[
  {
    "objectID": "teaching/stat-learn/material/09_Trees.html#fitting-regression-trees",
    "href": "teaching/stat-learn/material/09_Trees.html#fitting-regression-trees",
    "title": "Statistical Learning - Lab Session 09",
    "section": "Fitting Regression Trees",
    "text": "Fitting Regression Trees\nHere we fit a regression tree to the Boston data set. First, we create a training set, and fit the tree to the training data.\n\nset.seed(1)\ntrain &lt;- sample(1:nrow(Boston), nrow(Boston) / 2)\ntree.boston &lt;- tree(medv ~ ., Boston, subset = train)\nsummary(tree.boston)\n\n\nRegression tree:\ntree(formula = medv ~ ., data = Boston, subset = train)\nVariables actually used in tree construction:\n[1] \"rm\"    \"lstat\" \"crim\"  \"age\"  \nNumber of terminal nodes:  7 \nResidual mean deviance:  10.38 = 2555 / 246 \nDistribution of residuals:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-10.1800  -1.7770  -0.1775   0.0000   1.9230  16.5800 \n\n\nNotice that the output of summary() indicates that only four of the variables have been used in constructing the tree. In the context of a regression tree, the deviance is simply the sum of squared errors for the tree. We now plot the tree.\n\nplot(tree.boston)\ntext(tree.boston, pretty = 0)\n\n\n\n\n\n\n\n\nThe variable lstat measures the percentage of individuals with {lower socioeconomic status}, while the variable rm corresponds to the average number of rooms. The tree indicates that larger values of rm, or lower values of lstat, correspond to more expensive houses. For example, the tree predicts a median house price of $\\(45{,}400\\) for homes in census tracts in which rm &gt;= 7.553.\n\nBoston %&gt;%\n  ggplot(aes(x = rm, y = medv)) +geom_point() +\n  geom_segment(aes(x = 6.959, xend = 6.959, y = 0, yend = 60), \n               linetype = \"dashed\", color = \"purple\", size = 1) +\n    geom_segment(aes(x = 7.553, xend = 7.553, y = 0, yend = 60), \n               linetype = \"dashed\", color = \"purple\", size = 1) +\n    geom_segment(aes(x = 6.959, xend = 7.553, y = 33.42, yend = 33.42), \n               linetype = \"dashed\", color = \"darkorange\", size = 1.5) +\n    geom_segment(aes(x = 7.553, xend = 9, y = 45.38, yend = 45.38), \n               linetype = \"dashed\", color = \"darkorange\", size = 1.5) +\n    theme_minimal() +\n    labs(title  = \"Right side of regression tree\") +\n    theme(plot.title = element_text(hjust = 1))\n\n\n\n\n\n\n\n\nNow we use the cv.tree() function to see whether pruning the tree will improve performance.\n\ncv.boston &lt;- cv.tree(tree.boston)\ndf_viz_cv = data.frame(size = cv.boston[1], dev = cv.boston[2])\ndf_viz_cv %&gt;%\n  ggplot(aes(x = size, y = dev)) +\n  geom_point(size = 1) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn this case, the most complex tree under consideration is selected by cross-validation. However, if we wish to prune the tree, we could do so as follows, using the prune.tree() function:\n\nprune.boston &lt;- prune.tree(tree.boston, best = 5)\nplot(prune.boston)\ntext(prune.boston, pretty = 0)\n\n\n\n\n\n\n\n\nIn keeping with the cross-validation results, we use the unpruned tree to make predictions on the test set.\n\nyhat &lt;- predict(tree.boston, newdata = Boston[-train, ])\nboston.test &lt;- Boston[-train, \"medv\"]\nplot(yhat, boston.test)\nabline(0, 1)\n\n\n\n\n\n\n\nmean((yhat - boston.test)^2)\n\n[1] 35.28688\n\n\nIn other words, the test set MSE associated with the regression tree is \\(35.29\\). The square root of the MSE is therefore around \\(5.941\\), indicating that this model leads to test predictions that are (on average) within approximately \\(5.941\\) of the true median home value for the census tract."
  },
  {
    "objectID": "teaching/stat-learn/material/09_Trees.html#bagging-and-random-forests",
    "href": "teaching/stat-learn/material/09_Trees.html#bagging-and-random-forests",
    "title": "Statistical Learning - Lab Session 09",
    "section": "Bagging and Random Forests",
    "text": "Bagging and Random Forests\nHere we apply bagging and random forests to the Boston data, using the randomForest package in R. The exact results obtained in this section may depend on the version of R and the version of the randomForest package installed on your computer. Recall that bagging is simply a special case of a random forest with \\(m=p\\). Therefore, the randomForest() function can be used to perform both random forests and bagging. We perform bagging as follows:\n\nlibrary(randomForest)\nset.seed(1)\nbag.boston &lt;- randomForest(medv ~ ., data = Boston,\n    subset = train, mtry = 12, importance = TRUE)\nbag.boston\n\n\nCall:\n randomForest(formula = medv ~ ., data = Boston, mtry = 12, importance = TRUE,      subset = train) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 12\n\n          Mean of squared residuals: 11.40162\n                    % Var explained: 85.17\n\n\nThe argument mtry = 12 indicates that all \\(12\\) predictors should be considered for each split of the tree‚Äîin other words, that bagging should be done. How well does this bagged model perform on the test set?\n\nyhat.bag &lt;- predict(bag.boston, newdata = Boston[-train, ])\nplot(yhat.bag, boston.test)\nabline(0, 1)\n\n\n\n\n\n\n\nmean((yhat.bag - boston.test)^2)\n\n[1] 23.41916\n\n\nThe test set MSE associated with the bagged regression tree is \\(23.42\\), about two-thirds of that obtained using an optimally-pruned single tree. We could change the number of trees grown by randomForest() using the ntree argument:\n\nbag.boston25 &lt;- randomForest(medv ~ ., data = Boston, subset = train, mtry = 12, ntree = 25)\nyhat.bag25 &lt;- predict(bag.boston25, newdata = Boston[-train, ])\nmean((yhat.bag25 - boston.test)^2)\n\n[1] 25.75055\n\n\nGrowing a random forest proceeds in exactly the same way, except that we use a smaller value of the mtry argument. By default, randomForest() uses \\(p/3\\) variables when building a random forest of regression trees, and \\(\\sqrt{p}\\) variables when building a random forest of classification trees. Here we use mtry = 6.\n\nset.seed(1)\nrf.boston &lt;- randomForest(medv ~ ., data = Boston,\n    subset = train, mtry = 6, importance = TRUE)\nyhat.rf &lt;- predict(rf.boston, newdata = Boston[-train, ])\nmean((yhat.rf - boston.test)^2)\n\n[1] 20.06644\n\n\nThe test set MSE is \\(20.07\\); this indicates that random forests yielded an improvement over bagging in this case.\nUsing the importance() function, we can view the importance of each variable.\n\nimportance(rf.boston)\n\n          %IncMSE IncNodePurity\ncrim    19.435587    1070.42307\nzn       3.091630      82.19257\nindus    6.140529     590.09536\nchas     1.370310      36.70356\nnox     13.263466     859.97091\nrm      35.094741    8270.33906\nage     15.144821     634.31220\ndis      9.163776     684.87953\nrad      4.793720      83.18719\ntax      4.410714     292.20949\nptratio  8.612780     902.20190\nlstat   28.725343    5813.04833\n\n\nTwo measures of variable importance are reported. The first is based upon the mean decrease of accuracy in predictions on the out of bag samples when a given variable is permuted. The second is a measure of the total decrease in node impurity that results from splits over that variable, averaged over all trees (this was plotted in Figure 8.9). In the case of regression trees, the node impurity is measured by the training RSS, and for classification trees by the deviance. Plots of these importance measures can be produced using the varImpPlot() function.\n\nvarImpPlot(rf.boston)\n\n\n\n\n\n\n\n\nThe results indicate that across all of the trees considered in the random forest, the wealth of the community (lstat) and the house size (rm) are by far the two most important variables.\nIf we want to limit the number of fitted trees, we consult the OutofBag-error, stored as \\(mse\\) in our randomForest object. We observe that the error stabilizes after a while and additional trees bring little to no benefit.\n\ndfviz_B = data.frame(ntrees = rep(seq(1, length(rf.boston$mse)),2),\n                     mse = c(rf.boston$mse, bag.boston$mse),\n                     model = c(rep(\"rf\", 500), rep(\"bag\", 500)) )\n\ndfviz_B %&gt;%\n  ggplot(aes(x = ntrees, y = mse, color = model)) +\n  geom_line(size = 1) +\n  theme_minimal()"
  },
  {
    "objectID": "teaching/stat-learn/material/09_Trees.html#boosting",
    "href": "teaching/stat-learn/material/09_Trees.html#boosting",
    "title": "Statistical Learning - Lab Session 09",
    "section": "Boosting",
    "text": "Boosting\nThe ISLR2 book uses the \\(gbm\\) library which is being deprecated (can still be used but authors recommend switching). We will use the authors newer \\(gbm3\\) library.\nTo install run the following chunk. This will take a couple of minutes.\n\n\n#install.packages(\"remotes\")\n#remotes::install_github(\"gbm-developers/gbm3\", build_vignettes = TRUE, force = TRUE)\n\nHere we use the gbm3 package, and within it the gbm() function, to fit boosted regression trees to the Boston data set. We run gbm() with the option distribution = \"gaussian\" since this is a regression problem; if it were a binary classification problem, we would use distribution = \"bernoulli\". The argument n.trees = 5000 indicates that we want \\(5000\\) trees, and the option interaction.depth = 4 limits the depth of each tree.\n\nlibrary(gbm3)\n\nError in library(gbm3): there is no package called 'gbm3'\n\nset.seed(0)\nboost.boston &lt;- gbm(medv ~ ., data = Boston[train, ],\n    distribution = \"gaussian\", n.trees = 8000,\n    interaction.depth = 4)\n\nError in gbm(medv ~ ., data = Boston[train, ], distribution = \"gaussian\", : could not find function \"gbm\"\n\n\nThe summary() function produces a relative influence plot and also outputs the relative influence statistics.\n\nsummary(boost.boston)\n\nError: object 'boost.boston' not found\n\n\nWe see that lstat and rm are by far the most important variables. We can also produce partial dependence plots for these two variables. These plots illustrate the marginal effect of the selected variables on the response after integrating out the other variables. In this case, as we might expect, median house prices are increasing with rm and decreasing with lstat.\n\nplot(boost.boston, var_index = c(\"rm\", \"lstat\"))\n\nError: object 'boost.boston' not found\n\n\nWe now use the boosted model to predict medv on the test set:\n\nyhat.boost &lt;- predict(boost.boston, newdata = Boston[-train, ], n.trees = 8000)\n\nError: object 'boost.boston' not found\n\nmean((yhat.boost - boston.test)^2)\n\nError: object 'yhat.boost' not found\n\n\nThe test MSE obtained is \\(19.921\\): this is superior to the test MSE of random forests and bagging. If we want to, we can perform boosting with a different value of the shrinkage parameter \\(\\lambda\\) in (8.10). The default value is \\(0.001\\), but this is easily modified. Here we take \\(\\lambda=0.2\\).\n\nboost.boston_lam &lt;- gbm(medv ~ ., data = Boston[train, ],\n                    distribution = \"gaussian\",\n                    n.trees = 8000,\n                    interaction.depth = 4,\n                    shrinkage = 0.2,\n                    verbose = F)\n\nError in gbm(medv ~ ., data = Boston[train, ], distribution = \"gaussian\", : could not find function \"gbm\"\n\nyhat.boost_lam &lt;- predict(boost.boston_lam,\n                      newdata = Boston[-train, ],\n                      n.trees = 8000)\n\nError: object 'boost.boston_lam' not found\n\nmean((yhat.boost_lam - boston.test)^2)\n\nError: object 'yhat.boost_lam' not found\n\n\nIn this case, using \\(\\lambda=0.2\\) leads to a lower test MSE than \\(\\lambda=0.001\\).\nTo understand why this happens we can look at the training performance of our model. Remember that a boosted model learns iteratively with each new tree. As we increase the learning rate, the model converges faster and displays a lower training error earlier.\n\ndf_viz_boost = data.frame(errors = c(boost.boston$train.error, boost.boston_lam$train.error),\n                          trees = c(rep(seq(1,8000),2)),\n                          l2 = c(rep(\"l2 = 0.001\", 8000), rep(\"l2 = 0.2\", 8000)))\n\nError: object 'boost.boston' not found\n\ndf_viz_boost %&gt;%\n  ggplot(aes(x = trees, y = errors, color = l2)) + geom_point() #+ coord_cartesian(ylim = c(0,5))\n\nError: object 'df_viz_boost' not found"
  },
  {
    "objectID": "rpackage/index.html",
    "href": "rpackage/index.html",
    "title": "R packages",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "project/index.html",
    "href": "project/index.html",
    "title": "Projects",
    "section": "",
    "text": "Statistical Entropy Analysis of Network Data\n\n\nIn this project, a general framework for using statistical entropies to capture interdependencies among node and tie variables in multivariate networks is developed.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultigraph Representation of Network Data\n\n\nThe exploratory and confirmatory statistical analysis of multivariate social networks represented as multigraphs.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork of Interconnected Convoys\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Co-Evolution of Network Structure and Individual Outcomes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGender Dependent Structures in Charachter Networks\n\n\nUsing network analysis to analyze gender representation in popular cinema.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNEXUS1492 - Reconstructing Archaeological Networks\n\n\nReconstructing Archaeological Networks And Their Transformations Across The Historical Divide.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#general",
    "href": "index.html#general",
    "title": "Termeh Shafie",
    "section": "General",
    "text": "General\n\n09.09.2024: Netropy 0.2.0\nThe new version of netropy is published on CRAN.\n\n\n05.06.2024: Inaugural Professorial Lecture ü§ì\nI‚Äôll be giving my inaugural professorial lecture. which is open for anyone who wants join. Expect a lot of big words, personal anecdotes, some mildly amusing jokes, and maybe even a technical hiccup or two ü§ìüìö‚ú®  Join afterward for finger food and drinks ü•Ç  üìÖ Date: June 5th üìç Venue: Data Theatre ZT1204, University of Konstanz\n\n\n20.02.2024: Multigraphr 0.2.0\nThe new version of multigraphr is published on CRAN. No user facing changes, just improved performance thanks to David Schoch.\n\n\n10.08.2023: Maternity Leave üê£\nOn parental leave until end of the year.\n\n\n27.07.2023: Professor of Computational Social Science and Data Science\nToday I start my new position as a Professor (Professorin mit Schwerpunkt Lehre) at the Department of Politics and Public Administration, University of Konstanz."
  },
  {
    "objectID": "index.html#recent-publications",
    "href": "index.html#recent-publications",
    "title": "Termeh Shafie",
    "section": "Recent Publications",
    "text": "Recent Publications\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nThe interplay of structural features and observed dissimilarities among centrality indices\n\n\nDec 6, 2023\n\n\n\n\nGoodness of fit tests for random multigraph models\n\n\nJul 21, 2022\n\n\n\n\n\nNo matching items\n\n\n\nall publications"
  },
  {
    "objectID": "index.html#latest-talks",
    "href": "index.html#latest-talks",
    "title": "Termeh Shafie",
    "section": "Latest Talks",
    "text": "Latest Talks\n\n\n\n\n\n\n\n\n\n\n‚ÄúThe Interplay of Structural Features and Observed Dissimilarities Among Centrality Indices‚Äù\n\n\nThe XLIV Social Networks Conference of INSNA\n\n\n\n\n\nJun 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Analysis & Modeling of Multivariate Networks\n\n\nInaugural Lecture, University of Konstanz\n\n\n\n\n\nJun 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n‚ÄúStatistical Analysis of Multivariate Egocentric Networks‚Äù\n\n\nThe XLII Social Networks Conference of INSNA\n\n\n\n\n\nJul 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n‚ÄúStatistical Entropy Analysis of Network Data‚Äù\n\n\nThe Women in Network Science (WiNS) seminar\n\n\n\n\n\nMay 5, 2022\n\n\n\n\n\n\nNo matching items\n\n\n\nmore talks"
  },
  {
    "objectID": "index.html#r-packages",
    "href": "index.html#r-packages",
    "title": "Termeh Shafie",
    "section": "R packages",
    "text": "R packages\n\n\n\n\n\n\n\n\n\nLatest published version 0.2.0 on CRAN is up to date.\n\n\n\n\n\n\n\n\n\n\n\nLatest published version 0.2.0 on CRAN is up to date.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#github-activity",
    "href": "index.html#github-activity",
    "title": "Termeh Shafie",
    "section": "GitHub Activity",
    "text": "GitHub Activity"
  },
  {
    "objectID": "index.html#social-media",
    "href": "index.html#social-media",
    "title": "Termeh Shafie",
    "section": "Social Media",
    "text": "Social Media\n\n¬†\n\n¬† ¬† Toots"
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "The interplay of structural features and observed dissimilarities among centrality indices\n\n\nThe association of network topology with dissimilarities of indices is assessed\n\n\n\n\n\n\n\n\n\n\n\n\n\nGoodness of fit tests for random multigraph models\n\n\nGoodness of fit tests for different probability models for random multigraphs.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiplexity Analysis of Networks using Multigraph Representations\n\n\nA method for performing multiplexity analysis in social networks with several node covariates is presented.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSocial Network Analysis\n\n\nA review chapter on social network analysis aimed towards undergraduate students.\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Framework for Reconstructing Archaeological Networks using Exponential Random Graph Models\n\n\nWe present a general framework in which we combine exponential random graph models with archaeological substantiations of mechanisms for network formation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nReconstructing Archaeological Networks with Structural Holes\n\n\nWe consider exponential random graph models, and show how they can be applied to reconstruct networks coherent with Burt‚Äôs arguments on closure and structural holes.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Multigraphs and Aggregated Triads with Fixed Degrees\n\n\nNew combinatorial results are given for the global probability distribution of edge multiplicities and its marginal local distributions of loops and edges.\n\n\n\n\n\n\n\n\n\n\n\n\n\nHypergraph Representations: A Study of Carib Attacks on Colonial Forces, 1509-1700\n\n\nWe perform a centrality analysis of a directed hypergraph representing attacks by indigenous peoples from the Lesser Antilles on European colonial settlements, 1509-1700.\n\n\n\n\n\n\n\n\n\n\n\n\n\nInvestigating Human Geographic Origins using Dual-Isotope (87Sr/86Sr, d18O) Assignment approaches\n\n\nWe develop and test more standardized and quantitative approaches to geographic assignment of individual origins using multivariate isotopic data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Local and Global Properties of Multigraphs\n\n\nThe local and global structures of undirected multigraphs under two random multigraph models are analyzed and compared.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNation Building and Social Signaling in Southern Ontario AD 1350-1650\n\n\nSocial network analysis is used to demonstrates the signaling practices reflecting regional patterns.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariate Entropy Analysis of Network Data\n\n\nWe show how it is possible to systematically check for tendencies in data, such as independencies or conditional independencies, using multivariate entropies.\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Protection for Online Social Networks and P-Stability for Graphs\n\n\nWe consider different approaches for data privacy in online social networks and for developing graph protection.\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Multigraph Approach to Social Network Analysis\n\n\nThe theoretical background for analyzing multivariate social networks using multigraph representations is introduced.\n\n\n\n\n\n\n\n\n\n\n\n\n\nComplexity of Families of Multigraphs\n\n\nComplexity measured for multigraphs are specified and their applicability is discussed.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/stat-learn/material/05_Class-II.html",
    "href": "teaching/stat-learn/material/05_Class-II.html",
    "title": "Classification Methods II",
    "section": "",
    "text": "To continue our dive into classification methods we will take a loook at the Insurance dataset stored as ‚ÄúCaravan‚Äù.\nThis data set includes 85 predictors that measure demographic characteristics for 5,822 individuals. The response variable is Purchase, which indicates whether or not a given individual purchases a caravan insurance policy. In this data set, only 6% of people purchased caravan insurance.\n\nlibrary(ISLR2)\nlibrary(class)\nattach(Caravan)\ndim(Caravan)\n\n[1] 5822   86\n\n\n\nsummary(Caravan$Purchase)\n\n  No  Yes \n5474  348 \n\n348 / 5474\n\n[1] 0.06357326\n\n\nBecause the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale. For instance, imagine a data set that contains two variables, salary and age (measured in dollars and years, respectively). As far as KNN is concerned, a difference of $1,000 in salary is enormous compared to a difference of 50 years in age. Consequently, salary will drive the KNN classification results, and age will have almost no effect. This is contrary to our intuition that a salary difference of $1,000 is quite small compared to an age difference of 50 years. Furthermore, the importance of scale to the KNN classifier leads to another issue: if we measured salary in Japanese yen, or if we measured age in minutes, then we‚Äôd get quite different classification results from what we get if these two variables are measured in dollars and years.\nA good way to handle this problem is to standardize the data so that all variables are given a mean of zero and a standard deviation of one. Then all variables will be on a comparable scale. The scale() function does just this. In standardizing the data, we exclude column 86, because that is the qualitative Purchase variable.\n\nstandardized.X &lt;- scale(Caravan[, -86])\ncat(\"Unstandardized:\\n\")\n\nUnstandardized:\n\nvar(Caravan[, 1])\n\n[1] 165.0378\n\nvar(Caravan[, 2])\n\n[1] 0.1647078\n\ncat(\"Standardized:\\n\")\n\nStandardized:\n\nvar(standardized.X[, 1])\n\n[1] 1\n\nvar(standardized.X[, 2])\n\n[1] 1\n\n\nNow every column of standardized.X has a standard deviation of one and a mean of zero.\nWe now split the observations into a test set, containing the first 1,000 observations, and a training set, containing the remaining observations. We fit a KNN model on the training data using K=1, and evaluate its performance on the test data.\n\ntest &lt;- 1:1000\ntrain.X &lt;- standardized.X[-test, ]\ntest.X &lt;- standardized.X[test, ]\ntrain.Y &lt;- Purchase[-test]\ntest.Y &lt;- Purchase[test]\nset.seed(1)\nknn.pred &lt;- knn(train.X, test.X, train.Y, k = 1)\nmean(test.Y != knn.pred)\n\n[1] 0.118\n\n\n\ntable(knn.pred, test.Y)\n\n        test.Y\nknn.pred  No Yes\n     No  873  50\n     Yes  68   9\n\n9 / (68 + 9)\n\n[1] 0.1168831\n\n\nThis looks like a nice result - we have high accuracy. But do we care about how many people are not buying insurance in this example?\n\n\nTune your hyperparameter k and compare to our previous result. Also Compare to random guessing (50:50)\n\n\n\n\nNow we will perform LDA on the Smarket data. In R, we fit an LDA model using the lda() function, which is part of the MASS library. Notice that the syntax for the lda() function is identical to that of lm(), and to that of glm() except for the absence of the family option. We fit the model using only the observations before 2005.\n\nattach(Smarket)\ntrain &lt;- (Year &lt; 2005)\nSmarket.2005 &lt;- Smarket[!train, ]\ndim(Smarket.2005)\n\n[1] 252   9\n\nDirection.2005 &lt;- Direction[!train]\n\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:ISLR2':\n\n    Boston\n\nlda.fit &lt;- lda(Direction ~ Lag1 + Lag2, data = Smarket,\n    subset = train)\nlda.fit\n\nCall:\nlda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\nCoefficients of linear discriminants:\n            LD1\nLag1 -0.6420190\nLag2 -0.5135293\n\n\n\nplot(lda.fit)\n\n\n\n\n\n\n\n\nThe LDA output indicates that \\(\\hat\\pi_1=0.492\\) and \\(\\hat\\pi_2=0.508\\); in other words, \\(49.2\\) % of the training observations correspond to days during which the market went down. It also provides the group means; these are the average of each predictor within each class, and are used by LDA as estimates of \\(\\mu_k\\). These suggest that there is a tendency for the previous 2~days‚Äô returns to be negative on days when the market increases, and a tendency for the previous days‚Äô returns to be positive on days when the market declines. The coefficients of linear discriminants output provides the linear combination of lagone and lagtwo that are used to form the LDA decision rule. In other words, these are the multipliers of the elements of \\(X=x\\) in (4.24). If $-0.642 \\(`lagone`\\) - 0.514 $lagtwo is large, then the LDA classifier will predict a market increase, and if it is small, then the LDA classifier will predict a market decline.\nThe plot() function produces plots of the linear discriminants, obtained by computing $-0.642 \\(`lagone`\\) - 0.514 $lagtwo for each of the training observations. The Up and Down observations are displayed separately.\nThe predict() function returns a list with three elements. The first element, class, contains LDA‚Äôs predictions about the movement of the market. The second element, posterior, is a matrix whose \\(k\\)th column contains the posterior probability that the corresponding observation belongs to the \\(k\\)th class, computed from (4.15). Finally, x contains the linear discriminants, described earlier.\n\nlda.pred &lt;- predict(lda.fit, Smarket.2005)\nnames(lda.pred)\n\n[1] \"class\"     \"posterior\" \"x\"        \n\n\nAs we observed in Section 4.5, the LDA and logistic regression predictions are almost identical.\n\nlda.class &lt;- lda.pred$class\ntable(lda.class, Direction.2005)\n\n         Direction.2005\nlda.class Down  Up\n     Down   35  35\n     Up     76 106\n\nmean(lda.class == Direction.2005)\n\n[1] 0.5595238\n\n\nApplying a \\(50\\) % threshold to the posterior probabilities allows us to recreate the predictions contained in lda.pred$class.\n\nsum(lda.pred$posterior[, 1] &gt;= .5)\n\n[1] 70\n\nsum(lda.pred$posterior[, 1] &lt; .5)\n\n[1] 182\n\n\nNotice that the posterior probability output by the model corresponds to the probability that the market will decrease:\n\nlda.pred$posterior[1:20, 1]\n\n      999      1000      1001      1002      1003      1004      1005      1006 \n0.4901792 0.4792185 0.4668185 0.4740011 0.4927877 0.4938562 0.4951016 0.4872861 \n     1007      1008      1009      1010      1011      1012      1013      1014 \n0.4907013 0.4844026 0.4906963 0.5119988 0.4895152 0.4706761 0.4744593 0.4799583 \n     1015      1016      1017      1018 \n0.4935775 0.5030894 0.4978806 0.4886331 \n\nlda.class[1:20]\n\n [1] Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Down Up   Up   Up  \n[16] Up   Up   Down Up   Up  \nLevels: Down Up\n\n\nIf we wanted to use a posterior probability threshold other than \\(50\\) % in order to make predictions, then we could easily do so. For instance, suppose that we wish to predict a market decrease only if we are very certain that the market will indeed decrease on that day‚Äîsay, if the posterior probability is at least \\(90\\) %.\n\nsum(lda.pred$posterior[, 1] &gt; .9)\n\n[1] 0\n\n\nNo days in 2005 meet that threshold! In fact, the greatest posterior probability of decrease in all of 2005 was \\(52.02\\)\n\n\n\nWe will now fit a QDA model to the Smarket data. QDA is implemented in R using the qda() function, which is also part of the MASS library. The syntax is identical to that of lda().\n\nqda.fit &lt;- qda(Direction ~ Lag1 + Lag2, data = Smarket,\n    subset = train)\nqda.fit\n\nCall:\nqda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\n\nThe output contains the group means. But it does not contain the coefficients of the linear discriminants, because the QDA classifier involves a quadratic, rather than a linear, function of the predictors. The predict() function works in exactly the same fashion as for LDA.\n\nqda.class &lt;- predict(qda.fit, Smarket.2005)$class\ntable(qda.class, Direction.2005)\n\n         Direction.2005\nqda.class Down  Up\n     Down   30  20\n     Up     81 121\n\nmean(qda.class == Direction.2005)\n\n[1] 0.5992063\n\n\nInterestingly, the QDA predictions are accurate almost \\(60\\) % of the time, even though the 2005 data was not used to fit the model. This level of accuracy is quite impressive for stock market data, which is known to be quite hard to model accurately. This suggests that the quadratic form assumed by QDA may capture the true relationship more accurately than the linear forms assumed by LDA and logistic regression. However, we recommend evaluating this method‚Äôs performance on a larger test set before betting that this approach will consistently beat the market!\n\n\n\nNext, we fit a naive Bayes model to the Smarket data. Naive Bayes is implemented in R using the naiveBayes() function, which is part of the e1071 library. The syntax is identical to that of lda() and qda(). By default, this implementation of the naive Bayes classifier models each quantitative feature using a Gaussian distribution. However, a kernel density method can also be used to estimate the distributions.\n\n# install.packages(\"e1071\")\nlibrary(e1071)\nnb.fit &lt;- naiveBayes(Direction ~ Lag1 + Lag2, data = Smarket,\n    subset = train)\nnb.fit\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n    Down       Up \n0.491984 0.508016 \n\nConditional probabilities:\n      Lag1\nY             [,1]     [,2]\n  Down  0.04279022 1.227446\n  Up   -0.03954635 1.231668\n\n      Lag2\nY             [,1]     [,2]\n  Down  0.03389409 1.239191\n  Up   -0.03132544 1.220765\n\n\nThe output contains the estimated mean and standard deviation for each variable in each class. For example, the mean for lagone is \\(0.0428\\) for\nDirection=Down, and the standard deviation is \\(1.23\\). We can easily verify this:\n\nmean(Lag1[train][Direction[train] == \"Down\"])\n\n[1] 0.04279022\n\nsd(Lag1[train][Direction[train] == \"Down\"])\n\n[1] 1.227446\n\n\nThe predict() function is straightforward.\n\nnb.class &lt;- predict(nb.fit, Smarket.2005)\ntable(nb.class, Direction.2005)\n\n        Direction.2005\nnb.class Down  Up\n    Down   28  20\n    Up     83 121\n\nmean(nb.class == Direction.2005)\n\n[1] 0.5912698\n\n\nNaive Bayes performs very well on this data, with accurate predictions over \\(59\\%\\) of the time. This is slightly worse than QDA, but much better than LDA.\nThe predict() function can also generate estimates of the probability that each observation belongs to a particular class. %\n\nnb.preds &lt;- predict(nb.fit, Smarket.2005, type = \"raw\")\nnb.preds[1:5, ]\n\n          Down        Up\n[1,] 0.4873164 0.5126836\n[2,] 0.4762492 0.5237508\n[3,] 0.4653377 0.5346623\n[4,] 0.4748652 0.5251348\n[5,] 0.4901890 0.5098110"
  },
  {
    "objectID": "teaching/stat-learn/material/05_Class-II.html#caravan-data-and-knn",
    "href": "teaching/stat-learn/material/05_Class-II.html#caravan-data-and-knn",
    "title": "Classification Methods II",
    "section": "",
    "text": "To continue our dive into classification methods we will take a loook at the Insurance dataset stored as ‚ÄúCaravan‚Äù.\nThis data set includes 85 predictors that measure demographic characteristics for 5,822 individuals. The response variable is Purchase, which indicates whether or not a given individual purchases a caravan insurance policy. In this data set, only 6% of people purchased caravan insurance.\n\nlibrary(ISLR2)\nlibrary(class)\nattach(Caravan)\ndim(Caravan)\n\n[1] 5822   86\n\n\n\nsummary(Caravan$Purchase)\n\n  No  Yes \n5474  348 \n\n348 / 5474\n\n[1] 0.06357326\n\n\nBecause the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale. For instance, imagine a data set that contains two variables, salary and age (measured in dollars and years, respectively). As far as KNN is concerned, a difference of $1,000 in salary is enormous compared to a difference of 50 years in age. Consequently, salary will drive the KNN classification results, and age will have almost no effect. This is contrary to our intuition that a salary difference of $1,000 is quite small compared to an age difference of 50 years. Furthermore, the importance of scale to the KNN classifier leads to another issue: if we measured salary in Japanese yen, or if we measured age in minutes, then we‚Äôd get quite different classification results from what we get if these two variables are measured in dollars and years.\nA good way to handle this problem is to standardize the data so that all variables are given a mean of zero and a standard deviation of one. Then all variables will be on a comparable scale. The scale() function does just this. In standardizing the data, we exclude column 86, because that is the qualitative Purchase variable.\n\nstandardized.X &lt;- scale(Caravan[, -86])\ncat(\"Unstandardized:\\n\")\n\nUnstandardized:\n\nvar(Caravan[, 1])\n\n[1] 165.0378\n\nvar(Caravan[, 2])\n\n[1] 0.1647078\n\ncat(\"Standardized:\\n\")\n\nStandardized:\n\nvar(standardized.X[, 1])\n\n[1] 1\n\nvar(standardized.X[, 2])\n\n[1] 1\n\n\nNow every column of standardized.X has a standard deviation of one and a mean of zero.\nWe now split the observations into a test set, containing the first 1,000 observations, and a training set, containing the remaining observations. We fit a KNN model on the training data using K=1, and evaluate its performance on the test data.\n\ntest &lt;- 1:1000\ntrain.X &lt;- standardized.X[-test, ]\ntest.X &lt;- standardized.X[test, ]\ntrain.Y &lt;- Purchase[-test]\ntest.Y &lt;- Purchase[test]\nset.seed(1)\nknn.pred &lt;- knn(train.X, test.X, train.Y, k = 1)\nmean(test.Y != knn.pred)\n\n[1] 0.118\n\n\n\ntable(knn.pred, test.Y)\n\n        test.Y\nknn.pred  No Yes\n     No  873  50\n     Yes  68   9\n\n9 / (68 + 9)\n\n[1] 0.1168831\n\n\nThis looks like a nice result - we have high accuracy. But do we care about how many people are not buying insurance in this example?\n\n\nTune your hyperparameter k and compare to our previous result. Also Compare to random guessing (50:50)"
  },
  {
    "objectID": "teaching/stat-learn/material/05_Class-II.html#lda-with-the-stocks-data",
    "href": "teaching/stat-learn/material/05_Class-II.html#lda-with-the-stocks-data",
    "title": "Classification Methods II",
    "section": "",
    "text": "Now we will perform LDA on the Smarket data. In R, we fit an LDA model using the lda() function, which is part of the MASS library. Notice that the syntax for the lda() function is identical to that of lm(), and to that of glm() except for the absence of the family option. We fit the model using only the observations before 2005.\n\nattach(Smarket)\ntrain &lt;- (Year &lt; 2005)\nSmarket.2005 &lt;- Smarket[!train, ]\ndim(Smarket.2005)\n\n[1] 252   9\n\nDirection.2005 &lt;- Direction[!train]\n\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:ISLR2':\n\n    Boston\n\nlda.fit &lt;- lda(Direction ~ Lag1 + Lag2, data = Smarket,\n    subset = train)\nlda.fit\n\nCall:\nlda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\nCoefficients of linear discriminants:\n            LD1\nLag1 -0.6420190\nLag2 -0.5135293\n\n\n\nplot(lda.fit)\n\n\n\n\n\n\n\n\nThe LDA output indicates that \\(\\hat\\pi_1=0.492\\) and \\(\\hat\\pi_2=0.508\\); in other words, \\(49.2\\) % of the training observations correspond to days during which the market went down. It also provides the group means; these are the average of each predictor within each class, and are used by LDA as estimates of \\(\\mu_k\\). These suggest that there is a tendency for the previous 2~days‚Äô returns to be negative on days when the market increases, and a tendency for the previous days‚Äô returns to be positive on days when the market declines. The coefficients of linear discriminants output provides the linear combination of lagone and lagtwo that are used to form the LDA decision rule. In other words, these are the multipliers of the elements of \\(X=x\\) in (4.24). If $-0.642 \\(`lagone`\\) - 0.514 $lagtwo is large, then the LDA classifier will predict a market increase, and if it is small, then the LDA classifier will predict a market decline.\nThe plot() function produces plots of the linear discriminants, obtained by computing $-0.642 \\(`lagone`\\) - 0.514 $lagtwo for each of the training observations. The Up and Down observations are displayed separately.\nThe predict() function returns a list with three elements. The first element, class, contains LDA‚Äôs predictions about the movement of the market. The second element, posterior, is a matrix whose \\(k\\)th column contains the posterior probability that the corresponding observation belongs to the \\(k\\)th class, computed from (4.15). Finally, x contains the linear discriminants, described earlier.\n\nlda.pred &lt;- predict(lda.fit, Smarket.2005)\nnames(lda.pred)\n\n[1] \"class\"     \"posterior\" \"x\"        \n\n\nAs we observed in Section 4.5, the LDA and logistic regression predictions are almost identical.\n\nlda.class &lt;- lda.pred$class\ntable(lda.class, Direction.2005)\n\n         Direction.2005\nlda.class Down  Up\n     Down   35  35\n     Up     76 106\n\nmean(lda.class == Direction.2005)\n\n[1] 0.5595238\n\n\nApplying a \\(50\\) % threshold to the posterior probabilities allows us to recreate the predictions contained in lda.pred$class.\n\nsum(lda.pred$posterior[, 1] &gt;= .5)\n\n[1] 70\n\nsum(lda.pred$posterior[, 1] &lt; .5)\n\n[1] 182\n\n\nNotice that the posterior probability output by the model corresponds to the probability that the market will decrease:\n\nlda.pred$posterior[1:20, 1]\n\n      999      1000      1001      1002      1003      1004      1005      1006 \n0.4901792 0.4792185 0.4668185 0.4740011 0.4927877 0.4938562 0.4951016 0.4872861 \n     1007      1008      1009      1010      1011      1012      1013      1014 \n0.4907013 0.4844026 0.4906963 0.5119988 0.4895152 0.4706761 0.4744593 0.4799583 \n     1015      1016      1017      1018 \n0.4935775 0.5030894 0.4978806 0.4886331 \n\nlda.class[1:20]\n\n [1] Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Down Up   Up   Up  \n[16] Up   Up   Down Up   Up  \nLevels: Down Up\n\n\nIf we wanted to use a posterior probability threshold other than \\(50\\) % in order to make predictions, then we could easily do so. For instance, suppose that we wish to predict a market decrease only if we are very certain that the market will indeed decrease on that day‚Äîsay, if the posterior probability is at least \\(90\\) %.\n\nsum(lda.pred$posterior[, 1] &gt; .9)\n\n[1] 0\n\n\nNo days in 2005 meet that threshold! In fact, the greatest posterior probability of decrease in all of 2005 was \\(52.02\\)"
  },
  {
    "objectID": "teaching/stat-learn/material/05_Class-II.html#quadratic-discriminant-analysis",
    "href": "teaching/stat-learn/material/05_Class-II.html#quadratic-discriminant-analysis",
    "title": "Classification Methods II",
    "section": "",
    "text": "We will now fit a QDA model to the Smarket data. QDA is implemented in R using the qda() function, which is also part of the MASS library. The syntax is identical to that of lda().\n\nqda.fit &lt;- qda(Direction ~ Lag1 + Lag2, data = Smarket,\n    subset = train)\nqda.fit\n\nCall:\nqda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\n\nThe output contains the group means. But it does not contain the coefficients of the linear discriminants, because the QDA classifier involves a quadratic, rather than a linear, function of the predictors. The predict() function works in exactly the same fashion as for LDA.\n\nqda.class &lt;- predict(qda.fit, Smarket.2005)$class\ntable(qda.class, Direction.2005)\n\n         Direction.2005\nqda.class Down  Up\n     Down   30  20\n     Up     81 121\n\nmean(qda.class == Direction.2005)\n\n[1] 0.5992063\n\n\nInterestingly, the QDA predictions are accurate almost \\(60\\) % of the time, even though the 2005 data was not used to fit the model. This level of accuracy is quite impressive for stock market data, which is known to be quite hard to model accurately. This suggests that the quadratic form assumed by QDA may capture the true relationship more accurately than the linear forms assumed by LDA and logistic regression. However, we recommend evaluating this method‚Äôs performance on a larger test set before betting that this approach will consistently beat the market!"
  },
  {
    "objectID": "teaching/stat-learn/material/05_Class-II.html#naive-bayes",
    "href": "teaching/stat-learn/material/05_Class-II.html#naive-bayes",
    "title": "Classification Methods II",
    "section": "",
    "text": "Next, we fit a naive Bayes model to the Smarket data. Naive Bayes is implemented in R using the naiveBayes() function, which is part of the e1071 library. The syntax is identical to that of lda() and qda(). By default, this implementation of the naive Bayes classifier models each quantitative feature using a Gaussian distribution. However, a kernel density method can also be used to estimate the distributions.\n\n# install.packages(\"e1071\")\nlibrary(e1071)\nnb.fit &lt;- naiveBayes(Direction ~ Lag1 + Lag2, data = Smarket,\n    subset = train)\nnb.fit\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n    Down       Up \n0.491984 0.508016 \n\nConditional probabilities:\n      Lag1\nY             [,1]     [,2]\n  Down  0.04279022 1.227446\n  Up   -0.03954635 1.231668\n\n      Lag2\nY             [,1]     [,2]\n  Down  0.03389409 1.239191\n  Up   -0.03132544 1.220765\n\n\nThe output contains the estimated mean and standard deviation for each variable in each class. For example, the mean for lagone is \\(0.0428\\) for\nDirection=Down, and the standard deviation is \\(1.23\\). We can easily verify this:\n\nmean(Lag1[train][Direction[train] == \"Down\"])\n\n[1] 0.04279022\n\nsd(Lag1[train][Direction[train] == \"Down\"])\n\n[1] 1.227446\n\n\nThe predict() function is straightforward.\n\nnb.class &lt;- predict(nb.fit, Smarket.2005)\ntable(nb.class, Direction.2005)\n\n        Direction.2005\nnb.class Down  Up\n    Down   28  20\n    Up     83 121\n\nmean(nb.class == Direction.2005)\n\n[1] 0.5912698\n\n\nNaive Bayes performs very well on this data, with accurate predictions over \\(59\\%\\) of the time. This is slightly worse than QDA, but much better than LDA.\nThe predict() function can also generate estimates of the probability that each observation belongs to a particular class. %\n\nnb.preds &lt;- predict(nb.fit, Smarket.2005, type = \"raw\")\nnb.preds[1:5, ]\n\n          Down        Up\n[1,] 0.4873164 0.5126836\n[2,] 0.4762492 0.5237508\n[3,] 0.4653377 0.5346623\n[4,] 0.4748652 0.5251348\n[5,] 0.4901890 0.5098110"
  },
  {
    "objectID": "teaching/stat-learn/material/02-linear-regression-I.html",
    "href": "teaching/stat-learn/material/02-linear-regression-I.html",
    "title": "Linear Regression I",
    "section": "",
    "text": "The library() function is used to load libraries, or groups of functions and data sets that are not included in the base R distribution. Basic functions that perform least squares linear regression and other simple analyses come standard with the base distribution, but more exotic functions require additional libraries. Here we load the MASS package, which is a very large collection of data sets and functions. We also load the ISLR2 package, which includes the data sets associated with this book.\n\nlibrary(MASS)\nlibrary(ISLR2)\n\n\nAttaching package: 'ISLR2'\n\n\nThe following object is masked from 'package:MASS':\n\n    Boston\n\n\nIf you receive an error message when loading any of these libraries, it likely indicates that the corresponding library has not yet been installed on your system. Some libraries, such as MASS, come with R and do not need to be separately installed on your computer. However, other packages, such as ISLR2, must be downloaded the first time they are used. This can be done directly from within R. For example, on a Windows system, select the Install package option under the Packages tab. After you select any mirror site, a list of available packages will appear. Simply select the package you wish to install and R will automatically download the package. Alternatively, this can be done at the R command line via install.packages(\"ISLR2\"). This installation only needs to be done the first time you use a package. However, the library() function must be called within each R session.\n\n\n\nThe ISLR2 library contains the Boston data set, which records medv (median house value) for \\(506\\) census tracts in Boston. We will seek to predict medv using \\(12\\) predictors such as rmvar (average number of rooms per house), age (proportion of owner-occupied units built prior to 1940) and lstat (percent of households with low socioeconomic status).\n\nknitr::kable(head(Boston)) # note: you only write head(Boston) here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nlstat\nmedv\n\n\n\n\n0.00632\n18\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n4.98\n24.0\n\n\n0.02731\n0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n9.14\n21.6\n\n\n0.02729\n0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n4.03\n34.7\n\n\n0.03237\n0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n2.94\n33.4\n\n\n0.06905\n0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n5.33\n36.2\n\n\n0.02985\n0\n2.18\n0\n0.458\n6.430\n58.7\n6.0622\n3\n222\n18.7\n5.21\n28.7\n\n\n\n\n\nTo find out more about the data set, we can type ?Boston.\nWe will start by using the lm() function to fit a simple linear regression model, with medv as the response and lstat as the predictor. The basic syntax is lm(y ~ x, data), where y is the response, x is the predictor, and data is the data set in which these two variables are kept.\n\nlm.fit &lt;- lm(medv ~ lstat)\n\nError in eval(predvars, data, env): object 'medv' not found\n\n\nThe command causes an error because R does not know where to find the variables medv and lstat. The next line tells R that the variables are in Boston. If we attach Boston, the first line works fine because R now recognizes the variables.\n\nlm.fit &lt;- lm(medv ~ lstat, data = Boston)\nattach(Boston)\nlm.fit &lt;- lm(medv ~ lstat)\n\nIf we type lm.fit, some basic information about the model is output. For more detailed information, we use summary(lm.fit). This gives us \\(p\\)-values and standard errors for the coefficients, as well as the \\(R^2\\) statistic and \\(F\\)-statistic for the model.\n\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\nWe can use the names() function in order to find out what other pieces of information are stored in lm.fit. Although we can extract these quantities by name‚Äîe.g.¬†lm.fit$coefficients‚Äîit is safer to use the extractor functions like coef() to access them.\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\n\nIn order to obtain a confidence interval for the coefficient estimates, we can use the confint() command.\n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\n\nThe predict() function can be used to produce confidence intervals and prediction intervals for the prediction of medv for a given value of lstat.\n\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))),\n    interval = \"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))),\n    interval = \"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n\nFor instance, the 95 % confidence interval associated with a lstat value of 10 is \\((24.47, 25.63)\\), and the 95 % prediction interval is \\((12.828, 37.28)\\). As expected, the confidence and prediction intervals are centered around the same point (a predicted value of \\(25.05\\) for medv when lstat equals 10), but the latter are substantially wider.\nWe will now plot medv and lstat along with the least squares regression line using the plot() and abline() functions.\n\nplot(lstat, medv)\nabline(lm.fit)\n\n\n\n\n\n\n\n\nThere is some evidence for non-linearity in the relationship between lstat and medv. We will explore this issue later in this lab.\nThe abline() function can be used to draw any line, not just the least squares regression line. To draw a line with intercept a and slope b, we type abline(a, b). Below we experiment with some additional settings for plotting lines and points. The lwd = 3 command causes the width of the regression line to be increased by a factor of 3; this works for the plot() and lines() functions also. We can also use the pch option to create different plotting symbols.\n\nplot(lstat, medv)\nabline(lm.fit, lwd = 3)\nabline(lm.fit, lwd = 3, col = \"red\")\n\n\n\n\n\n\n\nplot(lstat, medv, col = \"red\")\n\n\n\n\n\n\n\nplot(lstat, medv, pch = 20)\n\n\n\n\n\n\n\nplot(lstat, medv, pch = \"+\")\n\n\n\n\n\n\n\nplot(1:20, 1:20, pch = 1:20)\n\n\n\n\n\n\n\n\nNext we examine some diagnostic plots, several of which were discussed in Section 3.3.3. Four diagnostic plots are automatically produced by applying the plot() function directly to the output from lm(). In general, this command will produce one plot at a time, and hitting Enter will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the par() and mfrow() functions, which tell R to split the display screen into separate panels so that multiple plots can be viewed simultaneously. For example, par(mfrow = c(2, 2)) divides the plotting region into a \\(2 \\times 2\\) grid of panels.\n\npar(mfrow = c(2, 2))\nplot(lm.fit)\n\n\n\n\n\n\n\n\nAlternatively, we can compute the residuals from a linear regression fit using the residuals() function. The function rstudent() will return the studentized residuals, and we can use this function to plot the residuals against the fitted values.\n\nplot(predict(lm.fit), residuals(lm.fit))\n\n\n\n\n\n\n\nplot(predict(lm.fit), rstudent(lm.fit))\n\n\n\n\n\n\n\n\nOn the basis of the residual plots, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the hatvalues() function.\n\nplot(hatvalues(lm.fit))\n\n\n\n\n\n\n\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\nThe which.max() function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic.\n\n\n\nIn order to fit a multiple linear regression model using least squares, we again use the lm() function. The syntax lm(y ~ x1 + x2 + x3) is used to fit a model with three predictors, x1, x2, and x3. The summary() function now outputs the regression coefficients for all the predictors.\n\nlm.fit &lt;- lm(medv ~ lstat + age, data = Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\nThe Boston data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:\n\nlm.fit &lt;- lm(medv ~ ., data = Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1304  -2.7673  -0.5814   1.9414  26.2526 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.617270   4.936039   8.431 3.79e-16 ***\ncrim         -0.121389   0.033000  -3.678 0.000261 ***\nzn            0.046963   0.013879   3.384 0.000772 ***\nindus         0.013468   0.062145   0.217 0.828520    \nchas          2.839993   0.870007   3.264 0.001173 ** \nnox         -18.758022   3.851355  -4.870 1.50e-06 ***\nrm            3.658119   0.420246   8.705  &lt; 2e-16 ***\nage           0.003611   0.013329   0.271 0.786595    \ndis          -1.490754   0.201623  -7.394 6.17e-13 ***\nrad           0.289405   0.066908   4.325 1.84e-05 ***\ntax          -0.012682   0.003801  -3.337 0.000912 ***\nptratio      -0.937533   0.132206  -7.091 4.63e-12 ***\nlstat        -0.552019   0.050659 -10.897  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.798 on 493 degrees of freedom\nMultiple R-squared:  0.7343,    Adjusted R-squared:  0.7278 \nF-statistic: 113.5 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\n\nWe can access the individual components of a summary object by name (type ?summary.lm to see what is available). Hence summary(lm.fit)$r.sq gives us the \\(R^2\\), and summary(lm.fit)$sigma gives us the RSE. The vif() function, part of the car package, can be used to compute variance inflation factors. Most VIF‚Äôs are low to moderate for this data. The car package is not part of the base R installation so it must be downloaded the first time you use it via the install.packages() function in R.\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.767486 2.298459 3.987181 1.071168 4.369093 1.912532 3.088232 3.954037 \n     rad      tax  ptratio    lstat \n7.445301 9.002158 1.797060 2.870777 \n\n\nWhat if we would like to perform a regression using all of the variables but one? For example, in the above regression output, age has a high \\(p\\)-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except age.\n\nlm.fit1 &lt;- lm(medv ~ . - age, data = Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1851  -2.7330  -0.6116   1.8555  26.3838 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.525128   4.919684   8.441 3.52e-16 ***\ncrim         -0.121426   0.032969  -3.683 0.000256 ***\nzn            0.046512   0.013766   3.379 0.000785 ***\nindus         0.013451   0.062086   0.217 0.828577    \nchas          2.852773   0.867912   3.287 0.001085 ** \nnox         -18.485070   3.713714  -4.978 8.91e-07 ***\nrm            3.681070   0.411230   8.951  &lt; 2e-16 ***\ndis          -1.506777   0.192570  -7.825 3.12e-14 ***\nrad           0.287940   0.066627   4.322 1.87e-05 ***\ntax          -0.012653   0.003796  -3.333 0.000923 ***\nptratio      -0.934649   0.131653  -7.099 4.39e-12 ***\nlstat        -0.547409   0.047669 -11.483  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.794 on 494 degrees of freedom\nMultiple R-squared:  0.7343,    Adjusted R-squared:  0.7284 \nF-statistic: 124.1 on 11 and 494 DF,  p-value: &lt; 2.2e-16\n\n\nAlternatively, the update() function can be used.\n\nlm.fit1 &lt;- update(lm.fit, ~ . - age)\n\n\n\n\nIt is easy to include interaction terms in a linear model using the lm() function. The syntax lstat:age tells R to include an interaction term between lstat and age. The syntax lstat * age simultaneously includes lstat, age, and the interaction term lstat\\(\\times\\)age as predictors; it is a shorthand for lstat + age + lstat:age. %We can also pass in transformed versions of the predictors.\n\nsummary(lm(medv ~ lstat * age, data = Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nThe lm() function can also accommodate non-linear transformations of the predictors. For instance, given a predictor \\(X\\), we can create a predictor \\(X^2\\) using I(X^2). The function I() is needed since the ^ has a special meaning in a formula object; wrapping as we do allows the standard usage in R, which is to raise X to the power 2. We now perform a regression of medv onto lstat and lstat^2.\n\nlm.fit2 &lt;- lm(medv ~ lstat + I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\nThe near-zero \\(p\\)-value associated with the quadratic term suggests that it leads to an improved model. We use the anova() function to further quantify the extent to which the quadratic fit is superior to the linear fit.\n\nlm.fit &lt;- lm(medv ~ lstat)\nanova(lm.fit, lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere Model 1 represents the linear submodel containing only one predictor, lstat, while Model 2 corresponds to the larger quadratic model that has two predictors, lstat and lstat^2. The anova() function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Here the \\(F\\)-statistic is \\(135\\) and the associated \\(p\\)-value is virtually zero. This provides very clear evidence that the model containing the predictors lstat and lstat^2 is far superior to the model that only contains the predictor lstat. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between medv and lstat. If we type\n\npar(mfrow = c(2, 2))\nplot(lm.fit2)\n\n\n\n\n\n\n\n\nthen we see that when the lstat^2 term is included in the model, there is little discernible pattern in the residuals.\nIn order to create a cubic fit, we can include a predictor of the form I(X^3). However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the poly() function to create the polynomial within lm(). For example, the following command produces a fifth-order polynomial fit:\n\nlm.fit5 &lt;- lm(medv ~ poly(lstat, 5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\n\nThis suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant \\(p\\)-values in a regression fit.\nBy default, the poly() function orthogonalizes the predictors: this means that the features output by this function are not simply a sequence of powers of the argument. However, a linear model applied to the output of the poly() function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). In order to obtain the raw polynomials from the poly() function, the argument raw = TRUE must be used.\nOf course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.\n\nsummary(lm(medv ~ log(rm), data = Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nWe will now examine the Carseats data, which is part of the ISLR2 library. We will attempt to predict Sales (child car seat sales) in \\(400\\) locations based on a number of predictors.\n\nhead(Carseats)\n\n  Sales CompPrice Income Advertising Population Price ShelveLoc Age Education\n1  9.50       138     73          11        276   120       Bad  42        17\n2 11.22       111     48          16        260    83      Good  65        10\n3 10.06       113     35          10        269    80    Medium  59        12\n4  7.40       117    100           4        466    97    Medium  55        14\n5  4.15       141     64           3        340   128       Bad  38        13\n6 10.81       124    113          13        501    72       Bad  78        16\n  Urban  US\n1   Yes Yes\n2   Yes Yes\n3   Yes Yes\n4   Yes Yes\n5   Yes  No\n6    No Yes\n\n\nThe Carseats data includes qualitative predictors such as shelveloc, an indicator of the quality of the shelving location‚Äîthat is, the space within a store in which the car seat is displayed‚Äîat each location. The predictor shelveloc takes on three possible values: Bad, Medium, and Good. Given a qualitative variable such as shelveloc, R generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.\n\nlm.fit &lt;- lm(Sales ~ . + Income:Advertising + Price:Age, \n    data = Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\n\nThe contrasts() function returns the coding that R uses for the dummy variables.\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\nUse ?contrasts to learn about other contrasts, and how to set them.\nR has created a ShelveLocGood dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a ShelveLocMedium dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for ShelveLocGood in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And ShelveLocMedium has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location.\n\n\n\nAs we have seen, R comes with many useful functions, and still more functions are available by way of R libraries. However, we will often be interested in performing an operation for which no function is available. In this setting, we may want to write our own function. For instance, below we provide a simple function that reads in the ISLR2 and MASS libraries, called LoadLibraries(). Before we have created the function, R returns an error if we try to call it.\n\nLoadLibraries\n\nError: object 'LoadLibraries' not found\n\nLoadLibraries()\n\nError in LoadLibraries(): could not find function \"LoadLibraries\"\n\n\nWe now create the function. Note that the + symbols are printed by R and should not be typed in. The { symbol informs R that multiple commands are about to be input. Hitting Enter after typing { will cause R to print the + symbol. We can then input as many commands as we wish, hitting {Enter} after each one. Finally the } symbol informs R that no further commands will be entered.\n\nLoadLibraries &lt;- function() {\n library(ISLR2)\n library(MASS)\n print(\"The libraries have been loaded.\")\n}\n\nNow if we type in LoadLibraries, R will tell us what is in the function.\n\nLoadLibraries\n\nfunction () \n{\n    library(ISLR2)\n    library(MASS)\n    print(\"The libraries have been loaded.\")\n}\n\n\nIf we call the function, the libraries are loaded in and the print statement is output.\n\nLoadLibraries()\n\n[1] \"The libraries have been loaded.\""
  },
  {
    "objectID": "teaching/stat-learn/material/02-linear-regression-I.html#libraries",
    "href": "teaching/stat-learn/material/02-linear-regression-I.html#libraries",
    "title": "Linear Regression I",
    "section": "",
    "text": "The library() function is used to load libraries, or groups of functions and data sets that are not included in the base R distribution. Basic functions that perform least squares linear regression and other simple analyses come standard with the base distribution, but more exotic functions require additional libraries. Here we load the MASS package, which is a very large collection of data sets and functions. We also load the ISLR2 package, which includes the data sets associated with this book.\n\nlibrary(MASS)\nlibrary(ISLR2)\n\n\nAttaching package: 'ISLR2'\n\n\nThe following object is masked from 'package:MASS':\n\n    Boston\n\n\nIf you receive an error message when loading any of these libraries, it likely indicates that the corresponding library has not yet been installed on your system. Some libraries, such as MASS, come with R and do not need to be separately installed on your computer. However, other packages, such as ISLR2, must be downloaded the first time they are used. This can be done directly from within R. For example, on a Windows system, select the Install package option under the Packages tab. After you select any mirror site, a list of available packages will appear. Simply select the package you wish to install and R will automatically download the package. Alternatively, this can be done at the R command line via install.packages(\"ISLR2\"). This installation only needs to be done the first time you use a package. However, the library() function must be called within each R session."
  },
  {
    "objectID": "teaching/stat-learn/material/02-linear-regression-I.html#simple-linear-regression",
    "href": "teaching/stat-learn/material/02-linear-regression-I.html#simple-linear-regression",
    "title": "Linear Regression I",
    "section": "",
    "text": "The ISLR2 library contains the Boston data set, which records medv (median house value) for \\(506\\) census tracts in Boston. We will seek to predict medv using \\(12\\) predictors such as rmvar (average number of rooms per house), age (proportion of owner-occupied units built prior to 1940) and lstat (percent of households with low socioeconomic status).\n\nknitr::kable(head(Boston)) # note: you only write head(Boston) here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nlstat\nmedv\n\n\n\n\n0.00632\n18\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n4.98\n24.0\n\n\n0.02731\n0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n9.14\n21.6\n\n\n0.02729\n0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n4.03\n34.7\n\n\n0.03237\n0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n2.94\n33.4\n\n\n0.06905\n0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n5.33\n36.2\n\n\n0.02985\n0\n2.18\n0\n0.458\n6.430\n58.7\n6.0622\n3\n222\n18.7\n5.21\n28.7\n\n\n\n\n\nTo find out more about the data set, we can type ?Boston.\nWe will start by using the lm() function to fit a simple linear regression model, with medv as the response and lstat as the predictor. The basic syntax is lm(y ~ x, data), where y is the response, x is the predictor, and data is the data set in which these two variables are kept.\n\nlm.fit &lt;- lm(medv ~ lstat)\n\nError in eval(predvars, data, env): object 'medv' not found\n\n\nThe command causes an error because R does not know where to find the variables medv and lstat. The next line tells R that the variables are in Boston. If we attach Boston, the first line works fine because R now recognizes the variables.\n\nlm.fit &lt;- lm(medv ~ lstat, data = Boston)\nattach(Boston)\nlm.fit &lt;- lm(medv ~ lstat)\n\nIf we type lm.fit, some basic information about the model is output. For more detailed information, we use summary(lm.fit). This gives us \\(p\\)-values and standard errors for the coefficients, as well as the \\(R^2\\) statistic and \\(F\\)-statistic for the model.\n\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\nWe can use the names() function in order to find out what other pieces of information are stored in lm.fit. Although we can extract these quantities by name‚Äîe.g.¬†lm.fit$coefficients‚Äîit is safer to use the extractor functions like coef() to access them.\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\n\nIn order to obtain a confidence interval for the coefficient estimates, we can use the confint() command.\n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\n\nThe predict() function can be used to produce confidence intervals and prediction intervals for the prediction of medv for a given value of lstat.\n\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))),\n    interval = \"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))),\n    interval = \"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n\nFor instance, the 95 % confidence interval associated with a lstat value of 10 is \\((24.47, 25.63)\\), and the 95 % prediction interval is \\((12.828, 37.28)\\). As expected, the confidence and prediction intervals are centered around the same point (a predicted value of \\(25.05\\) for medv when lstat equals 10), but the latter are substantially wider.\nWe will now plot medv and lstat along with the least squares regression line using the plot() and abline() functions.\n\nplot(lstat, medv)\nabline(lm.fit)\n\n\n\n\n\n\n\n\nThere is some evidence for non-linearity in the relationship between lstat and medv. We will explore this issue later in this lab.\nThe abline() function can be used to draw any line, not just the least squares regression line. To draw a line with intercept a and slope b, we type abline(a, b). Below we experiment with some additional settings for plotting lines and points. The lwd = 3 command causes the width of the regression line to be increased by a factor of 3; this works for the plot() and lines() functions also. We can also use the pch option to create different plotting symbols.\n\nplot(lstat, medv)\nabline(lm.fit, lwd = 3)\nabline(lm.fit, lwd = 3, col = \"red\")\n\n\n\n\n\n\n\nplot(lstat, medv, col = \"red\")\n\n\n\n\n\n\n\nplot(lstat, medv, pch = 20)\n\n\n\n\n\n\n\nplot(lstat, medv, pch = \"+\")\n\n\n\n\n\n\n\nplot(1:20, 1:20, pch = 1:20)\n\n\n\n\n\n\n\n\nNext we examine some diagnostic plots, several of which were discussed in Section 3.3.3. Four diagnostic plots are automatically produced by applying the plot() function directly to the output from lm(). In general, this command will produce one plot at a time, and hitting Enter will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the par() and mfrow() functions, which tell R to split the display screen into separate panels so that multiple plots can be viewed simultaneously. For example, par(mfrow = c(2, 2)) divides the plotting region into a \\(2 \\times 2\\) grid of panels.\n\npar(mfrow = c(2, 2))\nplot(lm.fit)\n\n\n\n\n\n\n\n\nAlternatively, we can compute the residuals from a linear regression fit using the residuals() function. The function rstudent() will return the studentized residuals, and we can use this function to plot the residuals against the fitted values.\n\nplot(predict(lm.fit), residuals(lm.fit))\n\n\n\n\n\n\n\nplot(predict(lm.fit), rstudent(lm.fit))\n\n\n\n\n\n\n\n\nOn the basis of the residual plots, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the hatvalues() function.\n\nplot(hatvalues(lm.fit))\n\n\n\n\n\n\n\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\nThe which.max() function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic."
  },
  {
    "objectID": "teaching/stat-learn/material/02-linear-regression-I.html#multiple-linear-regression",
    "href": "teaching/stat-learn/material/02-linear-regression-I.html#multiple-linear-regression",
    "title": "Linear Regression I",
    "section": "",
    "text": "In order to fit a multiple linear regression model using least squares, we again use the lm() function. The syntax lm(y ~ x1 + x2 + x3) is used to fit a model with three predictors, x1, x2, and x3. The summary() function now outputs the regression coefficients for all the predictors.\n\nlm.fit &lt;- lm(medv ~ lstat + age, data = Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\nThe Boston data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:\n\nlm.fit &lt;- lm(medv ~ ., data = Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1304  -2.7673  -0.5814   1.9414  26.2526 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.617270   4.936039   8.431 3.79e-16 ***\ncrim         -0.121389   0.033000  -3.678 0.000261 ***\nzn            0.046963   0.013879   3.384 0.000772 ***\nindus         0.013468   0.062145   0.217 0.828520    \nchas          2.839993   0.870007   3.264 0.001173 ** \nnox         -18.758022   3.851355  -4.870 1.50e-06 ***\nrm            3.658119   0.420246   8.705  &lt; 2e-16 ***\nage           0.003611   0.013329   0.271 0.786595    \ndis          -1.490754   0.201623  -7.394 6.17e-13 ***\nrad           0.289405   0.066908   4.325 1.84e-05 ***\ntax          -0.012682   0.003801  -3.337 0.000912 ***\nptratio      -0.937533   0.132206  -7.091 4.63e-12 ***\nlstat        -0.552019   0.050659 -10.897  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.798 on 493 degrees of freedom\nMultiple R-squared:  0.7343,    Adjusted R-squared:  0.7278 \nF-statistic: 113.5 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\n\nWe can access the individual components of a summary object by name (type ?summary.lm to see what is available). Hence summary(lm.fit)$r.sq gives us the \\(R^2\\), and summary(lm.fit)$sigma gives us the RSE. The vif() function, part of the car package, can be used to compute variance inflation factors. Most VIF‚Äôs are low to moderate for this data. The car package is not part of the base R installation so it must be downloaded the first time you use it via the install.packages() function in R.\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.767486 2.298459 3.987181 1.071168 4.369093 1.912532 3.088232 3.954037 \n     rad      tax  ptratio    lstat \n7.445301 9.002158 1.797060 2.870777 \n\n\nWhat if we would like to perform a regression using all of the variables but one? For example, in the above regression output, age has a high \\(p\\)-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except age.\n\nlm.fit1 &lt;- lm(medv ~ . - age, data = Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1851  -2.7330  -0.6116   1.8555  26.3838 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.525128   4.919684   8.441 3.52e-16 ***\ncrim         -0.121426   0.032969  -3.683 0.000256 ***\nzn            0.046512   0.013766   3.379 0.000785 ***\nindus         0.013451   0.062086   0.217 0.828577    \nchas          2.852773   0.867912   3.287 0.001085 ** \nnox         -18.485070   3.713714  -4.978 8.91e-07 ***\nrm            3.681070   0.411230   8.951  &lt; 2e-16 ***\ndis          -1.506777   0.192570  -7.825 3.12e-14 ***\nrad           0.287940   0.066627   4.322 1.87e-05 ***\ntax          -0.012653   0.003796  -3.333 0.000923 ***\nptratio      -0.934649   0.131653  -7.099 4.39e-12 ***\nlstat        -0.547409   0.047669 -11.483  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.794 on 494 degrees of freedom\nMultiple R-squared:  0.7343,    Adjusted R-squared:  0.7284 \nF-statistic: 124.1 on 11 and 494 DF,  p-value: &lt; 2.2e-16\n\n\nAlternatively, the update() function can be used.\n\nlm.fit1 &lt;- update(lm.fit, ~ . - age)"
  },
  {
    "objectID": "teaching/stat-learn/material/02-linear-regression-I.html#interaction-terms",
    "href": "teaching/stat-learn/material/02-linear-regression-I.html#interaction-terms",
    "title": "Linear Regression I",
    "section": "",
    "text": "It is easy to include interaction terms in a linear model using the lm() function. The syntax lstat:age tells R to include an interaction term between lstat and age. The syntax lstat * age simultaneously includes lstat, age, and the interaction term lstat\\(\\times\\)age as predictors; it is a shorthand for lstat + age + lstat:age. %We can also pass in transformed versions of the predictors.\n\nsummary(lm(medv ~ lstat * age, data = Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "teaching/stat-learn/material/02-linear-regression-I.html#non-linear-transformations-of-the-predictors",
    "href": "teaching/stat-learn/material/02-linear-regression-I.html#non-linear-transformations-of-the-predictors",
    "title": "Linear Regression I",
    "section": "",
    "text": "The lm() function can also accommodate non-linear transformations of the predictors. For instance, given a predictor \\(X\\), we can create a predictor \\(X^2\\) using I(X^2). The function I() is needed since the ^ has a special meaning in a formula object; wrapping as we do allows the standard usage in R, which is to raise X to the power 2. We now perform a regression of medv onto lstat and lstat^2.\n\nlm.fit2 &lt;- lm(medv ~ lstat + I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\nThe near-zero \\(p\\)-value associated with the quadratic term suggests that it leads to an improved model. We use the anova() function to further quantify the extent to which the quadratic fit is superior to the linear fit.\n\nlm.fit &lt;- lm(medv ~ lstat)\nanova(lm.fit, lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere Model 1 represents the linear submodel containing only one predictor, lstat, while Model 2 corresponds to the larger quadratic model that has two predictors, lstat and lstat^2. The anova() function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Here the \\(F\\)-statistic is \\(135\\) and the associated \\(p\\)-value is virtually zero. This provides very clear evidence that the model containing the predictors lstat and lstat^2 is far superior to the model that only contains the predictor lstat. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between medv and lstat. If we type\n\npar(mfrow = c(2, 2))\nplot(lm.fit2)\n\n\n\n\n\n\n\n\nthen we see that when the lstat^2 term is included in the model, there is little discernible pattern in the residuals.\nIn order to create a cubic fit, we can include a predictor of the form I(X^3). However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the poly() function to create the polynomial within lm(). For example, the following command produces a fifth-order polynomial fit:\n\nlm.fit5 &lt;- lm(medv ~ poly(lstat, 5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\n\nThis suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant \\(p\\)-values in a regression fit.\nBy default, the poly() function orthogonalizes the predictors: this means that the features output by this function are not simply a sequence of powers of the argument. However, a linear model applied to the output of the poly() function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). In order to obtain the raw polynomials from the poly() function, the argument raw = TRUE must be used.\nOf course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.\n\nsummary(lm(medv ~ log(rm), data = Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "teaching/stat-learn/material/02-linear-regression-I.html#qualitative-predictors",
    "href": "teaching/stat-learn/material/02-linear-regression-I.html#qualitative-predictors",
    "title": "Linear Regression I",
    "section": "",
    "text": "We will now examine the Carseats data, which is part of the ISLR2 library. We will attempt to predict Sales (child car seat sales) in \\(400\\) locations based on a number of predictors.\n\nhead(Carseats)\n\n  Sales CompPrice Income Advertising Population Price ShelveLoc Age Education\n1  9.50       138     73          11        276   120       Bad  42        17\n2 11.22       111     48          16        260    83      Good  65        10\n3 10.06       113     35          10        269    80    Medium  59        12\n4  7.40       117    100           4        466    97    Medium  55        14\n5  4.15       141     64           3        340   128       Bad  38        13\n6 10.81       124    113          13        501    72       Bad  78        16\n  Urban  US\n1   Yes Yes\n2   Yes Yes\n3   Yes Yes\n4   Yes Yes\n5   Yes  No\n6    No Yes\n\n\nThe Carseats data includes qualitative predictors such as shelveloc, an indicator of the quality of the shelving location‚Äîthat is, the space within a store in which the car seat is displayed‚Äîat each location. The predictor shelveloc takes on three possible values: Bad, Medium, and Good. Given a qualitative variable such as shelveloc, R generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.\n\nlm.fit &lt;- lm(Sales ~ . + Income:Advertising + Price:Age, \n    data = Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\n\nThe contrasts() function returns the coding that R uses for the dummy variables.\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\nUse ?contrasts to learn about other contrasts, and how to set them.\nR has created a ShelveLocGood dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a ShelveLocMedium dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for ShelveLocGood in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And ShelveLocMedium has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location."
  },
  {
    "objectID": "teaching/stat-learn/material/02-linear-regression-I.html#writing-functions",
    "href": "teaching/stat-learn/material/02-linear-regression-I.html#writing-functions",
    "title": "Linear Regression I",
    "section": "",
    "text": "As we have seen, R comes with many useful functions, and still more functions are available by way of R libraries. However, we will often be interested in performing an operation for which no function is available. In this setting, we may want to write our own function. For instance, below we provide a simple function that reads in the ISLR2 and MASS libraries, called LoadLibraries(). Before we have created the function, R returns an error if we try to call it.\n\nLoadLibraries\n\nError: object 'LoadLibraries' not found\n\nLoadLibraries()\n\nError in LoadLibraries(): could not find function \"LoadLibraries\"\n\n\nWe now create the function. Note that the + symbols are printed by R and should not be typed in. The { symbol informs R that multiple commands are about to be input. Hitting Enter after typing { will cause R to print the + symbol. We can then input as many commands as we wish, hitting {Enter} after each one. Finally the } symbol informs R that no further commands will be entered.\n\nLoadLibraries &lt;- function() {\n library(ISLR2)\n library(MASS)\n print(\"The libraries have been loaded.\")\n}\n\nNow if we type in LoadLibraries, R will tell us what is in the function.\n\nLoadLibraries\n\nfunction () \n{\n    library(ISLR2)\n    library(MASS)\n    print(\"The libraries have been loaded.\")\n}\n\n\nIf we call the function, the libraries are loaded in and the print statement is output.\n\nLoadLibraries()\n\n[1] \"The libraries have been loaded.\""
  },
  {
    "objectID": "teaching/stat-learn/material/03-linear-regression-II.html",
    "href": "teaching/stat-learn/material/03-linear-regression-II.html",
    "title": "Linear Regression II",
    "section": "",
    "text": "Our goal is to run a few models of different complexity and evaluate the performance based on a test-train split of the data. Can you understand why we do not use the full data to test the different models?\nWe will start by creating a function that allows us to assess model performance:\n\nrmse = function(actual, predicted) {\n  sqrt(mean((actual - predicted) ^ 2))\n}\n\nWe will also use a function that gives the complexity of the linear model as number of independent variables defined.\n\nget_complexity = function(model) {\n  length(coef(model)) - 1\n}\n\nCan you understand why we take -1 in the above function?\n\n\nWe will use the Advertisement data from ISLR2 which is in a compressed folder for download. Make sure you have set the correct working directory for reading the data. You can also load the data from the ISLR2 package. The Advertising data set consists of the sales of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: TV, radio, and newspaper.\n\nlibrary(readr) # install if not in your library\nAdvertising = read_csv(\"03-data/Advertising.csv\") \nknitr::kable(head(Advertising,10)) # look at 10 first rows of data\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n230.1\n37.8\n69.2\n22.1\n\n\n44.5\n39.3\n45.1\n10.4\n\n\n17.2\n45.9\n69.3\n9.3\n\n\n151.5\n41.3\n58.5\n18.5\n\n\n180.8\n10.8\n58.4\n12.9\n\n\n8.7\n48.9\n75.0\n7.2\n\n\n57.5\n32.8\n23.5\n11.8\n\n\n120.2\n19.6\n11.6\n13.2\n\n\n8.6\n2.1\n1.0\n4.8\n\n\n199.8\n2.6\n21.2\n10.6\n\n\n\n\n\nExplore the data by doing some plots. Example shown below.\n\nplot(Sales ~ TV, data = Advertising, col = \"dodgerblue\", pch = 20, cex = 1.5,\n     main = \"Sales vs Television Advertising\")\n\n\n\n\n\n\n\n\nWe can also explore the data by using a correlation matrix plot:\n\npairs(Advertising)\n\n\n\n\n\n\n\n\nWhat are the main patterns you detect?\n\n\n\nWe will now split the data in two. One part of the datasets will be used to fit (train) a model, which we will call the training data. The remainder of the original data will be used to assess how well the model is predicting, which we will call the test data.\nWe use the sample() function to obtain a random sample of the rows of the original data. We then use those row numbers (and remaining row numbers) to split the data accordingly. Notice we used the set.seed() function to reproduce the same random split each time we perform this analysis.\n\nset.seed(1)\nnum_obs = nrow(Advertising)\n\ntrain_index = sample(num_obs, size = trunc(0.50 * num_obs))\ntrain_data = Advertising[train_index, ]\ntest_data = Advertising[-train_index, ]\n\n\n\n\nWe start by fitting the simplest linear model, a model with no predictors:\n\nfit_0 = lm(Sales ~ 1, data = train_data)\nget_complexity(fit_0)\n\n[1] 0\n\n\nAs seen, the complexity of this model is 0. We compute the Test and Train RMSE for this model, via the function specified above and a direct formula:\n\n# train RMSE\nrmse(actual = train_data$Sales, predicted = predict(fit_0, train_data))\n\n[1] 5.297333\n\nsqrt(mean((train_data$Sales - predict(fit_0, train_data)) ^ 2))\n\n[1] 5.297333\n\n# test RMSE\nrmse(actual = test_data$Sales, predicted = predict(fit_0, test_data))\n\n[1] 5.112073\n\nsqrt(mean((test_data$Sales - predict(fit_0, test_data)) ^ 2)) \n\n[1] 5.112073\n\n\nLet‚Äôs make the computations of RMSE even more easy by using the below function:\n\n# train RMSE\nget_rmse = function(model, data, response) {\n  rmse(actual = subset(data, select = response, drop = TRUE),\n       predicted = predict(model, data))\n}\n\nCan you figure out how to use the above function?\nWe will fit 5 models, compute the train and test MSE for each model to evaluate it and visualize how it relates to the complexity (i.e.¬†model size in terms of parameters) of the model. Furthermore, we will interpret the results in terms of underfitting or overfitting. We will conclude with dertemrineing which model to choose. The five models to fit are listed below:\n\nSales ~ Radio + Newspaper + TV\nSales ~ Radio * Newspaper * TV\nSales ~ Radio * Newspaper * TV + I(TV ^ 2)\nSales ~ Radio * Newspaper * TV + I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2)\nSales ~ Radio * Newspaper * TV + I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2)\n\nNote that the specification first*second indicates the cross of first and second. This is the same as first + second + first:second.\nWe fit all five models and print the output by creating a list of the model fits. We then obtain train RMSE, test RMSE, and model complexity for each. Finally, plot the results. The train RMSE can be seen in blue, while the test RMSE is given in orange.\n\nfit_1 = lm(Sales ~ Radio + Newspaper + TV, data = train_data)\nfit_2 = lm(Sales ~ Radio * Newspaper * TV, data = train_data)\nfit_3 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2), data = train_data)\nfit_4 = lm(Sales ~ Radio * Newspaper * TV + \n             I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2), data = train_data)\nfit_5 = lm(Sales ~ Radio * Newspaper * TV +\n             I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2), data = train_data)\n\nmodel_list = list(fit_1, fit_2, fit_3, fit_4, fit_5)\n#model_list #uncomment if you wish to see this list object\n\ntrain_rmse = sapply(model_list, get_rmse, data = train_data, response = \"Sales\")\ntest_rmse = sapply(model_list, get_rmse, data = test_data, response = \"Sales\")\nmodel_complexity = sapply(model_list, get_complexity)\n\ntrain_rmse = sapply(model_list, get_rmse, data = train_data, response = \"Sales\")\ntest_rmse = sapply(model_list, get_rmse, data = test_data, response = \"Sales\")\nmodel_complexity = sapply(model_list, get_complexity)\n\nplot(model_complexity, train_rmse, type = \"b\", \n     ylim = c(min(c(train_rmse, test_rmse)) - 0.02, \n              max(c(train_rmse, test_rmse)) + 0.02), \n     col = \"blue\", \n     lwd = 2,\n     xlab = \"Complexity (Model Size)\",\n     ylab = \"RMSE\")\nlines(model_complexity, test_rmse, type = \"b\", col = \"red\", lwd = 2)\ngrid()\n\n\n\n\n\n\n\n\nConclusion:\n\nUnderfitting models: In general High Train RMSE, High Test RMSE. Seen in fit_1 and fit_2.\nOverfitting models: In general Low Train RMSE, High Test RMSE. Seen in fit_4 and fit_5.\n\nWe see that the Test RMSE is smallest for fit_3, and it is the model we believe will perform the best on future data not used to train the model.\n\n\n\n\nWrite functions for the some of the other loss functions we covered in the lecture (e.g.¬†MAE, MAPE). Apply them to asses the same five models above. Do you see similar or deviant results in terms of overfitting/underfitting?\nReplicate the simulation example from lecture. Try other models based on different order polynomials."
  },
  {
    "objectID": "teaching/stat-learn/material/03-linear-regression-II.html#data",
    "href": "teaching/stat-learn/material/03-linear-regression-II.html#data",
    "title": "Linear Regression II",
    "section": "",
    "text": "We will use the Advertisement data from ISLR2 which is in a compressed folder for download. Make sure you have set the correct working directory for reading the data. You can also load the data from the ISLR2 package. The Advertising data set consists of the sales of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: TV, radio, and newspaper.\n\nlibrary(readr) # install if not in your library\nAdvertising = read_csv(\"03-data/Advertising.csv\") \nknitr::kable(head(Advertising,10)) # look at 10 first rows of data\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n230.1\n37.8\n69.2\n22.1\n\n\n44.5\n39.3\n45.1\n10.4\n\n\n17.2\n45.9\n69.3\n9.3\n\n\n151.5\n41.3\n58.5\n18.5\n\n\n180.8\n10.8\n58.4\n12.9\n\n\n8.7\n48.9\n75.0\n7.2\n\n\n57.5\n32.8\n23.5\n11.8\n\n\n120.2\n19.6\n11.6\n13.2\n\n\n8.6\n2.1\n1.0\n4.8\n\n\n199.8\n2.6\n21.2\n10.6\n\n\n\n\n\nExplore the data by doing some plots. Example shown below.\n\nplot(Sales ~ TV, data = Advertising, col = \"dodgerblue\", pch = 20, cex = 1.5,\n     main = \"Sales vs Television Advertising\")\n\n\n\n\n\n\n\n\nWe can also explore the data by using a correlation matrix plot:\n\npairs(Advertising)\n\n\n\n\n\n\n\n\nWhat are the main patterns you detect?"
  },
  {
    "objectID": "teaching/stat-learn/material/03-linear-regression-II.html#test-train-split",
    "href": "teaching/stat-learn/material/03-linear-regression-II.html#test-train-split",
    "title": "Linear Regression II",
    "section": "",
    "text": "We will now split the data in two. One part of the datasets will be used to fit (train) a model, which we will call the training data. The remainder of the original data will be used to assess how well the model is predicting, which we will call the test data.\nWe use the sample() function to obtain a random sample of the rows of the original data. We then use those row numbers (and remaining row numbers) to split the data accordingly. Notice we used the set.seed() function to reproduce the same random split each time we perform this analysis.\n\nset.seed(1)\nnum_obs = nrow(Advertising)\n\ntrain_index = sample(num_obs, size = trunc(0.50 * num_obs))\ntrain_data = Advertising[train_index, ]\ntest_data = Advertising[-train_index, ]"
  },
  {
    "objectID": "teaching/stat-learn/material/03-linear-regression-II.html#the-model",
    "href": "teaching/stat-learn/material/03-linear-regression-II.html#the-model",
    "title": "Linear Regression II",
    "section": "",
    "text": "We start by fitting the simplest linear model, a model with no predictors:\n\nfit_0 = lm(Sales ~ 1, data = train_data)\nget_complexity(fit_0)\n\n[1] 0\n\n\nAs seen, the complexity of this model is 0. We compute the Test and Train RMSE for this model, via the function specified above and a direct formula:\n\n# train RMSE\nrmse(actual = train_data$Sales, predicted = predict(fit_0, train_data))\n\n[1] 5.297333\n\nsqrt(mean((train_data$Sales - predict(fit_0, train_data)) ^ 2))\n\n[1] 5.297333\n\n# test RMSE\nrmse(actual = test_data$Sales, predicted = predict(fit_0, test_data))\n\n[1] 5.112073\n\nsqrt(mean((test_data$Sales - predict(fit_0, test_data)) ^ 2)) \n\n[1] 5.112073\n\n\nLet‚Äôs make the computations of RMSE even more easy by using the below function:\n\n# train RMSE\nget_rmse = function(model, data, response) {\n  rmse(actual = subset(data, select = response, drop = TRUE),\n       predicted = predict(model, data))\n}\n\nCan you figure out how to use the above function?\nWe will fit 5 models, compute the train and test MSE for each model to evaluate it and visualize how it relates to the complexity (i.e.¬†model size in terms of parameters) of the model. Furthermore, we will interpret the results in terms of underfitting or overfitting. We will conclude with dertemrineing which model to choose. The five models to fit are listed below:\n\nSales ~ Radio + Newspaper + TV\nSales ~ Radio * Newspaper * TV\nSales ~ Radio * Newspaper * TV + I(TV ^ 2)\nSales ~ Radio * Newspaper * TV + I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2)\nSales ~ Radio * Newspaper * TV + I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2)\n\nNote that the specification first*second indicates the cross of first and second. This is the same as first + second + first:second.\nWe fit all five models and print the output by creating a list of the model fits. We then obtain train RMSE, test RMSE, and model complexity for each. Finally, plot the results. The train RMSE can be seen in blue, while the test RMSE is given in orange.\n\nfit_1 = lm(Sales ~ Radio + Newspaper + TV, data = train_data)\nfit_2 = lm(Sales ~ Radio * Newspaper * TV, data = train_data)\nfit_3 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2), data = train_data)\nfit_4 = lm(Sales ~ Radio * Newspaper * TV + \n             I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2), data = train_data)\nfit_5 = lm(Sales ~ Radio * Newspaper * TV +\n             I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2), data = train_data)\n\nmodel_list = list(fit_1, fit_2, fit_3, fit_4, fit_5)\n#model_list #uncomment if you wish to see this list object\n\ntrain_rmse = sapply(model_list, get_rmse, data = train_data, response = \"Sales\")\ntest_rmse = sapply(model_list, get_rmse, data = test_data, response = \"Sales\")\nmodel_complexity = sapply(model_list, get_complexity)\n\ntrain_rmse = sapply(model_list, get_rmse, data = train_data, response = \"Sales\")\ntest_rmse = sapply(model_list, get_rmse, data = test_data, response = \"Sales\")\nmodel_complexity = sapply(model_list, get_complexity)\n\nplot(model_complexity, train_rmse, type = \"b\", \n     ylim = c(min(c(train_rmse, test_rmse)) - 0.02, \n              max(c(train_rmse, test_rmse)) + 0.02), \n     col = \"blue\", \n     lwd = 2,\n     xlab = \"Complexity (Model Size)\",\n     ylab = \"RMSE\")\nlines(model_complexity, test_rmse, type = \"b\", col = \"red\", lwd = 2)\ngrid()\n\n\n\n\n\n\n\n\nConclusion:\n\nUnderfitting models: In general High Train RMSE, High Test RMSE. Seen in fit_1 and fit_2.\nOverfitting models: In general Low Train RMSE, High Test RMSE. Seen in fit_4 and fit_5.\n\nWe see that the Test RMSE is smallest for fit_3, and it is the model we believe will perform the best on future data not used to train the model."
  },
  {
    "objectID": "teaching/stat-learn/material/03-linear-regression-II.html#exercises",
    "href": "teaching/stat-learn/material/03-linear-regression-II.html#exercises",
    "title": "Linear Regression II",
    "section": "",
    "text": "Write functions for the some of the other loss functions we covered in the lecture (e.g.¬†MAE, MAPE). Apply them to asses the same five models above. Do you see similar or deviant results in terms of overfitting/underfitting?\nReplicate the simulation example from lecture. Try other models based on different order polynomials."
  },
  {
    "objectID": "teaching/tidyverse/material/nobel-laureates.html",
    "href": "teaching/tidyverse/material/nobel-laureates.html",
    "title": "Nobel Laureates",
    "section": "",
    "text": "In January 2017, Buzzfeed published an article on why Nobel laureates show immigration is so important for American science. You can read the article here. In the article they show that while most living Nobel laureates in the sciences are based in the US, many of them were born in other countries. This is one reason why scientific leaders say that immigration is vital for progress. In this lab we will work with the data from this article to recreate some of their visualizations as well as explore new questions."
  },
  {
    "objectID": "teaching/tidyverse/material/nobel-laureates.html#packages",
    "href": "teaching/tidyverse/material/nobel-laureates.html#packages",
    "title": "Nobel Laureates",
    "section": "Packages",
    "text": "Packages\nWe‚Äôll use the tidyverse package for much of the data wrangling. This package is already installed for you. You can load them by running the following in your Console:\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "teaching/tidyverse/material/nobel-laureates.html#data",
    "href": "teaching/tidyverse/material/nobel-laureates.html#data",
    "title": "Nobel Laureates",
    "section": "Data",
    "text": "Data\nThe dataset for this assignment can be found as a CSV (comma separated values) file. You can read it in using the following.\n\nnobel &lt;- read_csv(\"data/nobel.csv\")\n\nThe variable descriptions are as follows:\n\nid: ID number\nfirstname: First name of laureate\nsurname: Surname\nyear: Year prize won\ncategory: Category of prize\naffiliation: Affiliation of laureate\ncity: City of laureate in prize year\ncountry: Country of laureate in prize year\nborn_date: Birth date of laureate\ndied_date: Death date of laureate\ngender: Gender of laureate\nborn_city: City where laureate was born\nborn_country: Country where laureate was born\nborn_country_code: Code of country where laureate was born\ndied_city: City where laureate died\ndied_country: Country where laureate died\ndied_country_code: Code of country where laureate died\noverall_motivation: Overall motivation for recognition\nshare: Number of other winners award is shared with\nmotivation: Motivation for recognition\n\nIn a few cases the name of the city/country changed after laureate was given (e.g.¬†in 1975 Bosnia and Herzegovina was called the Socialist Federative Republic of Yugoslavia). In these cases the variables below reflect a different name than their counterparts without the suffix `_original`.\n\nborn_country_original: Original country where laureate was born\nborn_city_original: Original city where laureate was born\ndied_country_original: Original country where laureate died\ndied_city_original: Original city where laureate died\ncity_original: Original city where laureate lived at the time of winning the award\ncountry_original: Original country where laureate lived at the time of winning the award"
  },
  {
    "objectID": "teaching/tidyverse/material/nobel-laureates.html#get-to-know-your-data",
    "href": "teaching/tidyverse/material/nobel-laureates.html#get-to-know-your-data",
    "title": "Nobel Laureates",
    "section": "Get to know your data",
    "text": "Get to know your data\n\nHow many observations and how many variables are in the dataset? Use inline code to answer this question. What does each row represent?\n\nThere are some observations in this dataset that we will exclude from our analysis to match the Buzzfeed results.\n\nCreate a new data frame called nobel_living that filters for\n\n\nlaureates for whom country is available\nlaureates who are people as opposed to organizations (organizations are denoted with \"org\" as their gender)\nlaureates who are still alive (their died_date is NA)\n\nConfirm that once you have filtered for these characteristics you are left with a data frame with 228 observations, once again using inline code."
  },
  {
    "objectID": "teaching/tidyverse/material/nobel-laureates.html#most-living-nobel-laureates-were-based-in-the-us-when-they-won-their-prizes",
    "href": "teaching/tidyverse/material/nobel-laureates.html#most-living-nobel-laureates-were-based-in-the-us-when-they-won-their-prizes",
    "title": "Nobel Laureates",
    "section": "Most living Nobel laureates were based in the US when they won their prizes",
    "text": "Most living Nobel laureates were based in the US when they won their prizes\n‚Ä¶ says the Buzzfeed article. Let‚Äôs see if that‚Äôs true.\nFirst, we‚Äôll create a new variable to identify whether the laureate was in the US when they won their prize. We‚Äôll use the mutate() function for this. The following pipeline mutates the nobel_living data frame by adding a new variable called country_us. We use an if statement to create this variable. The first argument in the if_else() function we‚Äôre using to write this if statement is the condition we‚Äôre testing for. If country is equal to \"USA\", we set country_us to \"USA\". If not, we set the country_us to \"Other\".\nNote: we can achieve the same result using the fct_other() function we‚Äôve seen before (i.e.¬†with country_us = fct_other(country, \"USA\")). We decided to use the if_else() here to show you one example of an if statement in R.\n\nnobel_living &lt;- nobel_living %&gt;%\n  mutate(\n    country_us = if_else(country == \"USA\", \"USA\", \"Other\")\n  )\n\nNext, we will limit our analysis to only the following categories: Physics, Medicine, Chemistry, and Economics.\n\nnobel_living_science &lt;- nobel_living %&gt;%\n  filter(category %in% c(\"Physics\", \"Medicine\", \"Chemistry\", \"Economics\"))\n\nFor the next exercise work with the nobel_living_science data frame you created above.\n\nCreate a faceted bar plot visualizing the relationship between the category of prize and whether the laureate was in the US when they won the nobel prize. Interpret your visualization, and say a few words about whether the Buzzfeed headline is supported by the data.\n\nYour visualization should be faceted by category.\nFor each facet you should have two bars, one for winners in the US and one for Other.\nFlip the coordinates so the bars are horizontal, not vertical."
  },
  {
    "objectID": "teaching/tidyverse/material/nobel-laureates.html#but-of-those-us-based-nobel-laureates-many-were-born-in-other-countries",
    "href": "teaching/tidyverse/material/nobel-laureates.html#but-of-those-us-based-nobel-laureates-many-were-born-in-other-countries",
    "title": "Nobel Laureates",
    "section": "But of those US-based Nobel laureates, many were born in other countries",
    "text": "But of those US-based Nobel laureates, many were born in other countries\nHint: You should be able to cheat borrow from code you used earlier to create the country_us variable.\n\nCreate a new variable called born_country_us that has the value \"USA\" if the laureate is born in the US, and \"Other\" otherwise. How many of the winners are born in the US?\nAdd a second variable to your visualization from Exercise 3 based on whether the laureate was born in the US or not. Based on your visualization, do the data appear to support Buzzfeed‚Äôs claim? Explain your reasoning in 1-2 sentences.\n\nYour final visualization should contain a facet for each category.\nWithin each facet, there should be a bar for whether the laureate won the award in the US or not.\nEach bar should have segments for whether the laureate was born in the US or not."
  },
  {
    "objectID": "teaching/tidyverse/material/nobel-laureates.html#heres-where-those-immigrant-nobelists-were-born",
    "href": "teaching/tidyverse/material/nobel-laureates.html#heres-where-those-immigrant-nobelists-were-born",
    "title": "Nobel Laureates",
    "section": "Here‚Äôs where those immigrant Nobelists were born",
    "text": "Here‚Äôs where those immigrant Nobelists were born\nNote: your bar plot won‚Äôt exactly match the one from the Buzzfeed article. This is likely because the data has been updated since the article was published.\n\nIn a single pipeline, filter for laureates who won their prize in the US, but were born outside of the US, and then create a frequency table (with the count() function) for their birth country (born_country) and arrange the resulting data frame in descending order of number of observations for each country. Which country is the most common?"
  },
  {
    "objectID": "teaching/tidyverse/material/uoe-art.html",
    "href": "teaching/tidyverse/material/uoe-art.html",
    "title": "University of Edinburgh Art Collection",
    "section": "",
    "text": "The University of Edinburgh Art Collection ‚Äúsupports the world-leading research and teaching that happens within the University. Comprised of an astonishing range of objects and ideas spanning two millennia and a multitude of artistic forms, the collection reflects not only the long and rich trajectory of the University, but also major national and international shifts in art history.‚Äù\nIn this practical we‚Äôll scrape data on all art pieces in the Edinburgh College of Art collection."
  },
  {
    "objectID": "teaching/tidyverse/material/uoe-art.html#r-scripts-vs.-quarto-documents",
    "href": "teaching/tidyverse/material/uoe-art.html#r-scripts-vs.-quarto-documents",
    "title": "University of Edinburgh Art Collection",
    "section": "R scripts vs.¬†Quarto documents",
    "text": "R scripts vs.¬†Quarto documents\nToday you‚Äôll be using both R scripts and R Markdown documents:\n\nuse R scripts in the web scraping stage and ultimately save the scraped data as a csv.\nuse an Quarto document in the web analysis stage, where we start off by reading in the csv file we wrote out in the scraping stage."
  },
  {
    "objectID": "teaching/tidyverse/material/uoe-art.html#packages",
    "href": "teaching/tidyverse/material/uoe-art.html#packages",
    "title": "University of Edinburgh Art Collection",
    "section": "Packages",
    "text": "Packages\nWe‚Äôll use the tidyverse package for much of the data wrangling and visualisation, the robotstxt package to check if we‚Äôre allowed to scrape the data, the rvest package for data scraping.\n\nlibrary(tidyverse) \nlibrary(robotstxt)\nlibrary(rvest)"
  },
  {
    "objectID": "teaching/tidyverse/material/uoe-art.html#data",
    "href": "teaching/tidyverse/material/uoe-art.html#data",
    "title": "University of Edinburgh Art Collection",
    "section": "Data",
    "text": "Data\nThis assignment does not come with any prepared datasets. Instead you‚Äôll be scraping the data! But before doing so, let‚Äôs check that a bot has permissions to access pages on this domain.\n\npaths_allowed(\"https://collections.ed.ac.uk/art)\")\n\n\n collections.ed.ac.uk                      \n\n\n[1] TRUE"
  },
  {
    "objectID": "teaching/tidyverse/material/uoe-art.html#scraping-a-single-page",
    "href": "teaching/tidyverse/material/uoe-art.html#scraping-a-single-page",
    "title": "University of Edinburgh Art Collection",
    "section": "Scraping a single page",
    "text": "Scraping a single page\nWe will start off by scraping data on the first 10 pieces in the collection from here.\nFirst, we define a new object called first_url, which is the link above. Then, we read the page at this url with the read_html() function from the rvest package. The code for this is already provided in 01-scrape-page-one.R.\n\n# set url\nfirst_url &lt;- \"https://collections.ed.ac.uk/art/search/*:*/Collection:%22edinburgh+college+of+art%7C%7C%7CEdinburgh+College+of+Art%22?offset=0\"\n\n# read html page\npage &lt;- read_html(first_url)\n\nFor the ten pieces on this page we will extract title, artist, and link information, and put these three variables in a data frame.\n\nTitles\nLet‚Äôs start with titles. We make use of the SelectorGadget to identify the tags for the relevant nodes:\n\n\n\n\n\n\n\n\n\n\npage %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%\n  html_node(\"h3 a\")\n\n{xml_nodeset (10)}\n [1] &lt;a href=\"./record/20567?highlight=*:*\"&gt;Tomb of Ferry de Gros, Lord of Dy ...\n [2] &lt;a href=\"./record/22534?highlight=*:*\"&gt;Tomb                              ...\n [3] &lt;a href=\"./record/21218?highlight=*:*\"&gt;Portrait of a Woman               ...\n [4] &lt;a href=\"./record/21881?highlight=*:*\"&gt;Untitled                          ...\n [5] &lt;a href=\"./record/20907?highlight=*:*\"&gt;Standing Male Nude                ...\n [6] &lt;a href=\"./record/20972?highlight=*:*\"&gt;Seated Male Nude with Blanket     ...\n [7] &lt;a href=\"./record/99439?highlight=*:*\"&gt;Untitled - Woman and Man          ...\n [8] &lt;a href=\"./record/50508?highlight=*:*\"&gt;Unknown                           ...\n [9] &lt;a href=\"./record/21054?highlight=*:*\"&gt;Striding Male Nude                ...\n[10] &lt;a href=\"./record/21038?highlight=*:*\"&gt;Standing Male Nude                ...\n\n\nThen we extract the text with html_text():\n\npage %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%\n  html_node(\"h3 a\") %&gt;%\n  html_text()\n\n [1] \"Tomb of Ferry de Gros, Lord of Dyeghem Nieunland                                    \"                           \n [2] \"Tomb                                    \"                                                                       \n [3] \"Portrait of a Woman                                                                            (1955)\"          \n [4] \"Untitled                                                                            (Unknown)\"                  \n [5] \"Standing Male Nude                                                                            (JAN 1958)\"       \n [6] \"Seated Male Nude with Blanket                                                                            (1959)\"\n [7] \"Untitled - Woman and Man                                                                            (1981)\"     \n [8] \"Unknown                                                                            (1957)\"                      \n [9] \"Striding Male Nude                                                                            (1950)\"           \n[10] \"Standing Male Nude                                    \"                                                         \n\n\nAnd get rid of all the spurious white space in the text with str_squish(), which reduces repeated whitespace inside a string.\nTake a look at the help for str_squish() to find out more about how it works and how it‚Äôs different from str_trim().\n\npage %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%\n  html_node(\"h3 a\") %&gt;%\n  html_text() %&gt;% \n  str_squish()\n\n [1] \"Tomb of Ferry de Gros, Lord of Dyeghem Nieunland\"\n [2] \"Tomb\"                                            \n [3] \"Portrait of a Woman (1955)\"                      \n [4] \"Untitled (Unknown)\"                              \n [5] \"Standing Male Nude (JAN 1958)\"                   \n [6] \"Seated Male Nude with Blanket (1959)\"            \n [7] \"Untitled - Woman and Man (1981)\"                 \n [8] \"Unknown (1957)\"                                  \n [9] \"Striding Male Nude (1950)\"                       \n[10] \"Standing Male Nude\"                              \n\n\nAnd finally save the resulting data as a vector of length 10:\n\ntitles &lt;- page %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%\n  html_node(\"h3 a\") %&gt;%\n  html_text() %&gt;%\n  str_squish()\n\n\n\nLinks\nThe same nodes that contain the text for the titles also contains information on the links to individual art piece pages for each title. We can extract this information using a new function from the rvest package, html_attr(), which extracts attributes.\nA mini HTML lesson! The following is how we define hyperlinked text in HTML:\n&lt;a href=\"https://www.google.com\"&gt;Search on Google&lt;/a&gt;\nAnd this is how the text would look like on a webpage: Search on Google.\nHere the text is Search on Google and the href attribute contains the url of the website you‚Äôd go to if you click on the hyperlinked text: https://www.google.com.\nThe moral of the story is: the link is stored in the href attribute.\n\npage %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%   # same nodes\n  html_node(\"h3 a\") %&gt;%         # as before\n  html_attr(\"href\")             # but get href attribute instead of text\n\n [1] \"./record/20567?highlight=*:*\" \"./record/22534?highlight=*:*\"\n [3] \"./record/21218?highlight=*:*\" \"./record/21881?highlight=*:*\"\n [5] \"./record/20907?highlight=*:*\" \"./record/20972?highlight=*:*\"\n [7] \"./record/99439?highlight=*:*\" \"./record/50508?highlight=*:*\"\n [9] \"./record/21054?highlight=*:*\" \"./record/21038?highlight=*:*\"\n\n\nThese don‚Äôt really look like URLs as we know then though. They‚Äôre relative links.\nSee the help for str_replace() to find out how it works. Remember that the first argument is passed in from the pipeline, so you just need to define the pattern and replacement arguments.\n\nClick on one of art piece titles in your browser and take note of the url of the webpage it takes you to. Think about how that url compares to what we scraped above? How is it different? Using str_replace(), fix the URLs. You‚Äôll note something special happening in the pattern to replace. We want to replace the ., but we have it as \\\\.. This is because the period . is a special character and so we need to escape it first with backslashes, \\\\s.\n\n\n\nArtists\n\nFill in the blanks to scrape artist names.\n\n\n\nPut it altogether\n\nFill in the blanks to organize everything in a tibble.\n\n\n\nScrape the next page\n\nClick on the next page, and grab its url. Fill in the blank in to define a new object: second_url. Copy-paste code from top of the R script to scrape the new set of art pieces, and save the resulting data frame as second_ten."
  },
  {
    "objectID": "teaching/tidyverse/material/uoe-art.html#functions",
    "href": "teaching/tidyverse/material/uoe-art.html#functions",
    "title": "University of Edinburgh Art Collection",
    "section": "Functions",
    "text": "Functions\nYou‚Äôve been using R functions, now it‚Äôs time to write your own!\nLet‚Äôs start simple. Here is a function that takes in an argument x, and adds 2 to it.\n\nadd_two &lt;- function(x){\n  x + 2\n}\n\nLet‚Äôs test it:\n\nadd_two(3)\n\n[1] 5\n\nadd_two(10)\n\n[1] 12\n\n\nThe skeleton for defining functions in R is as follows:\n\nfunction_name &lt;- function(input){\n  # do something with the input(s)\n  # return something\n}\n\nThen, a function for scraping a page should look something like:\nReminder: Function names should be short but evocative verbs.\n\nfunction_name &lt;- function(url){\n  # read page at url\n  # extract title, link, artist info for n pieces on page\n  # return a n x 3 tibble\n}\n\n\nFill in the blanks using code you already developed in the previous exercises. Name the function scrape_page.\n\nTest out your new function by running the following in the console. Does the output look right? Discuss with teammates whether you‚Äôre getting the same results as before.\n\nscrape_page(first_url)\nscrape_page(second_url)"
  },
  {
    "objectID": "teaching/tidyverse/material/uoe-art.html#iteration",
    "href": "teaching/tidyverse/material/uoe-art.html#iteration",
    "title": "University of Edinburgh Art Collection",
    "section": "Iteration",
    "text": "Iteration\nWe went from manually scraping individual pages to writing a function to do the same. Next, we will work on making our workflow a little more efficient by using R to iterate over all pages that contain information on the art collection.\nThat means we give develop a list of URLs (of pages that each have 10 art pieces), and write some code that applies the scrape_page() function to each page, and combines the resulting data frames from each page into a single data frame with 3289 rows and 3 columns.\n\nList of URLs\nClick through the first few of the pages in the art collection and observe their URLs to confirm the following pattern:\n[sometext]offset=0     # Pieces 1-10\n[sometext]offset=10    # Pieces 11-20\n[sometext]offset=20    # Pieces 21-30\n[sometext]offset=30    # Pieces 31-40\n...\n[sometext]offset=3280  # Pieces 3281-3289\nWe can construct these URLs in R by pasting together two pieces: (1) a common (root) text for the beginning of the URL, and (2) numbers starting at 0, increasing by 10, all the way up to 3289. Two new functions are helpful for accomplishing this: glue() for pasting two pieces of text and seq() for generating a sequence of numbers.\n\nFill in the blanks to construct the list of URLs.\n\n\n\nMapping\nFinally, we‚Äôre ready to iterate over the list of URLs we constructed. We will do this by mapping the function we developed over the list of URLs. There are a series of mapping functions in R and they each take the following form:\nmap([x], [function to apply to each element of x])\nIn our case x is the list of URLs we constructed and the function to apply to each element of x is the function we developed earlier, scrape_page. And as a result we want a data frame, so we use map_dfr function:\n\nmap_dfr(urls, scrape_page)\n\n\nFill in the blanks to scrape all pages, and to create a new data frame called uoe_art.\n\n\n\nWrite out data\n\nFinally write out the data frame you constructed into the data folder so that you can use it in the analysis section."
  },
  {
    "objectID": "teaching/tidyverse/material/uoe-art.html#analysis",
    "href": "teaching/tidyverse/material/uoe-art.html#analysis",
    "title": "University of Edinburgh Art Collection",
    "section": "Analysis",
    "text": "Analysis\nFor the rest of the exercises you can work in Quarto/R Markdown.\nNow that we have a tidy dataset that we can analyze, let‚Äôs do that!\nWe‚Äôll start with some data cleaning, to clean up the dates that appear at the end of some title text in parentheses. Some of these are years, others are more specific dates, some art pieces have no date information whatsoever, and others have some non-date information in parentheses. This should be interesting to clean up!\nFirst thing we‚Äôll try is to separate the title column into two: one for the actual title and the other for the date if it exists. In human speak, we need to\n‚Äúseparate the title column at the first occurrence of ( and put the contents on one side of the ( into a column called title and the contents on the other side into a column called date‚Äù\nLuckily, there‚Äôs a function that does just this: separate()!\nAnd once we have completed separating the single title column into title and date, we need to do further clean-up in the date column to get rid of extraneous )s with str_remove(), capture year information, and save the data as a numeric variable.\n\nFill in the blanks in to implement the data wrangling we described above. Note that this will result in some warnings when you run the code, and that‚Äôs OK! Read the warnings, and explain what they mean, and why we are ok with leaving them in given that our objective is to just capture year where it‚Äôs convenient to do so.\nPrint out a summary of the data frame using the skim() function. How many pieces have artist info missing? How many have year info missing?\nMake a histogram of years. Use a reasonable binwidth. Do you see anything out of the ordinary?\nFind which piece has the out of the ordinary year and go to its page on the art collection website to find the correct year for it. Can you tell why our code didn‚Äôt capture the correct year information? Correct the error in the data frame and visualize the data again.\n\nHint: You‚Äôll want to use mutate() and if_else() or case_when() to implement the correction.\n\nWho is the most commonly featured artist in the collection? Do you know them? Any guess as to why the university has so many pieces from them?\nFinal question! How many art pieces have the word ‚Äúchild‚Äù in their title? Try to figure it out, and ask for help if you‚Äôre stuck.\n\nHint: str_subset() can be helful here. You should consider how you might capture titles where the word appears as ‚Äúchild‚Äù and ‚ÄúChild‚Äù.\nSource: https://collections.ed.ac.uk/art/about"
  },
  {
    "objectID": "teaching/tidyverse/material/plastic-waste.html",
    "href": "teaching/tidyverse/material/plastic-waste.html",
    "title": "Global plastic waste",
    "section": "",
    "text": "Plastic pollution is a major and growing problem, negatively affecting oceans and wildlife health. Our World in Data has a lot of great data at various levels including globally, per country, and over time. For this lab we focus on data from 2010.\nAdditionally, National Geographic ran a data visualization communication contest on plastic waste as seen here."
  },
  {
    "objectID": "teaching/tidyverse/material/plastic-waste.html#packages",
    "href": "teaching/tidyverse/material/plastic-waste.html#packages",
    "title": "Global plastic waste",
    "section": "Packages",
    "text": "Packages\nWe‚Äôll use the tidyverse package for this analysis.\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "teaching/tidyverse/material/plastic-waste.html#data",
    "href": "teaching/tidyverse/material/plastic-waste.html#data",
    "title": "Global plastic waste",
    "section": "Data",
    "text": "Data\nThe dataset for this assignment can be found as a csv file. You can read it in using the following (make sure you save the data in your working directory).\n\nplastic_waste &lt;- read_csv(\"data/plastic-waste.csv\")\n\nThe variable descriptions are as follows:\n\ncode: 3 Letter country code\nentity: Country name\ncontinent: Continent name\nyear: Year\ngdp_per_cap: GDP per capita constant 2011 international $, rate\nplastic_waste_per_cap: Amount of plastic waste per capita in kg/day\nmismanaged_plastic_waste_per_cap: Amount of mismanaged plastic waste per capita in kg/day\nmismanaged_plastic_waste: Tonnes of mismanaged plastic waste\ncoastal_pop: Number of individuals living on/near coast\ntotal_pop: Total population according to Gapminder"
  },
  {
    "objectID": "teaching/tidyverse/material/nobels-csv.html",
    "href": "teaching/tidyverse/material/nobels-csv.html",
    "title": "Nobel winners",
    "section": "",
    "text": "library(tidyverse)\n\nLet‚Äôs first load the data:\n\nnobel &lt;- ___(___)\n\nThen let‚Äôs split the data into two:\n\n# stem laureates\n___ &lt;- nobel %&gt;%\n  filter(___)\n\n# non-steam laureates\n___ &lt;- nobel %&gt;%\n  filter(___)\n\nAnd finally write out the data:\n\n# add code for writing out the two data frames here"
  },
  {
    "objectID": "teaching/tidyverse/material/brexit.html",
    "href": "teaching/tidyverse/material/brexit.html",
    "title": "Brexit",
    "section": "",
    "text": "library(tidyverse)\n\nIn September 2019, YouGov survey asked 1,639 GB adults the following question:\n\nIn hindsight, do you think Britain was right/wrong to vote to leave EU?\n\nRight to leave\n\nWrong to leave\n\nDon‚Äôt know\n\n\nThe data from the survey is in data/brexit.csv.\n\nbrexit &lt;- read_csv(\"data11/brexit.csv\")\n\nIn the course video we made the following visualisation.\n\nbrexit &lt;- brexit %&gt;%\n  mutate(\n    region = fct_relevel(region, \"london\", \"rest_of_south\", \"midlands_wales\", \"north\", \"scot\"),\n    region = fct_recode(region, London = \"london\", `Rest of South` = \"rest_of_south\", `Midlands / Wales` = \"midlands_wales\", North = \"north\", Scotland = \"scot\")\n  )\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  ) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\",\n    \"Right\" = \"#67a9cf\",\n    \"Don't know\" = \"gray\"\n  )) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn this application exercise we tell different stories with the same data.\n\nExercise 1 - Free scales\nAdd scales = \"free_x\" as an argument to the facet_wrap() function. How does the visualisation change? How is the story this visualisation telling different than the story the original plot tells?\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region,\n    nrow = 1, labeller = label_wrap_gen(width = 12),\n    # ___\n  ) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  ) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\",\n    \"Right\" = \"#67a9cf\",\n    \"Don't know\" = \"gray\"\n  )) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nExercise 2 - Comparing proportions across facets\nFirst, calculate the proportion of wrong, right, and don‚Äôt know answers in each category and then plot these proportions (rather than the counts) and then improve axis labeling. How is the story this visualisation telling different than the story the original plot tells? Hint: You‚Äôll need the scales package to improve axis labeling, which means you‚Äôll need to load it on top of the document as well.\n\n\nExercise 3 - Comparing proportions across bars\nRecreate the same visualisation from the previous exercise, this time dodging the bars for opinion proportions for each region, rather than faceting by region and then improve the legend. How is the story this visualisation telling different than the story the previous plot tells?"
  },
  {
    "objectID": "teaching/tidyverse/material/countries-of-the-world.html",
    "href": "teaching/tidyverse/material/countries-of-the-world.html",
    "title": "Countries of the world",
    "section": "",
    "text": "In order to complete this assignment you will need a Chrome browser with the Selector Gadget extension installed.\nThis website lists the names of 250 countries, as well as their flag, capital, population and size in square kilometres. Our goal could be to read this information into R for each country so that we can potentially analyse it further.\nBefore we start, we should load the required packages (we will also need the tidyverse package this time) and read the website with the function read_html() and assign it to an R object.\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(DT)\n\npage &lt;- read_html(\"https://scrapethissite.com/pages/simple/\")"
  },
  {
    "objectID": "teaching/tidyverse/material/countries-of-the-world.html#country-names",
    "href": "teaching/tidyverse/material/countries-of-the-world.html#country-names",
    "title": "Countries of the world",
    "section": "Country names",
    "text": "Country names\nUse the Selector Gadget to identify the CSS selectors needed to extract country names.\n\ncountry &lt;- page %&gt;%\n  html_elements(\".country-name\") %&gt;%\n  html_text(trim = TRUE) \n\nhead(country)\n\n[1] \"Andorra\"              \"United Arab Emirates\" \"Afghanistan\"         \n[4] \"Antigua and Barbuda\"  \"Anguilla\"             \"Albania\""
  },
  {
    "objectID": "teaching/tidyverse/material/countries-of-the-world.html#capitals-population-and-area",
    "href": "teaching/tidyverse/material/countries-of-the-world.html#capitals-population-and-area",
    "title": "Countries of the world",
    "section": "Capitals, population and area",
    "text": "Capitals, population and area\nLet us now turn to the further information for each country. Again use the selector gadget to identify the CSS selector needed which in this case is .country-info:\n\npage %&gt;%\n  html_elements(\".country-info\") %&gt;%\n  html_text(trim = TRUE) %&gt;% \n  head(n = 10)\n\n [1] \"Capital: Andorra la VellaPopulation: 84000Area (km2): 468.0\"   \n [2] \"Capital: Abu DhabiPopulation: 4975593Area (km2): 82880.0\"      \n [3] \"Capital: KabulPopulation: 29121286Area (km2): 647500.0\"        \n [4] \"Capital: St. John'sPopulation: 86754Area (km2): 443.0\"         \n [5] \"Capital: The ValleyPopulation: 13254Area (km2): 102.0\"         \n [6] \"Capital: TiranaPopulation: 2986952Area (km2): 28748.0\"         \n [7] \"Capital: YerevanPopulation: 2968000Area (km2): 29800.0\"        \n [8] \"Capital: LuandaPopulation: 13068161Area (km2): 1246700.0\"      \n [9] \"Capital: NonePopulation: 0Area (km2): 1.4E7\"                   \n[10] \"Capital: Buenos AiresPopulation: 41343201Area (km2): 2766890.0\"\n\n\nSo we get the names of the capitals, but also the population and the size of the country. The selector was not specific enough and we have to tell html_elements() more precisely which of these we are interested in. These CSS selectors differ between the three countries‚Äô information:\n\nThe selector country-capital gives us the capital of the countries:\n\n\ncapital &lt;- page %&gt;%\n  html_elements(\".country-capital\") %&gt;%\n  html_text(trim = TRUE) \n\nhead(capital)\n\n[1] \"Andorra la Vella\" \"Abu Dhabi\"        \"Kabul\"            \"St. John's\"      \n[5] \"The Valley\"       \"Tirana\"          \n\n\n\nThe selector country-population gives us the population of the countries:\n\n\npopulation &lt;-  page %&gt;%\n  html_elements(\".country-population\") %&gt;%\n  html_text() %&gt;% \n  as.numeric()\nhead(population)\n\n[1]    84000  4975593 29121286    86754    13254  2986952\n\n\n\nThe selector country-area gives us the area of the countries:\n\n\narea &lt;-  page %&gt;%\n  html_elements(\".country-area\") %&gt;%\n  html_text() %&gt;% \n  as.numeric()\nhead(area)\n\n[1]    468  82880 647500    443    102  28748\n\n\nNote that we need to tell R to interpret the ‚Äútext‚Äù read from the HTML code as numbers using the function as.numeric()."
  },
  {
    "objectID": "teaching/tidyverse/material/countries-of-the-world.html#merge-into-one-tibble",
    "href": "teaching/tidyverse/material/countries-of-the-world.html#merge-into-one-tibble",
    "title": "Countries of the world",
    "section": "Merge into one tibble",
    "text": "Merge into one tibble\nWe could already continue working with this, but for many applications it is more practical if we combine the data in a vertical form:\n\ncountries &lt;- tibble(\n  country = country,\n  capital = capital,\n  population = population,\n  area = area\n)\ncountries\n\n# A tibble: 250 √ó 4\n   country              capital          population     area\n   &lt;chr&gt;                &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;\n 1 Andorra              Andorra la Vella      84000      468\n 2 United Arab Emirates Abu Dhabi           4975593    82880\n 3 Afghanistan          Kabul              29121286   647500\n 4 Antigua and Barbuda  St. John's            86754      443\n 5 Anguilla             The Valley            13254      102\n 6 Albania              Tirana              2986952    28748\n 7 Armenia              Yerevan             2968000    29800\n 8 Angola               Luanda             13068161  1246700\n 9 Antarctica           None                      0 14000000\n10 Argentina            Buenos Aires       41343201  2766890\n# ‚Ñπ 240 more rows"
  },
  {
    "objectID": "teaching/tidyverse/material/countries-of-the-world.html#all-in-one-step",
    "href": "teaching/tidyverse/material/countries-of-the-world.html#all-in-one-step",
    "title": "Countries of the world",
    "section": "All in one step",
    "text": "All in one step\nIf we are sure that we do not need the individual vectors, we can also perform the reading of the data and the creation of the tibble in a single step. Below you can see how the complete scraping process can be completed in relatively few lines.\n\npage &lt;- \"https://scrapethissite.com/pages/simple/\" %&gt;%\n  read_html()\n\ncountries_2 &lt;- tibble(\n  Land = page %&gt;%\n    html_elements(css = \".country-name\") %&gt;% \n    html_text(trim = TRUE),\n  capital = page %&gt;% \n    html_elements(css = \".country-capital\") %&gt;% \n    html_text(),\n  population = page %&gt;% \n    html_elements(css = \".country-population\") %&gt;% \n    html_text() %&gt;% \n    as.numeric(),\n  area = page %&gt;% \n    html_elements(css = \".country-area\") %&gt;% \n    html_text() %&gt;% \n    as.numeric()\n)\n\ncountries_2\n\n# A tibble: 250 √ó 4\n   Land                 capital          population     area\n   &lt;chr&gt;                &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;\n 1 Andorra              Andorra la Vella      84000      468\n 2 United Arab Emirates Abu Dhabi           4975593    82880\n 3 Afghanistan          Kabul              29121286   647500\n 4 Antigua and Barbuda  St. John's            86754      443\n 5 Anguilla             The Valley            13254      102\n 6 Albania              Tirana              2986952    28748\n 7 Armenia              Yerevan             2968000    29800\n 8 Angola               Luanda             13068161  1246700\n 9 Antarctica           None                      0 14000000\n10 Argentina            Buenos Aires       41343201  2766890\n# ‚Ñπ 240 more rows"
  },
  {
    "objectID": "teaching/tidyverse/material/nyc-flights.html",
    "href": "teaching/tidyverse/material/nyc-flights.html",
    "title": "NYC flights",
    "section": "",
    "text": "For the below exercies we use the tidyverse package for much of the data wrangling and visualisation and the data lives in the nycflights13 package. This is data a dataset of flights departing from New York City (NYC) airports in the year 2013.\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\nThe data sets available in the package can be viewed using the following syntax:\nYou will need one or combinations of these to solve the following exercises. These tables are organized as the figure below shows."
  },
  {
    "objectID": "teaching/tidyverse/material/nyc-flights.html#packages-and-data",
    "href": "teaching/tidyverse/material/nyc-flights.html#packages-and-data",
    "title": "NYC flights",
    "section": "",
    "text": "For the below exercies we use the tidyverse package for much of the data wrangling and visualisation and the data lives in the nycflights13 package. This is data a dataset of flights departing from New York City (NYC) airports in the year 2013.\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\nThe data sets available in the package can be viewed using the following syntax:\nYou will need one or combinations of these to solve the following exercises. These tables are organized as the figure below shows."
  },
  {
    "objectID": "teaching/tidyverse/material/nyc-flights.html#familiarizing-ourselves-with-the-dataset",
    "href": "teaching/tidyverse/material/nyc-flights.html#familiarizing-ourselves-with-the-dataset",
    "title": "NYC flights",
    "section": "Familiarizing ourselves with the dataset",
    "text": "Familiarizing ourselves with the dataset\n\nWhat variables are included in the flights dataset? How many rows are there?\nWhat variables are included in the airports dataset? How many rows are there?\nWhich variables are included in the airlines dataset? How many rows are there?"
  },
  {
    "objectID": "teaching/tidyverse/material/nyc-flights.html#focusing-on-atlanta",
    "href": "teaching/tidyverse/material/nyc-flights.html#focusing-on-atlanta",
    "title": "NYC flights",
    "section": "Focusing on Atlanta",
    "text": "Focusing on Atlanta\n\nLet‚Äôs focus on flights from NYC area airports to Atlanta GA (FAA code ATL). Create a new object atlanta that includes only these flights. Hint: use filter()). How many flights to Atlanta were there in 2013?"
  },
  {
    "objectID": "teaching/tidyverse/material/nyc-flights.html#seasonality",
    "href": "teaching/tidyverse/material/nyc-flights.html#seasonality",
    "title": "NYC flights",
    "section": "Seasonality",
    "text": "Seasonality\n\nIs there a difference in the number of flights per month?\nSummarize the number of flights for each month and provide a sorted list with the months with the most flights first. Hint: use group_by() in combination with summarize())."
  },
  {
    "objectID": "teaching/tidyverse/material/nyc-flights.html#use-filter",
    "href": "teaching/tidyverse/material/nyc-flights.html#use-filter",
    "title": "NYC flights",
    "section": "Use filter()",
    "text": "Use filter()\n\nFind all flights that\n\n\nHad an arrival delay of two or more hours.\nFlew to Houston (IAH or HOU)\nWere operated by United, American, or Delta. Hint: In the flights dataset, the column carrier indicates the airline, but it uses two-character carrier codes. You can find the carrier codes for the airlines in the airlines dataset. Since the carrier code dataset only has 16 rows, and the names of the airlines in that dataset are not exactly ‚ÄúUnited‚Äù, ‚ÄúAmerican‚Äù, or ‚ÄúDelta‚Äù, it is easiest to manually look up their carrier codes in that data.\nDeparted in summer (July, August, and September). Hint: the summer flights are those that departed in months 7 (July), 8 (August), and 9 (September).\nArrived more than two hours late, but didn‚Äôt leave late. Hint: Flights that arrived more than two hours late, but didn‚Äôt leave late will have an arrival delay of more than 120 minutes (arr_delay &gt; 120) and a non-positive departure delay (dep_delay &lt;=0)\nWere delayed by at least an hour, but made up over 30 minutes in flight. Hint: If a flight was delayed by at least an hour, then dep_delay &gt;= 60. If the flight didn‚Äôt make up any time in the air, then its arrival would be delayed by the same amount as its departure, meaning dep_delay == arr_delay, or alternatively, dep_delay - arr_delay == 0. If it makes up over 30 minutes in the air, then the arrival delay must be at least 30 minutes less than the departure delay, which is stated as dep_delay - arr_delay &gt; 30.\nDeparted between midnight and 6 am (inclusive). Hint: In dep_time, midnight is represented by 2400, not 0. You can verify this by checking the minimum and maximum of dep_time."
  },
  {
    "objectID": "teaching/tidyverse/material/nyc-flights.html#arrange-rows-with-arrange",
    "href": "teaching/tidyverse/material/nyc-flights.html#arrange-rows-with-arrange",
    "title": "NYC flights",
    "section": "Arrange rows with arrange()",
    "text": "Arrange rows with arrange()\n\nHow could you use arrange() to sort all missing values to the start? Hint: use is.na()) and add an indicator of whether the column has a missing value, the flights will first be sorted by desc(is.na(dep_time)). Since desc(is.na(dep_time)) is either TRUE when dep_time is missing, or FALSE, when it is not, the rows with missing values of dep_time will come first, since TRUE &gt; FALSE.\nSort flights to find the most delayed flights. Find the flights that left earliest.\nSort flights to find the fastest flights."
  },
  {
    "objectID": "teaching/tidyverse/material/nyc-flights.html#seelct-variables-with-select",
    "href": "teaching/tidyverse/material/nyc-flights.html#seelct-variables-with-select",
    "title": "NYC flights",
    "section": "Seelct variables with select()",
    "text": "Seelct variables with select()\n\nWhat does the one_of() function do? Why might it be helpful in conjunction with this vector?"
  },
  {
    "objectID": "teaching/tidyverse/material/nyc-flights.html#add-new-variables-with-mutate",
    "href": "teaching/tidyverse/material/nyc-flights.html#add-new-variables-with-mutate",
    "title": "NYC flights",
    "section": "Add new variables with mutate()",
    "text": "Add new variables with mutate()\n\nCompare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it?\nCome up with another approach that will give you the same output as not_cancelled %&gt;% count(dest) and not_cancelled %&gt;% count(tailnum, wt = distance) (without using count()).\nLook at the number of cancelled flights per day. Is there a pattern? Create a plot to visualize your answers.\nFor each destination, compute the total minutes of delay. For each flight, compute the proportion of the total delay for its destination.\nDelays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag() to explore how the delay of a flight is related to the delay of the immediately preceding flight. Use a plot to visualize this."
  },
  {
    "objectID": "teaching/tidyverse/material/nyc-flights.html#more-viz",
    "href": "teaching/tidyverse/material/nyc-flights.html#more-viz",
    "title": "NYC flights",
    "section": "More Viz",
    "text": "More Viz\n\nVisualize the distribution of on time departure rate across the three airports using a segmented bar plot. Hint: Remove NA‚Äôs and suppose that a flight that is delayed for less than 5 minutes is basically ‚Äúon time‚Äù."
  },
  {
    "objectID": "teaching/tidyverse/material/nyc-flights.html#advanced-exercises",
    "href": "teaching/tidyverse/material/nyc-flights.html#advanced-exercises",
    "title": "NYC flights",
    "section": "Advanced Exercises:",
    "text": "Advanced Exercises:\n\nImagine you wanted to draw (approximately) the route each plane flies from its origin to its destination. What variables would you need? What tables from the package you loaded would you need to combine?\nThis plots the approximate flight paths of the first 100 flights in the flights dataset. Try reproducing it. Hint: you can create a layer of map borders using borders(state).\n\n\n\n\n\n\n\n\n\n\n\nWe know that some days of the year are ‚Äúspecial‚Äù, and fewer people than usual fly on them. Since it is US data for 2013 we will consider: New Years Day, Independence Day, Thanksgiving Day, Christmas Day.\n\nHow might you represent that data as a data frame? What would be the primary keys of that table? How would it connect to the existing tables?\nWe can add a table of special dates, similar to the following table.\n\nspecial_days &lt;- tribble(\n  ~year, ~month, ~day, ~holiday,\n  2013, 01, 01, \"New Years Day\",\n  2013, 07, 04, \"Independence Day\",\n  2013, 11, 29, \"Thanksgiving Day\",\n  2013, 12, 25, \"Christmas Day\"\n)\n\nThe primary key of the table would be the (year, month, day) columns. The (year, month, day) columns could be used to join special_days with other tables.\n\nCreate a visualization fo your own to illustrate if indeed fewer people than usual fly on the above special days.\nCompute the average delay by destination, then join on the airports data frame so you can show the spatial distribution of delays. Here‚Äôs an easy way to draw a map of the United States (can you understand why we choose semi-join?):\n\n\nairports %&gt;%\n  semi_join(flights, c(\"faa\" = \"dest\")) %&gt;%\n  ggplot(aes(lon, lat)) +\n  borders(\"state\") +\n  geom_point() +\n  coord_quickmap() + \n  theme_void()\n\n\n\n\n\n\n\n\nHint: You might want to use the size or color of the points to display the average delay for each airport.\n\nWhat weather conditions make it more likely to see a delay? Use the variable precip (precipitation) from the weather dataset to answer this.\nWhat happened on June 13, 2013? Reproduce the following plot which displays the spatial pattern of delays, and then use Google to cross-reference with the weather. Hint: use library(viridis) to get the same colors."
  },
  {
    "objectID": "teaching/tidyverse/material/hotels-datawrangling.html",
    "href": "teaching/tidyverse/material/hotels-datawrangling.html",
    "title": "Hotel bookings - data wrangling",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)\n# From TidyTuesday: https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-02-11/readme.md\nhotels &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\nThe data is also available as a csv file which you can import directly."
  },
  {
    "objectID": "teaching/tidyverse/material/hotels-datawrangling.html#exercises",
    "href": "teaching/tidyverse/material/hotels-datawrangling.html#exercises",
    "title": "Hotel bookings - data wrangling",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\nWarm up! Take a look at an overview of the data with the skim() function.\nNote: I already gave you the answer to this exercise. You just need to knit the document and view the output. A definition of all variables is given in the Data dictionary section at the end, though you don‚Äôt need to familiarize yourself with all variables in order to work through these exercises.\n\nskim(hotels)\n\n\nData summary\n\n\nName\nhotels\n\n\nNumber of rows\n119390\n\n\nNumber of columns\n32\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n13\n\n\nDate\n1\n\n\nnumeric\n18\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nhotel\n0\n1\n10\n12\n0\n2\n0\n\n\narrival_date_month\n0\n1\n3\n9\n0\n12\n0\n\n\nmeal\n0\n1\n2\n9\n0\n5\n0\n\n\ncountry\n0\n1\n2\n4\n0\n178\n0\n\n\nmarket_segment\n0\n1\n6\n13\n0\n8\n0\n\n\ndistribution_channel\n0\n1\n3\n9\n0\n5\n0\n\n\nreserved_room_type\n0\n1\n1\n1\n0\n10\n0\n\n\nassigned_room_type\n0\n1\n1\n1\n0\n12\n0\n\n\ndeposit_type\n0\n1\n10\n10\n0\n3\n0\n\n\nagent\n0\n1\n1\n4\n0\n334\n0\n\n\ncompany\n0\n1\n1\n4\n0\n353\n0\n\n\ncustomer_type\n0\n1\n5\n15\n0\n4\n0\n\n\nreservation_status\n0\n1\n7\n9\n0\n3\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nreservation_status_date\n0\n1\n2014-10-17\n2017-09-14\n2016-08-07\n926\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nis_canceled\n0\n1\n0.37\n0.48\n0.00\n0.00\n0.00\n1\n1\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÖ\n\n\nlead_time\n0\n1\n104.01\n106.86\n0.00\n18.00\n69.00\n160\n737\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\narrival_date_year\n0\n1\n2016.16\n0.71\n2015.00\n2016.00\n2016.00\n2017\n2017\n‚ñÉ‚ñÅ‚ñá‚ñÅ‚ñÜ\n\n\narrival_date_week_number\n0\n1\n27.17\n13.61\n1.00\n16.00\n28.00\n38\n53\n‚ñÖ‚ñá‚ñá‚ñá‚ñÖ\n\n\narrival_date_day_of_month\n0\n1\n15.80\n8.78\n1.00\n8.00\n16.00\n23\n31\n‚ñá‚ñá‚ñá‚ñá‚ñÜ\n\n\nstays_in_weekend_nights\n0\n1\n0.93\n1.00\n0.00\n0.00\n1.00\n2\n19\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nstays_in_week_nights\n0\n1\n2.50\n1.91\n0.00\n1.00\n2.00\n3\n50\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nadults\n0\n1\n1.86\n0.58\n0.00\n2.00\n2.00\n2\n55\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nchildren\n4\n1\n0.10\n0.40\n0.00\n0.00\n0.00\n0\n10\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nbabies\n0\n1\n0.01\n0.10\n0.00\n0.00\n0.00\n0\n10\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nis_repeated_guest\n0\n1\n0.03\n0.18\n0.00\n0.00\n0.00\n0\n1\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nprevious_cancellations\n0\n1\n0.09\n0.84\n0.00\n0.00\n0.00\n0\n26\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nprevious_bookings_not_canceled\n0\n1\n0.14\n1.50\n0.00\n0.00\n0.00\n0\n72\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nbooking_changes\n0\n1\n0.22\n0.65\n0.00\n0.00\n0.00\n0\n21\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\ndays_in_waiting_list\n0\n1\n2.32\n17.59\n0.00\n0.00\n0.00\n0\n391\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nadr\n0\n1\n101.83\n50.54\n-6.38\n69.29\n94.58\n126\n5400\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nrequired_car_parking_spaces\n0\n1\n0.06\n0.25\n0.00\n0.00\n0.00\n0\n8\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\ntotal_of_special_requests\n0\n1\n0.57\n0.79\n0.00\n0.00\n0.00\n1\n5\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\n\n\nExercise 2.\nAre people traveling on a whim? Let‚Äôs see‚Ä¶\nFill in the blanks for filtering for hotel bookings where the guest is not from the US (country code \"USA\") and the lead_time is less than 1 day.\n\nhotels %&gt;%\n  filter(\n    country ____ \"USA\", \n    lead_time ____ ____\n    )\n\n\n\nExercise 3.\nHow many bookings involve at least 1 child or baby?\nIn the following chunk, replace\n\n[AT LEAST] with the logical operator for ‚Äúat least‚Äù (in two places)\n[OR] with the logical operator for ‚Äúor‚Äù\n\nNote: You will need to set eval=TRUE when you have an answer you want to try out in the qmd file.\n\nhotels %&gt;%\n  filter(\n    children [AT LEAST] 1 [OR] babies [AT LEAST] 1\n    )\n\n\n\nExercise 4.\nDo you think it‚Äôs more likely to find bookings with children or babies in city hotels or resort hotels? Test your intuition. Using filter() determine the number of bookings in resort hotels that have more than 1 child or baby in the room? Then, do the same for city hotels, and compare the numbers of rows in the resulting filtered data frames.\n\n# add code here\n# pay attention to correctness and code style\n\n\n# add code here\n# pay attention to correctness and code style\n\n\n\nExercise 5.\nCreate a frequency table of the number of adults in a booking. Display the results in descending order so the most common observation is on top. What is the most common number of adults in bookings in this dataset? Are there any surprising results?\n\n# add code here\n# pay attention to correctness and code style\n\n\n\nExercise 6.\nRepeat Exercise 5, once for canceled bookings (is_canceled coded as 1) and once for not canceled bookings (is_canceled coded as 0). What does this reveal about the surprising results you spotted in the previous exercise?\n\n# add code here\n# pay attention to correctness and code style\n\n\n\nExercise 7.\nCalculate minimum, mean, median, and maximum average daily rate (adr) grouped by hotel type so that you can get these statistics separately for resort and city hotels. Which type of hotel is higher, on average?\n\n# add code here\n# pay attention to correctness and code style\n\n\n\nExercise 8.\nWe observe two unusual values in the summary statistics above ‚Äì a negative minimum, and a very high maximum). What types of hotels are these? Locate these observations in the dataset and find out the arrival date (year and month) as well as how many people (adults, children, and babies) stayed in the room. You can investigate the data in the viewer to locate these values, but preferably you should identify them in a reproducible way with some code.\nHint: For example, you can filter for the given adr amounts and select the relevant columns.\n\n# add code here\n# pay attention to correctness and code style"
  },
  {
    "objectID": "teaching/tidyverse/material/hotels-datawrangling.html#data-dictionary",
    "href": "teaching/tidyverse/material/hotels-datawrangling.html#data-dictionary",
    "title": "Hotel bookings - data wrangling",
    "section": "Data dictionary",
    "text": "Data dictionary\nBelow is the full data dictionary. Note that it is long (there are lots of variables in the data), but we will be using a limited set of the variables for our analysis.\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nhotel\ncharacter\nHotel (H1 = Resort Hotel or H2 = City Hotel)\n\n\nis_canceled\ndouble\nValue indicating if the booking was canceled (1) or not (0)\n\n\nlead_time\ndouble\nNumber of days that elapsed between the entering date of the booking into the PMS and the arrival date\n\n\narrival_date_year\ndouble\nYear of arrival date\n\n\narrival_date_month\ncharacter\nMonth of arrival date\n\n\narrival_date_week_number\ndouble\nWeek number of year for arrival date\n\n\narrival_date_day_of_month\ndouble\nDay of arrival date\n\n\nstays_in_weekend_nights\ndouble\nNumber of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel\n\n\nstays_in_week_nights\ndouble\nNumber of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel\n\n\nadults\ndouble\nNumber of adults\n\n\nchildren\ndouble\nNumber of children\n\n\nbabies\ndouble\nNumber of babies\n\n\nmeal\ncharacter\nType of meal booked. Categories are presented in standard hospitality meal packages:  Undefined/SC ‚Äì no meal package;BB ‚Äì Bed & Breakfast;  HB ‚Äì Half board (breakfast and one other meal ‚Äì usually dinner);  FB ‚Äì Full board (breakfast, lunch and dinner)\n\n\ncountry\ncharacter\nCountry of origin. Categories are represented in the ISO 3155‚Äì3:2013 format\n\n\nmarket_segment\ncharacter\nMarket segment designation. In categories, the term ‚ÄúTA‚Äù means ‚ÄúTravel Agents‚Äù and ‚ÄúTO‚Äù means ‚ÄúTour Operators‚Äù\n\n\ndistribution_channel\ncharacter\nBooking distribution channel. The term ‚ÄúTA‚Äù means ‚ÄúTravel Agents‚Äù and ‚ÄúTO‚Äù means ‚ÄúTour Operators‚Äù\n\n\nis_repeated_guest\ndouble\nValue indicating if the booking name was from a repeated guest (1) or not (0)\n\n\nprevious_cancellations\ndouble\nNumber of previous bookings that were cancelled by the customer prior to the current booking\n\n\nprevious_bookings_not_canceled\ndouble\nNumber of previous bookings not cancelled by the customer prior to the current booking\n\n\nreserved_room_type\ncharacter\nCode of room type reserved. Code is presented instead of designation for anonymity reasons\n\n\nassigned_room_type\ncharacter\nCode for the type of room assigned to the booking. Sometimes the assigned room type differs from the reserved room type due to hotel operation reasons (e.g.¬†overbooking) or by customer request. Code is presented instead of designation for anonymity reasons\n\n\nbooking_changes\ndouble\nNumber of changes/amendments made to the booking from the moment the booking was entered on the PMS until the moment of check-in or cancellation\n\n\ndeposit_type\ncharacter\nIndication on if the customer made a deposit to guarantee the booking. This variable can assume three categories:No Deposit ‚Äì no deposit was made;Non Refund ‚Äì a deposit was made in the value of the total stay cost;Refundable ‚Äì a deposit was made with a value under the total cost of stay.\n\n\nagent\ncharacter\nID of the travel agency that made the booking\n\n\ncompany\ncharacter\nID of the company/entity that made the booking or responsible for paying the booking. ID is presented instead of designation for anonymity reasons\n\n\ndays_in_waiting_list\ndouble\nNumber of days the booking was in the waiting list before it was confirmed to the customer\n\n\ncustomer_type\ncharacter\nType of booking, assuming one of four categories:Contract - when the booking has an allotment or other type of contract associated to it;Group ‚Äì when the booking is associated to a group;Transient ‚Äì when the booking is not part of a group or contract, and is not associated to other transient booking;Transient-party ‚Äì when the booking is transient, but is associated to at least other transient booking\n\n\nadr\ndouble\nAverage Daily Rate as defined by dividing the sum of all lodging transactions by the total number of staying nights\n\n\nrequired_car_parking_spaces\ndouble\nNumber of car parking spaces required by the customer\n\n\ntotal_of_special_requests\ndouble\nNumber of special requests made by the customer (e.g.¬†twin bed or high floor)\n\n\nreservation_status\ncharacter\nReservation last status, assuming one of three categories:Canceled ‚Äì booking was canceled by the customer;Check-Out ‚Äì customer has checked in but already departed;No-Show ‚Äì customer did not check-in and did inform the hotel of the reason why\n\n\nreservation_status_date\ndouble\nDate at which the last status was set. This variable can be used in conjunction with the ReservationStatus to understand when was the booking canceled or when did the customer checked-out of the hotel"
  },
  {
    "objectID": "teaching/tidyverse/index.html",
    "href": "teaching/tidyverse/index.html",
    "title": "Data Science with Tidyverse",
    "section": "",
    "text": "Make sure to install and load Tidyverse:\n\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n\nSchedule\n\n\n\n\ndate\nslides\npractical\ndata\n\n\n\n\n1: Meet the toolkit\n12.04.2024\n\n\n\n\n\n2: Data visualization and ggplot\n19.04.2024\n\n\n\n\n\n3: Visualizing numerical and categorical data\n26.04.2024\n\n\n.zip\n\n\n4: Grammar of data wrangling I\n10.05.2024\n\n \n.zip\n\n\n5: Grammar of data wrangling II\n17.05.2024\n\n\n.zip\n\n\n6: Tidying Data\n23.05.2024\n\n\n.zip\n\n\n¬†¬†¬†¬† More Practicals\n07.06.2024\n\n \n.zip\n\n\n7: Data Types and Data Classes\n14.06.2024\n\n  \n.zip\n\n\n8: Importing and Recoding Data\n21.06.2024\n\n  \n.zip\n\n\n9: Web Scraping\n05.07.2024\n\n\n\n\n\n10: Functions and Iteration\n12.07.2024\n\n\n\n\n\n11: Effective Visualization\n19.07.2024\n\n\n.zip"
  },
  {
    "objectID": "teaching/math-ss/index.html",
    "href": "teaching/math-ss/index.html",
    "title": "Mathematics for Social Scientists",
    "section": "",
    "text": "Schedule\n\n\n\n\ndate\nslides\nhandout\n\n\n\n\n1: Preliminaries\n22.10.2024\n\n\n\n\n2: Algebra Review, Modular Arithmetic & Boolean Algebra\n29.10.2024\n\n\n\n\n3: Functions & Relations\n05.11.2024\n\n\n\n\n4: Sequences & Series, Limits & Continuity\n12.11.2024\n\n\n\n\n5: Calculus Fundamentals: Differentiation\n19.11.2024\n\n\n\n\n6: Calculus Fundamentals: The Integral\n26.11.2024\n\n\n\n\n7:Extrema in One Dimension\n03.12.2024\n\n\n\n\n8: Introduction to Probability\n10.12.2024\n\n\n\n\n9: Discrete Distributions\n17.12.2024"
  },
  {
    "objectID": "project/rmm/index.html",
    "href": "project/rmm/index.html",
    "title": "Multigraph Representation of Network Data",
    "section": "",
    "text": "Multigraphs are network representations in which multiple edges and edge loops (self edges) are permitted. Multigraph data structure can be observed directly but the possibility to obtain multigraphs using blocking, aggregation and scaling makes them widely applicable.\nFor the past decade, I‚Äôve been working on a statistical framework for analyzing network data using this representation. I have developed different multigraph multigraph models and derived several statistics under these models that can be used to analyse local and global network properties in order to convey important social phenomena and processes. The latest contribution in this framework is formal goodness of fit tests for the developed probability models for random multigraphs.\nThe proposed framework is in full implemented in the R package ‚Äòmultigraphr‚Äô and a description of various functions implemented in the package are given in the following. More details are provided in the package vignetteand the references listed."
  },
  {
    "objectID": "project/rmm/index.html#project-summary",
    "href": "project/rmm/index.html#project-summary",
    "title": "Multigraph Representation of Network Data",
    "section": "",
    "text": "Multigraphs are network representations in which multiple edges and edge loops (self edges) are permitted. Multigraph data structure can be observed directly but the possibility to obtain multigraphs using blocking, aggregation and scaling makes them widely applicable.\nFor the past decade, I‚Äôve been working on a statistical framework for analyzing network data using this representation. I have developed different multigraph multigraph models and derived several statistics under these models that can be used to analyse local and global network properties in order to convey important social phenomena and processes. The latest contribution in this framework is formal goodness of fit tests for the developed probability models for random multigraphs.\nThe proposed framework is in full implemented in the R package ‚Äòmultigraphr‚Äô and a description of various functions implemented in the package are given in the following. More details are provided in the package vignetteand the references listed."
  },
  {
    "objectID": "project/rmm/index.html#r-package-multigraphr",
    "href": "project/rmm/index.html#r-package-multigraphr",
    "title": "Multigraph Representation of Network Data",
    "section": "R package multigraphr",
    "text": "R package multigraphr\n\nPackage overview\n  \nThis package introduces the multigraph framework for analyzing social network data. Brief description of various functions implemented in the package are given in the following but more details are provided in the package vignettes and the references listed.\n\n\nInstallation\nYou can install the released version of multigraphr from CRAN with:\ninstall.packages(\"multigraphr\")\nThe development version from GitHub with:\n# install.packages(\"devtools\")\ndevtools::install_github(\"termehs/multigraphr\")"
  },
  {
    "objectID": "project/rmm/index.html#multigraphs-and-applicability",
    "href": "project/rmm/index.html#multigraphs-and-applicability",
    "title": "Multigraph Representation of Network Data",
    "section": "Multigraphs and applicability",
    "text": "Multigraphs and applicability\nMultigraphs are network representations in which multiple edges and edge loops (self edges) are permitted. These data structures can be either directly observed or aggregated by classifying or cross-classifying node attributes into meta nodes. For the latter case, within group edges correspond to self-edges. See example below where the original graph with 15 nodes and 12 edges (left) is aggregated based on node categories into a small multigraph with 4 nodes (right).\n\nEdge aggregation can also be used to obtain multigraphs. Assume that we study a graph with three different types of relations over three periods of time: \nIf we aggregate over time periods, we obtain for each edge category a multigraph for the total time period of three days:\n\nFor more details on these kinds of aggregations, see Shafie (2015;2016)."
  },
  {
    "objectID": "project/rmm/index.html#multigraph-representation-of-network-data",
    "href": "project/rmm/index.html#multigraph-representation-of-network-data",
    "title": "Multigraph Representation of Network Data",
    "section": "Multigraph representation of network data",
    "text": "Multigraph representation of network data\nMultigraphs are represented by their edge multiplicity sequence M with elements M(i,j), denoting the number of edges at vertex pair sites (i,j) ordered according to (1,1) &lt; (1,2) &lt;¬∑¬∑¬∑&lt; (1,n) &lt; (2,2) &lt; (2,3) &lt;¬∑¬∑¬∑&lt; (n,n), where n is number of nodes. The number of vertex pair sites is given by r = n(n+1)/2.\n\nRandom multigraph models\nTwo probability models for generating undirected random multigraphs are implemented in the package together with several statistics under these two models. Moreover, functions for goodness of fit tests are available for the presented models.\nNote that some of the functions are only practical for small scale multigraphs.\nThe first model is obtained by random stub matching (RSM) given observed degree sequence of a multigraphs, so that edge assignments to vertex pair sites are dependent. The second is obtained by independent edge assignments (IEA) according to a common probability distribution. There are two ways in which an approximate IEA model can be obtained from an RSM model, thus facilitating the structural analysis. These two ways are\n\nindependent stub assignment (ISA)\nindependent edge assignment of stubs (IEAS)\n\n(Shafie, 2016).\n\n\nExample\n\nlibrary('multigraphr')\n\nConsider a small graph on 3 nodes and the following adjacency matrix:\n\nA &lt;-  matrix(c(1, 1, 0, \n               1, 2, 2, \n               0, 2, 0), \n             nrow = 3, ncol = 3)\nA\n\n     [,1] [,2] [,3]\n[1,]    1    1    0\n[2,]    1    2    2\n[3,]    0    2    0\n\n\nThe degree sequence of the multigraph has double counted diagonals (edge stubs for loops) and is given by\n\nD &lt;- get_degree_seq(adj = A, type = 'graph')\nD\n\n[1] 3 7 2\n\n\nso that number of edges in the multigraph is half the sum of the degree sequence which is equal to 6.\nThe RSM model given observed degree sequence shows the sample space consists of 7 possible multigraphs, as represented by their multiplicity sequence m.seq (each row correspond to the edge multiplicity sequence of a unique multigraph):\n\nrsm_1 &lt;- rsm_model(deg.seq = D)\nrsm_1$m.seq\n\n  M11 M12 M13 M22 M23 M33\n1   1   1   0   3   0   1\n2   1   1   0   2   2   0\n3   1   0   1   3   1   0\n4   0   3   0   2   0   1\n5   0   3   0   1   2   0\n6   0   2   1   2   1   0\n7   0   1   2   3   0   0\n\n\nwith probabilities associated with each multigraph, together with statistics ‚Äònumber of loops‚Äô, ‚Äònumber of multiple edges‚Äô and ‚Äòsimple graphs or not‚Äô:\n\nrsm_1$prob.dists\n\n    prob.rsm loops multiedges simple\n1 0.03030303     5          1      0\n2 0.18181818     3          3      0\n3 0.06060606     4          2      0\n4 0.06060606     3          3      0\n5 0.24242424     1          5      0\n6 0.36363636     2          4      0\n7 0.06060606     3          3      0\n\n\nConsider using the IEA model to approximate the RSM model so that edge assignment probabilities are functions of observed degree sequence. Note that the sample space for multigraphs is much bigger than for the RSM model so the multiplicity sequences are not printed (they can be found using the function get_edgemultip_seq for very small multigraphs and their probabilities can be found using the multinomial distribution). The following shows the number of multigraphs under either of the IEA models:\n\nieas_1 &lt;-   iea_model(adj = A , type = 'graph',  model = 'IEAS', K = 0, apx = TRUE)\nieas_1$nr.multigraphs\n\n[1] 462"
  },
  {
    "objectID": "project/rmm/index.html#statistics-to-analyze-structural-properties",
    "href": "project/rmm/index.html#statistics-to-analyze-structural-properties",
    "title": "Multigraph Representation of Network Data",
    "section": "Statistics to analyze structural properties",
    "text": "Statistics to analyze structural properties\nThese statistics include number of loops (indicator of e.g.¬†homophily) and number of multiple edges (indicator of e.g.¬†multiplexity/interlocking), which are implemented in the package together with their probability distributions, moments and interval estimates under the different multigraph models.\n\nExample (cont‚Äôd)\nUnder the RSM model, the first two moments and interval estimates of the statistics M1 = ‚Äònumber of loops‚Äô and M2 = ‚Äònumber of multiple edges‚Äô are given by\n\nrsm_1$M\n\n             M1    M2\nExpected  2.273 3.727\nVariance  0.986 0.986\nUpper 95% 4.259 5.713\nLower 95% 0.287 1.741\n\n\nwhich are calculated using the numerically found probability distributions under RSM (no analytical solutions exist for these moments).\nUnder the IEA models (IEAS or ISA), moments of these statistics, together with the complexity statistic \\(R_k\\) representing the sequence of frequencies of edge sites with multiplicities 0,1,‚Ä¶,k, are found using derived formulas. Thus, there is no limit on multigraph size to use these. When the IEAS model is used to approximate the RSM model as shown above:\n\nieas_1$M\n\n              M1    M2\nObserved   3.000 3.000\nExpected   2.273 3.727\nVariance   1.412 1.412\nUpper 95%  4.649 6.104\nLower 95% -0.104 1.351\n\nieas_1$R\n\n             R0     R1     R2\nObserved  2.000  2.000  2.000\nExpected  2.674  1.588  1.030\nVariance  0.575  1.129  0.760\nUpper 95% 4.191  3.713  2.773\nLower 95% 1.156 -0.537 -0.713\n\n\nWhen the ISA model is used to approximate the RSM model (see above):\n\nisa_1 &lt;-   iea_model(adj = A , type = 'graph',  \n                     model = 'ISA', K = 0, apx = TRUE)\nisa_1$M\n\n             M1    M2\nObserved  3.000 3.000\nExpected  2.583 3.417\nVariance  1.471 1.471\nUpper 95% 5.009 5.842\nLower 95% 0.158 0.991\n\nisa_1$R\n\n             R0     R1     R2\nObserved  2.000  2.000  2.000\nExpected  2.599  1.703  1.018\nVariance  0.622  1.223  0.748\nUpper 95% 4.176  3.915  2.748\nLower 95% 1.021 -0.509 -0.711\n\n\nThe IEA models can also be used independent of the RSM model. For example, the IEAS model can be used where edge assignment probabilities are estimated using the observed edge multiplicities (maximum likelihood estimates):\n\nieas_2 &lt;-   iea_model(adj = A , type = 'graph',  \n                      model = 'IEAS', K = 0, apx = FALSE)\nieas_2$M\n\n             M1    M2\nObserved  3.000 3.000\nExpected  3.000 3.000\nVariance  1.500 1.500\nUpper 95% 5.449 5.449\nLower 95% 0.551 0.551\n\nieas_2$R\n\n             R0     R1     R2\nObserved  2.000  2.000  2.000\nExpected  2.845  1.331  1.060\nVariance  0.434  0.805  0.800\nUpper 95% 4.163  3.125  2.849\nLower 95% 1.528 -0.464 -0.729\n\n\nThe ISA model can also be used independent of the RSM model. Then, a sequence containing the stub assignment probabilities (for example based on prior belief) should be given as argument:\n\nisa_2 &lt;-   iea_model(adj = A , type = 'graph',  \n                     model = 'ISA', K = 0, apx = FALSE, p.seq = c(1/3, 1/3, 1/3))\nisa_2$M\n\n              M1    M2\nObserved   3.000 3.000\nExpected   2.000 4.000\nVariance   1.333 1.333\nUpper 95%  4.309 6.309\nLower 95% -0.309 1.691\n\nisa_2$R\n\n             R0     R1     R2\nObserved  2.000  2.000  2.000\nExpected  2.144  2.248  1.160\nVariance  0.632  1.487  0.710\nUpper 95% 3.734  4.687  2.845\nLower 95% 0.554 -0.190 -0.525\n\n\nThe interval estimates can then be visualized to detect discrepancies between observed and expected values thus indicating social mechanisms at play in the generation of edges, and to detect interval overlap and potential interdependence between different types of edges (see Shafie 2015,2016; Shafie & Schoch 2021)."
  },
  {
    "objectID": "project/rmm/index.html#goodness-of-fit-tests",
    "href": "project/rmm/index.html#goodness-of-fit-tests",
    "title": "Multigraph Representation of Network Data",
    "section": "Goodness of fit tests",
    "text": "Goodness of fit tests\nGoodness of fits tests of multigraph models using Pearson (S) and information divergence (A) test statistics under the random stub matching (RSM) and by independent edge assignments (IEA) model, where the latter is either independent edge assignments of stubs (IEAS) or independent stub assignment (ISA). The tests are performed using goodness-of-fit measures between the edge multiplicity sequence of a specified model or an observed multigraph, and the expected multiplicity sequence according to a simple or composite hypothesis."
  },
  {
    "objectID": "project/rmm/index.html#simulated-goodness-of-fit-tests",
    "href": "project/rmm/index.html#simulated-goodness-of-fit-tests",
    "title": "Multigraph Representation of Network Data",
    "section": "Simulated goodness of fit tests",
    "text": "Simulated goodness of fit tests\nProbability distributions of test statistics, summary of tests, moments of tests statistics, adjusted test statistics, critical values, significance level according to asymptotic distribution, and power of tests can be examined using gof_sim given a specified model from which we simulate observed values from, and a null or non-null hypothesis from which we calculate expected values from. This in order to investigate the behavior of the null and non-null distributions of the test statistics and their fit to to asymptotic chi-square distributions.\n\nExample\nSimulated goodness of fit tests for multigraphs with n=4 nodes and m=10 edges.\n(1) Testing a simple IEAS hypothesis with degree sequence (6,6,6,2) against a RSM model with degrees (8,8,2,2):\n\ngof1 &lt;- gof_sim(m = 10, model = 'IEAS', deg.mod = c(8,8,2,2), \n                hyp = 'IEAS', deg.hyp = c(6,6,6,2))\n\n(2) Testing a correctly specified simple IEAS hypothesis with degree sequence (14,2,2,2):\n\ngof2 &lt;- gof_sim(m = 10, model = 'IEAS', deg.mod = c(14,2,2,2), \n                hyp = 'IEAS', deg.hyp = c(14,2,2,2))\n\nThe non-null (gof1) and null (gof2) distributions of the test statistics together with their asymptotic chi2-distribution can be visualized using ggplot2:\n \n(3) Testing a composite IEAS hypothesis against a RSM model with degree sequence (14,2,2,2):\n\ngof3 &lt;- gof_sim(m = 10, model = 'RSM', deg.mod = c(14,2,2,2), \n                hyp = 'IEAS', deg.hyp = 0)\n\n(4) Testing a composite ISA hypothesis against a ISA model with degree sequence (14,2,2,2):\n\ngof4 &lt;- gof_sim(m = 10, model = 'ISA', deg.mod = c(14,2,2,2), \n                hyp = 'ISA', deg.hyp = 0)\n\nThe non-null (gof3) and null (gof4) distributions of the test statistics can then be visualized as shown above to check their fit to the asymptotic œá¬≤-distribution."
  },
  {
    "objectID": "project/rmm/index.html#performing-the-goodness-of-fit-test-on-your-data",
    "href": "project/rmm/index.html#performing-the-goodness-of-fit-test-on-your-data",
    "title": "Multigraph Representation of Network Data",
    "section": "Performing the goodness of fit test on your data",
    "text": "Performing the goodness of fit test on your data\nUse function gof_test to test whether the observed data follows IEA approximations of the RSM model. The null hypotheses can be simple or composite, although the latter is not recommended for small multigraphs as it is difficult to detect a false composite hypothesis under an RSM model and under IEA models (this can be checked and verified using gof_sim to simulate these cases).\nNon-rejection of the null implies that the approximations fit the data, thus implying that above statistics under the IEA models can be used to further analyze the observed network. Consider the following multigraph from the well known Florentine family network with marital. This multigraphs is aggregated based on the three actor attributes wealth (W), number of priorates (P) and total number of ties (T) which are all dichotomized to reflect high or low economic, political and social influence (details on the aggregation can be found in Shafie, 2015):\n\nThe multiplicity sequence represented as an upper triangular matrix for this mutigrpah is given by\n\nflor_m &lt;- t(matrix(c (0, 0, 1, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 2, 0, 0, 1, 5,\n                      0, 0, 0, 0, 0, 0, 1, 1,\n                      0, 0, 0, 0, 0, 0, 1, 2,\n                      0, 0, 0, 0, 0, 0, 2, 1,\n                      0, 0, 0, 0, 0, 0, 0, 2,\n                      0, 0, 0, 0, 0, 0, 0, 1), nrow= 8, ncol=8))\n\nThe equivalence of adjacency matrix for the multigraph is given by\n\nflor_adj &lt;- flor_m+t(flor_m)\nflor_adj \n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    0    0    1    0    0    0    0    0\n[2,]    0    0    0    0    0    0    0    0\n[3,]    1    0    0    2    0    0    1    5\n[4,]    0    0    2    0    0    0    1    1\n[5,]    0    0    0    0    0    0    1    2\n[6,]    0    0    0    0    0    0    2    1\n[7,]    0    0    1    1    1    2    0    2\n[8,]    0    0    5    1    2    1    2    2\n\n\nwith the diagonal representing the loops double counted (Shafie, 2016). The function get_degree_seq can now be used to find the degree sequence for this multigraph:\n\nflor_d &lt;- get_degree_seq(adj = flor_adj, type = 'multigraph')\nflor_d\n\n[1]  1  0  9  4  3  3  7 13\n\n\nNow we test whether the observed network fits the IEAS or the ISA model. The \\(p\\)-values for testing whether there is a significant difference between observed and expected edge multiplicity values according to the two approximate IEA models are given in the output tables below. Note that the asymptotic œá¬≤-distribution has \\(r-1 = (n(n+1)/2) - 1 =35\\) degrees of freedom.\n\nflor_ieas_test &lt;- gof_test(flor_adj, 'multigraph', 'IEAS', flor_d, 35)\nflor_ieas_test\n\n  Stat dof Stat(obs) p-value\n1    S  35    15.762   0.998\n2    A  35    18.905   0.988\n\n\n\nflor_isa_test &lt;- gof_test(flor_adj, 'multigraph', 'ISA', flor_d, 35)\nflor_isa_test \n\n  Stat dof Stat(obs) p-value\n1    S  35    16.572   0.997\n2    A  35    19.648   0.983\n\n\nThe results show that we have strong evidence for the null such that we fail to reject it. Thus, there is not a significant difference between the observed and the expected edge multiplicity sequence according on the two IEA models. Statistics derived under these models presented above can thus be used to analyze the structure of these multigraphs."
  },
  {
    "objectID": "project/rmm/index.html#references",
    "href": "project/rmm/index.html#references",
    "title": "Multigraph Representation of Network Data",
    "section": "References",
    "text": "References\n\nShafie, T. (2015). A multigraph approach to social network analysis. Journal of Social Structure, 16. Link\nShafie, T. (2016). Analyzing local and global properties of multigraphs. The Journal of Mathematical Sociology, 40(4), 239-264. Link\nFrank, O., Shafie, T., (2018). Random Multigraphs and Aggregated Triads with Fixed Degrees. Network Science, 6(2), 232-250. Link\nShafie, T., Schoch, D. (2021) Multiplexity analysis of networks using multigraph representations. Statistical Methods & Applications 30, 1425‚Äì1444. Link\nShafie, T. (2022). Goodness of fit tests for random multigraph models, Journal of Applied Statistics. Link"
  },
  {
    "objectID": "project/seand/index.html",
    "href": "project/seand/index.html",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "",
    "text": "In multivariate statistics, there is an abundance of different measures of centrality and spread, many of which cannot be applied on variables measured on nominal or ordinal scale. Since network data in majority comprises such variables, alternative measures for analyzing spread, flatness and association is needed. This is also of particular relevance given the special feature of interdependent observations in networks.\nMultivariate entropy analysis is a general statistical method for analyzing and finding dependence structure in data consisting of repeated observations of variables with a common domain and with discrete finite range spaces. Only nominal scale is required for each variable, so only the size of the variable‚Äôs range space is important but not its actual values. Variables on ordinal or numerical scales, even continuous numerical scales, can be used, but they should be aggregated so that their ranges match the number of available repeated observations. By investigating the frequencies of occurrences of joint variable outcomes, complicated dependence structures, partial independence and conditional independence as well as redundancies and functional dependence can be found.\nSince 2015, I am working with Ove Frank and Krzysztof Nowicki on a project in which we build a systematic framework for using statistical entropy tools to analyze network data.\nThe proposed framework is implemented in the R package ‚Äònetropy‚Äô and a description of various functions implemented in the package are given in the following. More details are provided in the package vignettes and the references listed."
  },
  {
    "objectID": "project/seand/index.html#project-summary",
    "href": "project/seand/index.html#project-summary",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "",
    "text": "In multivariate statistics, there is an abundance of different measures of centrality and spread, many of which cannot be applied on variables measured on nominal or ordinal scale. Since network data in majority comprises such variables, alternative measures for analyzing spread, flatness and association is needed. This is also of particular relevance given the special feature of interdependent observations in networks.\nMultivariate entropy analysis is a general statistical method for analyzing and finding dependence structure in data consisting of repeated observations of variables with a common domain and with discrete finite range spaces. Only nominal scale is required for each variable, so only the size of the variable‚Äôs range space is important but not its actual values. Variables on ordinal or numerical scales, even continuous numerical scales, can be used, but they should be aggregated so that their ranges match the number of available repeated observations. By investigating the frequencies of occurrences of joint variable outcomes, complicated dependence structures, partial independence and conditional independence as well as redundancies and functional dependence can be found.\nSince 2015, I am working with Ove Frank and Krzysztof Nowicki on a project in which we build a systematic framework for using statistical entropy tools to analyze network data.\nThe proposed framework is implemented in the R package ‚Äònetropy‚Äô and a description of various functions implemented in the package are given in the following. More details are provided in the package vignettes and the references listed."
  },
  {
    "objectID": "project/seand/index.html#r-package-netropy",
    "href": "project/seand/index.html#r-package-netropy",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "R package netropy",
    "text": "R package netropy\n\n\nPackage overview\n  \nThis package introduces these entropy tools in the context of network data. Brief description of various functions implemented in the package are given in the following but more details are provided in the package vignettes and the references listed.\n\n\nInstallation\nYou can install the released version of netropy from CRAN with:\ninstall.packages(\"netropy\")\nThe development version from GitHub with:\n# install.packages(\"devtools\")\ndevtools::install_github(\"termehs/netropy\")\nTo load the package:\n\nlibrary('netropy')\n\n\n\nLoading internal data\nThe different entropy tools are explained and illustrated by exploring data from a network study of a corporate law firm, which has previously been analysed by several authors (link). The data set is included in the package as a list with objects representing adjacency matrices for each of the three networks advice (directed), friendship (directed) and co-work (undirected), together with a data frame comprising 8 attributes on each of the 71 lawyers.\nTo load the data, extract each object and assign the correct names to them:\n\ndata(lawdata) \nadj.advice &lt;- lawdata[[1]]\nadj.friend &lt;- lawdata[[2]]\nadj.cowork &lt;-lawdata[[3]]\ndf.att &lt;- lawdata[[4]]"
  },
  {
    "objectID": "project/seand/index.html#variable-domains-and-data-editing",
    "href": "project/seand/index.html#variable-domains-and-data-editing",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "Variable domains and data editing",
    "text": "Variable domains and data editing\nA requirement for the applicability of these entropy tools is the specification of discrete variables with finite range spaces on the same domain: either node attributes/vertex variables, edges/dyad variables or triad variables. These can be either observed or transformed as shown in the following using the above example data set.\nWe have 8 vertex variables with 71 observations, two of which (years and age) are numerical and needs categorization based on their cumulative distributions. This categorization is in details described in the vignette ‚Äúvariable domains and data editing‚Äù. Here we just show the new dataframe created (note that variable senior is omitted as it only comprises unique values and that we edit all variable to start from 0):\n\natt.var &lt;-\n  data.frame(\n    status   = df.att$status-1,\n    gender   = df.att$gender,\n    office   = df.att$office-1,\n    years    = ifelse(df.att$years &lt;= 3,0,\n                      ifelse(df.att$years &lt;= 13,1,2)),\n    age      = ifelse(df.att$age &lt;= 35,0,\n                      ifelse(df.att$age &lt;= 45,1,2)),\n    practice = df.att$practice,\n    lawschool= df.att$lawschool-1\n    )\nhead(att.var)\n\n  status gender office years age practice lawschool\n1      0      1      0     2   2        1         0\n2      0      1      0     2   2        0         0\n3      0      1      1     1   2        1         0\n4      0      1      0     2   2        0         2\n5      0      1      1     2   2        1         1\n6      0      1      1     2   2        1         0\n\n\nThese vertex variables can be transformed into dyad variables by using the function get_dyad_var(). Observed node attributes in the dataframe att_var are then transformed into pairs of individual attributes. For example, status with binary outcomes is transformed into dyads having 4 possible outcomes (0,0), (0,1), (1,0), (1,1):\n\ndyad.status    &lt;- get_dyad_var(att.var$status, type = 'att')\ndyad.gender    &lt;- get_dyad_var(att.var$gender, type = 'att')\ndyad.office    &lt;- get_dyad_var(att.var$office, type = 'att')\ndyad.years     &lt;- get_dyad_var(att.var$years, type = 'att')\ndyad.age       &lt;- get_dyad_var(att.var$age, type = 'att')\ndyad.practice  &lt;- get_dyad_var(att.var$practice, type = 'att')\ndyad.lawschool &lt;- get_dyad_var(att.var$lawschool, type = 'att')\n\nSimilarly, dyad variables can be created based on observed ties. For the undirected edges, we use indicator variables read directly from the adjacency matrix for the dyad in question, while for the directed ones (advice and friendship) we have pairs of indicators representing sending and receiving ties with 4 possible outcomes :\n\ndyad.cwk    &lt;- get_dyad_var(adj.cowork, type = 'tie')\ndyad.adv    &lt;- get_dyad_var(adj.advice, type = 'tie')\ndyad.frn    &lt;- get_dyad_var(adj.friend, type = 'tie')\n\nAll 10 dyad variables are merged into one data frame for subsequent entropy analysis:\n\ndyad.var &lt;-\n  data.frame(cbind(status   = dyad.status$var,\n                  gender    = dyad.gender$var,\n                  office    = dyad.office$var,\n                  years     = dyad.years$var,\n                  age       = dyad.age$var,\n                  practice  = dyad.practice$var,\n                  lawschool = dyad.lawschool$var,\n                  cowork    = dyad.cwk$var,\n                  advice    = dyad.adv$var,\n                  friend    = dyad.frn$var)\n                  )\nhead(dyad.var)\n\n  status gender office years age practice lawschool cowork advice friend\n1      3      3      0     8   8        1         0      0      3      2\n2      3      3      3     5   8        3         0      0      0      0\n3      3      3      3     5   8        2         0      0      1      0\n4      3      3      0     8   8        1         6      0      1      2\n5      3      3      0     8   8        0         6      0      1      1\n6      3      3      1     7   8        1         6      0      1      1\n\n\nA similar function get_triad_var() is implemented for transforming vertex variables and different relation types into triad variables. This is described in more detail in the vignette ‚Äúvariable domains and data editing‚Äù."
  },
  {
    "objectID": "project/seand/index.html#univariate-bivariate-and-trivariate-entropies",
    "href": "project/seand/index.html#univariate-bivariate-and-trivariate-entropies",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "Univariate, bivariate and trivariate entropies",
    "text": "Univariate, bivariate and trivariate entropies\nThe function entropy_bivar() computes the bivariate entropies of all pairs of variables in the dataframe. The output is given as an upper triangular matrix with cells giving the bivariate entropies of row and column variables. The diagonal thus gives the univariate entropies for each variable in the dataframe:\n\nH2 &lt;- entropy_bivar(dyad.var)\nH2\n\n          status gender office years   age practice lawschool cowork advice\nstatus     1.493  2.868  3.640 3.370 3.912    3.453     4.363  2.092  2.687\ngender        NA  1.547  3.758 3.939 4.274    3.506     4.439  2.158  2.785\noffice        NA     NA  2.239 4.828 4.901    4.154     5.058  2.792  3.388\nyears         NA     NA     NA 2.671 4.857    4.582     5.422  3.268  3.868\nage           NA     NA     NA    NA 2.801    4.743     5.347  3.411  4.028\npractice      NA     NA     NA    NA    NA    1.962     4.880  2.530  3.127\nlawschool     NA     NA     NA    NA    NA       NA     2.953  3.567  4.186\ncowork        NA     NA     NA    NA    NA       NA        NA  0.615  1.687\nadvice        NA     NA     NA    NA    NA       NA        NA     NA  1.248\nfriend        NA     NA     NA    NA    NA       NA        NA     NA     NA\n          friend\nstatus     2.324\ngender     2.415\noffice     3.044\nyears      3.483\nage        3.637\npractice   2.831\nlawschool  3.812\ncowork     1.456\nadvice     1.953\nfriend     0.881\n\n\nBivariate entropies can be used to detect redundant variables that should be omitted from the dataframe for further analysis. This occurs when the univariate entropy for a variable is equal to the bivariate entropies for pairs including that variable. As seen above, the dataframe dyad.var has no redundant variables. This can also be checked using the function redundancy() which yields a binary matrix as output indicating which row and column variables are hold the same information:\n\nredundancy(dyad.var)\n\nNULL\n\n\nMore examples of using the function redundancy() is given in the vignette ‚Äúunivariate bivariate and trivariate entropies‚Äù.\nTrivariate entropies can be computed using the function entropy_trivar() which returns a dataframe with the first three columns representing possible triples of variables V1,V2, and V3 from the dataframe in question, and their entropies H(V1,V2,V3) as the fourth column. We illustrated this on the dataframe dyad.var:\n\nH3 &lt;- entropy_trivar(dyad.var)\nhead(H3, 10) # view first 10 rows of dataframe\n\n       V1     V2        V3 H(V1,V2,V3)\n1  status gender    office       4.938\n2  status gender     years       4.609\n3  status gender       age       5.129\n4  status gender  practice       4.810\n5  status gender lawschool       5.664\n6  status gender    cowork       3.464\n7  status gender    advice       4.048\n8  status gender    friend       3.685\n9  status office     years       5.321\n10 status office       age       5.721"
  },
  {
    "objectID": "project/seand/index.html#joint-entropy-and-association-graphs",
    "href": "project/seand/index.html#joint-entropy-and-association-graphs",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "Joint entropy and association graphs",
    "text": "Joint entropy and association graphs\nJoint entropies is a non-negative measure of association among pairs of variables. It is equal to 0 if and only if two variables are completely independent of each other.\nThe function joint_entropy() computes the joint entropies between all pairs of variables in a given dataframe and returns a list consisting of the upper triangular joint entropy matrix (univariate entropies in the diagonal) and a dataframe giving the frequency distributions of unique joint entropy values. A function argument specifies the precision given in number of decimals for which the frequency distribution of unique entropy values is created (default is 3). Applying the function on the dataframe dyad.var with two decimals:\n\nJ &lt;- joint_entropy(dyad.var, 2)\nJ$matrix\n\n          status gender office years  age practice lawschool cowork advice\nstatus      1.49   0.17   0.09  0.79 0.38     0.00      0.08   0.02   0.05\ngender        NA   1.55   0.03  0.28 0.07     0.00      0.06   0.00   0.01\noffice        NA     NA   2.24  0.08 0.14     0.05      0.13   0.06   0.10\nyears         NA     NA     NA  2.67 0.61     0.05      0.20   0.02   0.05\nage           NA     NA     NA    NA 2.80     0.02      0.41   0.01   0.02\npractice      NA     NA     NA    NA   NA     1.96      0.04   0.05   0.08\nlawschool     NA     NA     NA    NA   NA       NA      2.95   0.00   0.01\ncowork        NA     NA     NA    NA   NA       NA        NA   0.62   0.18\nadvice        NA     NA     NA    NA   NA       NA        NA     NA   1.25\nfriend        NA     NA     NA    NA   NA       NA        NA     NA     NA\n          friend\nstatus      0.05\ngender      0.01\noffice      0.08\nyears       0.07\nage         0.05\npractice    0.01\nlawschool   0.02\ncowork      0.04\nadvice      0.18\nfriend      0.88\n\nJ$freq\n\n      j  #(J = j) #(J &gt;= j)\n1  0.79         1         1\n2  0.61         1         2\n3  0.41         1         3\n4  0.38         1         4\n5  0.28         1         5\n6   0.2         1         6\n7  0.18         2         8\n8  0.17         1         9\n9  0.14         1        10\n10 0.13         1        11\n11  0.1         1        12\n12 0.09         1        13\n13 0.08         4        17\n14 0.07         2        19\n15 0.06         2        21\n16 0.05         7        28\n17 0.04         2        30\n18 0.03         1        31\n19 0.02         5        36\n20 0.01         5        41\n21    0         4        45\n\n\nAs seen, the strongest association is between the variables status and years with joint entropy values of 0.79. We have independence (joint entropy value of 0) between two pairs of variables: (status,practice), (practise,gender), (cowork,gender),and (cowork,lawschool).\nThese results can be illustrated in a association graph using the function assoc_graph() which returns a ggraph object in which nodes represent variables and links represent strength of association (thicker links indicate stronger dependence). To use the function we need to load the ggraph library and to determine a threshold which the graph drawn is based on. We set it to 0.15 so that we only visualize the strongest associations\n\nlibrary(ggraph)\nassoc_graph(dyad.var, 0.15)\n\n\n\n\n\n\n\n\nGiven this threshold, we see isolated and disconnected nodes representing independent variables. We note strong dependence between the three dyadic variables status,years and age, but also a somewhat strong dependence among the three variables lawschool, years and age, and the three variables status, years and gender. The association graph can also be interpreted as a tendency for relations cowork and friend to be independent conditionally on relation advice, that is, any dependence between dyad variables cowork and friend is explained by advice.\nA threshold that gives a graph with reasonably many small independent or conditionally independent subsets of variables can be considered to represent a multivariate model for further testing.\nMore details and examples of joint entropies and association graphs are given in the vignette ‚Äújoint entropies and association graphs‚Äù."
  },
  {
    "objectID": "project/seand/index.html#prediction-power-based-on-expected-conditional-entropies",
    "href": "project/seand/index.html#prediction-power-based-on-expected-conditional-entropies",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "Prediction power based on expected conditional entropies",
    "text": "Prediction power based on expected conditional entropies\nThe function prediction_power() computes prediction power when pairs of variables in a given dataframe are used to predict a third variable from the same dataframe. The variable to be predicted and the dataframe in which this variable also is part of is given as input arguments, and the output is an upper triangular matrix giving the expected conditional entropies of pairs of row and column variables (denoted \\(X\\) and \\(Y\\)) of the matrix, i.e.¬†EH(Z|X,Y) where \\(Z\\) is the variable to be predicted. The diagonal gives EH(Z|X) , that is when only one variable as a predictor. Note that NA‚Äôs are in the row and column representing the variable being predicted.\nAssume we are interested in predicting variable status (that is whether a lawyer in the data set is an associate or partner). This is done by running the following syntax\n\nprediction_power('status', dyad.var)\n\n          status gender office years   age practice lawschool cowork advice\nstatus        NA     NA     NA    NA    NA       NA        NA     NA     NA\ngender        NA  1.375  1.180 0.670 0.855    1.304     1.225  1.306  1.263\noffice        NA     NA  2.147 0.493 0.820    1.374     1.245  1.373  1.325\nyears         NA     NA     NA 2.265 0.573    0.682     0.554  0.691  0.667\nage           NA     NA     NA    NA 1.877    1.089     0.958  1.087  1.052\npractice      NA     NA     NA    NA    NA    2.446     1.388  1.459  1.410\nlawschool     NA     NA     NA    NA    NA       NA     3.335  1.390  1.337\ncowork        NA     NA     NA    NA    NA       NA        NA  2.419  1.400\nadvice        NA     NA     NA    NA    NA       NA        NA     NA  2.781\nfriend        NA     NA     NA    NA    NA       NA        NA     NA     NA\n          friend\nstatus        NA\ngender     1.270\noffice     1.334\nyears      0.684\nage        1.058\npractice   1.427\nlawschool  1.350\ncowork     1.411\nadvice     1.407\nfriend     3.408\n\n\nFor better readability, the powers of different predictors can be conveniently compared by using prediction plots that display a color matrix with rows for \\(X\\) and columns for \\(Y\\) with darker colors in the cells when we have higher prediction power for \\(Z\\).\nMore details and examples of expected conditional entropies and prediction power are given in the package vignette."
  },
  {
    "objectID": "project/seand/index.html#divergence-tests-of-goodness-of-fit",
    "href": "project/seand/index.html#divergence-tests-of-goodness-of-fit",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "Divergence Tests of Goodness of Fit",
    "text": "Divergence Tests of Goodness of Fit\nOccurring cliques in association graphs represent connected components of dependent variables, and by comparing the graphs for different thresholds, specific structural models of multivariate dependence can be suggested and tested. The function div_gof() allows such hypothesis tests for pairwise independence of \\(X\\) and \\(Y\\): \\(X \\bot Y\\), and pairwise independence conditional a third variable \\(Z\\): \\(X\\bot Y|Z\\).\nTo test friend\\(\\bot\\) cowork\\(|\\)advice, that is whether dyad variable friend is independent of cowork given advice we use the function as shown below:\n\ndiv_gof(dat = dyad.var, var1 = \"friend\", var2 = \"cowork\", var_cond = \"advice\")\n\n     D df(D)\n1 0.94    12\n\n\nNot specifying argument var_cond would instead test friend\\(\\bot\\)cowork without any conditioning."
  },
  {
    "objectID": "project/seand/index.html#references",
    "href": "project/seand/index.html#references",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "References",
    "text": "References\nParts of the theoretical background is provided in the package vignettes, but for more details, consult the following literature:\n\nFrank, O., & Shafie, T. (2016). Multivariate entropy analysis of network data. Bulletin of Sociological Methodology/Bulletin de M√©thodologie Sociologique, 129(1), 45-63. link"
  },
  {
    "objectID": "talks/SUNBELT2024/index.html",
    "href": "talks/SUNBELT2024/index.html",
    "title": "‚ÄúThe Interplay of Structural Features and Observed Dissimilarities Among Centrality Indices‚Äù",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "talks/EUSN2019/index.html",
    "href": "talks/EUSN2019/index.html",
    "title": "‚ÄúGender Dependent Structures of Dialogue Networks in Films‚Äù",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "talks/SUNBELT2022/index.html",
    "href": "talks/SUNBELT2022/index.html",
    "title": "‚ÄúStatistical Analysis of Multivariate Egocentric Networks‚Äù",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "talks/NETWORKS2021/index.html",
    "href": "talks/NETWORKS2021/index.html",
    "title": "‚ÄúGoodness of Fit Tests for Random Multigraph Models‚Äù",
    "section": "",
    "text": "Slides\n\n\nVideo"
  },
  {
    "objectID": "publications/ontario_sna/index.html",
    "href": "publications/ontario_sna/index.html",
    "title": "Nation Building and Social Signaling in Southern Ontario AD 1350-1650",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/ontario_sna/index.html#abstract",
    "href": "publications/ontario_sna/index.html#abstract",
    "title": "Nation Building and Social Signaling in Southern Ontario AD 1350-1650",
    "section": "Abstract",
    "text": "Abstract\nPottery is a mainstay of archaeological analysis worldwide. Often, high proportions of the pottery recovered from a given site are decorated in some manner. In northern Iroquoia, late pre-contact pottery and early contact decoration commonly occur on collars‚Äîthick bands of clay that encircle a pot and extend several centimeters down from the lip. These decorations constitute signals that conveyed information about a pot‚Äôs user(s). In southern Ontario the period A.D. 1350 to 1650 witnessed substantial changes in socio-political and settlement systems that included population movement, coalescence of formerly separate communities into large villages and towns, waxing and waning of regional strife, the formation of nations, and finally the development of three confederacies that each occupied distinct, constricted areas. Social network analysis demonstrates that signaling practices changed to reflect these regional patterns. Networks become more consolidated through time ultimately resulting in a ‚Äúsmall world‚Äù network with small degrees of separation between sites reflecting the integration of communities within and between the three confederacies.\nPLoS ONE 11(5), e0156178"
  },
  {
    "objectID": "publications/isotopes/index.html",
    "href": "publications/isotopes/index.html",
    "title": "Investigating Human Geographic Origins using Dual-Isotope (87Sr/86Sr, d18O) Assignment approaches",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/isotopes/index.html#abstract",
    "href": "publications/isotopes/index.html#abstract",
    "title": "Investigating Human Geographic Origins using Dual-Isotope (87Sr/86Sr, d18O) Assignment approaches",
    "section": "Abstract",
    "text": "Abstract\nSubstantial progress in the application of multiple isotope analyses has greatly improved the ability to identify nonlocal individuals amongst archaeological populations over the past decades. More recently the development of large scale models of spatial isotopic variation (isoscapes) has contributed to improved geographic assignments of human and animal origins. Persistent challenges remain, however, in the accurate identification of individual geographic origins from skeletal isotope data in studies of human (and animal) migration and provenance. In an attempt to develop and test more standardized and quantitative approaches to geographic assignment of individual origins using isotopic data two methods, combining 87Sr/86Sr and d18O isoscapes, are examined for the Circum-Caribbean region 1) an Interval approach using a defined range of fixed isotopic variation per location and 2) a Likelihood assignment approach using univariate and bivariate probability density functions. These two methods are tested with enamel isotope data from a modern sample of known origin from Caracas, Venezuela and further explored with two archaeological samples of unknown origin recovered from Cuba and Trinidad. The results emphasize both the potential and limitation of the different approaches. Validation tests on the known origin sample exclude most areas of the Circum-Caribbean region and correctly highlight Caracas as a possible place of origin with both approaches. The positive validation results clearly demonstrate the overall efficacy of a dual-isotope approach to geoprovenance. The accuracy and precision of geographic assignments may be further improved by better understanding of the relationships between environmental and biological isotope variation; continued development and refinement of relevant isoscapes; and the eventual incorporation of a broader array of isotope proxy data.\nPLoS ONE 12(2), e0172562"
  },
  {
    "objectID": "publications/data_protection/index.html",
    "href": "publications/data_protection/index.html",
    "title": "Data Protection for Online Social Networks and P-Stability for Graphs",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/data_protection/index.html#abstract",
    "href": "publications/data_protection/index.html#abstract",
    "title": "Data Protection for Online Social Networks and P-Stability for Graphs",
    "section": "Abstract",
    "text": "Abstract\nGraphs can be used as a model for online social networks. In this framework, vertices represent individuals and edges relationships between individuals. In recent years, different approaches have been considered to offer data privacy to online social networks and for developing graph protection. Perturbative approaches are formally defined in terms of perturbation and modification of graphs. In this paper, we discuss the concept of P -stability on graphs and its relation to data privacy. The concept of P-stability is rooted in the number of graphs given a fixed degree sequence. In this paper, we show that for any graph there exists a class of P-stable graphs. This result implies that there is a fully polynomial randomized approximation for graph masking for the graphs in the class. In order to further refine the classification of a given graph, we introduce the concept of natural class of a graph. It is based on a class of scale-free networks.\nIEEE Transactions on Emerging Topics in Computing 4(3), 374-381"
  },
  {
    "objectID": "publications/global_local_multigraphs/index.html",
    "href": "publications/global_local_multigraphs/index.html",
    "title": "Analyzing Local and Global Properties of Multigraphs",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/global_local_multigraphs/index.html#abstract",
    "href": "publications/global_local_multigraphs/index.html#abstract",
    "title": "Analyzing Local and Global Properties of Multigraphs",
    "section": "Abstract",
    "text": "Abstract\nThe local structure of undirected multigraphs under two random multigraph models is analyzed and compared. The first model generates multigraphs by randomly coupling pairs of stubs according to a fixed degree sequence so that edge assignments to vertex pair sites are dependent. The second model is a simplification that ignores the dependency between the edge assignments. It is investigated when this ignorance is justified so that the simplified model can be used as an approximation, thus facilitating the structural analysis of network data with multiple relations and loops. The comparison is based on the local properties of multigraphs given by marginal distribution of edge multiplicities and some local properties that are aggregations of global properties.\nJournal of Mathematical Sociology 40(4), 239-264"
  },
  {
    "objectID": "publications/gof_multigraph/index.html",
    "href": "publications/gof_multigraph/index.html",
    "title": "Goodness of fit tests for random multigraph models",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/gof_multigraph/index.html#abstract",
    "href": "publications/gof_multigraph/index.html#abstract",
    "title": "Goodness of fit tests for random multigraph models",
    "section": "Abstract",
    "text": "Abstract\nGoodness of fit tests for two probabilistic multigraph models are presented. The first model is random stub matching given fixed degrees (RSM) so that edge assignments to vertex pair sites are dependent, and the second is independent edge assignments (IEA) according to a common probability distribution. Tests are performed using goodness of fit measures between the edge multiplicity sequence of an observed multigraph, and the expected one according to a simple or composite hypothesis. Test statistics of Pearson type and of likelihood ratio type are used, and the expected values of the Pearson statistic under the different models are derived. Test performances based on simulations indicate that even for small number of edges, the null distributions of both statistics are well approximated by their asymptotic œá2-distribution. The non-null distributions of the test statistics can be well approximated by proposed adjusted œá2-distributions used for power approximations. The influence of RSM on both test statistics is substantial for small number of edges and implies a shift of their distributions towards smaller values compared to what holds true for the null distributions under IEA. Two applications on social networks are included to illustrate how the tests can guide in the analysis of social structure.\nJournal of Applied Statistics"
  },
  {
    "objectID": "publications/aggregated_triads/index.html",
    "href": "publications/aggregated_triads/index.html",
    "title": "Random Multigraphs and Aggregated Triads with Fixed Degrees",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/aggregated_triads/index.html#abstract",
    "href": "publications/aggregated_triads/index.html#abstract",
    "title": "Random Multigraphs and Aggregated Triads with Fixed Degrees",
    "section": "Abstract",
    "text": "Abstract\nRandom multigraphs with fixed degrees are obtained by the configuration model or by so called random stub matching. New combinatorial results are given for the global probability distribution of edge multiplicities and its marginal local distributions of loops and edges. The number of multigraphs on triads is determined for arbitrary degrees, and aggregated triads are shown to be useful for analyzing regular and almost regular multigraphs. Relationships between entropy and complexity are given and numerically illustrated for multigraphs with different number of vertices and specified average and variance for the degrees.\nNetwork Science 6(2), 232-250"
  },
  {
    "objectID": "publications/interplay_str_cent/index.html",
    "href": "publications/interplay_str_cent/index.html",
    "title": "The interplay of structural features and observed dissimilarities among centrality indices",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/interplay_str_cent/index.html#abstract",
    "href": "publications/interplay_str_cent/index.html#abstract",
    "title": "The interplay of structural features and observed dissimilarities among centrality indices",
    "section": "Abstract",
    "text": "Abstract\nAn abundance of centrality indices has been proposed which capture the importance of nodes in a network based on different structural features. While there remains a persistent belief that similarities in outcomes of indices is contingent on their technical definitions, a growing body of research shows that structural features affect observed similarities more than technicalities. We conduct a series of experiments on artificial networks to trace the influence of specific structural features on the similarity of indices which confirm previous results in the literature. Our analysis on 1163 real-world networks, however, shows that little of the observations on synthetic networks convincingly carry over to empirical settings. Our findings suggest that although it seems clear that (dis)similarities among centralities depend on structural properties of the network, using correlation type analyses do not seem to be a promising approach to uncover such connections.\nSocial Networks"
  },
  {
    "objectID": "publications/reconstructing_arch_nets/index.html",
    "href": "publications/reconstructing_arch_nets/index.html",
    "title": "Reconstructing Archaeological Networks with Structural Holes",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/reconstructing_arch_nets/index.html#abstract",
    "href": "publications/reconstructing_arch_nets/index.html#abstract",
    "title": "Reconstructing Archaeological Networks with Structural Holes",
    "section": "Abstract",
    "text": "Abstract\nModel-based reconstruction is an approach to infer network structures where they cannot be observed. For archaeological networks, several models based on assumptions concerning distance among sites, site size, or costs and benefits have been proposed to infer missing ties. Since these assumptions are formulated at a dyadic level, they do not provide means to express dependencies among ties and therefore include less plausible network scenarios. In this paper we investigate the use of network models that explicitly incorporate tie dependence. In particular, we consider exponential random graph models, and show how they can be applied to reconstruct networks coherent with Burt‚Äôs arguments on closure and structural holes (Burt 2001). The approach is illustrated on data from the Middle Bronze Age in the Aegean. authors:\nJournal of Archaeological Method and Theory 25(1), 226-253"
  },
  {
    "objectID": "publications/multivariate_entropy_analysis/index.html",
    "href": "publications/multivariate_entropy_analysis/index.html",
    "title": "Multivariate Entropy Analysis of Network Data",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/multivariate_entropy_analysis/index.html#abstract",
    "href": "publications/multivariate_entropy_analysis/index.html#abstract",
    "title": "Multivariate Entropy Analysis of Network Data",
    "section": "Abstract",
    "text": "Abstract\nMultigraphs with numerical or qualitative attributes defined on vertices and edges can benefit from systematic methods based on multivariate entropies for describing and analysing the interdependencies that are present between vertex and edge attributes. This is here illustrated by application of these tools to a subset of data on the social relations among Renaissance Florentine families collected by John Padgett. Using multivariate entropies we show how it is possible to systematically check for tendencies in data that can be described as independencies or conditional independencies, or as dependencies allowing certain combinations of variables to predict other variables. We also show how different structural models can be tested by divergence measures obtained from the multivariate entropies.\nBulletin of Sociological Methodology/Bulletin de M√©thodologie Sociologique 120(1), 45-63"
  },
  {
    "objectID": "publications/multiplexity/index.html",
    "href": "publications/multiplexity/index.html",
    "title": "Multiplexity Analysis of Networks using Multigraph Representations",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/multiplexity/index.html#abstract",
    "href": "publications/multiplexity/index.html#abstract",
    "title": "Multiplexity Analysis of Networks using Multigraph Representations",
    "section": "Abstract",
    "text": "Abstract\nMultivariate networks comprising several compositional and structural variables can be represented as multigraphs by various forms of aggregations based on vertex attributes. We propose a framework to perform exploratory and confirmatory multiplexity analysis of aggregated multigraphs in order to find relevant associations between vertex and edge attributes. The exploration is performed by comparing frequencies of the different edges within and between aggregated vertex categories, while the confirmatory analysis is performed using derived complexity or multiplexity statistics under different random multigraph models. These statistics are defined by the distribution of edge multiplicities and provide information on the covariation and dependencies of different edges given vertex attributes. The presented approach highlights the need to further analyse and model structural dependencies with respect to edge entrainment. We illustrate the approach by applying it on a well known multivariate network dataset which has previously been analysed in the context of multiplexity.\npublication: Statistical Methods & Applications 30, 1425‚Äì1444"
  },
  {
    "objectID": "publications/hypergraph/index.html",
    "href": "publications/hypergraph/index.html",
    "title": "Hypergraph Representations: A Study of Carib Attacks on Colonial Forces, 1509-1700",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/hypergraph/index.html#abstract",
    "href": "publications/hypergraph/index.html#abstract",
    "title": "Hypergraph Representations: A Study of Carib Attacks on Colonial Forces, 1509-1700",
    "section": "Abstract",
    "text": "Abstract\nNetwork data consisting of recorded historical events can be represented as hyper-graphs where the ties or events can connect any number of nodes or event related attributes. In this paper, we perform a centrality analysis of a directed hypergraph representing attacks by indigenous peoples from the Lesser Antilles on European colonial settlements, 1509-1700. The results of central attacks with respect to at- tacked colonial force, member of attack alliances, and year and location of attack are discussed and compared to a non-relational exploratory analysis of the data. This comparison points to the importance of a mixed methods approach to enhance the analysis and to obtain a complementary understanding of a network study.\npublication: ‚ÄòJournal of Historical Network Research 1(1), 52-70‚Äô"
  },
  {
    "objectID": "publications/complexity_multigraphs/index.html",
    "href": "publications/complexity_multigraphs/index.html",
    "title": "Complexity of Families of Multigraphs",
    "section": "",
    "text": "pdf"
  },
  {
    "objectID": "publications/complexity_multigraphs/index.html#abstract",
    "href": "publications/complexity_multigraphs/index.html#abstract",
    "title": "Complexity of Families of Multigraphs",
    "section": "Abstract",
    "text": "Abstract\nThis article describes families of finite multigraphs with labeled or unlabeled edges and vertices. It shows how size and complexity vary for different types of equivalence classes of graphs defined by ignoring only edge labels or ignoring both edge and vertex labels. Complexity is quantified by the distribution of edge multiplicities, and different complexity measures are discussed. Basic occupancy models for multigraphs are used to illustrate different graph distributions on isomorphism and complexity. The loss of information caused by ignoring edge and vertex labels is quantified by entropy and joint information that provide tools for studying properties of and relations between different graph families.\nIn 2012 JSM Proceedings: Papers Presented at the Joint Statistical Meetings, San Diego, California, July 28-August 2, 2012, and Other ASA-sponsored Conferences, American Statistical Association, 2012"
  },
  {
    "objectID": "publications/sna_chapter/index.html",
    "href": "publications/sna_chapter/index.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "book chapter"
  },
  {
    "objectID": "publications/sna_chapter/index.html#abstract",
    "href": "publications/sna_chapter/index.html#abstract",
    "title": "Social Network Analysis",
    "section": "Abstract",
    "text": "Abstract\nSocial networks comprise a set of nodes or actors (representing, e.g., individuals, groups, organisations) that are pairwise connected by edges or ties (representing, e.g., relationships, interactions, communication). The social systems arising exhibit patterns of interest, and social network analysis is the study of how and why these patterns emerge, sustain, and evolve. Of primary interest is thus to understand and describe the social processes that support the observed structure. These processes are founded in theories about network representation and theories about observed social phenomena. The benefit from network conceptualisation is thus obtained by outlining the association and distinction between these theories. This entry serves as an introduction to fundamental network concepts and analytical approaches, their potential for studying social phenomena, and a description of why they are central to theoretical constructs. This entry also provides a short introduction to statistical network modelling for cross-sectional and longitudinal network data.\nSAGE Research Methods Foundations"
  },
  {
    "objectID": "publications/multigraph_approach/index.html",
    "href": "publications/multigraph_approach/index.html",
    "title": "A Multigraph Approach to Social Network Analysis",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/multigraph_approach/index.html#abstract",
    "href": "publications/multigraph_approach/index.html#abstract",
    "title": "A Multigraph Approach to Social Network Analysis",
    "section": "Abstract",
    "text": "Abstract\nMultigraphs are graphs where multiple edges and edge loops are permitted. The main purpose of this article is to show the versatility of a multigraph approach when analysing social networks. Multigraph data structures are described and it is exemplified how they naturally occur in many contexts but also how they can be constructed by different kinds of aggregation in graphs. Special attention is given to a random multigraph model based on independent edge assignments to sites of vertex pairs and some useful measures of the local and global structure under this model are presented. Further, it is shown how some general measures of simplicity and complexity of multigraphs are easily handled under the present model.\n‚ÄòJournal of Social Structure 16, 1-22‚Äô"
  },
  {
    "objectID": "publications/ergm_framework_arch/index.html",
    "href": "publications/ergm_framework_arch/index.html",
    "title": "A Framework for Reconstructing Archaeological Networks using Exponential Random Graph Models",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/ergm_framework_arch/index.html#abstract",
    "href": "publications/ergm_framework_arch/index.html#abstract",
    "title": "A Framework for Reconstructing Archaeological Networks using Exponential Random Graph Models",
    "section": "Abstract",
    "text": "Abstract\nReconstructing ties between archaeological contexts may contribute to explain and describe a variety of past social phenomena. Several models have been formulated to infer the structure of such archaeological networks. The underlying propositions about mechanisms regulating the formation of ties in the past are often articulated on a dyadic basis and therefore rarely account for dependencies among ties. Here, we present a general framework in which we combine exponential random graph models with archaeological substantiations of mechanisms that may be responsible for network formation to account for tie dependence. We use data collected over a set of sites in the Caribbean during the period AD 100 - 400 to illustrate the steps to obtain a network reconstruction.\nJournal of Archaeological Method and Theory 27, 192-219"
  },
  {
    "objectID": "talks/WiNS2022/index.html",
    "href": "talks/WiNS2022/index.html",
    "title": "‚ÄúStatistical Entropy Analysis of Network Data‚Äù",
    "section": "",
    "text": "Slides\n\n\nExamples\n\n\nVideo"
  },
  {
    "objectID": "talks/Inaugural-KN/index.html",
    "href": "talks/Inaugural-KN/index.html",
    "title": "Statistical Analysis & Modeling of Multivariate Networks",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "talks/CMB2022/index.html",
    "href": "talks/CMB2022/index.html",
    "title": "‚ÄúAnalyzing Social Structure using Multigraph Representations‚Äù",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "talks/EUSN2021/index.html",
    "href": "talks/EUSN2021/index.html",
    "title": "Multiplexity Analysis of Networks using Multigraph Representations",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "project/movienetworks/index.html",
    "href": "project/movienetworks/index.html",
    "title": "Gender Dependent Structures in Charachter Networks",
    "section": "",
    "text": "Since 2019, I‚Äôm sporadically working on a project with Pete Jones which aims to apply existing and novel quantitative methods (e.g.¬†relational event models, entropy based natural language processing and multigraph representations), to study the gendered inequalities in popular cinema. See Pete‚Äôs website for more information and some awesome packages such as charinet which he has developed on the topic."
  },
  {
    "objectID": "project/movienetworks/index.html#project-summary",
    "href": "project/movienetworks/index.html#project-summary",
    "title": "Gender Dependent Structures in Charachter Networks",
    "section": "",
    "text": "Since 2019, I‚Äôm sporadically working on a project with Pete Jones which aims to apply existing and novel quantitative methods (e.g.¬†relational event models, entropy based natural language processing and multigraph representations), to study the gendered inequalities in popular cinema. See Pete‚Äôs website for more information and some awesome packages such as charinet which he has developed on the topic."
  },
  {
    "objectID": "project/nexus/index.html",
    "href": "project/nexus/index.html",
    "title": "NEXUS1492 - Reconstructing Archaeological Networks",
    "section": "",
    "text": "Between 2013-2018, I worked in international ERC-Synergy research project NEXUS1492 investigates the impacts of colonial encounters in the Caribbean through the reconstruction and modelling of archaeological networks.\nYou can read more about the project and its output here."
  },
  {
    "objectID": "project/nexus/index.html#project-summary",
    "href": "project/nexus/index.html#project-summary",
    "title": "NEXUS1492 - Reconstructing Archaeological Networks",
    "section": "",
    "text": "Between 2013-2018, I worked in international ERC-Synergy research project NEXUS1492 investigates the impacts of colonial encounters in the Caribbean through the reconstruction and modelling of archaeological networks.\nYou can read more about the project and its output here."
  },
  {
    "objectID": "project/nexus/index.html#references",
    "href": "project/nexus/index.html#references",
    "title": "NEXUS1492 - Reconstructing Archaeological Networks",
    "section": "References",
    "text": "References\n\nShafie T., Schoch D., Mans J., Hofman C., Brandes U., (2017). Hypergraph Representations: A Study of Carib Attacks on Colonial Forces, 1509-1700. Journal of Historical Network Research,1(1), 52-70. Link\nLaffoon, J.E., Sonnemann, T.F., Shafie, T., Hofman, C.L., Brandes, U. and Davies, G.R., (2017). Investigating human geographic origins using dual-isotope (87Sr/86Sr, Œ¥18O) assignment approaches. PloS one, 12(2), p.e0172562. Link\nAmati, V., Shafie, T., Brandes U., (2018) Reconstructing Archaeological Networks with Structural Holes. Journal of Archaeological Method and Theory volume 25, 226‚Äì253. Link\nAmati, V., Mol, A., Shafie, T., Hofman, C., Brandes U., (2020). A Framework for Reconstructing Archaeological Networks Using Exponential Random Graph Models. Journal of Archaeological Method and Theory volume 27, 192‚Äì219. Link"
  },
  {
    "objectID": "teaching/tidyverse/material/bechdel.html",
    "href": "teaching/tidyverse/material/bechdel.html",
    "title": "Bechdel",
    "section": "",
    "text": "In this mini analysis we work with the data used in the FiveThirtyEight story titled ‚ÄúThe Dollar-And-Cents Case Against Hollywood‚Äôs Exclusion of Women‚Äù. We will together fill in the blanks denoted by ___."
  },
  {
    "objectID": "teaching/tidyverse/material/bechdel.html#data-and-packages",
    "href": "teaching/tidyverse/material/bechdel.html#data-and-packages",
    "title": "Bechdel",
    "section": "Data and packages",
    "text": "Data and packages\nWe start with loading the packages we‚Äôll use.\n\nlibrary(fivethirtyeight)\nlibrary(tidyverse)\n\nThe dataset contains information on 1794 movies released between 1970 and 2013. However we‚Äôll focus our analysis on movies released between 1990 and 2013.\n\nbechdel90_13 &lt;- bechdel %&gt;% \n  filter(between(year, 1990, 2013))\n\nThere are ___ such movies.\nThe financial variables we‚Äôll focus on are the following:\n\nbudget_2013: Budget in 2013 inflation adjusted dollars\ndomgross_2013: Domestic gross (US) in 2013 inflation adjusted dollars\nintgross_2013: Total International (i.e., worldwide) gross in 2013 inflation adjusted dollars\n\nAnd we‚Äôll also use the binary and clean_test variables for grouping."
  },
  {
    "objectID": "teaching/tidyverse/material/bechdel.html#analysis",
    "href": "teaching/tidyverse/material/bechdel.html#analysis",
    "title": "Bechdel",
    "section": "Analysis",
    "text": "Analysis\nLet‚Äôs take a look at how median budget and gross vary by whether the movie passed the Bechdel test, which is stored in the binary variable.\n\nbechdel90_13 %&gt;%\n  group_by(binary) %&gt;%\n  summarise(\n    med_budget = median(budget_2013),\n    med_domgross = median(domgross_2013, na.rm = TRUE),\n    med_intgross = median(intgross_2013, na.rm = TRUE)\n    )\n\n# A tibble: 2 √ó 4\n  binary med_budget med_domgross med_intgross\n  &lt;chr&gt;       &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 FAIL    48385984.    57318606.    104475669\n2 PASS    31070724     45330446.     80124349\n\n\nNext, let‚Äôs take a look at how median budget and gross vary by a more detailed indicator of the Bechdel test result. This information is stored in the clean_test variable, which takes on the following values:\n\nok = passes test\ndubious\nmen = women only talk about men\nnotalk = women don‚Äôt talk to each other\nnowomen = fewer than two women\n\n\nbechdel90_13 %&gt;%\n  #group_by(___) %&gt;%\n  summarise(\n    med_budget = median(budget_2013),\n    med_domgross = median(domgross_2013, na.rm = TRUE),\n    med_intgross = median(intgross_2013, na.rm = TRUE)\n    )\n\n# A tibble: 1 √ó 3\n  med_budget med_domgross med_intgross\n       &lt;int&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1   37878971     52270207     93523336\n\n\nIn order to evaluate how return on investment varies among movies that pass and fail the Bechdel test, we‚Äôll first create a new variable called roi as the ratio of the gross to budget.\n\nbechdel90_13 &lt;- bechdel90_13 %&gt;%\n  mutate(roi = (intgross_2013 + domgross_2013) / budget_2013)\n\nLet‚Äôs see which movies have the highest return on investment.\n\nbechdel90_13 %&gt;%\n  arrange(desc(roi)) %&gt;% \n  select(title, roi, year)\n\n# A tibble: 1,615 √ó 3\n   title                     roi  year\n   &lt;chr&gt;                   &lt;dbl&gt; &lt;int&gt;\n 1 Paranormal Activity      671.  2007\n 2 The Blair Witch Project  648.  1999\n 3 El Mariachi              583.  1992\n 4 Clerks.                  258.  1994\n 5 In the Company of Men    231.  1997\n 6 Napoleon Dynamite        227.  2004\n 7 Once                     190.  2006\n 8 The Devil Inside         155.  2012\n 9 Primer                   142.  2004\n10 Fireproof                134.  2008\n# ‚Ñπ 1,605 more rows\n\n\nBelow is a visualization of the return on investment by test result, however it‚Äôs difficult to see the distributions due to a few extreme observations.\n\nggplot(data = bechdel90_13, \n       mapping = aes(x = clean_test, y = roi, color = binary)) +\n  geom_boxplot() +\n  labs(\n    title = \"Return on investment vs. Bechdel test result\",\n    x = \"Detailed Bechdel result\",\n    y = \"___\",\n    color = \"Binary Bechdel result\"\n    )\n\n\n\n\n\n\n\n\nWhat are those movies with very high returns on investment?\n\nbechdel90_13 %&gt;%\n  filter(roi &gt; 400) %&gt;%\n  select(title, budget_2013, domgross_2013, year)\n\n# A tibble: 3 √ó 4\n  title                   budget_2013 domgross_2013  year\n  &lt;chr&gt;                         &lt;int&gt;         &lt;dbl&gt; &lt;int&gt;\n1 Paranormal Activity          505595     121251476  2007\n2 The Blair Witch Project      839077     196538593  1999\n3 El Mariachi                   11622       3388636  1992\n\n\nZooming in on the movies with roi &lt; ___ provides a better view of how the medians across the categories compare:\n\nggplot(data = bechdel90_13, mapping = aes(x = clean_test, y = roi, color = binary)) +\n  geom_boxplot() +\n  labs(\n    title = \"Return on investment vs. Bechdel test result\",\n    subtitle = \"___\", # Something about zooming in to a certain level\n    x = \"Detailed Bechdel result\",\n    y = \"Return on investment\",\n    color = \"Binary Bechdel result\"\n    ) +\n  coord_cartesian(ylim = c(0, 15))"
  },
  {
    "objectID": "teaching/tidyverse/material/sales-excel.html",
    "href": "teaching/tidyverse/material/sales-excel.html",
    "title": "Sales",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl)\n\n\nRead in the Excel file called sales.xlsx from the data-raw/ folder such that it looks like the following.\n\n\n\n\n\n\n\n\n\n\n\nStretch goal: Manipulate the sales data such such that it looks like the following."
  },
  {
    "objectID": "teaching/tidyverse/material/data-type-class-exercises.html",
    "href": "teaching/tidyverse/material/data-type-class-exercises.html",
    "title": "Data Type and Data Classes: Exercises",
    "section": "",
    "text": "Double check that you do not have stored objects in your current session with the following command. This will list all objects that you have in your current R session.\n\nls()\n\n[1] \"pandoc_dir\"      \"quarto_bin_path\"\n\n\nIn case you have objects that you want to remove from the current session you can do so with the rm() function.  This command will remove all objects available in your current environment.\n\nrm(list = ls())\n\nThis command uses commands that we have not talked about yet. If you do not understand how it works now, you will do so after tomorrows lectures and exercises.\n\nCreate variables var1 and var2 and initialize them with two integers of choice.\nAdd the two variables and save them as a new variable named var3 and print the result.\nCheck the class, mode, and type for var1, var2, var3 and œÄ (is found under the variable name pi in R)\nCreate two character variables containing a text of choice. Check the mode, class, and type of the first one.\n\nAdd var1 to it. What is the result and why?\n\n\n\n\nConvert var3 to an integer, cast an integer variable to double, cast a string to a double.\nReport floor and ceiling of œÄ and round œÄ to 3 decimal places.\nIs floor of œÄ an integer?\nTreat \"3.56437\" string as number.\nDivide ‚àû by - ‚àû\nPrint a truth table for OR (for three distinct logical values). Read about truth tables here.\nMultiply a logical TRUE by a logical FALSE. Rise the logical true to the 7-th power.\nCreate two character variables containing two verses of a chosen song.\n\n\nConcatenate the two variables,\n\nPaste the variables with ‚Äò*‚Äô as separator.\n\nFind if ‚Äòand‚Äô occurs in the second line,\n\nSubstitute a word for another,\n\nExtract substring starting at the 5th character and 5 characters long."
  },
  {
    "objectID": "teaching/tidyverse/material/data-type-class-exercises.html#exercise",
    "href": "teaching/tidyverse/material/data-type-class-exercises.html#exercise",
    "title": "Data Type and Data Classes: Exercises",
    "section": "",
    "text": "Double check that you do not have stored objects in your current session with the following command. This will list all objects that you have in your current R session.\n\nls()\n\n[1] \"pandoc_dir\"      \"quarto_bin_path\"\n\n\nIn case you have objects that you want to remove from the current session you can do so with the rm() function.  This command will remove all objects available in your current environment.\n\nrm(list = ls())\n\nThis command uses commands that we have not talked about yet. If you do not understand how it works now, you will do so after tomorrows lectures and exercises.\n\nCreate variables var1 and var2 and initialize them with two integers of choice.\nAdd the two variables and save them as a new variable named var3 and print the result.\nCheck the class, mode, and type for var1, var2, var3 and œÄ (is found under the variable name pi in R)\nCreate two character variables containing a text of choice. Check the mode, class, and type of the first one.\n\nAdd var1 to it. What is the result and why?\n\n\n\n\nConvert var3 to an integer, cast an integer variable to double, cast a string to a double.\nReport floor and ceiling of œÄ and round œÄ to 3 decimal places.\nIs floor of œÄ an integer?\nTreat \"3.56437\" string as number.\nDivide ‚àû by - ‚àû\nPrint a truth table for OR (for three distinct logical values). Read about truth tables here.\nMultiply a logical TRUE by a logical FALSE. Rise the logical true to the 7-th power.\nCreate two character variables containing two verses of a chosen song.\n\n\nConcatenate the two variables,\n\nPaste the variables with ‚Äò*‚Äô as separator.\n\nFind if ‚Äòand‚Äô occurs in the second line,\n\nSubstitute a word for another,\n\nExtract substring starting at the 5th character and 5 characters long."
  },
  {
    "objectID": "teaching/tidyverse/material/hotels-forcats.html",
    "href": "teaching/tidyverse/material/hotels-forcats.html",
    "title": "Hotel bookings - factors",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)\n\nLoad the hotels data set we used in a previous practical. Render and view the following visualisation. How are the months ordered? What would be a better order? Then, reorder the months on the x-axis (levels of arrival_date_month) in a way that makes more sense. You will want to use a function from the forcats package, see https://forcats.tidyverse.org/reference/index.html for inspiration and help.\nStretch goal: If you finish the above task before time is up, change the y-axis label so the values are shown with dollar signs, e.g.¬†$80 instead of 80. You will want to use a function from the scales package, see https://scales.r-lib.org/reference/index.html for inspiration and help.\n\nhotels %&gt;%\n  group_by(hotel, arrival_date_month) %&gt;%   # group by hotel type and arrival month\n  summarise(mean_adr = mean(adr)) %&gt;%       # calculate mean adr for each group\n  ggplot(aes(\n    x = arrival_date_month,                 # x-axis = arrival_date_month\n    y = mean_adr,                           # y-axis = mean_adr calculated above\n    group = hotel,                          # group lines by hotel type\n    color = hotel)                          # and color by hotel type\n    ) +\n  geom_line() +                             # use lines to represent data\n  theme_minimal() +                         # use a minimal theme\n  labs(\n    x = \"Arrival month\",                 # customize labels\n    y = \"Mean ADR (average daily rate)\",\n    title = \"Comparison of resort and city hotel prices across months\",\n    subtitle = \"Resort hotel prices soar in the summer while ciry hotel prices remain relatively constant throughout the year\",\n    color = \"Hotel type\"\n    )"
  },
  {
    "objectID": "teaching/tidyverse/material/legos.html",
    "href": "teaching/tidyverse/material/legos.html",
    "title": "Legos",
    "section": "",
    "text": "Here, we work with (simulated) data from Lego sales in 2018 for a sample of customers who bought Legos in the US."
  },
  {
    "objectID": "teaching/tidyverse/material/legos.html#data-and-packages",
    "href": "teaching/tidyverse/material/legos.html#data-and-packages",
    "title": "Legos",
    "section": "Data and Packages",
    "text": "Data and Packages\nWe‚Äôll use the tidyverse package for much of the data wrangling and visualisation and the data is given to import.\n\nlibrary(tidyverse)\n\nThe following variables are available in the data set:\n\nfirst_name: First name of customer\nlast_name: Last name of customer\nage: Age of customer\nphone_number: Phone number of customer\nset_id: Set ID of lego set purchased\nnumber: Item number of lego set purchased\ntheme: Theme of lego set purchased\nsubtheme: Sub theme of lego set purchased\nyear: Year of purchase\nname: Name of lego set purchased\npieces: Number of pieces of legos in set purchased\nus_price: Price of set purchase in US Dollars\nimage_url: Image URL of lego set purchased\nquantity: Quantity of lego set(s) purchased"
  },
  {
    "objectID": "teaching/tidyverse/material/starwars.html",
    "href": "teaching/tidyverse/material/starwars.html",
    "title": "Visualizing Starwars characters",
    "section": "",
    "text": "Glimpse at the starwars data frame.\n\n\nglimpse(starwars)\n\nRows: 87\nColumns: 14\n$ name       &lt;chr&gt; \"Luke Skywalker\", \"C-3PO\", \"R2-D2\", \"Darth Vader\", \"Leia Or‚Ä¶\n$ height     &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 2‚Ä¶\n$ mass       &lt;dbl&gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77.‚Ä¶\n$ hair_color &lt;chr&gt; \"blond\", NA, NA, \"none\", \"brown\", \"brown, grey\", \"brown\", N‚Ä¶\n$ skin_color &lt;chr&gt; \"fair\", \"gold\", \"white, blue\", \"white\", \"light\", \"light\", \"‚Ä¶\n$ eye_color  &lt;chr&gt; \"blue\", \"yellow\", \"red\", \"yellow\", \"brown\", \"blue\", \"blue\",‚Ä¶\n$ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57.0, ‚Ä¶\n$ sex        &lt;chr&gt; \"male\", \"none\", \"none\", \"male\", \"female\", \"male\", \"female\",‚Ä¶\n$ gender     &lt;chr&gt; \"masculine\", \"masculine\", \"masculine\", \"masculine\", \"femini‚Ä¶\n$ homeworld  &lt;chr&gt; \"Tatooine\", \"Tatooine\", \"Naboo\", \"Tatooine\", \"Alderaan\", \"T‚Ä¶\n$ species    &lt;chr&gt; \"Human\", \"Droid\", \"Droid\", \"Human\", \"Human\", \"Human\", \"Huma‚Ä¶\n$ films      &lt;list&gt; &lt;\"A New Hope\", \"The Empire Strikes Back\", \"Return of the J‚Ä¶\n$ vehicles   &lt;list&gt; &lt;\"Snowspeeder\", \"Imperial Speeder Bike\"&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, \"Imp‚Ä¶\n$ starships  &lt;list&gt; &lt;\"X-wing\", \"Imperial shuttle\"&gt;, &lt;&gt;, &lt;&gt;, \"TIE Advanced x1\",‚Ä¶\n\n\n\nModify the following plot to change the color of all points to \"pink\".\n\n\nggplot(starwars, \n       aes(x = height, y = mass, color = gender, size = birth_year)) +\n  geom_point(color = \"pink\")\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nAdd labels for title, x and y axes, and size of points. Uncomment to see the effect.\n\n\nggplot(starwars, \n       aes(x = height, y = mass, color = gender, size = birth_year)) +\n  geom_point(color = \"#30509C\") +\n  labs(\n    #title = \"___\",\n    #x = \"___\", \n    #y = \"___\",\n    #___\n    )\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nPick a single numerical variable and make a histogram of it. Select a reasonable binwidth for it.\n\n(A little bit of starter code is provided below, and the code chunk is set to not be evaluated with eval: false because the current code in there is not valid code and hence the document wouldn‚Äôt knit. Once you replace the code with valid code, set the chunk option to eval: true, or remove the eval option altogether since it‚Äôs set to true by default.)\n\nggplot(starwars, aes(___)) +\n  geom___\n\n\nPick a numerical variable and a categorical variable and make a visualization (you pick the type!) to visualization the relationship between the two variables. Along with your code and output, provide an interpretation of the visualization.\n\nInterpretation goes here‚Ä¶\n\nPick a single categorical variable from the data set and make a bar plot of its distribution.\n\n\nPick two categorical variables and make a visualization to visualize the relationship between the two variables. Along with your code and output, provide an interpretation of the visualization.\n\nInterpretation goes here‚Ä¶\n\nPick two numerical variables and two categorical variables and make a visualization that incorporates all of them and provide an interpretation with your answer.\n\n(This time no starter code is provided, you‚Äôre on your own!)\nInterpretation goes here‚Ä¶"
  },
  {
    "objectID": "teaching/tidyverse/material/type-coercion.html",
    "href": "teaching/tidyverse/material/type-coercion.html",
    "title": "Type coercion",
    "section": "",
    "text": "c(1, 1L, \"C\")\n\n\nc(1, 1L, \"C\")\n\n[1] \"1\" \"1\" \"C\"\n\n\n\n1\n\n[1] 1\n\n1L\n\n[1] 1\n\n\"C\"\n\n[1] \"C\"\n\n\n\n#typeof(c(1, 1L, \"C\"))\n\n\nc(1L / 0, \"A\")\n\n\nc(1L / 0, \"A\")\n\n[1] \"Inf\" \"A\"  \n\n\n\ntypeof(1L)\n\n[1] \"integer\"\n\ntypeof(0)\n\n[1] \"double\"\n\ntypeof(1L/0)\n\n[1] \"double\"\n\ntypeof(\"A\")\n\n[1] \"character\"\n\n\n\n#typeof(c(1L / 0, \"A\"))\n\n\nc(1:3, 5)\n\n\nc(1:3, 5)\n\n[1] 1 2 3 5\n\n\n\ntypeof(1:3)\n\n[1] \"integer\"\n\ntypeof(5)\n\n[1] \"double\"\n\n\n\n#typeof(c(1:3, 5))\n\n\nc(3, \"3+\")\n\n\nc(3, \"3+\")\n\n[1] \"3\"  \"3+\"\n\n\n\ntypeof(3)\n\n[1] \"double\"\n\ntypeof(\"3+\")\n\n[1] \"character\"\n\n\n\n#typeof(c(3, \"3+\"))\n\n\nc(NA, TRUE)\n\n\nc(NA, TRUE)\n\n[1]   NA TRUE\n\n\n\ntypeof(NA)\n\n[1] \"logical\"\n\ntypeof(TRUE)\n\n[1] \"logical\"\n\n\n\n#typeof(c(NA, TRUE))"
  },
  {
    "objectID": "teaching/tidyverse/material/la-quinta.html",
    "href": "teaching/tidyverse/material/la-quinta.html",
    "title": "La Quinta is Spanish for next to Denny‚Äôs",
    "section": "",
    "text": "Have you ever taken a road trip in the US and thought to yourself ‚ÄúI wonder what La Quinta means‚Äù. Well, the late comedian Mitch Hedberg thinks it‚Äôs Spanish for next to Denny‚Äôs.\nIf you‚Äôre not familiar with these two establishments, Denny‚Äôs is a casual diner chain that is open 24 hours and La Quinta Inn and Suites is a hotel chain.\nThese two establishments tend to be clustered together, or at least this observation is a joke made famous by Mitch Hedberg. In this lab we explore the validity of this joke and along the way learn some more data wrangling and tips for visualizing spatial data.\nThe inspiration for this comes from a blog post by John Reiser on his new jersey geographer blog. You can read that analysis here. Reiser‚Äôs blog post focuses on scraping data from Denny‚Äôs and La Quinta Inn and Suites websites using Python. Here, we focus on visualization and analysis of these data."
  },
  {
    "objectID": "teaching/tidyverse/material/la-quinta.html#packages",
    "href": "teaching/tidyverse/material/la-quinta.html#packages",
    "title": "La Quinta is Spanish for next to Denny‚Äôs",
    "section": "Packages",
    "text": "Packages\nWe‚Äôll use the tidyverse package for much of the data wrangling and visualisation and the data lives in the dsbox package. These packages are already installed for you. You can load them by running the following in your Console:\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "teaching/tidyverse/material/la-quinta.html#data",
    "href": "teaching/tidyverse/material/la-quinta.html#data",
    "title": "La Quinta is Spanish for next to Denny‚Äôs",
    "section": "Data",
    "text": "Data\nThe data sets we‚Äôll use are called dennys and laquinta and are available for download. Note that these data were scraped from here and here, respectively. You can find information about the data sets here and here. To help with our analysis we will also use a data set on US states.\n\nlaquinta &lt;- read_csv(\"data/laquinta.csv\")\ndennys &lt;- read_csv(\"data/dennys.csv\")\nstates &lt;- read_csv(\"data/states.csv\")\n\nEach observation in the states dataset represents a state, including DC. Along with the name of the state we have the two-letter abbreviation and we have the geographic area of the state (in square miles)."
  },
  {
    "objectID": "teaching/tidyverse/material/college-majors.html",
    "href": "teaching/tidyverse/material/college-majors.html",
    "title": "What should I major in?",
    "section": "",
    "text": "The first step in the process of turning information into knowledge process is to summarize and describe the raw information - the data. In this assignment we explore data on college majors and earnings, specifically the data begin the FiveThirtyEight story ‚ÄúThe Economic Guide To Picking A College Major‚Äù.\nThese data originally come from the American Community Survey (ACS) 2010-2012 Public Use Microdata Series. While this is outside the scope of this assignment, if you are curious about how raw data from the ACS were cleaned and prepared, see the code FiveThirtyEight authors used.\nWe should also note that there are many considerations that go into picking a major. Earnings potential and employment prospects are two of them, and they are important, but they don‚Äôt tell the whole story. Keep this in mind as you analyze the data."
  },
  {
    "objectID": "teaching/tidyverse/material/college-majors.html#packages",
    "href": "teaching/tidyverse/material/college-majors.html#packages",
    "title": "What should I major in?",
    "section": "Packages",
    "text": "Packages\nWe‚Äôll use the tidyverse package for much of the data wrangling and visualisation, the scales package for better formatting of labels on visualisations, and the data lives in the fivethirtyeight package. These packages are already installed for you. You can load them by running the following in your Console:\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(fivethirtyeight)"
  },
  {
    "objectID": "teaching/tidyverse/material/college-majors.html#data",
    "href": "teaching/tidyverse/material/college-majors.html#data",
    "title": "What should I major in?",
    "section": "Data",
    "text": "Data\nThe data can be found in the fivethirtyeight package, and it‚Äôs called college_recent_grads. Since the dataset is distributed with the package, we don‚Äôt need to load it separately; it becomes available to us when we load the package. You can find out more about the dataset by inspecting its documentation, which you can access by running ?college_recent_grads in the Console or using the Help menu in RStudio to search for college_recent_grads. You can also find this information here.\nYou can also take a quick peek at your data frame and view its dimensions with the glimpse function.\n\nglimpse(college_recent_grads)\n\nRows: 173\nColumns: 21\n$ rank                        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,‚Ä¶\n$ major_code                  &lt;int&gt; 2419, 2416, 2415, 2417, 2405, 2418, 6202, ‚Ä¶\n$ major                       &lt;chr&gt; \"Petroleum Engineering\", \"Mining And Miner‚Ä¶\n$ major_category              &lt;chr&gt; \"Engineering\", \"Engineering\", \"Engineering‚Ä¶\n$ total                       &lt;int&gt; 2339, 756, 856, 1258, 32260, 2573, 3777, 1‚Ä¶\n$ sample_size                 &lt;int&gt; 36, 7, 3, 16, 289, 17, 51, 10, 1029, 631, ‚Ä¶\n$ men                         &lt;int&gt; 2057, 679, 725, 1123, 21239, 2200, 2110, 8‚Ä¶\n$ women                       &lt;int&gt; 282, 77, 131, 135, 11021, 373, 1667, 960, ‚Ä¶\n$ sharewomen                  &lt;dbl&gt; 0.1205643, 0.1018519, 0.1530374, 0.1073132‚Ä¶\n$ employed                    &lt;int&gt; 1976, 640, 648, 758, 25694, 1857, 2912, 15‚Ä¶\n$ employed_fulltime           &lt;int&gt; 1849, 556, 558, 1069, 23170, 2038, 2924, 1‚Ä¶\n$ employed_parttime           &lt;int&gt; 270, 170, 133, 150, 5180, 264, 296, 553, 1‚Ä¶\n$ employed_fulltime_yearround &lt;int&gt; 1207, 388, 340, 692, 16697, 1449, 2482, 82‚Ä¶\n$ unemployed                  &lt;int&gt; 37, 85, 16, 40, 1672, 400, 308, 33, 4650, ‚Ä¶\n$ unemployment_rate           &lt;dbl&gt; 0.018380527, 0.117241379, 0.024096386, 0.0‚Ä¶\n$ p25th                       &lt;dbl&gt; 95000, 55000, 50000, 43000, 50000, 50000, ‚Ä¶\n$ median                      &lt;dbl&gt; 110000, 75000, 73000, 70000, 65000, 65000,‚Ä¶\n$ p75th                       &lt;dbl&gt; 125000, 90000, 105000, 80000, 75000, 10200‚Ä¶\n$ college_jobs                &lt;int&gt; 1534, 350, 456, 529, 18314, 1142, 1768, 97‚Ä¶\n$ non_college_jobs            &lt;int&gt; 364, 257, 176, 102, 4440, 657, 314, 500, 1‚Ä¶\n$ low_wage_jobs               &lt;int&gt; 193, 50, 0, 0, 972, 244, 259, 220, 3253, 3‚Ä¶\n\n\nThe college_recent_grads data frame is a trove of information. Let‚Äôs think about some questions we might want to answer with these data:\n\nWhich major has the lowest unemployment rate?\nWhich major has the highest percentage of women?\nHow do the distributions of median income compare across major categories?\nDo women tend to choose majors with lower or higher earnings?\n\nIn the next section we aim to answer these questions."
  },
  {
    "objectID": "teaching/tidyverse/material/college-majors.html#which-major-has-the-lowest-unemployment-rate",
    "href": "teaching/tidyverse/material/college-majors.html#which-major-has-the-lowest-unemployment-rate",
    "title": "What should I major in?",
    "section": "Which major has the lowest unemployment rate?",
    "text": "Which major has the lowest unemployment rate?\nIn order to answer this question all we need to do is sort the data. We use the arrange function to do this, and sort it by the unemployment_rate variable. By default arrange sorts in ascending order, which is what we want here ‚Äì we‚Äôre interested in the major with the lowest unemployment rate.\n\ncollege_recent_grads %&gt;%\n  arrange(unemployment_rate)\n\n# A tibble: 173 √ó 21\n    rank major_code major           major_category total sample_size   men women\n   &lt;int&gt;      &lt;int&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;int&gt;       &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1    53       4005 Mathematics An‚Ä¶ Computers & M‚Ä¶   609           7   500   109\n 2    74       3801 Military Techn‚Ä¶ Industrial Ar‚Ä¶   124           4   124     0\n 3    84       3602 Botany          Biology & Lif‚Ä¶  1329           9   626   703\n 4   113       1106 Soil Science    Agriculture &‚Ä¶   685           4   476   209\n 5   121       2301 Educational Ad‚Ä¶ Education        804           5   280   524\n 6    15       2409 Engineering Me‚Ä¶ Engineering     4321          30  3526   795\n 7    20       3201 Court Reporting Law & Public ‚Ä¶  1148          14   877   271\n 8   120       2305 Mathematics Te‚Ä¶ Education      14237         123  3872 10365\n 9     1       2419 Petroleum Engi‚Ä¶ Engineering     2339          36  2057   282\n10    65       1100 General Agricu‚Ä¶ Agriculture &‚Ä¶ 10399         158  6053  4346\n# ‚Ñπ 163 more rows\n# ‚Ñπ 13 more variables: sharewomen &lt;dbl&gt;, employed &lt;int&gt;,\n#   employed_fulltime &lt;int&gt;, employed_parttime &lt;int&gt;,\n#   employed_fulltime_yearround &lt;int&gt;, unemployed &lt;int&gt;,\n#   unemployment_rate &lt;dbl&gt;, p25th &lt;dbl&gt;, median &lt;dbl&gt;, p75th &lt;dbl&gt;,\n#   college_jobs &lt;int&gt;, non_college_jobs &lt;int&gt;, low_wage_jobs &lt;int&gt;\n\n\nThis gives us what we wanted, but not in an ideal form. First, the name of the major barely fits on the page. Second, some of the variables are not that useful (e.g.¬†major_code, major_category) and some we might want front and center are not easily viewed (e.g.¬†unemployment_rate).\nWe can use the select function to choose which variables to display, and in which order:\n\ncollege_recent_grads %&gt;%\n  arrange(unemployment_rate) %&gt;%\n  select(rank, major, unemployment_rate)\n\n# A tibble: 173 √ó 3\n    rank major                                      unemployment_rate\n   &lt;int&gt; &lt;chr&gt;                                                  &lt;dbl&gt;\n 1    53 Mathematics And Computer Science                     0      \n 2    74 Military Technologies                                0      \n 3    84 Botany                                               0      \n 4   113 Soil Science                                         0      \n 5   121 Educational Administration And Supervision           0      \n 6    15 Engineering Mechanics Physics And Science            0.00633\n 7    20 Court Reporting                                      0.0117 \n 8   120 Mathematics Teacher Education                        0.0162 \n 9     1 Petroleum Engineering                                0.0184 \n10    65 General Agriculture                                  0.0196 \n# ‚Ñπ 163 more rows\n\n\nOk, this is looking better, but do we really need to display all those decimal places in the unemployment variable? Not really!\nWe can use the percent() function to clean up the display a bit.\n\ncollege_recent_grads %&gt;%\n  arrange(unemployment_rate) %&gt;%\n  select(rank, major, unemployment_rate) %&gt;%\n  mutate(unemployment_rate = percent(unemployment_rate))\n\n# A tibble: 173 √ó 3\n    rank major                                      unemployment_rate\n   &lt;int&gt; &lt;chr&gt;                                      &lt;chr&gt;            \n 1    53 Mathematics And Computer Science           0.00000%         \n 2    74 Military Technologies                      0.00000%         \n 3    84 Botany                                     0.00000%         \n 4   113 Soil Science                               0.00000%         \n 5   121 Educational Administration And Supervision 0.00000%         \n 6    15 Engineering Mechanics Physics And Science  0.63343%         \n 7    20 Court Reporting                            1.16897%         \n 8   120 Mathematics Teacher Education              1.62028%         \n 9     1 Petroleum Engineering                      1.83805%         \n10    65 General Agriculture                        1.96425%         \n# ‚Ñπ 163 more rows"
  },
  {
    "objectID": "teaching/tidyverse/material/college-majors.html#which-major-has-the-highest-percentage-of-women",
    "href": "teaching/tidyverse/material/college-majors.html#which-major-has-the-highest-percentage-of-women",
    "title": "What should I major in?",
    "section": "Which major has the highest percentage of women?",
    "text": "Which major has the highest percentage of women?\nTo answer such a question we need to arrange the data in descending order. For example, if earlier we were interested in the major with the highest unemployment rate, we would use the following:\n\nThe `desc` function specifies that we want `unemployment_rate` in descending order.\n\n\ncollege_recent_grads %&gt;%\n  arrange(desc(unemployment_rate)) %&gt;%\n  select(rank, major, unemployment_rate)\n\n# A tibble: 173 √ó 3\n    rank major                                      unemployment_rate\n   &lt;int&gt; &lt;chr&gt;                                                  &lt;dbl&gt;\n 1     6 Nuclear Engineering                                    0.177\n 2    90 Public Administration                                  0.159\n 3    85 Computer Networking And Telecommunications             0.152\n 4   171 Clinical Psychology                                    0.149\n 5    30 Public Policy                                          0.128\n 6   106 Communication Technologies                             0.120\n 7     2 Mining And Mineral Engineering                         0.117\n 8    54 Computer Programming And Data Processing               0.114\n 9    80 Geography                                              0.113\n10    59 Architecture                                           0.113\n# ‚Ñπ 163 more rows\n\n\n\nUsing what you‚Äôve learned so far, arrange the data in descending order with respect to proportion of women in a major, and display only the major, the total number of people with major, and proportion of women. Show only the top 3 majors by adding top_n(3) at the end of the pipeline."
  },
  {
    "objectID": "teaching/tidyverse/material/college-majors.html#how-do-the-distributions-of-median-income-compare-across-major-categories",
    "href": "teaching/tidyverse/material/college-majors.html#how-do-the-distributions-of-median-income-compare-across-major-categories",
    "title": "What should I major in?",
    "section": "How do the distributions of median income compare across major categories?",
    "text": "How do the distributions of median income compare across major categories?\nNote: A percentile is a measure used in statistics indicating the value below which a given percentage of observations in a group of observations fall. For example, the 20th percentile is the value below which 20% of the observations may be found. (Source: Wikipedia\nThere are three types of incomes reported in this data frame: p25th, median, and p75th. These correspond to the 25th, 50th, and 75th percentiles of the income distribution of sampled individuals for a given major.\n\nWhy do we often choose the median, rather than the mean, to describe the typical income of a group of people?\n\nThe question we want to answer ‚ÄúHow do the distributions of median income compare across major categories?‚Äù. We need to do a few things to answer this question: First, we need to group the data by major_category. Then, we need a way to summarize the distributions of median income within these groups. This decision will depend on the shapes of these distributions. So first, we need to visualize the data.\nWe use the ggplot() function to do this. The first argument is the data frame, and the next argument gives the mapping of the variables of the data to the aesthetic elements of the plot.\nLet‚Äôs start simple and take a look at the distribution of all median incomes, without considering the major categories.\n\nggplot(data = college_recent_grads, mapping = aes(x = median)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\nAlong with the plot, we get a message:\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nThis is telling us that we might want to reconsider the binwidth we chose for our histogram ‚Äì or more accurately, the binwidth we didn‚Äôt specify. It‚Äôs good practice to always think in the context of the data and try out a few binwidths before settling on a binwidth. You might ask yourself: ‚ÄúWhat would be a meaningful difference in median incomes?‚Äù $1 is obviously too little, $10000 might be too high.\n\nTry binwidths of $1000 and $5000 and choose one. Explain your reasoning for your choice. Note that the binwidth is an argument for the geom_histogram function. So to specify a binwidth of $1000, you would use geom_histogram(binwidth = 1000).\n\nWe can also calculate summary statistics for this distribution using the summarise function:\n\ncollege_recent_grads %&gt;%\n  summarise(min = min(median), max = max(median),\n            mean = mean(median), med = median(median),\n            sd = sd(median), \n            q1 = quantile(median, probs = 0.25),\n            q3 = quantile(median, probs = 0.75))\n\n# A tibble: 1 √ó 7\n    min    max   mean   med     sd    q1    q3\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 22000 110000 40151. 36000 11470. 33000 45000\n\n\n\nBased on the shape of the histogram you created in the previous exercise, determine which of these summary statistics is useful for describing the distribution. Write up your description (remember shape, center, spread, any unusual observations) and include the summary statistic output as well.\nPlot the distribution of median income using a histogram, faceted by major_category. Use the binwidth you chose in the earlier exercise.\n\nNow that we‚Äôve seen the shapes of the distributions of median incomes for each major category, we should have a better idea for which summary statistic to use to quantify the typical median income.\n\nWhich major category has the highest typical (you‚Äôll need to decide what this means) median income? Use the partial code below, filling it in with the appropriate statistic and function. Also note that we are looking for the highest statistic, so make sure to arrange in the correct direction.\n\n\ncollege_recent_grads %&gt;%\n  group_by(major_category) %&gt;%\n  summarise(___ = ___(median)) %&gt;%\n  arrange(___)\n\n\nWhich major category is the least popular in this sample? To answer this question we use a new function called count, which first groups the data and then counts the number of observations in each category (see below). Add to the pipeline appropriately to arrange the results so that the major with the lowest observations is on top.\n\n\ncollege_recent_grads %&gt;%\n  count(major_category)\n\n# A tibble: 16 √ó 2\n   major_category                          n\n   &lt;chr&gt;                               &lt;int&gt;\n 1 Agriculture & Natural Resources        10\n 2 Arts                                    8\n 3 Biology & Life Science                 14\n 4 Business                               13\n 5 Communications & Journalism             4\n 6 Computers & Mathematics                11\n 7 Education                              16\n 8 Engineering                            29\n 9 Health                                 12\n10 Humanities & Liberal Arts              15\n11 Industrial Arts & Consumer Services     7\n12 Interdisciplinary                       1\n13 Law & Public Policy                     5\n14 Physical Sciences                      10\n15 Psychology & Social Work                9\n16 Social Science                          9"
  },
  {
    "objectID": "teaching/tidyverse/material/college-majors.html#all-stem-fields-arent-the-same",
    "href": "teaching/tidyverse/material/college-majors.html#all-stem-fields-arent-the-same",
    "title": "What should I major in?",
    "section": "All STEM fields aren‚Äôt the same",
    "text": "All STEM fields aren‚Äôt the same\nOne of the sections of the FiveThirtyEight story is ‚ÄúAll STEM fields aren‚Äôt the same‚Äù. Let‚Äôs see if this is true.\nFirst, let‚Äôs create a new vector called stem_categories that lists the major categories that are considered STEM fields.\n\nstem_categories &lt;- c(\"Biology & Life Science\",\n                     \"Computers & Mathematics\",\n                     \"Engineering\",\n                     \"Physical Sciences\")\n\nThen, we can use this to create a new variable in our data frame indicating whether a major is STEM or not.\n\ncollege_recent_grads &lt;- college_recent_grads %&gt;%\n  mutate(major_type = ifelse(major_category %in% stem_categories, \"stem\", \"not stem\"))\n\nLet‚Äôs unpack this: with mutate we create a new variable called major_type, which is defined as \"stem\" if the major_category is in the vector called stem_categories we created earlier, and as \"not stem\" otherwise.\n%in% is a logical operator. Other logical operators that are commonly used are\n\n\n\nOperator\nOperation\n\n\n\n\nx &lt; y\nless than\n\n\nx &gt; y\ngreater than\n\n\nx &lt;= y\nless than or equal to\n\n\nx &gt;= y\ngreater than or equal to\n\n\nx != y\nnot equal to\n\n\nx == y\nequal to\n\n\nx %in% y\ncontains\n\n\nx | y\nor\n\n\nx & y\nand\n\n\n!x\nnot\n\n\n\nWe can use the logical operators to also filter our data for STEM majors whose median earnings is less than median for all majors‚Äô median earnings, which we found to be $36,000 earlier.\n\ncollege_recent_grads %&gt;%\n  filter(\n    major_type == \"stem\",\n    median &lt; 36000\n  )\n\n# A tibble: 10 √ó 22\n    rank major_code major        major_category  total sample_size    men  women\n   &lt;int&gt;      &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;           &lt;int&gt;       &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n 1    93       1301 Environment‚Ä¶ Biology & Lif‚Ä¶  25965         225  10787  15178\n 2    98       5098 Multi-Disci‚Ä¶ Physical Scie‚Ä¶  62052         427  27015  35037\n 3   102       3608 Physiology   Biology & Lif‚Ä¶  22060          99   8422  13638\n 4   106       2001 Communicati‚Ä¶ Computers & M‚Ä¶  18035         208  11431   6604\n 5   109       3611 Neuroscience Biology & Lif‚Ä¶  13663          53   4944   8719\n 6   111       5002 Atmospheric‚Ä¶ Physical Scie‚Ä¶   4043          32   2744   1299\n 7   123       3699 Miscellaneo‚Ä¶ Biology & Lif‚Ä¶  10706          63   4747   5959\n 8   124       3600 Biology      Biology & Lif‚Ä¶ 280709        1370 111762 168947\n 9   133       3604 Ecology      Biology & Lif‚Ä¶   9154          86   3878   5276\n10   169       3609 Zoology      Biology & Lif‚Ä¶   8409          47   3050   5359\n# ‚Ñπ 14 more variables: sharewomen &lt;dbl&gt;, employed &lt;int&gt;,\n#   employed_fulltime &lt;int&gt;, employed_parttime &lt;int&gt;,\n#   employed_fulltime_yearround &lt;int&gt;, unemployed &lt;int&gt;,\n#   unemployment_rate &lt;dbl&gt;, p25th &lt;dbl&gt;, median &lt;dbl&gt;, p75th &lt;dbl&gt;,\n#   college_jobs &lt;int&gt;, non_college_jobs &lt;int&gt;, low_wage_jobs &lt;int&gt;,\n#   major_type &lt;chr&gt;\n\n\n\nWhich STEM majors have median salaries equal to or less than the median for all majors‚Äô median earnings? Your output should only show the major name and median, 25th percentile, and 75th percentile earning for that major as and should be sorted such that the major with the highest median earning is on top."
  },
  {
    "objectID": "teaching/tidyverse/material/college-majors.html#what-types-of-majors-do-women-tend-to-major-in",
    "href": "teaching/tidyverse/material/college-majors.html#what-types-of-majors-do-women-tend-to-major-in",
    "title": "What should I major in?",
    "section": "What types of majors do women tend to major in?",
    "text": "What types of majors do women tend to major in?\n\nCreate a scatterplot of median income vs.¬†proportion of women in that major, coloured by whether the major is in a STEM field or not. Describe the association between these three variables."
  },
  {
    "objectID": "teaching/tidyverse/material/college-majors.html#further-exploration",
    "href": "teaching/tidyverse/material/college-majors.html#further-exploration",
    "title": "What should I major in?",
    "section": "Further exploration",
    "text": "Further exploration\n\nAsk a question of interest to you, and answer it using summary statistic(s) and/or visualization(s)."
  },
  {
    "objectID": "teaching/stat-learn/index.html",
    "href": "teaching/stat-learn/index.html",
    "title": "Statistical Learning",
    "section": "",
    "text": "Schedule\n\n\n\n\ndate\nslides\nhandout\npractical\ndata\n\n\n\n\n1: Introduction: What is Statistical Learning?\n22.10.2024\n\n\n\n.zip\n\n\n2: Linear Regression I\n29.10.2024\n\n\n\n\n\n\n3: Linear Regression II\n05.11.2024\n\n\n\n.zip\n\n\n4: Classification I\n12.11.2024\n\n\n\n\n\n\n5: Classification II\n19.11.2024\n\n\n \n\n\n\n6: Model Validation\n26.11.2024\n\n\n \n\n\n\n7: Model Selection & Regularization\n03.12.2024\n\n\n\n\n\n\n8: ‚ÄúNon-linear‚Äù Linear Regression\n10.12.2024\n\n\n \n\n\n\n9: Tree Based Methods\n17.12.2024"
  },
  {
    "objectID": "teaching/stat-learn/material/05-knn-reg-class.html",
    "href": "teaching/stat-learn/material/05-knn-reg-class.html",
    "title": "KNN classification + regression",
    "section": "",
    "text": "We are going to look at the probability version of the KNN classifier algorithm. We create a function called KNN_classifier‚Å† for performing KNN classification using the following arguments:\n\n\\(x_0\\) as the new point at which we wish to predict \\(y\\)\n\\({\\bf x} = (x_1,x_2, \\dots, x_n)\\) as the vector of training \\(x\\)‚Äôs, where \\(x_i\\) is real-valued\n\\({\\bf y} = (y_1,y_2, \\dots, y_n)\\) as the vector of training \\(y\\)‚Äôs, where \\(y_i\\) is 0 or 1\n\\(K\\) as number of neighbors to use\n\\(\\hat{p}_{1}\\) as the estimated probability of \\(y_0=1\\) given \\(x_0\\)\n\nThe function calculates the Euclidean distance between \\(x_0\\) and each of the \\(x_i\\)‚Äôs in the training set \\((x_1, x_2, \\dots, x_n)\\). Then we order them from nearest to furthest away and computes the fraction of \\(y\\) values of the \\(y\\) values of the \\(K\\) nearest training points that are equal to 1 and return this proportion as an estimated probability of \\(y_0=1\\). We can transform \\(\\hat{p}_{1}\\) to a prediction of the \\(y\\) value at \\(x_0\\) by using a threshold on \\(\\hat{p}_{1}\\) and return\n\nKNN_classifier = function(x0, x, y, K) {\n  distances = abs(x - x0)  \n  o = order(distances)  \n  p1_hat = mean(y[o[1:K]])  \n  return(p1_hat)  \n}\n\n\n\nWe simulate training vector \\(\\bf{x}\\) from a uniform distribution on the interval \\([0,5]\\) and the true probability \\(p_1(x)\\) of \\(y=1\\) given \\(x\\) (true relationship between \\(x\\) and \\(y\\)) according to \\[p_1(x) = \\frac{\\exp(2*\\cos(x))}{(1 + \\exp(2*\\cos(x)))}\\] We simulate simulate training \\(y\\)‚Äôs as Bernoulli random variables with probabilities \\(p_1(x)\\).\n\nset.seed(1)  # set random number generator\nn = 20 \nx = 5*runif(n)  \np1 = function(x) { exp(2*cos(x))/(1 + exp(2*cos(x))) }  \ny = rbinom(n,1,p1(x)) \n\n\n\n\nplot(x,y,col=2,pch=20,cex=2)  # plot training data\nx_grid = seq(from=0, to=5, by=0.01)  # grid of x values \nlines(x_grid,p1(x_grid))  # plot true p1(x) values for the grid\n\n\n\n\n\n\n\n\n\n\n\n\nNow we run the KNN_classifier function to predict \\(y\\) at each point on the grid of \\(x\\) values. For that we need to define \\(K\\), that is number of nearest neighbors to use. We start with setting it equal to 1 but this can be changed later as an exercise. Further, we predict the \\(y\\) values for each \\(x\\) in the grid by thresholding the estimated probabilities to \\(\\leq 0.5\\) and \\(&gt;0.5\\).\n\nK = 1\np1_grid_hat = sapply(x_grid, function(x0) { KNN_classifier(x0, x, y, K) })\ny_grid_hat = round(p1_grid_hat &gt; 0.5)  \n\nNext we add the predicted values to our plot:\n\nplot(x,y,col=2,pch=20,cex=2)  # plot training data\ntitle(paste(\"K =\",K))\nlines(x_grid,p1(x_grid))  # plot true p1(x) values \nlines(x_grid,p1_grid_hat,col=4)  # plot estimated probabilities of y=1 \nlines(x_grid,y_grid_hat,col=4)  # plot predicted y values for each x0\n\n\n\n\n\n\n\n\n\n\n\nThe training error rate is given by \\[\\textrm{training error} = \\frac{1}{n}\\sum_{i=1}^n  I(\\hat{y_i} \\neq y_i)\\] So we first run KNN classifier (probability version) at each \\(x\\) in the training set, then we predict the \\(y\\) values for each \\(x\\) in the training set (prediction version of KNN), and finally compute the compute the training error rate which is the on average misclassification rate.\n\np1_hat = sapply(x, function(x0) { KNN_classifier(x0, x, y, K) }) \ny_hat = round(p1_hat &gt; 0.5)  \ntrain_error = mean(y_hat != y)  \nprint(paste0(\"Training error rate (K = \",K,\") = \",train_error))\n\n[1] \"Training error rate (K = 1) = 0\"\n\n\nNow we compute the test error rate. We again simulate a large number of samples as test set, namely 10000. We simulate test \\(x\\)‚Äôs and test \\(y\\)‚Äôs and run the KNN classifier at each \\(x\\) in the test set. We then predict the \\(y\\) values for each \\(x\\) in the test set and compute the test error rate.\n\nn_test = 10000 \nx_test = 5*runif(n_test)  \ny_test = rbinom(n_test,1,p1(x_test))  \np1_test_hat = sapply(x_test, function(x0) { KNN_classifier(x0, x, y, K) })  \ny_test_hat = round(p1_test_hat &gt; 0.5) \ntest_error = mean(y_test_hat != y_test) \nprint(paste0(\"Test error rate (K = \",K,\"): \",test_error))\n\n[1] \"Test error rate (K = 1): 0.2869\"\n\n\nHow can we tell if the above is a good test error rate? We compute the test error rate for the Bayes optimal classifier.\n\n# Bayes optimal classifier\n# use the true p1(x) to make the best possible predictions on the training set\ny_hat_optimal = p1(x) &gt; 0.5  \n# compute the training error rate for the Bayes optimal classifier\ntrain_error_optimal = mean(y_hat_optimal != y)  \nprint(paste0(\"Training error rate (Optimal): \",train_error_optimal))\n\n[1] \"Training error rate (Optimal): 0.15\"\n\n# use the true p1(x) to make the best possible predictions on the test set\ny_test_hat_optimal = round(p1(x_test) &gt; 0.5)\n# compute the test error rate for the Bayes optimal classifier\ntest_error_optimal = mean(y_test_hat_optimal != y_test) \nprint(paste0(\"Test error rate (Optimal): \",test_error_optimal))\n\n[1] \"Test error rate (Optimal): 0.2495\""
  },
  {
    "objectID": "teaching/stat-learn/material/05-knn-reg-class.html#simulate-training-data",
    "href": "teaching/stat-learn/material/05-knn-reg-class.html#simulate-training-data",
    "title": "KNN classification + regression",
    "section": "",
    "text": "We simulate training vector \\(\\bf{x}\\) from a uniform distribution on the interval \\([0,5]\\) and the true probability \\(p_1(x)\\) of \\(y=1\\) given \\(x\\) (true relationship between \\(x\\) and \\(y\\)) according to \\[p_1(x) = \\frac{\\exp(2*\\cos(x))}{(1 + \\exp(2*\\cos(x)))}\\] We simulate simulate training \\(y\\)‚Äôs as Bernoulli random variables with probabilities \\(p_1(x)\\).\n\nset.seed(1)  # set random number generator\nn = 20 \nx = 5*runif(n)  \np1 = function(x) { exp(2*cos(x))/(1 + exp(2*cos(x))) }  \ny = rbinom(n,1,p1(x)) \n\n\n\n\nplot(x,y,col=2,pch=20,cex=2)  # plot training data\nx_grid = seq(from=0, to=5, by=0.01)  # grid of x values \nlines(x_grid,p1(x_grid))  # plot true p1(x) values for the grid"
  },
  {
    "objectID": "teaching/stat-learn/material/05-knn-reg-class.html#predicting-classes",
    "href": "teaching/stat-learn/material/05-knn-reg-class.html#predicting-classes",
    "title": "KNN classification + regression",
    "section": "",
    "text": "Now we run the KNN_classifier function to predict \\(y\\) at each point on the grid of \\(x\\) values. For that we need to define \\(K\\), that is number of nearest neighbors to use. We start with setting it equal to 1 but this can be changed later as an exercise. Further, we predict the \\(y\\) values for each \\(x\\) in the grid by thresholding the estimated probabilities to \\(\\leq 0.5\\) and \\(&gt;0.5\\).\n\nK = 1\np1_grid_hat = sapply(x_grid, function(x0) { KNN_classifier(x0, x, y, K) })\ny_grid_hat = round(p1_grid_hat &gt; 0.5)  \n\nNext we add the predicted values to our plot:\n\nplot(x,y,col=2,pch=20,cex=2)  # plot training data\ntitle(paste(\"K =\",K))\nlines(x_grid,p1(x_grid))  # plot true p1(x) values \nlines(x_grid,p1_grid_hat,col=4)  # plot estimated probabilities of y=1 \nlines(x_grid,y_grid_hat,col=4)  # plot predicted y values for each x0"
  },
  {
    "objectID": "teaching/stat-learn/material/05-knn-reg-class.html#error-rates",
    "href": "teaching/stat-learn/material/05-knn-reg-class.html#error-rates",
    "title": "KNN classification + regression",
    "section": "",
    "text": "The training error rate is given by \\[\\textrm{training error} = \\frac{1}{n}\\sum_{i=1}^n  I(\\hat{y_i} \\neq y_i)\\] So we first run KNN classifier (probability version) at each \\(x\\) in the training set, then we predict the \\(y\\) values for each \\(x\\) in the training set (prediction version of KNN), and finally compute the compute the training error rate which is the on average misclassification rate.\n\np1_hat = sapply(x, function(x0) { KNN_classifier(x0, x, y, K) }) \ny_hat = round(p1_hat &gt; 0.5)  \ntrain_error = mean(y_hat != y)  \nprint(paste0(\"Training error rate (K = \",K,\") = \",train_error))\n\n[1] \"Training error rate (K = 1) = 0\"\n\n\nNow we compute the test error rate. We again simulate a large number of samples as test set, namely 10000. We simulate test \\(x\\)‚Äôs and test \\(y\\)‚Äôs and run the KNN classifier at each \\(x\\) in the test set. We then predict the \\(y\\) values for each \\(x\\) in the test set and compute the test error rate.\n\nn_test = 10000 \nx_test = 5*runif(n_test)  \ny_test = rbinom(n_test,1,p1(x_test))  \np1_test_hat = sapply(x_test, function(x0) { KNN_classifier(x0, x, y, K) })  \ny_test_hat = round(p1_test_hat &gt; 0.5) \ntest_error = mean(y_test_hat != y_test) \nprint(paste0(\"Test error rate (K = \",K,\"): \",test_error))\n\n[1] \"Test error rate (K = 1): 0.2869\"\n\n\nHow can we tell if the above is a good test error rate? We compute the test error rate for the Bayes optimal classifier.\n\n# Bayes optimal classifier\n# use the true p1(x) to make the best possible predictions on the training set\ny_hat_optimal = p1(x) &gt; 0.5  \n# compute the training error rate for the Bayes optimal classifier\ntrain_error_optimal = mean(y_hat_optimal != y)  \nprint(paste0(\"Training error rate (Optimal): \",train_error_optimal))\n\n[1] \"Training error rate (Optimal): 0.15\"\n\n# use the true p1(x) to make the best possible predictions on the test set\ny_test_hat_optimal = round(p1(x_test) &gt; 0.5)\n# compute the test error rate for the Bayes optimal classifier\ntest_error_optimal = mean(y_test_hat_optimal != y_test) \nprint(paste0(\"Test error rate (Optimal): \",test_error_optimal))\n\n[1] \"Test error rate (Optimal): 0.2495\""
  },
  {
    "objectID": "teaching/stat-learn/material/05-knn-reg-class.html#simulate-training-data-1",
    "href": "teaching/stat-learn/material/05-knn-reg-class.html#simulate-training-data-1",
    "title": "KNN classification + regression",
    "section": "Simulate training data",
    "text": "Simulate training data\nWe simulate training vector \\(\\bf{x}\\) from a uniform distribution on the interval \\([0,5]\\) and simulate training vector \\(\\bf{y}\\) by assuming \\[y = f(x) + \\varepsilon\\] where \\(f(x) = \\cos(x)\\) and \\(\\varepsilon \\sim N(0, \\sigma^2)\\) and \\(\\sigma = 0.3\\).\n\nset.seed(1)  # set random number generator\nn = 20  # number of samples\nx = 5*runif(n)  \nsigma = 0.3  \nf = function(x) { cos(x) }  \ny = f(x) + sigma*rnorm(n)  \n\n\nPlot of the training data\n\nplot(x,y,col=2,pch=20,cex=2)  # plot training data\nx_grid = seq(from=0, to=5, by=0.01)  # grid of x values for plotting f(x) values\nlines(x_grid,f(x_grid))  # plot true f(x) values for the grid"
  },
  {
    "objectID": "teaching/stat-learn/material/05-knn-reg-class.html#predicting-values",
    "href": "teaching/stat-learn/material/05-knn-reg-class.html#predicting-values",
    "title": "KNN classification + regression",
    "section": "Predicting values",
    "text": "Predicting values\nNow we run the KNN function to predict \\(y\\) at each point on the grid of \\(x\\) values. For that we need to define \\(K\\), that is number of nearest neighbors to use. We start with setting it equal to 1 but this can be changed later as an exercise.\n\nK = 1 \ny_grid_hat = sapply(x_grid, function(x0) { KNN(x0, x, y, K) })\n\nNext we add the predicted values to our plot:\n\nplot(x,y,col=2,pch=20,cex=2)  # plot training data\ntitle(paste(\"K =\",K))\nlines(x_grid,f(x_grid))  # plot true f(x) values\nlines(x_grid,y_grid_hat,col=4)  # plot predicted y values \n\n\n\n\n\n\n\n\nWhat happens to predicted curve when you change the value of \\(K\\)?"
  },
  {
    "objectID": "teaching/stat-learn/material/05-knn-reg-class.html#bias-variance-trade-off",
    "href": "teaching/stat-learn/material/05-knn-reg-class.html#bias-variance-trade-off",
    "title": "KNN classification + regression",
    "section": "Bias-Variance Trade-Off",
    "text": "Bias-Variance Trade-Off\nWe are going to run through some code in order to illustrate the trade-off between bias and variance. We set \\(x_0\\) to 1.5, which is the point we wish to estimate \\(y\\) at.\nWe simulate 10000 data sets to approximate expectations over the \\(Y\\)‚Äôs (given fixed \\(x\\)). We initialize two vectors of zeros to hold predicted and true \\(y\\) values at \\(x_0\\). Then for each of the 10000 datasets simulated, we repeat the above syntax for prediction. For the first 5 datasets simulated, we plot out the results to see what is happening.\n\nK = 1  \nx0 = 1.5  \nn_datasets = 10000  \ny0_hat = rep(0,n_datasets)  \ny0 = rep(0,n_datasets) \nfor (i in 1:n_datasets) {\n  y = f(x) + sigma*rnorm(n) \n  y0[i] = f(x0) + sigma*rnorm(1) \n  y0_hat[i] = KNN(x0, x, y, K)  \n  if (i &lt;= 5) {\n    plot(x,y,col=2,pch=20,cex=2,ylim=c(-1.5,1.5))  \n    lines(x_grid,f(x_grid))  \n    y_grid_hat = sapply(x_grid, function(x0) { KNN(x0, x, y, K) })\n    lines(x_grid,y_grid_hat,col=4)  \n    points(x0,y0_hat[i],pch=20,cex=4,col=4)  # plot predicted value of y at x0\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext we calculate the bias and variance of the KNN predictions at \\(x_0\\).We also compute the variance of the noise at \\(x_0\\) in order to be able to get the test MSE both using the bias variance representation \\[\\textrm{test MSE} = \\textrm{bias}^2 + \\textrm{variance} + \\textrm{noise}\\] and the direct formula: \\[\\mathop{\\mathbb{E}} \\left(y_0- \\hat{f}(x_0)\\right)^2\\]\n\nbias = mean(y0_hat) - f(x0)  # bias of KNN predictions at x0\nvariance = var(y0_hat)  # variance of KNN predictions at x0\nnoise = sigma^2  # variance of the noise at x0\n\nbias^2 + variance + noise \n\n[1] 0.2086705\n\nmean((y0 - y0_hat)^2) \n\n[1] 0.2097131\n\n\nWhy do you think the two values differ?\nLet‚Äôs visualize and explain the bias-variance trade-off using the syntax above. We show how the bias, variance and test MSE is influenced by the choice of \\(K\\), and include three plots should be included showing the following:\n\nbias vs.¬†\\(K\\) (or flexibility)\nvariance vs.¬†\\(K\\) (or flexibility)\ntest MSE versus \\(K\\) (or flexibility)\n\nWe plot of MSE, bias^2 and variance against number of neighbors \\(K\\):\n\nK = 1:5\ny0_hat_matrix = matrix(0, nrow = n_datasets, ncol = length(K))\n\n\nstatsout &lt;- function(K) {\n  y0_hat_K = rep(0, n_datasets)\n  \n  for (i in 1:n_datasets) {\n    y = f(x) + sigma * rnorm(n)\n    y0[i] = f(x0) + sigma * rnorm(1)\n    y0_hat_K[i] = KNN(x0, x, y, K)\n  }\n  \n  bias_K = mean(y0_hat_K) - f(x0)\n  variance_K = var(y0_hat_K)\n  noise_K = sigma^2\n  MSE_K = bias_K^2 + variance_K + noise_K \n  \n  return(c(bias_K^2, variance_K, MSE_K))\n}\n\ntoplot = sapply(K, statsout)\n\n# Plot the bias-variance trade-off\nplot(K, toplot[1, ], type = \"l\", lty=1, xlab = \"K\", \n     ylab = \"Bias^2\", ylim = c(min(toplot), max(toplot)))\nlines(K, toplot[2, ], lty=2)\nlines(K, toplot[3, ],lty=3)\n# Add a legend\nlegend(\"topleft\", legend=c(\"Bias^2\", \"Variance\", \"MSE\"),lty=1:3, cex=0.8)"
  },
  {
    "objectID": "teaching/stat-learn/material/08-nonlinearity.html",
    "href": "teaching/stat-learn/material/08-nonlinearity.html",
    "title": "‚ÄòNon-Linear‚Äô Linear Regression",
    "section": "",
    "text": "Introduction\nWe will use multiple approaches for modelling non-linearity and apply it to the Boston dataset included in the R library MASS. This dataset consists of 506 samples. The response variable is median value of owner-occupied homes in Boston (medv). The dataset has 13 associated predictor variables.\n\nlibrary(MASS)\nhelp(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\nhead(Boston)\n\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n  medv\n1 24.0\n2 21.6\n3 34.7\n4 33.4\n5 36.2\n6 28.7\n\n\nWe will analyse medv with respect to the predictor lstat (percentage of lower status population).\n\nhead(cbind(Boston$medv, Boston$lstat))\n\n     [,1] [,2]\n[1,] 24.0 4.98\n[2,] 21.6 9.14\n[3,] 34.7 4.03\n[4,] 33.4 2.94\n[5,] 36.2 5.33\n[6,] 28.7 5.21\n\n\nFor convenience we can name the response as y and the predictor x. We will also pre-define the labels for the x and y-axes that we will use repeatedly in figures throughout this practical.\n\ny = Boston$medv\nx = Boston$lstat\ny.lab = 'Median Property Value'\nx.lab = 'Lower Status (%)'\n\n\nplot( x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n      main = \"\", bty = 'l' )\n\n\n\n\n\n\n\n\n\n\nPolynomial Regression\nStart by fitting to the data a degree-2 polynomial using the command lm() and summarizing the results using summary().\n\npoly2 = lm(y ~ poly(x,  2,  raw = TRUE))\nsummary(poly2)\n\n\nCall:\nlm(formula = y ~ poly(x, 2, raw = TRUE))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             42.862007   0.872084   49.15   &lt;2e-16 ***\npoly(x, 2, raw = TRUE)1 -2.332821   0.123803  -18.84   &lt;2e-16 ***\npoly(x, 2, raw = TRUE)2  0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\nThe argument raw = TRUE In terms of fitting the curve poly(x, 2, raw = TRUE)) and poly(x, 2)) will give the same result! They are just based on different (orthogonal) basis but with polynomial regression we are almost never interested in the regression coefficients.\nFor plotting th results, we need to create an object, which we name sort.x, which has the sorted values of predictor x in a ascending order. Without sort.x we will not be able to produce the plots since in lecture. Then, we need to use predict() with sort.x as input in order to proceed to the next steps.\n\nsort.x = sort(x)\nsort.x[1:10]     # the first 10 sorted values of x \n\n [1] 1.73 1.92 1.98 2.47 2.87 2.88 2.94 2.96 2.97 2.98\n\npred2 = predict(poly2, newdata = list(x = sort.x), se = TRUE)\nnames(pred2)\n\n[1] \"fit\"            \"se.fit\"         \"df\"             \"residual.scale\"\n\n\nThe object pred2 contains fit, which are the fitted values, and se.fit, which are the standard errors of the mean prediction, that we need in order to construct the approximate 95% confidence intervals (of the mean prediction). With this information we can construct the confidence intervals using cbind(). Lets see how the first 10 fitted values and confidence intervals look like.\n\npred2$fit[1:10]  # the first 10 fitted values of the curve\n\n       1        2        3        4        5        6        7        8 \n38.95656 38.54352 38.41374 37.36561 36.52550 36.50468 36.37992 36.33840 \n       9       10 \n36.31765 36.29691 \n\nse.bands2 = cbind( pred2$fit - 2 * pred2$se.fit, \n                   pred2$fit + 2 * pred2$se.fit )\nse.bands2[1:10,] # the first 10 confidence intervals of the curve\n\n       [,1]     [,2]\n1  37.58243 40.33069\n2  37.20668 39.88036\n3  37.08853 39.73895\n4  36.13278 38.59845\n5  35.36453 37.68647\n6  35.34546 37.66390\n7  35.23118 37.52865\n8  35.19314 37.48365\n9  35.17413 37.46117\n10 35.15513 37.43870\n\n\nNow we can plot the results:\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"Degree-2 polynomial\", bty = 'l')\nlines(sort.x, pred2$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands2, lwd = 1.4, col = \"firebrick\", lty = 3)\n\n\n\n\n\n\n\n\nNote: We use lines() for pred2$fit because this is a vector, but for se.bands2, which is a matrix, we have to use matlines().\nThen we do similar steps to produce a plot of degree-2 up to degree-5 polynomial fits.\n\npoly3 = lm(y ~ poly(x,  3))\npoly4 = lm(y ~ poly(x,  4))\npoly5 = lm(y ~ poly(x, 5))\n\npred3 = predict(poly3, newdata = list(x = sort.x), se = TRUE)\npred4 = predict(poly4, newdata = list(x = sort.x), se = TRUE)\npred5 = predict(poly5, newdata = list(x = sort.x), se = TRUE)\n\nse.bands3 = cbind(pred3$fit + 2*pred3$se.fit, pred3$fit-2*pred3$se.fit)\nse.bands4 = cbind(pred4$fit + 2*pred4$se.fit, pred4$fit-2*pred4$se.fit)\nse.bands5 = cbind(pred5$fit + 2*pred5$se.fit, pred5$fit-2*pred5$se.fit)\n\n\npar(mfrow = c(2,2))\n# Degree-2\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"Degree-2 polynomial\", bty = 'l')\nlines(sort.x, pred2$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands2, lwd = 2, col = \"firebrick\", lty = 3)\n\n# Degree-3\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"Degree-3 polynomial\", bty = 'l')\nlines(sort.x, pred3$fit, lwd = 2, col = \"darkviolet\")\nmatlines(sort.x, se.bands3, lwd = 2, col = \"darkviolet\", lty = 3)\n\n# Degree-4\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"Degree-4 polynomial\", bty = 'l')\nlines(sort.x, pred4$fit, lwd = 2, col = \"royalblue\")\nmatlines(sort.x, se.bands4, lwd = 2, col = \"royalblue\", lty = 3)\n\n# Degree-5\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"Degree-5 polynomial\", bty = 'l')\nlines(sort.x, pred5$fit, lwd = 2, col = \"darkgreen\")\nmatlines(sort.x, se.bands5, lwd = 2, col = \"darkgreen\", lty = 3)\n\n\n\n\n\n\n\n\nAll four curves look reasonable given the data available. We may choose the degree-2 polynomial since it is simpler and seems to do about as well as the others. However, if we want to base our decision on a more formal procedure, we can use analysis-of-variance (ANOVA). Specifically, we will perform sequential comparisons based on the F-test, comparing first the linear model vs.¬†the quadratic model (degree-2 polynomial), then the quadratic model vs.¬†the cubic model (degree-3 polynomial) and so on. We therefore have to fit the simple linear model, and we also choose to fit the degree-6 polynomial to investigate the effects of an additional predictor as well. We can perform this analysis in RStudio using the command anova() as displayed below.\n\npoly1 = lm(y ~ x)\npoly6 = lm(y ~ poly(x, 6))\nanova(poly1, poly2, poly3, poly4, poly5, poly6)\n\nAnalysis of Variance Table\n\nModel 1: y ~ x\nModel 2: y ~ poly(x, 2, raw = TRUE)\nModel 3: y ~ poly(x, 3)\nModel 4: y ~ poly(x, 4)\nModel 5: y ~ poly(x, 5)\nModel 6: y ~ poly(x, 6)\n  Res.Df   RSS Df Sum of Sq        F    Pr(&gt;F)    \n1    504 19472                                    \n2    503 15347  1    4125.1 151.8623 &lt; 2.2e-16 ***\n3    502 14616  1     731.8  26.9390 3.061e-07 ***\n4    501 13968  1     647.8  23.8477 1.406e-06 ***\n5    500 13597  1     370.7  13.6453 0.0002452 ***\n6    499 13555  1      42.4   1.5596 0.2123125    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWhich model would you choose?\n\n\nStep Functions\nFor step function regression we can make use of the command cut(), which automatically assigns samples to intervals given a specific number of intervals. We can check how this works by executing the following syntax:\n\ntable(cut(x, 2))\n\n\n(1.69,19.9]   (19.9,38] \n        430          76 \n\n\nWhat we see is that cut(x, 2) automatically created a factor with two levels, corresponding to the intervals \\((1.69,19.9]\\) and \\((19.9,38]\\) and assigned each entry in\nx to one of these factors depending on which interval it was in. The command table() tells us that 430 samples of x fall within the first interval and that 76 samples fall within the second interval. Note that cut(x, 2) generated 2 intervals, but this means there is only 1 cutpoint (at 19.9). The number of cutpoints is naturally one less than the number of intervals, but it is important to be aware that cut requires specification of the number of required intervals.\nSo, we can use cut() within lm() to easily fit regression models with step functions. Below we consider 4 models with 1, 2, 3 and 4 cutpoints (2, 3, 4 and 5 intervals) respectively.\n\nstep2 = lm(y ~ cut(x, 2))\nstep3 = lm(y ~ cut(x, 3))\nstep4 = lm(y ~ cut(x, 4))\nstep5 = lm(y ~ cut(x, 5))\n\nThe analysis then is essentially the same as previously. We plot the fitted lines of the four models, along with approximate 95% confidence intervals for the mean predictions.\n\npred2 = predict(step2, newdata = list(x = sort(x)), se = TRUE)\npred3 = predict(step3, newdata = list(x = sort(x)), se = TRUE)\npred4 = predict(step4, newdata = list(x = sort(x)), se = TRUE)\npred5 = predict(step5, newdata = list(x = sort(x)), se = TRUE)\n\nse.bands2 = cbind(pred2$fit + 2*pred2$se.fit, pred2$fit-2*pred2$se.fit)\nse.bands3 = cbind(pred3$fit + 2*pred3$se.fit, pred3$fit-2*pred3$se.fit)\nse.bands4 = cbind(pred4$fit + 2*pred4$se.fit, pred4$fit-2*pred4$se.fit)\nse.bands5 = cbind(pred5$fit + 2*pred5$se.fit, pred5$fit-2*pred5$se.fit)\n\npar(mfrow = c(2,2))\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"1 cutpoint\", bty = 'l')\nlines(sort(x), pred2$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort(x), se.bands2, lwd = 1.4, col = \"firebrick\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"2 cutpoints\", bty = 'l')\nlines(sort(x), pred3$fit, lwd = 2, col = \"darkviolet\")\nmatlines(sort(x), se.bands3, lwd = 1.4, col = \"darkviolet\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"3 cutpoints\", bty = 'l')\nlines(sort(x), pred4$fit, lwd = 2, col = \"royalblue\")\nmatlines(sort(x), se.bands4, lwd = 1.4, col = \"royalblue\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"4 cutpoints\", bty = 'l')\nlines(sort(x), pred5$fit, lwd = 2, col = \"darkgreen\")\nmatlines(sort(x), se.bands5, lwd = 1.4, col = \"darkgreen\", lty = 3)\n\n\n\n\n\n\n\n\nNote that we do not necessarily need to rely on the automatic selections of cutpoints used by cut(). We can define the intervals if we want to. For instance, if we want cutpoints at 10, 20 and 30 we can do the following\n\nbreaks4 = c(min(x), 10, 20, 30, max(x))\ntable(cut(x, breaks = breaks4))\n\n\n(1.73,10]   (10,20]   (20,30]   (30,38] \n      218       213        62        12 \n\n\nBy including min(x) and max(x) at the start and end, we ensure the intervals covered the entire range of x. Our model is then\n\nstep.new4 = lm(y ~ cut(x, breaks = breaks4))\nsummary(step.new4)\n\n\nCall:\nlm(formula = y ~ cut(x, breaks = breaks4))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.4803  -4.6239  -0.4239   2.8968  20.6197 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      29.3803     0.4415  66.540   &lt;2e-16 ***\ncut(x, breaks = breaks4)(10,20] -10.4563     0.6281 -16.648   &lt;2e-16 ***\ncut(x, breaks = breaks4)(20,30] -16.6770     0.9383 -17.773   &lt;2e-16 ***\ncut(x, breaks = breaks4)(30,38] -18.6886     1.9331  -9.668   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.519 on 501 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.4925,    Adjusted R-squared:  0.4895 \nF-statistic: 162.1 on 3 and 501 DF,  p-value: &lt; 2.2e-16\n\n\nWe can now make predictions at new data points using the constructed linear model as usual.\n\nnewx &lt;- c(10.56, 5.89)\npreds = predict(step.new4, newdata = list(x = newx), se = TRUE)\npreds\n\n$fit\n       1        2 \n18.92394 29.38028 \n\n$se.fit\n        1         2 \n0.4466955 0.4415432 \n\n$df\n[1] 501\n\n$residual.scale\n[1] 6.519307\n\n\n\n\nRegression Splines\nFor this analysis we will require package splines.\n\nlibrary(splines)\n\nInitially let‚Äôs fit regression splines by specifying knots. From the previous plot it is not clear where exactly we should place knots, so we will make use of the command summary in order to find the 25th, 50th and 75th percentiles ofx, which will be the positions where we will place the knots. We also sort the variable x before fitting the splines.\n\nsummary(x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.73    6.95   11.36   12.65   16.95   37.97 \n\ncuts = summary(x)[c(2, 3, 5)] \ncuts\n\n1st Qu.  Median 3rd Qu. \n  6.950  11.360  16.955 \n\nsort.x = sort(x)\n\nFor a start lets fit a linear spline using our selected placement of knots. For this we can use command lm() and inside it we use the command bs() in which we specify degree = 1 for a linear spline and knots = cuts for the placement of the knots at the three percentiles. We also calculate the corresponding fitted values and confidence intervals exactly in the same way we did in previous practical demonstrations.\n\nspline1 = lm(y ~  splines::bs(x, degree = 1, knots = cuts))\npred1 = predict(spline1, newdata = list(x = sort.x), se = TRUE)\nse.bands1 = cbind(pred1$fit + 2 * pred1$se.fit, \n                  pred1$fit - 2 * pred1$se.fit)\n\nLet‚Äôs plot the results:\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Linear Spline\", bty = 'l')\nlines(sort.x, pred1$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands1, lwd = 2, col = \"firebrick\", lty = 3)\n\n\n\n\n\n\n\n\nUsing ?bs we see that instead of using the argument knots we can use the argument df, which are the degrees of freedom. Splines have \\((d+1)+K\\) degrees of freedom, where \\(d\\) is the degree of the polynomial and \\(K\\) the number of knots. So in this case we have 1+1+3 = 5 degrees of freedom. Selecting df = 5 in bs() will automatically use 3 knots placed at the 25th, 50th and 75th percentiles. Below we check whether the plot based on df=5 is indeed the same as the previous plot and as we can see it is.\n\nspline1df = lm(y ~ splines::bs(x, degree = 1, df = 5))\npred1df = predict(spline1df, newdata = list(x = sort.x), se = TRUE)\nse.bands1df = cbind( pred1df$fit + 2 * pred1df$se.fit, \n                     pred1df$fit - 2 * pred1df$se.fit )\n\npar(mfrow = c(1, 2))\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Linear Spline (with knots)\", bty = 'l')\nlines(sort.x, pred1$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands1, lwd = 2, col = \"firebrick\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Linear Spline (with df)\", bty = 'l')\nlines(sort.x, pred1df$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands1df, lwd = 2, col = \"firebrick\", lty = 3)\nmatlines(sort.x, se.bands1, lwd = 2, col = \"firebrick\", lty = 3)\n\n\n\n\n\n\n\n\nHaving seen how this works we can also fit a degree-2 (quadratic) and degree-3 (cubic) spline to the data, all we have to do is change degree = 1 to degree = 2 and degree = 3 respectively. Also we increase the respective degrees of freedom from df = 5 to df = 6 and df = 7 in order to keep the same number (and position) of knots in the quadratic and cubic spline models.\n\nspline2 = lm(y ~ splines::bs(x, degree = 2, df = 6))\npred2 = predict(spline2, newdata = list(x = sort.x), se = TRUE)\nse.bands2 = cbind(pred2$fit + 2 * pred2$se.fit, pred2$fit - 2 * pred2$se.fit)\n\nspline3 = lm(y ~ splines::bs(x, degree = 3, df = 7))\npred3 = predict(spline3, newdata = list(x = sort.x), se = TRUE)\nse.bands3 = cbind(pred3$fit + 2 * pred3$se.fit, pred3$fit - 2 * pred3$se.fit)\n\npar(mfrow = c(1,3))\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Linear Spline\", bty = 'l')\nlines(sort.x, pred1$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands1, lwd = 2, col = \"firebrick\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Quadratic Spline\", bty = 'l')\nlines(sort.x, pred2$fit, lwd = 2, col = \"darkgreen\")\nmatlines(sort.x, se.bands2, lwd = 2, col = \"darkgreen\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Cubic Spline\", bty = 'l')\nlines(sort.x, pred3$fit, lwd = 2, col = \"royalblue\")\nmatlines(sort.x, se.bands3, lwd = 2, col = \"royalblue\", lty = 3)\n\n\n\n\n\n\n\n\n\n\nNatural Splines\nFor natural splines, we can use the command ns(). As with the command bs() previously, we again have the option to either specify the knots manually (via the argument knots) or to simply pre-define the degrees of freedom (via the argument df). Below we use the latter option to fit four natural splines with 1, 2, 3 and 4 degrees of freedom. As we see using 1 degree of freedom actually results in just a linear model.\n\nspline.ns1 = lm(y ~ splines::ns(x, df = 1))\npred.ns1 = predict(spline.ns1, newdata = list(x = sort.x), se = TRUE)\nse.bands.ns1 = cbind(pred.ns1$fit + 2 * pred.ns1$se.fit, \n                     pred.ns1$fit - 2 * pred.ns1$se.fit)\n\nspline.ns2 = lm(y ~ splines::ns(x, df = 2))\npred.ns2 = predict(spline.ns2, newdata = list(x = sort.x), se = TRUE)\nse.bands.ns2 = cbind(pred.ns2$fit + 2 * pred.ns2$se.fit, \n                     pred.ns2$fit - 2 * pred.ns2$se.fit)\n\nspline.ns3 = lm(y ~ splines::ns(x, df = 3))\npred.ns3 = predict(spline.ns3, newdata = list(x = sort.x), se = TRUE)\nse.bands.ns3 = cbind(pred.ns3$fit + 2 * pred.ns3$se.fit, \n                     pred.ns3$fit - 2 * pred.ns3$se.fit)\n\nspline.ns4 = lm(y ~ splines::ns(x, df = 4))\npred.ns4 = predict(spline.ns4, newdata = list(x = sort.x), se = TRUE)\nse.bands.ns4 = cbind(pred.ns4$fit + 2 * pred.ns4$se.fit, \n                     pred.ns4$fit - 2 * pred.ns4$se.fit)\n\npar(mfrow = c(2, 2))\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Natural Spline (1 df)\", bty = 'l')\nlines(sort.x, pred.ns1$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands.ns1, lwd = 2, col = \"firebrick\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Natural Spline (2 df)\", bty = 'l')\nlines(sort.x, pred.ns2$fit, lwd = 2, col = \"darkviolet\")\nmatlines(sort.x, se.bands.ns2, lwd = 2, col = \"darkviolet\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Natural Spline (3 df)\", bty = 'l')\nlines(sort.x, pred.ns3$fit, lwd = 2, col = \"royalblue\")\nmatlines(sort.x, se.bands.ns3, lwd = 2, col = \"royalblue\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Natural Spline (4 df)\", bty = 'l')\nlines(sort.x, pred.ns4$fit, lwd = 2, col = \"darkgreen\")\nmatlines(sort.x, se.bands.ns4, lwd = 2, col = \"darkgreen\", lty = 3)\n\n\n\n\n\n\n\n\nBelow we plot the cubic spline next to the natural cubic spline for comparison. As we can see, the natural cubic spline is generally smoother and closer to linear on the right boundary of the predictor space, where it has, additionally, narrower confidence intervals in comparison to the cubic spline.\n\npar(mfrow = c(1,2))\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Cubic Spline\", bty = 'l')\nlines(sort.x, pred3$fit, lwd = 2, col = \"darkblue\")\nmatlines(sort.x, se.bands3, lwd = 2, col = \"darkblue\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Natural Spline (3 df)\", bty = 'l')\nlines(sort.x, pred.ns3$fit, lwd = 2, col = \"royalblue\")\nmatlines(sort.x, se.bands.ns3, lwd = 2, col = \"royalblue\", lty = 3)\n\n\n\n\n\n\n\n\n\n\nSmoothing Splines\nFor fitting smoothing splines we use the command smooth.splines() instead of lm(). Under smoothing splines there are no knots to specify; the only parameter is \\(\\lambda\\). This can be specified via cross-validation by specifying cv = TRUE inside smooth.splines(). Alternatively, we can specify the effective degrees of freedom which correspond to some value of \\(\\lambda\\). Below we first fit a smoothing spline with 3 effective degrees of freedom (via the argument df = 3), and then also by tuning \\(\\lambda\\) via cross-validation. In this case we see that tuning \\(\\lambda\\) through cross-validation results in a curve which is slightly wiggly on the right boundary of the predictor space.\n\nsmooth1 = smooth.spline(x, y, df = 3)\nsmooth2 = smooth.spline(x, y, cv = TRUE)\n\nWarning in smooth.spline(x, y, cv = TRUE): cross-validation with non-unique 'x'\nvalues seems doubtful\n\npar(mfrow = c(1,2))\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Smoothing Spline (3 df)\", bty = 'l')\nlines(smooth1, lwd = 2, col = \"brown\")\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Smoothing Spline (CV)\", bty = 'l')\nlines(smooth2, lwd = 2, col = \"darkorange\")\n\n\n\n\n\n\n\n\nNote: the effective degrees of freedom of a smoothing spline are similar to the degrees of freedom in standard spline models and can be used as an alternative to cross-validation as a way to fix \\(\\lambda\\).\n\n\nGAMs\nIn this final part we will fit a generalised additive model (GAM) utilising more than one predictor from the Boston dataset.We first use the command names() in order to check once again the available predictor variables.\n\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n\nLet‚Äôs say that we want to use predictors lstat, indus and chas for the analysis (use ?Boston again to check what these refer to).\nFor GAMs we will make use of the library gam, so the first thing that we have to do is to install this package by executing install.packages(\"gam\") once. Then we load the library.\n\nlibrary(gam)\n\nThe main function is gam(). Inside this function we can use any combination of non-linear and linear modelling of the various predictors. For example below we use a cubic spline with 5 degrees of freedom for lstat, a smoothing spline with 5 degrees of freedom for indus and a simple linear model for variable chas. We then plot the contributions of each predictor using the command plot(). As we can see, GAMs are very useful as they estimate the contribution of the effects of each predictor.\n\ngam = gam( medv ~ bs(lstat, degree = 3, df = 5) + s(indus, df = 5) + chas, \n           data = Boston )\npar( mfrow = c(1,3) )\nplot( gam,  se = TRUE, col = \"blue\" )\n\n\n\n\n\n\n\n\nNote that simply using chas inside gam() is just fitting a linear model for this variable. However, one thing that we observe is that chas is a binary variable as it only takes the values of 0 and 1. This we can see from the x-axis of the chas plot on the right above. So, it would be preferable to use a step function for this variable. In order to do this we have to change the variable chas to a factor. We first create a second object called Boston1 (in order not to change the initial dataset Boston) and then we use the command factor() to change variable chas. Then we fit again the same model. As we can see below now gam() fits a step function for variable chas which is more appropriate.\n\nBoston1 = Boston\nBoston1$chas = factor(Boston1$chas)\n\ngam1 = gam( medv ~ bs(lstat, degree = 3, df = 5) + s(indus, df = 5) + chas, \n            data = Boston1 )\npar(mfrow = c(1,3))\nplot(gam1,  se = TRUE, col = \"blue\")\n\n\n\n\n\n\n\n\nWe can make predictions from gam objects, just like lm objects, using the predict() method for the class gam. Here we make predictions on some new data. Note that when assigning the value 0 to chas, we enclose it in ‚Äú‚Äù since we informed R to treat chas as a categorical factor with two levels: ‚Äú0‚Äù and ‚Äú1‚Äù.\n\npreds &lt;- predict( gam1, \n                  newdata = data.frame( chas = \"0\", indus = 3, lstat = 5 )  )\npreds\n\n       1 \n32.10065"
  },
  {
    "objectID": "teaching/stat-learn/material/07_model_selection_regularization.html",
    "href": "teaching/stat-learn/material/07_model_selection_regularization.html",
    "title": "Linear Models and Regularization Methods",
    "section": "",
    "text": "Here we apply the best subset selection approach to the Hitters data. We wish to predict a baseball player‚Äôs Salary on the basis of various statistics associated with performance in the previous year.\nFirst of all, we note that the Salary variable is missing for some of the players. The is.na() function can be used to identify the missing observations. It returns a vector of the same length as the input vector, with a TRUE for any elements that are missing, and a FALSE for non-missing elements. The sum() function can then be used to count all of the missing elements.\n\nlibrary(ISLR2)\nnames(Hitters)\n\n [1] \"AtBat\"     \"Hits\"      \"HmRun\"     \"Runs\"      \"RBI\"       \"Walks\"    \n [7] \"Years\"     \"CAtBat\"    \"CHits\"     \"CHmRun\"    \"CRuns\"     \"CRBI\"     \n[13] \"CWalks\"    \"League\"    \"Division\"  \"PutOuts\"   \"Assists\"   \"Errors\"   \n[19] \"Salary\"    \"NewLeague\"\n\ndim(Hitters)\n\n[1] 322  20\n\nsum(is.na(Hitters$Salary))\n\n[1] 59\n\n\nHence we see that Salary is missing for \\(59\\) players. The na.omit() function removes all of the rows that have missing values in any variable.\n\nHitters &lt;- na.omit(Hitters)\ndim(Hitters)\n\n[1] 263  20\n\nsum(is.na(Hitters))\n\n[1] 0\n\n\nThe regsubsets() function (part of the leaps library) performs best subset selection by identifying the best model that contains a given number of predictors, where best is quantified using RSS. The syntax is the same as for lm(). The summary() command outputs the best set of variables for each model size.\n\nlibrary(leaps)\nregfit.full &lt;- regsubsets(Salary ~ ., Hitters)\nsummary(regfit.full)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., Hitters)\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: exhaustive\n         AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 ) \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \"*\"    \" \"   \" \" \n8  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \"*\"    \"*\"   \" \" \n         CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 ) \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 ) \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 ) \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n\n\nAn asterisk indicates that a given variable is included in the corresponding model. For instance, this output indicates that the best two-variable model contains only Hits and CRBI. By default, regsubsets() only reports results up to the best eight-variable model. But the nvmax option can be used in order to return as many variables as are desired. Here we fit up to a 19-variable model.\n\nregfit.full &lt;- regsubsets(Salary ~ ., data = Hitters,\n    nvmax = 19)\nreg.summary &lt;- summary(regfit.full)\n\nThe summary() function also returns \\(R^2\\), RSS, adjusted \\(R^2\\), \\(C_p\\), and BIC. We can examine these to try to select the best overall model.\n\nnames(reg.summary)\n\n[1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"   \n\n\nFor instance, we see that the \\(R^2\\) statistic increases from \\(32 \\%\\), when only one variable is included in the model, to almost \\(55 \\%\\), when all variables are included. As expected, the \\(R^2\\) statistic increases monotonically as more variables are included.\n\nreg.summary$rsq\n\n [1] 0.3214501 0.4252237 0.4514294 0.4754067 0.4908036 0.5087146 0.5141227\n [8] 0.5285569 0.5346124 0.5404950 0.5426153 0.5436302 0.5444570 0.5452164\n[15] 0.5454692 0.5457656 0.5459518 0.5460945 0.5461159\n\n\nPlotting RSS, adjusted \\(R^2\\), \\(C_p\\), and BIC for all of the models at once will help us decide which model to select.\n\ndf = data.frame(adjR2 = reg.summary$adjr2,\n                rss = reg.summary$rss,\n                Cp = reg.summary$cp,\n                BIC = reg.summary$bic,\n                Variables = seq(1:19))\ndf_long = df %&gt;%\n  pivot_longer(cols = c(adjR2, rss, Cp, BIC),\n               names_to = \"Metric\",\n               values_to = \"Value\")\n\nError in df %&gt;% pivot_longer(cols = c(adjR2, rss, Cp, BIC), names_to = \"Metric\", : could not find function \"%&gt;%\"\n\ndf_long %&gt;%\n  ggplot(aes(x = Variables, y = Value)) +\n  geom_line() +\n  facet_wrap(~Metric, scales = \"free_y\") + \n  labs(x = \"Number of Variables\", y = \"Value\") +\n  theme_minimal()\n\nError in df_long %&gt;% ggplot(aes(x = Variables, y = Value)): could not find function \"%&gt;%\"\n\n\nThe which.max() function can be used to identify the location of the maximum point of a vector. We will now plot a red dot to indicate the model with the largest adjusted \\(R^2\\) statistic.\n\nmax_index &lt;- which.max(df$adjR2)\n\ndf %&gt;%\n  ggplot(aes(x = Variables, y = adjR2)) +\n  geom_line() + \n  geom_point(aes(x = max_index, y = adjR2[max_index]),\n             color = \"red\", size = 4, shape = 20) + \n  labs(x = \"Number of Variables\", y = \"Adjusted R¬≤\") +\n  theme_minimal()\n\nError in df %&gt;% ggplot(aes(x = Variables, y = adjR2)): could not find function \"%&gt;%\"\n\n\nIn a similar fashion we can plot the \\(C_p\\) and BIC statistics, and indicate the models with the smallest statistic using which.min().\n\nmin_cp &lt;- which.min(df$Cp)\nmin_bic &lt;- which.min(df$BIC)\n\ndf %&gt;%\n  ggplot(aes(x = Variables, y = Cp)) +\n  geom_line() + \n  geom_point(aes(x = min_cp, y = Cp[min_cp]),\n             color = \"red\", size = 4, shape = 20) + \n  labs(x = \"Number of Variables\", y = \"Cp\") +\n  theme_minimal()\n\nError in df %&gt;% ggplot(aes(x = Variables, y = Cp)): could not find function \"%&gt;%\"\n\ndf %&gt;%\n  ggplot(aes(x = Variables, y = BIC)) +\n  geom_line() + \n  geom_point(aes(x = min_bic, y = BIC[min_bic]),\n             color = \"red\", size = 4, shape = 20) + \n  labs(x = \"Number of Variables\", y = \"BIC\") +\n  theme_minimal()\n\nError in df %&gt;% ggplot(aes(x = Variables, y = BIC)): could not find function \"%&gt;%\"\n\n\nThe regsubsets() function has a built-in plot() command which can be used to display the selected variables for the best model with a given number of predictors, ranked according to the BIC, \\(C_p\\), adjusted \\(R^2\\), or AIC. To find out more about this function, type ?plot.regsubsets.\n\nplot(regfit.full, scale = \"r2\")\n\n\n\n\n\n\n\nplot(regfit.full, scale = \"adjr2\")\n\n\n\n\n\n\n\nplot(regfit.full, scale = \"Cp\")\n\n\n\n\n\n\n\nplot(regfit.full, scale = \"bic\")\n\n\n\n\n\n\n\n\nThe top row of each plot contains a black square for each variable selected according to the optimal model associated with that statistic. For instance, we see that several models share a BIC close to \\(-150\\). However, the model with the lowest BIC is the six-variable model that contains only AtBat, Hits, Walks, CRBI, DivisionW, and PutOuts. We can use the coef() function to see the coefficient estimates associated with this model.\n\ncoef(regfit.full, 6)\n\n (Intercept)        AtBat         Hits        Walks         CRBI    DivisionW \n  91.5117981   -1.8685892    7.6043976    3.6976468    0.6430169 -122.9515338 \n     PutOuts \n   0.2643076 \n\n\n\n\n\nWe can also use the regsubsets() function to perform forward stepwise or backward stepwise selection, using the argument method = \"forward\" or method = \"backward\".\n\nregfit.fwd &lt;- regsubsets(Salary ~ ., data = Hitters,\n                         nvmax = 19, method = \"forward\")\nsummary(regfit.fwd)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = \"forward\")\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: forward\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \"*\" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\nregfit.bwd &lt;- regsubsets(Salary ~ ., data = Hitters,\n                         nvmax = 19, method = \"backward\")\nsummary(regfit.bwd)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = \"backward\")\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: backward\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n4  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n7  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \"*\" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\n\nFor instance, we see that using forward stepwise selection, the best one-variable model contains only CRBI, and the best two-variable model additionally includes Hits. For this data, the best one-variable through six-variable models are each identical for best subset and forward selection. However, the best seven-variable models identified by forward stepwise selection, backward stepwise selection, and best subset selection are different.\n\ncoef(regfit.full, 7)\n\n (Intercept)         Hits        Walks       CAtBat        CHits       CHmRun \n  79.4509472    1.2833513    3.2274264   -0.3752350    1.4957073    1.4420538 \n   DivisionW      PutOuts \n-129.9866432    0.2366813 \n\ncoef(regfit.fwd, 7)\n\n (Intercept)        AtBat         Hits        Walks         CRBI       CWalks \n 109.7873062   -1.9588851    7.4498772    4.9131401    0.8537622   -0.3053070 \n   DivisionW      PutOuts \n-127.1223928    0.2533404 \n\ncoef(regfit.bwd, 7)\n\n (Intercept)        AtBat         Hits        Walks        CRuns       CWalks \n 105.6487488   -1.9762838    6.7574914    6.0558691    1.1293095   -0.7163346 \n   DivisionW      PutOuts \n-116.1692169    0.3028847 \n\n\n\n\n\nWe just saw that it is possible to choose among a set of models of different sizes using \\(C_p\\), BIC, and adjusted \\(R^2\\). We will now consider how to do this using the validation set and cross-validation approaches.\nIn order for these approaches to yield accurate estimates of the test error, we must use only the training observations to perform all aspects of model-fitting‚Äîincluding variable selection. Therefore, the determination of which model of a given size is best must be made using only the training observations. This point is subtle but important. If the full data set is used to perform the best subset selection step, the validation set errors and cross-validation errors that we obtain will not be accurate estimates of the test error.\n\nset.seed(1)\ntrain &lt;- sample(c(TRUE, FALSE),\n                nrow(Hitters),\n                replace = TRUE)\n\ntest &lt;- (!train)\n\nNow, we apply regsubsets() to the training set in order to perform best subset selection.\n\nregfit.best &lt;- regsubsets(Salary ~ .,\n                          data = Hitters[train, ],\n                          nvmax = 19)\n\nNotice that we subset the Hitters data frame directly in the call in order to access only the training subset of the data, using the expression Hitters[train, ]. We now compute the validation set error for the best model of each model size. We first make a model matrix from the test data.\n\ntest.mat &lt;- model.matrix(Salary ~ ., data = Hitters[test, ])\n\nThe model.matrix() function is used in many regression packages for building an ‚ÄúX‚Äù matrix from data. Now we run a loop, and for each size i, we extract the coefficients from regfit.best for the best model of that size, multiply them into the appropriate columns of the test model matrix to form the predictions, and compute the test MSE.\n\nval.errors &lt;- rep(NA, 19)\nfor (i in 1:19) {\n coefi &lt;- coef(regfit.best, id = i)\n pred &lt;- test.mat[, names(coefi)] %*% coefi\n val.errors[i] &lt;- mean((Hitters$Salary[test] - pred)^2)\n}\n\nWe find that the best model is the one that contains seven variables.\n\nval.errors\n\n [1] 164377.3 144405.5 152175.7 145198.4 137902.1 139175.7 126849.0 136191.4\n [9] 132889.6 135434.9 136963.3 140694.9 140690.9 141951.2 141508.2 142164.4\n[17] 141767.4 142339.6 142238.2\n\nwhich.min(val.errors)\n\n[1] 7\n\ncoef(regfit.best, 7)\n\n (Intercept)        AtBat         Hits        Walks        CRuns       CWalks \n  67.1085369   -2.1462987    7.0149547    8.0716640    1.2425113   -0.8337844 \n   DivisionW      PutOuts \n-118.4364998    0.2526925 \n\n\nThis was a little tedious, partly because there is no predict() method for regsubsets(). Since we will be using this function again, we can capture our steps above and write our own predict method.\n\n predict.regsubsets &lt;- function(object, newdata, id, ...) {\n  form &lt;- as.formula(object$call[[2]])\n  mat &lt;- model.matrix(form, newdata)\n  coefi &lt;- coef(object, id = id)\n  xvars &lt;- names(coefi)\n  mat[, xvars] %*% coefi\n }\n\nOur function pretty much mimics what we did above. The only complex part is how we extracted the formula used in the call to regsubsets(). We demonstrate how we use this function below, when we do cross-validation.\nFinally, we perform best subset selection on the full data set, and select the best seven-variable model. It is important that we make use of the full data set in order to obtain more accurate coefficient estimates.\nNote that we perform best subset selection on the full data set and select the best seven-variable model, rather than simply using the variables that were obtained from the training set, because the best seven-variable model on the full data set may differ from the corresponding model on the training set.\n\nregfit.best &lt;- regsubsets(Salary ~ ., data = Hitters,\n    nvmax = 19)\ncoef(regfit.best, 7)\n\n (Intercept)         Hits        Walks       CAtBat        CHits       CHmRun \n  79.4509472    1.2833513    3.2274264   -0.3752350    1.4957073    1.4420538 \n   DivisionW      PutOuts \n-129.9866432    0.2366813 \n\n\nIn fact, we see that the best seven-variable model on the full data set has a different set of variables than the best seven-variable model on the training set.\nWe now try to choose among the models of different sizes using cross-validation. This approach is somewhat involved, as we must perform best subset selection within each of the \\(k\\) training sets. Despite this, we see that with its clever subsetting syntax, R makes this job quite easy. First, we create a vector that allocates each observation to one of \\(k=10\\) folds, and we create a matrix in which we will store the results.\n\nk &lt;- 10\nn &lt;- nrow(Hitters)\nset.seed(1)\nfolds &lt;- sample(rep(1:k, length = n))\ncv.errors &lt;- matrix(NA, k, 19,\n    dimnames = list(NULL, paste(1:19)))\n\nNow we write a for loop that performs cross-validation. In the \\(j\\)th fold, the elements of folds that equal j are in the test set, and the remainder are in the training set. We make our predictions for each model size (using our new predict() method), compute the test errors on the appropriate subset, and store them in the appropriate slot in the matrix cv.errors. Note that in the following code R will automatically use our predict.regsubsets() function when we call predict() because the best.fit object has class regsubsets.\n\nfor (j in 1:k) {\n  best.fit &lt;- regsubsets(Salary ~ .,\n       data = Hitters[folds != j, ],\n       nvmax = 19)\n  for (i in 1:19) {\n    pred &lt;- predict(best.fit, Hitters[folds == j, ], id = i)\n    cv.errors[j, i] &lt;-\n         mean((Hitters$Salary[folds == j] - pred)^2)\n   }\n }\n\nThis has given us a \\(10 \\times 19\\) matrix, of which the \\((j,i)\\)th element corresponds to the test MSE for the \\(j\\)th cross-validation fold for the best \\(i\\)-variable model. We use the apply() function to average over the columns of this matrix in order to obtain a vector for which the \\(i\\)th element is the cross-validation error for the \\(i\\)-variable model.\n\nmean.cv.errors &lt;- apply(cv.errors, 2, mean)\nmean.cv.errors\n\n       1        2        3        4        5        6        7        8 \n143439.8 126817.0 134214.2 131782.9 130765.6 120382.9 121443.1 114363.7 \n       9       10       11       12       13       14       15       16 \n115163.1 109366.0 112738.5 113616.5 115557.6 115853.3 115630.6 116050.0 \n      17       18       19 \n116117.0 116419.3 116299.1 \n\npar(mfrow = c(1, 1))\nplot(mean.cv.errors, type = \"b\")\n\n\n\n\n\n\n\n\nWe see that cross-validation selects a 10-variable model. We now perform best subset selection on the full data set in order to obtain the 10-variable model.\n\nreg.best &lt;- regsubsets(Salary ~ ., data = Hitters,\n    nvmax = 19)\ncoef(reg.best, 10)\n\n (Intercept)        AtBat         Hits        Walks       CAtBat        CRuns \n 162.5354420   -2.1686501    6.9180175    5.7732246   -0.1300798    1.4082490 \n        CRBI       CWalks    DivisionW      PutOuts      Assists \n   0.7743122   -0.8308264 -112.3800575    0.2973726    0.2831680"
  },
  {
    "objectID": "teaching/stat-learn/material/07_model_selection_regularization.html#subset-selection-methods",
    "href": "teaching/stat-learn/material/07_model_selection_regularization.html#subset-selection-methods",
    "title": "Linear Models and Regularization Methods",
    "section": "",
    "text": "Here we apply the best subset selection approach to the Hitters data. We wish to predict a baseball player‚Äôs Salary on the basis of various statistics associated with performance in the previous year.\nFirst of all, we note that the Salary variable is missing for some of the players. The is.na() function can be used to identify the missing observations. It returns a vector of the same length as the input vector, with a TRUE for any elements that are missing, and a FALSE for non-missing elements. The sum() function can then be used to count all of the missing elements.\n\nlibrary(ISLR2)\nnames(Hitters)\n\n [1] \"AtBat\"     \"Hits\"      \"HmRun\"     \"Runs\"      \"RBI\"       \"Walks\"    \n [7] \"Years\"     \"CAtBat\"    \"CHits\"     \"CHmRun\"    \"CRuns\"     \"CRBI\"     \n[13] \"CWalks\"    \"League\"    \"Division\"  \"PutOuts\"   \"Assists\"   \"Errors\"   \n[19] \"Salary\"    \"NewLeague\"\n\ndim(Hitters)\n\n[1] 322  20\n\nsum(is.na(Hitters$Salary))\n\n[1] 59\n\n\nHence we see that Salary is missing for \\(59\\) players. The na.omit() function removes all of the rows that have missing values in any variable.\n\nHitters &lt;- na.omit(Hitters)\ndim(Hitters)\n\n[1] 263  20\n\nsum(is.na(Hitters))\n\n[1] 0\n\n\nThe regsubsets() function (part of the leaps library) performs best subset selection by identifying the best model that contains a given number of predictors, where best is quantified using RSS. The syntax is the same as for lm(). The summary() command outputs the best set of variables for each model size.\n\nlibrary(leaps)\nregfit.full &lt;- regsubsets(Salary ~ ., Hitters)\nsummary(regfit.full)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., Hitters)\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: exhaustive\n         AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 ) \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \"*\"    \" \"   \" \" \n8  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \"*\"    \"*\"   \" \" \n         CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 ) \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 ) \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 ) \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n\n\nAn asterisk indicates that a given variable is included in the corresponding model. For instance, this output indicates that the best two-variable model contains only Hits and CRBI. By default, regsubsets() only reports results up to the best eight-variable model. But the nvmax option can be used in order to return as many variables as are desired. Here we fit up to a 19-variable model.\n\nregfit.full &lt;- regsubsets(Salary ~ ., data = Hitters,\n    nvmax = 19)\nreg.summary &lt;- summary(regfit.full)\n\nThe summary() function also returns \\(R^2\\), RSS, adjusted \\(R^2\\), \\(C_p\\), and BIC. We can examine these to try to select the best overall model.\n\nnames(reg.summary)\n\n[1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"   \n\n\nFor instance, we see that the \\(R^2\\) statistic increases from \\(32 \\%\\), when only one variable is included in the model, to almost \\(55 \\%\\), when all variables are included. As expected, the \\(R^2\\) statistic increases monotonically as more variables are included.\n\nreg.summary$rsq\n\n [1] 0.3214501 0.4252237 0.4514294 0.4754067 0.4908036 0.5087146 0.5141227\n [8] 0.5285569 0.5346124 0.5404950 0.5426153 0.5436302 0.5444570 0.5452164\n[15] 0.5454692 0.5457656 0.5459518 0.5460945 0.5461159\n\n\nPlotting RSS, adjusted \\(R^2\\), \\(C_p\\), and BIC for all of the models at once will help us decide which model to select.\n\ndf = data.frame(adjR2 = reg.summary$adjr2,\n                rss = reg.summary$rss,\n                Cp = reg.summary$cp,\n                BIC = reg.summary$bic,\n                Variables = seq(1:19))\ndf_long = df %&gt;%\n  pivot_longer(cols = c(adjR2, rss, Cp, BIC),\n               names_to = \"Metric\",\n               values_to = \"Value\")\n\nError in df %&gt;% pivot_longer(cols = c(adjR2, rss, Cp, BIC), names_to = \"Metric\", : could not find function \"%&gt;%\"\n\ndf_long %&gt;%\n  ggplot(aes(x = Variables, y = Value)) +\n  geom_line() +\n  facet_wrap(~Metric, scales = \"free_y\") + \n  labs(x = \"Number of Variables\", y = \"Value\") +\n  theme_minimal()\n\nError in df_long %&gt;% ggplot(aes(x = Variables, y = Value)): could not find function \"%&gt;%\"\n\n\nThe which.max() function can be used to identify the location of the maximum point of a vector. We will now plot a red dot to indicate the model with the largest adjusted \\(R^2\\) statistic.\n\nmax_index &lt;- which.max(df$adjR2)\n\ndf %&gt;%\n  ggplot(aes(x = Variables, y = adjR2)) +\n  geom_line() + \n  geom_point(aes(x = max_index, y = adjR2[max_index]),\n             color = \"red\", size = 4, shape = 20) + \n  labs(x = \"Number of Variables\", y = \"Adjusted R¬≤\") +\n  theme_minimal()\n\nError in df %&gt;% ggplot(aes(x = Variables, y = adjR2)): could not find function \"%&gt;%\"\n\n\nIn a similar fashion we can plot the \\(C_p\\) and BIC statistics, and indicate the models with the smallest statistic using which.min().\n\nmin_cp &lt;- which.min(df$Cp)\nmin_bic &lt;- which.min(df$BIC)\n\ndf %&gt;%\n  ggplot(aes(x = Variables, y = Cp)) +\n  geom_line() + \n  geom_point(aes(x = min_cp, y = Cp[min_cp]),\n             color = \"red\", size = 4, shape = 20) + \n  labs(x = \"Number of Variables\", y = \"Cp\") +\n  theme_minimal()\n\nError in df %&gt;% ggplot(aes(x = Variables, y = Cp)): could not find function \"%&gt;%\"\n\ndf %&gt;%\n  ggplot(aes(x = Variables, y = BIC)) +\n  geom_line() + \n  geom_point(aes(x = min_bic, y = BIC[min_bic]),\n             color = \"red\", size = 4, shape = 20) + \n  labs(x = \"Number of Variables\", y = \"BIC\") +\n  theme_minimal()\n\nError in df %&gt;% ggplot(aes(x = Variables, y = BIC)): could not find function \"%&gt;%\"\n\n\nThe regsubsets() function has a built-in plot() command which can be used to display the selected variables for the best model with a given number of predictors, ranked according to the BIC, \\(C_p\\), adjusted \\(R^2\\), or AIC. To find out more about this function, type ?plot.regsubsets.\n\nplot(regfit.full, scale = \"r2\")\n\n\n\n\n\n\n\nplot(regfit.full, scale = \"adjr2\")\n\n\n\n\n\n\n\nplot(regfit.full, scale = \"Cp\")\n\n\n\n\n\n\n\nplot(regfit.full, scale = \"bic\")\n\n\n\n\n\n\n\n\nThe top row of each plot contains a black square for each variable selected according to the optimal model associated with that statistic. For instance, we see that several models share a BIC close to \\(-150\\). However, the model with the lowest BIC is the six-variable model that contains only AtBat, Hits, Walks, CRBI, DivisionW, and PutOuts. We can use the coef() function to see the coefficient estimates associated with this model.\n\ncoef(regfit.full, 6)\n\n (Intercept)        AtBat         Hits        Walks         CRBI    DivisionW \n  91.5117981   -1.8685892    7.6043976    3.6976468    0.6430169 -122.9515338 \n     PutOuts \n   0.2643076 \n\n\n\n\n\nWe can also use the regsubsets() function to perform forward stepwise or backward stepwise selection, using the argument method = \"forward\" or method = \"backward\".\n\nregfit.fwd &lt;- regsubsets(Salary ~ ., data = Hitters,\n                         nvmax = 19, method = \"forward\")\nsummary(regfit.fwd)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = \"forward\")\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: forward\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \"*\" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\nregfit.bwd &lt;- regsubsets(Salary ~ ., data = Hitters,\n                         nvmax = 19, method = \"backward\")\nsummary(regfit.bwd)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = \"backward\")\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: backward\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n4  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n7  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \"*\" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\n\nFor instance, we see that using forward stepwise selection, the best one-variable model contains only CRBI, and the best two-variable model additionally includes Hits. For this data, the best one-variable through six-variable models are each identical for best subset and forward selection. However, the best seven-variable models identified by forward stepwise selection, backward stepwise selection, and best subset selection are different.\n\ncoef(regfit.full, 7)\n\n (Intercept)         Hits        Walks       CAtBat        CHits       CHmRun \n  79.4509472    1.2833513    3.2274264   -0.3752350    1.4957073    1.4420538 \n   DivisionW      PutOuts \n-129.9866432    0.2366813 \n\ncoef(regfit.fwd, 7)\n\n (Intercept)        AtBat         Hits        Walks         CRBI       CWalks \n 109.7873062   -1.9588851    7.4498772    4.9131401    0.8537622   -0.3053070 \n   DivisionW      PutOuts \n-127.1223928    0.2533404 \n\ncoef(regfit.bwd, 7)\n\n (Intercept)        AtBat         Hits        Walks        CRuns       CWalks \n 105.6487488   -1.9762838    6.7574914    6.0558691    1.1293095   -0.7163346 \n   DivisionW      PutOuts \n-116.1692169    0.3028847 \n\n\n\n\n\nWe just saw that it is possible to choose among a set of models of different sizes using \\(C_p\\), BIC, and adjusted \\(R^2\\). We will now consider how to do this using the validation set and cross-validation approaches.\nIn order for these approaches to yield accurate estimates of the test error, we must use only the training observations to perform all aspects of model-fitting‚Äîincluding variable selection. Therefore, the determination of which model of a given size is best must be made using only the training observations. This point is subtle but important. If the full data set is used to perform the best subset selection step, the validation set errors and cross-validation errors that we obtain will not be accurate estimates of the test error.\n\nset.seed(1)\ntrain &lt;- sample(c(TRUE, FALSE),\n                nrow(Hitters),\n                replace = TRUE)\n\ntest &lt;- (!train)\n\nNow, we apply regsubsets() to the training set in order to perform best subset selection.\n\nregfit.best &lt;- regsubsets(Salary ~ .,\n                          data = Hitters[train, ],\n                          nvmax = 19)\n\nNotice that we subset the Hitters data frame directly in the call in order to access only the training subset of the data, using the expression Hitters[train, ]. We now compute the validation set error for the best model of each model size. We first make a model matrix from the test data.\n\ntest.mat &lt;- model.matrix(Salary ~ ., data = Hitters[test, ])\n\nThe model.matrix() function is used in many regression packages for building an ‚ÄúX‚Äù matrix from data. Now we run a loop, and for each size i, we extract the coefficients from regfit.best for the best model of that size, multiply them into the appropriate columns of the test model matrix to form the predictions, and compute the test MSE.\n\nval.errors &lt;- rep(NA, 19)\nfor (i in 1:19) {\n coefi &lt;- coef(regfit.best, id = i)\n pred &lt;- test.mat[, names(coefi)] %*% coefi\n val.errors[i] &lt;- mean((Hitters$Salary[test] - pred)^2)\n}\n\nWe find that the best model is the one that contains seven variables.\n\nval.errors\n\n [1] 164377.3 144405.5 152175.7 145198.4 137902.1 139175.7 126849.0 136191.4\n [9] 132889.6 135434.9 136963.3 140694.9 140690.9 141951.2 141508.2 142164.4\n[17] 141767.4 142339.6 142238.2\n\nwhich.min(val.errors)\n\n[1] 7\n\ncoef(regfit.best, 7)\n\n (Intercept)        AtBat         Hits        Walks        CRuns       CWalks \n  67.1085369   -2.1462987    7.0149547    8.0716640    1.2425113   -0.8337844 \n   DivisionW      PutOuts \n-118.4364998    0.2526925 \n\n\nThis was a little tedious, partly because there is no predict() method for regsubsets(). Since we will be using this function again, we can capture our steps above and write our own predict method.\n\n predict.regsubsets &lt;- function(object, newdata, id, ...) {\n  form &lt;- as.formula(object$call[[2]])\n  mat &lt;- model.matrix(form, newdata)\n  coefi &lt;- coef(object, id = id)\n  xvars &lt;- names(coefi)\n  mat[, xvars] %*% coefi\n }\n\nOur function pretty much mimics what we did above. The only complex part is how we extracted the formula used in the call to regsubsets(). We demonstrate how we use this function below, when we do cross-validation.\nFinally, we perform best subset selection on the full data set, and select the best seven-variable model. It is important that we make use of the full data set in order to obtain more accurate coefficient estimates.\nNote that we perform best subset selection on the full data set and select the best seven-variable model, rather than simply using the variables that were obtained from the training set, because the best seven-variable model on the full data set may differ from the corresponding model on the training set.\n\nregfit.best &lt;- regsubsets(Salary ~ ., data = Hitters,\n    nvmax = 19)\ncoef(regfit.best, 7)\n\n (Intercept)         Hits        Walks       CAtBat        CHits       CHmRun \n  79.4509472    1.2833513    3.2274264   -0.3752350    1.4957073    1.4420538 \n   DivisionW      PutOuts \n-129.9866432    0.2366813 \n\n\nIn fact, we see that the best seven-variable model on the full data set has a different set of variables than the best seven-variable model on the training set.\nWe now try to choose among the models of different sizes using cross-validation. This approach is somewhat involved, as we must perform best subset selection within each of the \\(k\\) training sets. Despite this, we see that with its clever subsetting syntax, R makes this job quite easy. First, we create a vector that allocates each observation to one of \\(k=10\\) folds, and we create a matrix in which we will store the results.\n\nk &lt;- 10\nn &lt;- nrow(Hitters)\nset.seed(1)\nfolds &lt;- sample(rep(1:k, length = n))\ncv.errors &lt;- matrix(NA, k, 19,\n    dimnames = list(NULL, paste(1:19)))\n\nNow we write a for loop that performs cross-validation. In the \\(j\\)th fold, the elements of folds that equal j are in the test set, and the remainder are in the training set. We make our predictions for each model size (using our new predict() method), compute the test errors on the appropriate subset, and store them in the appropriate slot in the matrix cv.errors. Note that in the following code R will automatically use our predict.regsubsets() function when we call predict() because the best.fit object has class regsubsets.\n\nfor (j in 1:k) {\n  best.fit &lt;- regsubsets(Salary ~ .,\n       data = Hitters[folds != j, ],\n       nvmax = 19)\n  for (i in 1:19) {\n    pred &lt;- predict(best.fit, Hitters[folds == j, ], id = i)\n    cv.errors[j, i] &lt;-\n         mean((Hitters$Salary[folds == j] - pred)^2)\n   }\n }\n\nThis has given us a \\(10 \\times 19\\) matrix, of which the \\((j,i)\\)th element corresponds to the test MSE for the \\(j\\)th cross-validation fold for the best \\(i\\)-variable model. We use the apply() function to average over the columns of this matrix in order to obtain a vector for which the \\(i\\)th element is the cross-validation error for the \\(i\\)-variable model.\n\nmean.cv.errors &lt;- apply(cv.errors, 2, mean)\nmean.cv.errors\n\n       1        2        3        4        5        6        7        8 \n143439.8 126817.0 134214.2 131782.9 130765.6 120382.9 121443.1 114363.7 \n       9       10       11       12       13       14       15       16 \n115163.1 109366.0 112738.5 113616.5 115557.6 115853.3 115630.6 116050.0 \n      17       18       19 \n116117.0 116419.3 116299.1 \n\npar(mfrow = c(1, 1))\nplot(mean.cv.errors, type = \"b\")\n\n\n\n\n\n\n\n\nWe see that cross-validation selects a 10-variable model. We now perform best subset selection on the full data set in order to obtain the 10-variable model.\n\nreg.best &lt;- regsubsets(Salary ~ ., data = Hitters,\n    nvmax = 19)\ncoef(reg.best, 10)\n\n (Intercept)        AtBat         Hits        Walks       CAtBat        CRuns \n 162.5354420   -2.1686501    6.9180175    5.7732246   -0.1300798    1.4082490 \n        CRBI       CWalks    DivisionW      PutOuts      Assists \n   0.7743122   -0.8308264 -112.3800575    0.2973726    0.2831680"
  },
  {
    "objectID": "teaching/stat-learn/material/07_model_selection_regularization.html#ridge-regression-and-the-lasso",
    "href": "teaching/stat-learn/material/07_model_selection_regularization.html#ridge-regression-and-the-lasso",
    "title": "Linear Models and Regularization Methods",
    "section": "Ridge Regression and the Lasso",
    "text": "Ridge Regression and the Lasso\nWe will use the glmnet package in order to perform ridge regression and the lasso. The main function in this package is glmnet(), which can be used to fit ridge regression models, lasso models, and more. This function has slightly different syntax from other model-fitting functions that we have encountered thus far in this book. In particular, we must pass in an x matrix as well as a y vector, and we do not use the y ~ x syntax. We will now perform ridge regression and the lasso in order to predict Salary on the Hitters data. Before proceeding ensure that the missing values have been removed from the data, as described in Section 6.5.1.\n\nx &lt;- model.matrix(Salary ~ ., Hitters)[, -1]\ny &lt;- Hitters$Salary\n\nThe model.matrix() function is particularly useful for creating x; not only does it produce a matrix corresponding to the \\(19\\) predictors but it also automatically transforms any qualitative variables into dummy variables. The latter property is important because glmnet() can only take numerical, quantitative inputs.\n\nRidge Regression\nThe glmnet() function has an alpha argument that determines what type of model is fit. If alpha=0 then a ridge regression model is fit, and if alpha=1 then a lasso model is fit. We first fit a ridge regression model.\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-8\n\ngrid &lt;- 10^seq(10, -2, length = 100)\nridge.mod &lt;- glmnet(x, y, alpha = 0, lambda = grid)\n\nBy default the glmnet() function performs ridge regression for an automatically selected range of \\(\\lambda\\) values. However, here we have chosen to implement the function over a grid of values ranging from \\(\\lambda=10^{10}\\) to \\(\\lambda=10^{-2}\\), essentially covering the full range of scenarios from the null model containing only the intercept, to the least squares fit. As we will see, we can also compute model fits for a particular value of \\(\\lambda\\) that is not one of the original grid values. Note that by default, the glmnet() function standardizes the variables so that they are on the same scale. To turn off this default setting, use the argument standardize = FALSE.\nAssociated with each value of \\(\\lambda\\) is a vector of ridge regression coefficients, stored in a matrix that can be accessed by coef(). In this case, it is a \\(20 \\times 100\\) matrix, with \\(20\\) rows (one for each predictor, plus an intercept) and \\(100\\) columns (one for each value of \\(\\lambda\\)).\n\ndim(coef(ridge.mod))\n\n[1]  20 100\n\n\nWe expect the coefficient estimates to be much smaller, in terms of \\(\\ell_2\\) norm, when a large value of \\(\\lambda\\) is used, as compared to when a small value of \\(\\lambda\\) is used. These are the coefficients when \\(\\lambda=11{,}498\\), along with their \\(\\ell_2\\) norm:\n\nridge.mod$lambda[50]\n\n[1] 11497.57\n\ncoef(ridge.mod)[, 50]\n\n  (Intercept)         AtBat          Hits         HmRun          Runs \n407.356050200   0.036957182   0.138180344   0.524629976   0.230701523 \n          RBI         Walks         Years        CAtBat         CHits \n  0.239841459   0.289618741   1.107702929   0.003131815   0.011653637 \n       CHmRun         CRuns          CRBI        CWalks       LeagueN \n  0.087545670   0.023379882   0.024138320   0.025015421   0.085028114 \n    DivisionW       PutOuts       Assists        Errors    NewLeagueN \n -6.215440973   0.016482577   0.002612988  -0.020502690   0.301433531 \n\nsqrt(sum(coef(ridge.mod)[-1, 50]^2))\n\n[1] 6.360612\n\n\nIn contrast, here are the coefficients when \\(\\lambda=705\\), along with their \\(\\ell_2\\) norm. Note the much larger \\(\\ell_2\\) norm of the coefficients associated with this smaller value of \\(\\lambda\\).\n\nridge.mod$lambda[60]\n\n[1] 705.4802\n\ncoef(ridge.mod)[, 60]\n\n (Intercept)        AtBat         Hits        HmRun         Runs          RBI \n 54.32519950   0.11211115   0.65622409   1.17980910   0.93769713   0.84718546 \n       Walks        Years       CAtBat        CHits       CHmRun        CRuns \n  1.31987948   2.59640425   0.01083413   0.04674557   0.33777318   0.09355528 \n        CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists \n  0.09780402   0.07189612  13.68370191 -54.65877750   0.11852289   0.01606037 \n      Errors   NewLeagueN \n -0.70358655   8.61181213 \n\nsqrt(sum(coef(ridge.mod)[-1, 60]^2))\n\n[1] 57.11001\n\n\nWe can use the predict() function for a number of purposes. For instance, we can obtain the ridge regression coefficients for a new value of \\(\\lambda\\), say \\(50\\):\n\npredict(ridge.mod, s = 50, type = \"coefficients\")[1:20, ]\n\n  (Intercept)         AtBat          Hits         HmRun          Runs \n 4.876610e+01 -3.580999e-01  1.969359e+00 -1.278248e+00  1.145892e+00 \n          RBI         Walks         Years        CAtBat         CHits \n 8.038292e-01  2.716186e+00 -6.218319e+00  5.447837e-03  1.064895e-01 \n       CHmRun         CRuns          CRBI        CWalks       LeagueN \n 6.244860e-01  2.214985e-01  2.186914e-01 -1.500245e-01  4.592589e+01 \n    DivisionW       PutOuts       Assists        Errors    NewLeagueN \n-1.182011e+02  2.502322e-01  1.215665e-01 -3.278600e+00 -9.496680e+00 \n\n\nWe now split the samples into a training set and a test set in order to estimate the test error of ridge regression and the lasso. There are two common ways to randomly split a data set. The first is to produce a random vector of TRUE, FALSE elements and select the observations corresponding to TRUE for the training data. The second is to randomly choose a subset of numbers between \\(1\\) and \\(n\\); these can then be used as the indices for the training observations. The two approaches work equally well. We used the former method in Section 6.5.1. Here we demonstrate the latter approach.\nWe first set a random seed so that the results obtained will be reproducible.\n\nset.seed(1)\ntrain &lt;- sample(1:nrow(x), nrow(x) / 2)\ntest &lt;- (-train)\ny.test &lt;- y[test]\n\nNext we fit a ridge regression model on the training set, and evaluate its MSE on the test set, using \\(\\lambda=4\\). Note the use of the predict() function again. This time we get predictions for a test set, by replacing type=\"coefficients\" with the newx argument.\n\nridge.mod &lt;- glmnet(x[train, ], y[train], alpha = 0,\n    lambda = grid, thresh = 1e-12)\n\nridge.pred &lt;- predict(ridge.mod, s = 4, newx = x[test, ])\nmean((ridge.pred - y.test)^2)\n\n[1] 142199.2\n\n\nThe test MSE is \\(142{,}199\\). Note that if we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations. In that case, we could compute the test set MSE like this:\n\nmean((mean(y[train]) - y.test)^2)\n\n[1] 224669.9\n\n\nWe could also get the same result by fitting a ridge regression model with a very large value of \\(\\lambda\\). Note that 1e10 means \\(10^{10}\\).\n\nridge.pred &lt;- predict(ridge.mod, s = 1e10, newx = x[test, ])\nmean((ridge.pred - y.test)^2)\n\n[1] 224669.8\n\n\nSo fitting a ridge regression model with \\(\\lambda=4\\) leads to a much lower test MSE than fitting a model with just an intercept. We now check whether there is any benefit to performing ridge regression with \\(\\lambda=4\\) instead of just performing least squares regression. Recall that least squares is simply ridge regression with \\(\\lambda=0\\). (In order for glmnet() to yield the exact least squares coefficients when \\(\\lambda=0\\), we use the argument exact = T when calling the predict() function. Otherwise, the predict() function will interpolate over the grid of \\(\\lambda\\) values used in fitting the glmnet() model, yielding approximate results. When we use exact = T, there remains a slight discrepancy in the third decimal place between the output of glmnet() when \\(\\lambda = 0\\) and the output of lm(); this is due to numerical approximation on the part of glmnet().)\n\nridge.pred &lt;- predict(ridge.mod, s = 0, newx = x[test, ],\n    exact = T, x = x[train, ], y = y[train])\nmean((ridge.pred - y.test)^2)\n\n[1] 168588.6\n\nlm(y ~ x, subset = train)\n\n\nCall:\nlm(formula = y ~ x, subset = train)\n\nCoefficients:\n(Intercept)       xAtBat        xHits       xHmRun        xRuns         xRBI  \n   274.0145      -0.3521      -1.6377       5.8145       1.5424       1.1243  \n     xWalks       xYears      xCAtBat       xCHits      xCHmRun       xCRuns  \n     3.7287     -16.3773      -0.6412       3.1632       3.4008      -0.9739  \n      xCRBI      xCWalks     xLeagueN   xDivisionW     xPutOuts     xAssists  \n    -0.6005       0.3379     119.1486    -144.0831       0.1976       0.6804  \n    xErrors  xNewLeagueN  \n    -4.7128     -71.0951  \n\npredict(ridge.mod, s = 0, exact = T, type = \"coefficients\",\n    x = x[train, ], y = y[train])[1:20, ]\n\n (Intercept)        AtBat         Hits        HmRun         Runs          RBI \n 274.0200994   -0.3521900   -1.6371383    5.8146692    1.5423361    1.1241837 \n       Walks        Years       CAtBat        CHits       CHmRun        CRuns \n   3.7288406  -16.3795195   -0.6411235    3.1629444    3.4005281   -0.9739405 \n        CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists \n  -0.6003976    0.3378422  119.1434637 -144.0853061    0.1976300    0.6804200 \n      Errors   NewLeagueN \n  -4.7127879  -71.0898914 \n\n\nIn general, if we want to fit a (unpenalized) least squares model, then we should use the lm() function, since that function provides more useful outputs, such as standard errors and p-values for the coefficients.\nIn general, instead of arbitrarily choosing \\(\\lambda=4\\), it would be better to use cross-validation to choose the tuning parameter \\(\\lambda\\). We can do this using the built-in cross-validation function, cv.glmnet(). By default, the function performs ten-fold cross-validation, though this can be changed using the argument nfolds. Note that we set a random seed first so our results will be reproducible, since the choice of the cross-validation folds is random.\n\nset.seed(1)\ncv.out &lt;- cv.glmnet(x[train, ], y[train], alpha = 0)\nplot(cv.out)\n\n\n\n\n\n\n\nbestlam &lt;- cv.out$lambda.min\nbestlam\n\n[1] 326.0828\n\n\nTherefore, we see that the value of \\(\\lambda\\) that results in the smallest cross-validation error is \\(326\\). What is the test MSE associated with this value of \\(\\lambda\\)?\n\nridge.pred &lt;- predict(ridge.mod, s = bestlam,\n    newx = x[test, ])\nmean((ridge.pred - y.test)^2)\n\n[1] 139856.6\n\n\nThis represents a further improvement over the test MSE that we got using \\(\\lambda=4\\). Finally, we refit our ridge regression model on the full data set, using the value of \\(\\lambda\\) chosen by cross-validation, and examine the coefficient estimates.\n\nout &lt;- glmnet(x, y, alpha = 0)\npredict(out, type = \"coefficients\", s = bestlam)[1:20, ]\n\n (Intercept)        AtBat         Hits        HmRun         Runs          RBI \n 15.44383120   0.07715547   0.85911582   0.60103106   1.06369007   0.87936105 \n       Walks        Years       CAtBat        CHits       CHmRun        CRuns \n  1.62444617   1.35254778   0.01134999   0.05746654   0.40680157   0.11456224 \n        CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists \n  0.12116504   0.05299202  22.09143197 -79.04032656   0.16619903   0.02941950 \n      Errors   NewLeagueN \n -1.36092945   9.12487765 \n\n\nAs expected, none of the coefficients are zero‚Äîridge regression does not perform variable selection!\n\n\nThe Lasso\nWe saw that ridge regression with a wise choice of \\(\\lambda\\) can outperform least squares as well as the null model on the Hitters data set. We now ask whether the lasso can yield either a more accurate or a more interpretable model than ridge regression. In order to fit a lasso model, we once again use the glmnet() function; however, this time we use the argument alpha=1. Other than that change, we proceed just as we did in fitting a ridge model.\n\nlasso.mod &lt;- glmnet(x[train, ], y[train], alpha = 1,\n    lambda = grid)\nplot(lasso.mod)\n\nWarning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\ncollapsing to unique 'x' values\n\n\n\n\n\n\n\n\n\nWe can see from the coefficient plot that depending on the choice of tuning parameter, some of the coefficients will be exactly equal to zero. We now perform cross-validation and compute the associated test error.\n\nset.seed(1)\ncv.out &lt;- cv.glmnet(x[train, ], y[train], alpha = 1)\nplot(cv.out)\n\n\n\n\n\n\n\nbestlam &lt;- cv.out$lambda.min\nlasso.pred &lt;- predict(lasso.mod, s = bestlam,\n    newx = x[test, ])\nmean((lasso.pred - y.test)^2)\n\n[1] 143673.6\n\n\nThis is substantially lower than the test set MSE of the null model and of least squares, and very similar to the test MSE of ridge regression with \\(\\lambda\\) chosen by cross-validation.\nHowever, the lasso has a substantial advantage over ridge regression in that the resulting coefficient estimates are sparse. Here we see that 8 of the 19 coefficient estimates are exactly zero. So the lasso model with \\(\\lambda\\) chosen by cross-validation contains only eleven variables.\n\nout &lt;- glmnet(x, y, alpha = 1, lambda = grid)\nlasso.coef &lt;- predict(out, type = \"coefficients\",\n    s = bestlam)[1:20, ]\nlasso.coef\n\n  (Intercept)         AtBat          Hits         HmRun          Runs \n   1.27479059   -0.05497143    2.18034583    0.00000000    0.00000000 \n          RBI         Walks         Years        CAtBat         CHits \n   0.00000000    2.29192406   -0.33806109    0.00000000    0.00000000 \n       CHmRun         CRuns          CRBI        CWalks       LeagueN \n   0.02825013    0.21628385    0.41712537    0.00000000   20.28615023 \n    DivisionW       PutOuts       Assists        Errors    NewLeagueN \n-116.16755870    0.23752385    0.00000000   -0.85629148    0.00000000 \n\nlasso.coef[lasso.coef != 0]\n\n  (Intercept)         AtBat          Hits         Walks         Years \n   1.27479059   -0.05497143    2.18034583    2.29192406   -0.33806109 \n       CHmRun         CRuns          CRBI       LeagueN     DivisionW \n   0.02825013    0.21628385    0.41712537   20.28615023 -116.16755870 \n      PutOuts        Errors \n   0.23752385   -0.85629148"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "‚ÄúThe Interplay of Structural Features and Observed Dissimilarities Among Centrality Indices‚Äù\n\n\nThe XLIV Social Networks Conference of INSNA\n\n\n\nJun 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Analysis & Modeling of Multivariate Networks\n\n\nInaugural Lecture, University of Konstanz\n\n\n\nJun 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚ÄúAnalyzing Social Structure using Multigraph Representations‚Äù\n\n\nPresentation @ Centre Marc Bloch, Berlin\n\n\n\nNov 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚ÄúStatistical Analysis of Multivariate Egocentric Networks‚Äù\n\n\nThe XLII Social Networks Conference of INSNA\n\n\n\nJul 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚ÄúStatistical Entropy Analysis of Network Data‚Äù\n\n\nThe Women in Network Science (WiNS) seminar\n\n\n\nMay 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiplexity Analysis of Networks using Multigraph Representations\n\n\nThe 5th European Conference on Social Networks\n\n\n\nSep 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚ÄúGoodness of Fit Tests for Random Multigraph Models‚Äù\n\n\nNetwork 2021 - A Joint Sunbelt and NetSci Conference\n\n\n\nJul 6, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚ÄúGender Dependent Structures of Dialogue Networks in Films‚Äù\n\n\nThe 4th European Conference on Social Networks\n\n\n\nSep 9, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "",
    "section": "",
    "text": "Hello World\n\n\n\ntest\n\n\n\nThis is a test entry‚Ä¶\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nSubtitle\n\n\n\n\n\n\nData Science with Tidyverse\n\n\nMSc course\n\n\n\n\nMathematics for Social Scientists\n\n\nMSc course\n\n\n\n\nStatistical Learning\n\n\nMSc course\n\n\n\n\n\nNo matching items"
  }
]