[
  {
    "objectID": "rpackage/index.html",
    "href": "rpackage/index.html",
    "title": "R packages",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "teaching/stat-learn/material/06_Model_Validation_Resampling.html",
    "href": "teaching/stat-learn/material/06_Model_Validation_Resampling.html",
    "title": "Cross-Validation and the Bootstrap",
    "section": "",
    "text": "In this lab, we explore the resampling techniques covered in this chapter. Some of the commands in this lab may take a while to run on your computer.\n\nlibrary(ISLR2)\nset.seed(1)\nattach(Auto)\n\n\n\nThe LOOCV estimate can be automatically computed for any generalized linear model using the glm() and cv.glm() functions. In the lab for Chapter 4, we used the glm() function to perform logistic regression by passing in the family = \"binomial\" argument. But if we use glm() to fit a model without passing in the family argument, then it performs linear regression, just like the lm() function.\n\nlibrary(boot)\nglm.fit &lt;- glm(mpg ~ horsepower, data = Auto)\ncv.err &lt;- cv.glm(Auto, glm.fit)\ncv.err$delta\n\n[1] 24.23151 24.23114\n\n\nThe cv.glm() function produces a list with several components. The two numbers in the delta vector contain the cross-validation results. In this case the numbers are identical (up to two decimal places) and correspond to the LOOCV statistic given in (5.1). Below, we discuss a situation in which the two numbers differ. Our cross-validation estimate for the test error is approximately \\(24.23\\).\nWe can repeat this procedure for increasingly complex polynomial fits. To automate the process, we use the for() function to initiate a for loop which iteratively fits polynomial regressions for polynomials of order \\(i=1\\) to \\(i=10\\), computes the associated cross-validation error, and stores it in the \\(i\\)th element of the vector cv.error. We begin by initializing the vector.\n\ncv.error &lt;- rep(0, 10)\nfor (i in 1:10) {\n  glm.fit &lt;- glm(mpg ~ poly(horsepower, i), data = Auto)\n  cv.error[i] &lt;- cv.glm(Auto, glm.fit)$delta[1]\n}\ncv.error\n\n [1] 24.23151 19.24821 19.33498 19.42443 19.03321 18.97864 18.83305 18.96115\n [9] 19.06863 19.49093\n\n\n\n\n\nThe cv.glm() function can also be used to implement \\(k\\)-fold CV. Below we use \\(k=10\\), a common choice for \\(k\\), on the Auto data set. We once again set a random seed and initialize a vector in which we will store the CV errors corresponding to the polynomial fits of orders one to ten.\n\nset.seed(17)\ncv.error.10 &lt;- rep(0, 10)\nfor (i in 1:10) {\n  glm.fit &lt;- glm(mpg ~ poly(horsepower, i), data = Auto)\n  cv.error.10[i] &lt;- cv.glm(Auto, glm.fit, K = 10)$delta[1]\n}\ncv.error.10\n\n [1] 24.27207 19.26909 19.34805 19.29496 19.03198 18.89781 19.12061 19.14666\n [9] 18.87013 20.95520\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\n\n\ncvviz = data.frame(err_LOOCV = cv.error, err_K10 = cv.error.10, poly_degree = seq(1:10))\ncvviz = cvviz %&gt;%\n  pivot_longer(cols = starts_with(\"err\"),\n               names_to = \"Model\",\n               values_to = \"Error\")\n\ncvviz %&gt;%\n  ggplot(aes(x = poly_degree, y = Error, color = Model)) +\n  geom_point() +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNotice that the computation time is shorter than that of LOOCV. (In principle, the computation time for LOOCV for a least squares linear model should be faster than for \\(k\\)-fold CV, due to the availability of the formula (5.2) for LOOCV; however, unfortunately the cv.glm() function does not make use of this formula.) We still see little evidence that using cubic or higher-order polynomial terms leads to lower test error than simply using a quadratic fit.\n\n\n\nWe illustrate the use of the bootstrap in the simple example of Section 5.2, as well as on an example involving estimating the accuracy of the linear regression model on the Auto data set.\n\n\nOne of the great advantages of the bootstrap approach is that it can be applied in almost all situations. No complicated mathematical calculations are required. Performing a bootstrap analysis in R entails only two steps. First, we must create a function that computes the statistic of interest. Second, we use the boot() function, which is part of the boot library, to perform the bootstrap by repeatedly sampling observations from the data set with replacement.\nThe Portfolio data set in the ISLR2 package is simulated data of \\(100\\) pairs of returns, generated in the fashion described in Section 5.2. To illustrate the use of the bootstrap on this data, we must first create a function.\nalpha.fn(), takes as input the \\((X,Y)\\) data as well as a vector indicating which observations should be used to estimate \\(\\alpha\\). The function then outputs the estimate for \\(\\alpha\\) based on the selected observations.\n\nalpha.fn &lt;- function(data, index) {\n  X &lt;- data$X[index]\n  Y &lt;- data$Y[index]\n  (var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2 * cov(X, Y))\n}\n\nThis function returns, or outputs, an estimate for \\(\\alpha\\) based on applying (5.7) to the observations indexed by the argument index. For instance, the following command tells R to estimate \\(\\alpha\\) using all \\(100\\) observations.\n\nalpha.fn(Portfolio, 1:100)\n\n[1] 0.5758321\n\n\nThe next command uses the sample() function to randomly select \\(100\\) observations from the range \\(1\\) to \\(100\\), with replacement. This is equivalent to constructing a new bootstrap data set and recomputing \\(\\hat{\\alpha}\\) based on the new data set.\n\nset.seed(7)\nalpha.fn(Portfolio, sample(100, 100, replace = T))\n\n[1] 0.5385326\n\n\nWe can implement a bootstrap analysis by performing this command many times, recording all of the corresponding estimates for \\(\\alpha\\), and computing the resulting standard deviation. However, the boot() function automates this approach. Below we produce \\(R=1,000\\) bootstrap estimates for \\(\\alpha\\).\n\nboot(Portfolio, alpha.fn, R = 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Portfolio, statistic = alpha.fn, R = 1000)\n\n\nBootstrap Statistics :\n     original       bias    std. error\nt1* 0.5758321 0.0007959475  0.08969074\n\n\nThe final output shows that using the original data, \\(\\hat{\\alpha}=0.5758\\), and that the bootstrap estimate for \\({\\rm SE}(\\hat{\\alpha})\\) is \\(0.0897\\).\n\n\n\nThe bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method. Here we use the bootstrap approach in order to assess the variability of the estimates for \\(\\beta_0\\) and \\(\\beta_1\\), the intercept and slope terms for the linear regression model that uses horsepower to predict mpg in the Auto data set. We will compare the estimates obtained using the bootstrap to those obtained using the formulas for \\({\\rm SE}(\\hat{\\beta}_0)\\) and \\({\\rm SE}(\\hat{\\beta}_1)\\) described in Section 3.1.2.\nWe first create a simple function, boot.fn(), which takes in the Auto data set as well as a set of indices for the observations, and returns the intercept and slope estimates for the linear regression model. We then apply this function to the full set of \\(392\\) observations in order to compute the estimates of \\(\\beta_0\\) and \\(\\beta_1\\) on the entire data set using the usual linear regression coefficient estimate formulas from Chapter 3. Note that we do not need the { and } at the beginning and end of the function because it is only one line long.\n\nboot.fn &lt;- function(data, index)\n  coef(lm(mpg ~ horsepower, data = data, subset = index))\nboot.fn(Auto, 1:392)\n\n(Intercept)  horsepower \n 39.9358610  -0.1578447 \n\n\nThe boot.fn() function can also be used in order to create bootstrap estimates for the intercept and slope terms by randomly sampling from among the observations with replacement. Here we give two examples.\n\nset.seed(1)\nboot.fn(Auto, sample(392, 392, replace = T))\n\n(Intercept)  horsepower \n 40.3404517  -0.1634868 \n\nboot.fn(Auto, sample(392, 392, replace = T))\n\n(Intercept)  horsepower \n 40.1186906  -0.1577063 \n\n\nNext, we use the boot() function to compute the standard errors of 1,000 bootstrap estimates for the intercept and slope terms.\n\nboot(Auto, boot.fn, 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot.fn, R = 1000)\n\n\nBootstrap Statistics :\n      original        bias    std. error\nt1* 39.9358610  0.0544513229 0.841289790\nt2* -0.1578447 -0.0006170901 0.007343073\n\n\nThis indicates that the bootstrap estimate for \\({\\rm SE}(\\hat{\\beta}_0)\\) is \\(0.84\\), and that the bootstrap estimate for \\({\\rm SE}(\\hat{\\beta}_1)\\) is \\(0.0073\\). As discussed in Section 3.1.2, standard formulas can be used to compute the standard errors for the regression coefficients in a linear model. These can be obtained using the summary() function.\n\nsummary(lm(mpg ~ horsepower, data = Auto))$coef\n\n              Estimate  Std. Error   t value      Pr(&gt;|t|)\n(Intercept) 39.9358610 0.717498656  55.65984 1.220362e-187\nhorsepower  -0.1578447 0.006445501 -24.48914  7.031989e-81\n\n\nThe standard error estimates for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) obtained using the formulas from Section 3.1.2 are \\(0.717\\) for the intercept and \\(0.0064\\) for the slope. Interestingly, these are somewhat different from the estimates obtained using the bootstrap. Does this indicate a problem with the bootstrap? In fact, it suggests the opposite. Recall that the standard formulas given in Equation 3.8 on page 66 rely on certain assumptions. For example, they depend on the unknown parameter \\(\\sigma^2\\), the noise variance. We then estimate \\(\\sigma^2\\) using the RSS. Now although the formulas for the standard errors do not rely on the linear model being correct, the estimate for \\(\\sigma^2\\) does. We see in Figure 3.8 on page 92 that there is a non-linear relationship in the data, and so the residuals from a linear fit will be inflated, and so will \\(\\hat{\\sigma}^2\\). Secondly, the standard formulas assume (somewhat unrealistically) that the \\(x_i\\) are fixed, and all the variability comes from the variation in the errors \\(\\epsilon_i\\). The bootstrap approach does not rely on any of these assumptions, and so it is likely giving a more accurate estimate of the standard errors of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) than is the summary() function.\nBelow we compute the bootstrap standard error estimates and the standard linear regression estimates that result from fitting the quadratic model to the data. Since this model provides a good fit to the data (Figure 3.8), there is now a better correspondence between the bootstrap estimates and the standard estimates of \\({\\rm SE}(\\hat{\\beta}_0)\\), \\({\\rm SE}(\\hat{\\beta}_1)\\) and \\({\\rm SE}(\\hat{\\beta}_2)\\).\n\nboot.fn &lt;- function(data, index)\n  coef(\n      lm(mpg ~ horsepower + I(horsepower^2), \n        data = data, subset = index)\n    )\nset.seed(1)\nboot(Auto, boot.fn, 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot.fn, R = 1000)\n\n\nBootstrap Statistics :\n        original        bias     std. error\nt1* 56.900099702  3.511640e-02 2.0300222526\nt2* -0.466189630 -7.080834e-04 0.0324241984\nt3*  0.001230536  2.840324e-06 0.0001172164\n\nsummary(\n    lm(mpg ~ horsepower + I(horsepower^2), data = Auto)\n  )$coef\n\n                    Estimate   Std. Error   t value      Pr(&gt;|t|)\n(Intercept)     56.900099702 1.8004268063  31.60367 1.740911e-109\nhorsepower      -0.466189630 0.0311246171 -14.97816  2.289429e-40\nI(horsepower^2)  0.001230536 0.0001220759  10.08009  2.196340e-21\n\n\n\n\n\n\nUse the Boston data set.\nImportant: Any function that you pass to boot needs to use indeces\n\nBased on this data set, provide an estimate for the population mean of medv. Call this estimate \\(\\hat{\\mu}\\)\n\n\nProvide an estimate of the standard error of \\(\\hat{\\mu}\\) . Interpret this result.\nHint: We can compute the standard error of the sample mean by dividing the sample standard deviation by the square root of the number of observations.\n\n\nNow estimate the standard error of \\(\\hat{\\mu}\\) using the bootstrap. How does this compare to your answer from (b)?\n\n\nBased on your bootstrap estimate from (c), provide a 95% confidence interval for the mean of medv. Compare it to the results obtained using t.test(Boston$medv).\nHint: You can approximate a 95% confidence interval using the formula\n[\\(\\hat{\\mu}\\) - 2SE(\\(\\hat{\\mu}\\)), \\(\\hat{\\mu}\\) + 2SE(\\(\\hat{\\mu}\\))] .\n\n\nBased on this data set, provide an estimate, \\(\\hat{\\mu}_{med}\\), for the median value of `medv` in the population.\n\n\nWe now would like to estimate the standard error of \\(\\hat{\\mu}_{med}\\). Unfortunately, there is no simple formula for computing the standard error of the median. Instead, estimate the standard error of the median using the bootstrap. Comment on your findings.\n\n\nBased on this data set, provide an estimate for the tenth per centile of medv in Boston census tracts. Call this quantity \\(\\hat{\\mu}_{0.1}\\). (You can use the quantile() function.)\n\n\nUse the bootstrap to estimate the standard error of \\(\\hat{\\mu}_{0.1}\\). Comment on your findings."
  },
  {
    "objectID": "teaching/stat-learn/material/06_Model_Validation_Resampling.html#leave-one-out-cross-validation",
    "href": "teaching/stat-learn/material/06_Model_Validation_Resampling.html#leave-one-out-cross-validation",
    "title": "Cross-Validation and the Bootstrap",
    "section": "",
    "text": "The LOOCV estimate can be automatically computed for any generalized linear model using the glm() and cv.glm() functions. In the lab for Chapter 4, we used the glm() function to perform logistic regression by passing in the family = \"binomial\" argument. But if we use glm() to fit a model without passing in the family argument, then it performs linear regression, just like the lm() function.\n\nlibrary(boot)\nglm.fit &lt;- glm(mpg ~ horsepower, data = Auto)\ncv.err &lt;- cv.glm(Auto, glm.fit)\ncv.err$delta\n\n[1] 24.23151 24.23114\n\n\nThe cv.glm() function produces a list with several components. The two numbers in the delta vector contain the cross-validation results. In this case the numbers are identical (up to two decimal places) and correspond to the LOOCV statistic given in (5.1). Below, we discuss a situation in which the two numbers differ. Our cross-validation estimate for the test error is approximately \\(24.23\\).\nWe can repeat this procedure for increasingly complex polynomial fits. To automate the process, we use the for() function to initiate a for loop which iteratively fits polynomial regressions for polynomials of order \\(i=1\\) to \\(i=10\\), computes the associated cross-validation error, and stores it in the \\(i\\)th element of the vector cv.error. We begin by initializing the vector.\n\ncv.error &lt;- rep(0, 10)\nfor (i in 1:10) {\n  glm.fit &lt;- glm(mpg ~ poly(horsepower, i), data = Auto)\n  cv.error[i] &lt;- cv.glm(Auto, glm.fit)$delta[1]\n}\ncv.error\n\n [1] 24.23151 19.24821 19.33498 19.42443 19.03321 18.97864 18.83305 18.96115\n [9] 19.06863 19.49093"
  },
  {
    "objectID": "teaching/stat-learn/material/06_Model_Validation_Resampling.html#k-fold-cross-validation",
    "href": "teaching/stat-learn/material/06_Model_Validation_Resampling.html#k-fold-cross-validation",
    "title": "Cross-Validation and the Bootstrap",
    "section": "",
    "text": "The cv.glm() function can also be used to implement \\(k\\)-fold CV. Below we use \\(k=10\\), a common choice for \\(k\\), on the Auto data set. We once again set a random seed and initialize a vector in which we will store the CV errors corresponding to the polynomial fits of orders one to ten.\n\nset.seed(17)\ncv.error.10 &lt;- rep(0, 10)\nfor (i in 1:10) {\n  glm.fit &lt;- glm(mpg ~ poly(horsepower, i), data = Auto)\n  cv.error.10[i] &lt;- cv.glm(Auto, glm.fit, K = 10)$delta[1]\n}\ncv.error.10\n\n [1] 24.27207 19.26909 19.34805 19.29496 19.03198 18.89781 19.12061 19.14666\n [9] 18.87013 20.95520\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\n\n\ncvviz = data.frame(err_LOOCV = cv.error, err_K10 = cv.error.10, poly_degree = seq(1:10))\ncvviz = cvviz %&gt;%\n  pivot_longer(cols = starts_with(\"err\"),\n               names_to = \"Model\",\n               values_to = \"Error\")\n\ncvviz %&gt;%\n  ggplot(aes(x = poly_degree, y = Error, color = Model)) +\n  geom_point() +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNotice that the computation time is shorter than that of LOOCV. (In principle, the computation time for LOOCV for a least squares linear model should be faster than for \\(k\\)-fold CV, due to the availability of the formula (5.2) for LOOCV; however, unfortunately the cv.glm() function does not make use of this formula.) We still see little evidence that using cubic or higher-order polynomial terms leads to lower test error than simply using a quadratic fit."
  },
  {
    "objectID": "teaching/stat-learn/material/06_Model_Validation_Resampling.html#the-bootstrap",
    "href": "teaching/stat-learn/material/06_Model_Validation_Resampling.html#the-bootstrap",
    "title": "Cross-Validation and the Bootstrap",
    "section": "",
    "text": "We illustrate the use of the bootstrap in the simple example of Section 5.2, as well as on an example involving estimating the accuracy of the linear regression model on the Auto data set.\n\n\nOne of the great advantages of the bootstrap approach is that it can be applied in almost all situations. No complicated mathematical calculations are required. Performing a bootstrap analysis in R entails only two steps. First, we must create a function that computes the statistic of interest. Second, we use the boot() function, which is part of the boot library, to perform the bootstrap by repeatedly sampling observations from the data set with replacement.\nThe Portfolio data set in the ISLR2 package is simulated data of \\(100\\) pairs of returns, generated in the fashion described in Section 5.2. To illustrate the use of the bootstrap on this data, we must first create a function.\nalpha.fn(), takes as input the \\((X,Y)\\) data as well as a vector indicating which observations should be used to estimate \\(\\alpha\\). The function then outputs the estimate for \\(\\alpha\\) based on the selected observations.\n\nalpha.fn &lt;- function(data, index) {\n  X &lt;- data$X[index]\n  Y &lt;- data$Y[index]\n  (var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2 * cov(X, Y))\n}\n\nThis function returns, or outputs, an estimate for \\(\\alpha\\) based on applying (5.7) to the observations indexed by the argument index. For instance, the following command tells R to estimate \\(\\alpha\\) using all \\(100\\) observations.\n\nalpha.fn(Portfolio, 1:100)\n\n[1] 0.5758321\n\n\nThe next command uses the sample() function to randomly select \\(100\\) observations from the range \\(1\\) to \\(100\\), with replacement. This is equivalent to constructing a new bootstrap data set and recomputing \\(\\hat{\\alpha}\\) based on the new data set.\n\nset.seed(7)\nalpha.fn(Portfolio, sample(100, 100, replace = T))\n\n[1] 0.5385326\n\n\nWe can implement a bootstrap analysis by performing this command many times, recording all of the corresponding estimates for \\(\\alpha\\), and computing the resulting standard deviation. However, the boot() function automates this approach. Below we produce \\(R=1,000\\) bootstrap estimates for \\(\\alpha\\).\n\nboot(Portfolio, alpha.fn, R = 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Portfolio, statistic = alpha.fn, R = 1000)\n\n\nBootstrap Statistics :\n     original       bias    std. error\nt1* 0.5758321 0.0007959475  0.08969074\n\n\nThe final output shows that using the original data, \\(\\hat{\\alpha}=0.5758\\), and that the bootstrap estimate for \\({\\rm SE}(\\hat{\\alpha})\\) is \\(0.0897\\).\n\n\n\nThe bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method. Here we use the bootstrap approach in order to assess the variability of the estimates for \\(\\beta_0\\) and \\(\\beta_1\\), the intercept and slope terms for the linear regression model that uses horsepower to predict mpg in the Auto data set. We will compare the estimates obtained using the bootstrap to those obtained using the formulas for \\({\\rm SE}(\\hat{\\beta}_0)\\) and \\({\\rm SE}(\\hat{\\beta}_1)\\) described in Section 3.1.2.\nWe first create a simple function, boot.fn(), which takes in the Auto data set as well as a set of indices for the observations, and returns the intercept and slope estimates for the linear regression model. We then apply this function to the full set of \\(392\\) observations in order to compute the estimates of \\(\\beta_0\\) and \\(\\beta_1\\) on the entire data set using the usual linear regression coefficient estimate formulas from Chapter 3. Note that we do not need the { and } at the beginning and end of the function because it is only one line long.\n\nboot.fn &lt;- function(data, index)\n  coef(lm(mpg ~ horsepower, data = data, subset = index))\nboot.fn(Auto, 1:392)\n\n(Intercept)  horsepower \n 39.9358610  -0.1578447 \n\n\nThe boot.fn() function can also be used in order to create bootstrap estimates for the intercept and slope terms by randomly sampling from among the observations with replacement. Here we give two examples.\n\nset.seed(1)\nboot.fn(Auto, sample(392, 392, replace = T))\n\n(Intercept)  horsepower \n 40.3404517  -0.1634868 \n\nboot.fn(Auto, sample(392, 392, replace = T))\n\n(Intercept)  horsepower \n 40.1186906  -0.1577063 \n\n\nNext, we use the boot() function to compute the standard errors of 1,000 bootstrap estimates for the intercept and slope terms.\n\nboot(Auto, boot.fn, 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot.fn, R = 1000)\n\n\nBootstrap Statistics :\n      original        bias    std. error\nt1* 39.9358610  0.0544513229 0.841289790\nt2* -0.1578447 -0.0006170901 0.007343073\n\n\nThis indicates that the bootstrap estimate for \\({\\rm SE}(\\hat{\\beta}_0)\\) is \\(0.84\\), and that the bootstrap estimate for \\({\\rm SE}(\\hat{\\beta}_1)\\) is \\(0.0073\\). As discussed in Section 3.1.2, standard formulas can be used to compute the standard errors for the regression coefficients in a linear model. These can be obtained using the summary() function.\n\nsummary(lm(mpg ~ horsepower, data = Auto))$coef\n\n              Estimate  Std. Error   t value      Pr(&gt;|t|)\n(Intercept) 39.9358610 0.717498656  55.65984 1.220362e-187\nhorsepower  -0.1578447 0.006445501 -24.48914  7.031989e-81\n\n\nThe standard error estimates for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) obtained using the formulas from Section 3.1.2 are \\(0.717\\) for the intercept and \\(0.0064\\) for the slope. Interestingly, these are somewhat different from the estimates obtained using the bootstrap. Does this indicate a problem with the bootstrap? In fact, it suggests the opposite. Recall that the standard formulas given in Equation 3.8 on page 66 rely on certain assumptions. For example, they depend on the unknown parameter \\(\\sigma^2\\), the noise variance. We then estimate \\(\\sigma^2\\) using the RSS. Now although the formulas for the standard errors do not rely on the linear model being correct, the estimate for \\(\\sigma^2\\) does. We see in Figure 3.8 on page 92 that there is a non-linear relationship in the data, and so the residuals from a linear fit will be inflated, and so will \\(\\hat{\\sigma}^2\\). Secondly, the standard formulas assume (somewhat unrealistically) that the \\(x_i\\) are fixed, and all the variability comes from the variation in the errors \\(\\epsilon_i\\). The bootstrap approach does not rely on any of these assumptions, and so it is likely giving a more accurate estimate of the standard errors of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) than is the summary() function.\nBelow we compute the bootstrap standard error estimates and the standard linear regression estimates that result from fitting the quadratic model to the data. Since this model provides a good fit to the data (Figure 3.8), there is now a better correspondence between the bootstrap estimates and the standard estimates of \\({\\rm SE}(\\hat{\\beta}_0)\\), \\({\\rm SE}(\\hat{\\beta}_1)\\) and \\({\\rm SE}(\\hat{\\beta}_2)\\).\n\nboot.fn &lt;- function(data, index)\n  coef(\n      lm(mpg ~ horsepower + I(horsepower^2), \n        data = data, subset = index)\n    )\nset.seed(1)\nboot(Auto, boot.fn, 1000)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Auto, statistic = boot.fn, R = 1000)\n\n\nBootstrap Statistics :\n        original        bias     std. error\nt1* 56.900099702  3.511640e-02 2.0300222526\nt2* -0.466189630 -7.080834e-04 0.0324241984\nt3*  0.001230536  2.840324e-06 0.0001172164\n\nsummary(\n    lm(mpg ~ horsepower + I(horsepower^2), data = Auto)\n  )$coef\n\n                    Estimate   Std. Error   t value      Pr(&gt;|t|)\n(Intercept)     56.900099702 1.8004268063  31.60367 1.740911e-109\nhorsepower      -0.466189630 0.0311246171 -14.97816  2.289429e-40\nI(horsepower^2)  0.001230536 0.0001220759  10.08009  2.196340e-21"
  },
  {
    "objectID": "teaching/stat-learn/material/06_Model_Validation_Resampling.html#exercises",
    "href": "teaching/stat-learn/material/06_Model_Validation_Resampling.html#exercises",
    "title": "Cross-Validation and the Bootstrap",
    "section": "",
    "text": "Use the Boston data set.\nImportant: Any function that you pass to boot needs to use indeces\n\nBased on this data set, provide an estimate for the population mean of medv. Call this estimate \\(\\hat{\\mu}\\)\n\n\nProvide an estimate of the standard error of \\(\\hat{\\mu}\\) . Interpret this result.\nHint: We can compute the standard error of the sample mean by dividing the sample standard deviation by the square root of the number of observations.\n\n\nNow estimate the standard error of \\(\\hat{\\mu}\\) using the bootstrap. How does this compare to your answer from (b)?\n\n\nBased on your bootstrap estimate from (c), provide a 95% confidence interval for the mean of medv. Compare it to the results obtained using t.test(Boston$medv).\nHint: You can approximate a 95% confidence interval using the formula\n[\\(\\hat{\\mu}\\) - 2SE(\\(\\hat{\\mu}\\)), \\(\\hat{\\mu}\\) + 2SE(\\(\\hat{\\mu}\\))] .\n\n\nBased on this data set, provide an estimate, \\(\\hat{\\mu}_{med}\\), for the median value of `medv` in the population.\n\n\nWe now would like to estimate the standard error of \\(\\hat{\\mu}_{med}\\). Unfortunately, there is no simple formula for computing the standard error of the median. Instead, estimate the standard error of the median using the bootstrap. Comment on your findings.\n\n\nBased on this data set, provide an estimate for the tenth per centile of medv in Boston census tracts. Call this quantity \\(\\hat{\\mu}_{0.1}\\). (You can use the quantile() function.)\n\n\nUse the bootstrap to estimate the standard error of \\(\\hat{\\mu}_{0.1}\\). Comment on your findings."
  },
  {
    "objectID": "teaching/stat-learn/material/06-model-validation-1.html",
    "href": "teaching/stat-learn/material/06-model-validation-1.html",
    "title": "Cross Validation and Model Selection",
    "section": "",
    "text": "iris data is a built-in data set in R that contains measurements for 50 flowers in 3 different species and 4 different attributes.\ncaret package is short for Classification And REgression Training. This is a useful tool for data splitting, pre-processing, feature selection and model tuning. In this simple example I will use this package to illustrate cross-validation methods.\ndplyr package is a commonly used tool for data manipulation.\ntidyverse package is for data manipulation and visualization (with dplyr included).\n\n\nlibrary(caret)\nlibrary(tidyverse)\n# Load data\ndata(iris)\n\n# Take a look at data \nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\n\nTo determine whether the designed model is performing well, we need to use the observations that are not being used during the training of the model. Therefore the test set will serve as the unseen data, then the values of the dependent variables are predicted and model accuracy will be evaluated based on the difference between actual values and predicted values of the dependent variable. We use following model performance metrics (consult lecture slides for more information):\n\n\\(R^2\\)\nRooted Mean Squared Error (RMSE)\nMean Absolute Error (MAE)\n\n\n\n\nEach methods below will be conducted in four steps:\n\nData splitting: split the data set into different subsets.\nTraining: build the model on the training data set.\nTesting: apply the resultant model to the unseen data (testing data set) to predict the outcome of new observations.\nEvaluating: calculate prediction error using the model performance metrics.\n\n\n\n\nIn this approach, the available data is divided into two subsets: a training set and a validation set. The training set is used to train the model, and the validation set is used to evaluate its performance. Predictions done by this method could be largely affected by the subset of observations used in testing set. If the test set is not representative of the entire data, this method may lead to overfitting.\n\n### Data splitting\n\n# set seed to generate a reproducible random sample\nset.seed(123)\n\n# create training and testing data set using index, training data contains 80% of the data set\n# 'list = FALSE' allows us to create a matrix data structure with the indices of the observations in the subsets along the rows.\ntrain.index.vsa &lt;- createDataPartition(iris$Species, p= 0.8, list = FALSE)\ntrain.vsa &lt;- iris[train.index.vsa,]\ntest.vsa &lt;- iris[-train.index.vsa,]\n\n# see how the the subsets are randomized\nrole = rep('train',nrow(iris))\nrole[-train.index.vsa] = 'test'\nggplot(data = cbind(iris,role)) + \n  geom_point(aes(x = Sepal.Length,\n                 y = Petal.Width,\n                 color = role)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n### Training: linear model is fit using all availbale predictors\nmodel.vsa &lt;- lm(Petal.Width ~., data = train.vsa)\n\n\n### Testing\npredictions.vsa &lt;- model.vsa %&gt;% predict(test.vsa)\n\n\n### Evaluating\ndata.frame(RMSE = RMSE(predictions.vsa, test.vsa$Petal.Width),\n           R2 = R2(predictions.vsa, test.vsa$Petal.Width),\n           MAE = MAE(predictions.vsa, test.vsa$Petal.Width))\n\n       RMSE        R2      MAE\n1 0.1675093 0.9497864 0.128837\n\n\n\n\n\n\n### Data splitting: leave one out\ntrain.loocv &lt;- trainControl(method = \"LOOCV\")\n\n### Training\nmodel.loocv &lt;- train(Petal.Width ~.,\n                     data = iris,\n                     method = \"lm\",\n                     trControl = train.loocv)\n\n### Present results\nprint(model.loocv)\n\nLinear Regression \n\n150 samples\n  4 predictor\n\nNo pre-processing\nResampling: Leave-One-Out Cross-Validation \nSummary of sample sizes: 149, 149, 149, 149, 149, 149, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.1705606  0.9496003  0.1268164\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\n\n\n### Data splitting\n\n# set seed to generate a reproducible random sample\nset.seed(123)\n# the number of K is set to be 5\ntrain.kfold &lt;- trainControl(method = \"cv\", number = 5)\n\n### Training\nmodel.kfold &lt;- train(Petal.Width ~.,\n                     data = iris,\n                     method = \"lm\",\n                     trControl = train.kfold)\n\n### Present results\nprint(model.kfold)\n\nLinear Regression \n\n150 samples\n  4 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 122, 120, 118, 121, 119 \nResampling results:\n\n  RMSE       Rsquared   MAE    \n  0.1704321  0.9514251  0.12891\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\n\n\n\n\n# set seed to generate a reproducible random sample\nset.seed(123)\n# the number of K is set to be 5\ntrain.rkfold &lt;- trainControl(method = \"repeatedcv\", number = 5, repeats = 3)\n\n### Training\nmodel.rkfold &lt;- train(Petal.Width ~.,\n                     data = iris,\n                     method = \"lm\",\n                     trControl = train.rkfold)\n\n### Present results\nprint(model.rkfold)\n\nLinear Regression \n\n150 samples\n  4 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold, repeated 3 times) \nSummary of sample sizes: 122, 120, 118, 121, 119, 119, ... \nResampling results:\n\n  RMSE      Rsquared   MAE      \n  0.168445  0.9525634  0.1266377\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\nprint(model.kfold)\n\nLinear Regression \n\n150 samples\n  4 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 122, 120, 118, 121, 119 \nResampling results:\n\n  RMSE       Rsquared   MAE    \n  0.1704321  0.9514251  0.12891\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\n\n\n\n\n\nCV method\nRMSE\nR2\nMAE\n\n\n\n\nValidation Set\n0.1675\n0.9498\n0.1288\n\n\nLOOCV\n0.1706\n0.9496\n0.1268\n\n\nK-Fold\n0.1704\n0.9514\n0.1289\n\n\nK-Fold repeat\n0.1704\n0.9514\n0.1289\n\n\n\nWhat do you note?"
  },
  {
    "objectID": "teaching/stat-learn/material/06-model-validation-1.html#model-performance-metrics",
    "href": "teaching/stat-learn/material/06-model-validation-1.html#model-performance-metrics",
    "title": "Cross Validation and Model Selection",
    "section": "",
    "text": "To determine whether the designed model is performing well, we need to use the observations that are not being used during the training of the model. Therefore the test set will serve as the unseen data, then the values of the dependent variables are predicted and model accuracy will be evaluated based on the difference between actual values and predicted values of the dependent variable. We use following model performance metrics (consult lecture slides for more information):\n\n\\(R^2\\)\nRooted Mean Squared Error (RMSE)\nMean Absolute Error (MAE)"
  },
  {
    "objectID": "teaching/stat-learn/material/06-model-validation-1.html#procedure-for-each-cv-method-applied",
    "href": "teaching/stat-learn/material/06-model-validation-1.html#procedure-for-each-cv-method-applied",
    "title": "Cross Validation and Model Selection",
    "section": "",
    "text": "Each methods below will be conducted in four steps:\n\nData splitting: split the data set into different subsets.\nTraining: build the model on the training data set.\nTesting: apply the resultant model to the unseen data (testing data set) to predict the outcome of new observations.\nEvaluating: calculate prediction error using the model performance metrics."
  },
  {
    "objectID": "teaching/stat-learn/material/06-model-validation-1.html#validation-set-approach",
    "href": "teaching/stat-learn/material/06-model-validation-1.html#validation-set-approach",
    "title": "Cross Validation and Model Selection",
    "section": "",
    "text": "In this approach, the available data is divided into two subsets: a training set and a validation set. The training set is used to train the model, and the validation set is used to evaluate its performance. Predictions done by this method could be largely affected by the subset of observations used in testing set. If the test set is not representative of the entire data, this method may lead to overfitting.\n\n### Data splitting\n\n# set seed to generate a reproducible random sample\nset.seed(123)\n\n# create training and testing data set using index, training data contains 80% of the data set\n# 'list = FALSE' allows us to create a matrix data structure with the indices of the observations in the subsets along the rows.\ntrain.index.vsa &lt;- createDataPartition(iris$Species, p= 0.8, list = FALSE)\ntrain.vsa &lt;- iris[train.index.vsa,]\ntest.vsa &lt;- iris[-train.index.vsa,]\n\n# see how the the subsets are randomized\nrole = rep('train',nrow(iris))\nrole[-train.index.vsa] = 'test'\nggplot(data = cbind(iris,role)) + \n  geom_point(aes(x = Sepal.Length,\n                 y = Petal.Width,\n                 color = role)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n### Training: linear model is fit using all availbale predictors\nmodel.vsa &lt;- lm(Petal.Width ~., data = train.vsa)\n\n\n### Testing\npredictions.vsa &lt;- model.vsa %&gt;% predict(test.vsa)\n\n\n### Evaluating\ndata.frame(RMSE = RMSE(predictions.vsa, test.vsa$Petal.Width),\n           R2 = R2(predictions.vsa, test.vsa$Petal.Width),\n           MAE = MAE(predictions.vsa, test.vsa$Petal.Width))\n\n       RMSE        R2      MAE\n1 0.1675093 0.9497864 0.128837"
  },
  {
    "objectID": "teaching/stat-learn/material/06-model-validation-1.html#leave-one-out-cross-validation-loocv",
    "href": "teaching/stat-learn/material/06-model-validation-1.html#leave-one-out-cross-validation-loocv",
    "title": "Cross Validation and Model Selection",
    "section": "",
    "text": "### Data splitting: leave one out\ntrain.loocv &lt;- trainControl(method = \"LOOCV\")\n\n### Training\nmodel.loocv &lt;- train(Petal.Width ~.,\n                     data = iris,\n                     method = \"lm\",\n                     trControl = train.loocv)\n\n### Present results\nprint(model.loocv)\n\nLinear Regression \n\n150 samples\n  4 predictor\n\nNo pre-processing\nResampling: Leave-One-Out Cross-Validation \nSummary of sample sizes: 149, 149, 149, 149, 149, 149, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.1705606  0.9496003  0.1268164\n\nTuning parameter 'intercept' was held constant at a value of TRUE"
  },
  {
    "objectID": "teaching/stat-learn/material/06-model-validation-1.html#k-fold-cross-validation",
    "href": "teaching/stat-learn/material/06-model-validation-1.html#k-fold-cross-validation",
    "title": "Cross Validation and Model Selection",
    "section": "",
    "text": "### Data splitting\n\n# set seed to generate a reproducible random sample\nset.seed(123)\n# the number of K is set to be 5\ntrain.kfold &lt;- trainControl(method = \"cv\", number = 5)\n\n### Training\nmodel.kfold &lt;- train(Petal.Width ~.,\n                     data = iris,\n                     method = \"lm\",\n                     trControl = train.kfold)\n\n### Present results\nprint(model.kfold)\n\nLinear Regression \n\n150 samples\n  4 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 122, 120, 118, 121, 119 \nResampling results:\n\n  RMSE       Rsquared   MAE    \n  0.1704321  0.9514251  0.12891\n\nTuning parameter 'intercept' was held constant at a value of TRUE"
  },
  {
    "objectID": "teaching/stat-learn/material/06-model-validation-1.html#repeated-k-fold-cross-validation",
    "href": "teaching/stat-learn/material/06-model-validation-1.html#repeated-k-fold-cross-validation",
    "title": "Cross Validation and Model Selection",
    "section": "",
    "text": "# set seed to generate a reproducible random sample\nset.seed(123)\n# the number of K is set to be 5\ntrain.rkfold &lt;- trainControl(method = \"repeatedcv\", number = 5, repeats = 3)\n\n### Training\nmodel.rkfold &lt;- train(Petal.Width ~.,\n                     data = iris,\n                     method = \"lm\",\n                     trControl = train.rkfold)\n\n### Present results\nprint(model.rkfold)\n\nLinear Regression \n\n150 samples\n  4 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold, repeated 3 times) \nSummary of sample sizes: 122, 120, 118, 121, 119, 119, ... \nResampling results:\n\n  RMSE      Rsquared   MAE      \n  0.168445  0.9525634  0.1266377\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\nprint(model.kfold)\n\nLinear Regression \n\n150 samples\n  4 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 122, 120, 118, 121, 119 \nResampling results:\n\n  RMSE       Rsquared   MAE    \n  0.1704321  0.9514251  0.12891\n\nTuning parameter 'intercept' was held constant at a value of TRUE"
  },
  {
    "objectID": "teaching/stat-learn/material/06-model-validation-1.html#lets-summarize-the-results",
    "href": "teaching/stat-learn/material/06-model-validation-1.html#lets-summarize-the-results",
    "title": "Cross Validation and Model Selection",
    "section": "",
    "text": "CV method\nRMSE\nR2\nMAE\n\n\n\n\nValidation Set\n0.1675\n0.9498\n0.1288\n\n\nLOOCV\n0.1706\n0.9496\n0.1268\n\n\nK-Fold\n0.1704\n0.9514\n0.1289\n\n\nK-Fold repeat\n0.1704\n0.9514\n0.1289\n\n\n\nWhat do you note?"
  },
  {
    "objectID": "teaching/stat-learn/material/06-model-validation-1.html#knn",
    "href": "teaching/stat-learn/material/06-model-validation-1.html#knn",
    "title": "Cross Validation and Model Selection",
    "section": "KNN",
    "text": "KNN\nRecall our KNN classifier from earlier where the following function was used:\n\nKNN = function(x0, x, y, K) {\n    distances = abs(x - x0)  # Euclidean distance between x0 and each x_i\n    o = order(distances)  # order of the training points by distance from x0 (nearest to farthest)\n    y0_hat = mean(y[o[1:K]])  # take average of the y values of the K nearest training points\n    return(y0_hat)  # return predicted value of y\n}\n\nwhere:\n\n\\(x_0\\) as the new point at which we wish to predict \\(y\\)\n\\({\\bf x} = (x_1,x_2, \\dots, x_n)\\) as the vector of training \\(x\\)’s\n\\({\\bf y} = (y_1,y_2, \\dots, y_n)\\) as the vector of training \\(y\\)’s\n\\(K\\) as number of neighbors to use\n\\(\\hat{y}_0\\) as the predicted value of \\(y\\) at \\(x_0\\)"
  },
  {
    "objectID": "teaching/stat-learn/material/06-model-validation-1.html#simulate-data",
    "href": "teaching/stat-learn/material/06-model-validation-1.html#simulate-data",
    "title": "Cross Validation and Model Selection",
    "section": "Simulate data",
    "text": "Simulate data\nWe also simulate training data as before and plot it:\n\nset.seed(1)  \nn = 100 \nx = 5*runif(n)\nsigma = 0.3  \nf = function(x) { cos(x) }  \ny = f(x) + sigma*rnorm(n)  \nplot(x,y,col=2,pch=20,cex=2)  # plot training data"
  },
  {
    "objectID": "teaching/stat-learn/material/06-model-validation-1.html#k-fold-cross-validation-1",
    "href": "teaching/stat-learn/material/06-model-validation-1.html#k-fold-cross-validation-1",
    "title": "Cross Validation and Model Selection",
    "section": "K-Fold Cross Validation",
    "text": "K-Fold Cross Validation\nHere we are going to use cross-validation to estimate test performance of the KNN classifier. We set number of neighbors as \\(K=1\\) and use the 10-fold cross validation. We do a random ordering of all the available data, and initialize a vector to hold MSE for each fold. For each fold, we then create a training and test (hold one out/validation) set, run KNN at each \\(x\\) in this test set (the one left out), and compute MSE on this test set. Then we average the MSE over all folds to obtain the CV estimate of test MSE:\n\nK = 1  \nnfolds = 10 \npermutation = sample(1:n)  \nMSE_fold = rep(0,nfolds)  \nfor (j in 1:nfolds) {\n    test = permutation[floor((j-1)*n/nfolds+1) : floor(j*n/nfolds)]  \n    train = setdiff(1:n, test)  \n    y_hat = sapply(x[test], function(x0) { KNN(x0, x[train], y[train], K) }) \n    MSE_fold[j] = mean((y[test] - y_hat)^2) \n}\nMSE_cv = mean(MSE_fold)  \nMSE_cv\n\n[1] 0.1630092\n\n\nNext we compare with the ground truth estimate of test performance, given this training set. Because this is a simulation example, we can generate lots of test data. We simulate \\(x\\)’s and \\(y\\)’s from the true data generating process. Then we run the KNN classifier at each \\(x\\) in the test set and compute the MSE on the test set:\n\nn_test = 100000\nx_test = 5*runif(n_test)  \ny_test = f(x_test) + sigma*rnorm(n_test)  \ny_test_hat = sapply(x_test, function(x0) { KNN(x0, x, y, K) })  \nMSE_test = mean((y_test - y_test_hat)^2)  \n\nLet’s compare the two values:\n\nMSE_test\n\n[1] 0.1682957\n\nMSE_cv\n\n[1] 0.1630092\n\n\nBe careful when calculating the root MSE (RMSE) since it corresponds to root mean squared error or square root of MSE: Let’s try with\n\nsqrt(MSE_test)  # test RMSE\n\n[1] 0.4102386\n\nsqrt(mean(MSE_fold))  # sqrt of MSE_cv\n\n[1] 0.403744\n\nmean(sqrt(MSE_fold))  # can we use this?\n\n[1] 0.3970696"
  },
  {
    "objectID": "teaching/stat-learn/material/06-model-validation-1.html#leave-one-out-cross-validation-loocv-1",
    "href": "teaching/stat-learn/material/06-model-validation-1.html#leave-one-out-cross-validation-loocv-1",
    "title": "Cross Validation and Model Selection",
    "section": "Leave-One Out Cross-Validation (LOOCV)",
    "text": "Leave-One Out Cross-Validation (LOOCV)\nUse the leave-one out cross-validation (LOOCV) in the above example and report the CV estimate of test MSE and the MSE given ground truth.\n\nK = 1  \nnfolds = n\npermutation = sample(1:n)  \nMSE_fold = rep(0,nfolds)  \nfor (j in 1:nfolds) {\n    test = permutation[floor((j-1)*n/nfolds+1) : floor(j*n/nfolds)]  \n    train = setdiff(1:n, test)  \n    y_hat = sapply(x[test], function(x0) { KNN(x0, x[train], y[train], K) }) \n    MSE_fold[j] = mean((y[test] - y_hat)^2) \n}\nMSE_loocv = mean(MSE_fold)  \nMSE_test = mean((y_test - y_test_hat)^2)  \nMSE_loocv\n\n[1] 0.1682862\n\nMSE_test\n\n[1] 0.1682957"
  },
  {
    "objectID": "teaching/stat-learn/material/06-model-validation-1.html#hyperparameter-tuning-choosing-model-settings",
    "href": "teaching/stat-learn/material/06-model-validation-1.html#hyperparameter-tuning-choosing-model-settings",
    "title": "Cross Validation and Model Selection",
    "section": "Hyperparameter Tuning: Choosing Model Settings",
    "text": "Hyperparameter Tuning: Choosing Model Settings\nWith the following example, we will illustrate how to use cross validation to choose the optimal number of neighbors \\(K\\) in KNN. We start with a rather high number of \\(K\\) to try for KNN (\\(K=30\\)) and use 10 folds for each of these cases in the cross validation. Then we do a random ordering of data and initialize vector for holding MSE’s. For each number of folds in the range, we compute the training and test set as before (this is again the validation set). For each \\(K\\) up to 30, we then run KNN at each \\(x\\) in the test set (the one left out), and compute MSE on the this test set. We average across folds to obtain CV estimate of test MSE for each \\(K\\) and plot the results:\n\nK_max = 30 \nnfolds = 10  \npermutation = sample(1:n)  \nMSE_fold = matrix(0,nfolds,K_max)  \nfor (j in 1:nfolds) {\n    test = permutation[floor((j-1)*n/nfolds+1) : floor(j*n/nfolds)]  \n    train = setdiff(1:n, test) \n    for (K in 1:K_max) {\n        y_hat = sapply(x[test], function(x0) { KNN(x0, x[train], y[train], K) })\n        MSE_fold[j,K] = mean((y[test] - y_hat)^2)  \n    }\n}\nMSE_cv = colMeans(MSE_fold)  \n\nWe plot CV estimate of test MSE against number of neighbors \\(K=1,2,\\dots,30\\), and choose the value of \\(K\\) that minimizes estimated test MSE. Compare with a ground truth estimate of test performance by using the chosen number of \\(K\\) and running KNN on each \\(x\\) in the test set (denoted x_test above). Why do you think the test performance estimate for the chosen \\(K\\) tend to be smaller than the ground truth estimate of test performance in this example?\n\nplot(1:K_max, MSE_cv, pch=19)  # plot CV estimate of test MSE for each K\n\n\n\n\n\n\n\n# Choose the value of K that minimizes estimated test MSE\nK_cv = which.min(MSE_cv)\nK_cv\n\n[1] 12\n\n\nAnswer: MSE_cv[K_cv] may systematically underestimate or overestimate test MSE! There are two sources of bias: K_cv is the minimum, and the pseudo-training set is smaller than \\(n\\)."
  },
  {
    "objectID": "teaching/stat-learn/material/06-model-validation-1.html#choosing-the-number-of-folds",
    "href": "teaching/stat-learn/material/06-model-validation-1.html#choosing-the-number-of-folds",
    "title": "Cross Validation and Model Selection",
    "section": "Choosing the number of folds",
    "text": "Choosing the number of folds\nWe start by simulating training data as before:\n\nset.seed(1) \nn = 20\nx = 5*runif(n)  \nsigma = 0.3 \ny = f(x) + sigma*rnorm(n)  \n\nWe then compute “ground truth” estimate of test performance, given this training set. We set \\(K=10\\), and run KNN at each \\(x\\) in the test set and compute MSE on the test set:\n\nK = 10\ny_test_hat = sapply(x_test, function(x0) { KNN(x0, x, y, K) })  \nMSE_test = mean((y_test - y_test_hat)^2)  \n\nNext, we repeatedly run CV for a range of number of folds nfolds up to maximum \\(n=20\\) (same as \\(n\\) above in our simulated data). We repeat the simulation 200 times, and for each repetition and number of folds, we split the training data into training and test (hold one out/validation set). We run KNN at each \\(x\\) in this test set and compute MSE. We then average the MSE’s for each case with a different number of folds:\n\nnfolds_max = n  # maximum value of nfolds to use for CV\nnreps = 200  # number of times to repeat the simulation\nMSE_cv = matrix(0,nreps,nfolds_max)  \nfor (r in 1:nreps) {  \n    for (nfolds in 1:nfolds_max) {\n        permutation = sample(1:n) \n        MSE_fold = rep(0,nfolds)  \n        for (j in 1:nfolds) {\n            test = permutation[floor((j-1)*n/nfolds+1) : floor(j*n/nfolds)]  \n            train = setdiff(1:n, test)  \n            y_hat = sapply(x[test], function(x0) { KNN(x0, x[train], y[train], K) }) \n            MSE_fold[j] = mean((y[test] - y_hat)^2) \n        }\n        MSE_cv[r,nfolds] = mean(MSE_fold)\n    }\n}\n\nWe compute the MSE, bias, and variance of the CV estimate of test MSE, for each value of nfolds and plot MSE, bias^2, and variance of the CV estimate, for each value of nfolds.\n\nmse = colMeans((MSE_cv - MSE_test)^2)\nbias = colMeans(MSE_cv) - MSE_test\nvariance = apply(MSE_cv,2,var)\n\n# plot of MSE, bias^2 and variance against number of folds\nplot(1:nfolds_max, type=\"n\", ylim=c(0,max(mse[2:nfolds_max])*1.1), xlab=\"nfolds\", ylab=\"mse\", main=\"MSE of the CV estimates\")\nlines(1:nfolds_max, mse, col=1, lty=2, lwd=2, ylim=c(0,0.2))\nlines(1:nfolds_max, bias^2, col=2, lwd=2)\nlines(1:nfolds_max, variance, col=4, lwd=2)\nlegend(\"topright\", legend=c(\"mse\",\"bias^2\",\"variance\"), col=c(1,2,4), lwd=2)\n\n\n\n\n\n\n\n# plot bias against number of folds\nplot(1:nfolds_max, bias)\nlines(1:nfolds_max, bias, col=2, lwd=2)\n\n\n\n\n\n\n\n\nIn the following plot below, why do you think the bias of the CV estimate of test MSE is always positive?\n\n# plot bias against number of folds\nplot(1:nfolds_max, bias)\nlines(1:nfolds_max, bias, col=2, lwd=2)\n\n\n\n\n\n\n\n\nAnswer: Because the the “pseudo”-training set (each fold) is smaller than the training set."
  },
  {
    "objectID": "teaching/stat-learn/material/02-linear-regression-I.html",
    "href": "teaching/stat-learn/material/02-linear-regression-I.html",
    "title": "Linear Regression I",
    "section": "",
    "text": "The library() function is used to load libraries, or groups of functions and data sets that are not included in the base R distribution. Basic functions that perform least squares linear regression and other simple analyses come standard with the base distribution, but more exotic functions require additional libraries. Here we load the MASS package, which is a very large collection of data sets and functions. We also load the ISLR2 package, which includes the data sets associated with this book.\n\nlibrary(MASS)\nlibrary(ISLR2)\n\n\nAttaching package: 'ISLR2'\n\n\nThe following object is masked from 'package:MASS':\n\n    Boston\n\n\nIf you receive an error message when loading any of these libraries, it likely indicates that the corresponding library has not yet been installed on your system. Some libraries, such as MASS, come with R and do not need to be separately installed on your computer. However, other packages, such as ISLR2, must be downloaded the first time they are used. This can be done directly from within R. For example, on a Windows system, select the Install package option under the Packages tab. After you select any mirror site, a list of available packages will appear. Simply select the package you wish to install and R will automatically download the package. Alternatively, this can be done at the R command line via install.packages(\"ISLR2\"). This installation only needs to be done the first time you use a package. However, the library() function must be called within each R session.\n\n\n\nThe ISLR2 library contains the Boston data set, which records medv (median house value) for \\(506\\) census tracts in Boston. We will seek to predict medv using \\(12\\) predictors such as rmvar (average number of rooms per house), age (proportion of owner-occupied units built prior to 1940) and lstat (percent of households with low socioeconomic status).\n\nknitr::kable(head(Boston)) # note: you only write head(Boston) here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nlstat\nmedv\n\n\n\n\n0.00632\n18\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n4.98\n24.0\n\n\n0.02731\n0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n9.14\n21.6\n\n\n0.02729\n0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n4.03\n34.7\n\n\n0.03237\n0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n2.94\n33.4\n\n\n0.06905\n0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n5.33\n36.2\n\n\n0.02985\n0\n2.18\n0\n0.458\n6.430\n58.7\n6.0622\n3\n222\n18.7\n5.21\n28.7\n\n\n\n\n\nTo find out more about the data set, we can type ?Boston.\nWe will start by using the lm() function to fit a simple linear regression model, with medv as the response and lstat as the predictor. The basic syntax is lm(y ~ x, data), where y is the response, x is the predictor, and data is the data set in which these two variables are kept.\n\nlm.fit &lt;- lm(medv ~ lstat)\n\nError in eval(predvars, data, env): object 'medv' not found\n\n\nThe command causes an error because R does not know where to find the variables medv and lstat. The next line tells R that the variables are in Boston. If we attach Boston, the first line works fine because R now recognizes the variables.\n\nlm.fit &lt;- lm(medv ~ lstat, data = Boston)\nattach(Boston)\nlm.fit &lt;- lm(medv ~ lstat)\n\nIf we type lm.fit, some basic information about the model is output. For more detailed information, we use summary(lm.fit). This gives us \\(p\\)-values and standard errors for the coefficients, as well as the \\(R^2\\) statistic and \\(F\\)-statistic for the model.\n\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\nWe can use the names() function in order to find out what other pieces of information are stored in lm.fit. Although we can extract these quantities by name—e.g. lm.fit$coefficients—it is safer to use the extractor functions like coef() to access them.\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\n\nIn order to obtain a confidence interval for the coefficient estimates, we can use the confint() command.\n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\n\nThe predict() function can be used to produce confidence intervals and prediction intervals for the prediction of medv for a given value of lstat.\n\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))),\n    interval = \"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))),\n    interval = \"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n\nFor instance, the 95 % confidence interval associated with a lstat value of 10 is \\((24.47, 25.63)\\), and the 95 % prediction interval is \\((12.828, 37.28)\\). As expected, the confidence and prediction intervals are centered around the same point (a predicted value of \\(25.05\\) for medv when lstat equals 10), but the latter are substantially wider.\nWe will now plot medv and lstat along with the least squares regression line using the plot() and abline() functions.\n\nplot(lstat, medv)\nabline(lm.fit)\n\n\n\n\n\n\n\n\nThere is some evidence for non-linearity in the relationship between lstat and medv. We will explore this issue later in this lab.\nThe abline() function can be used to draw any line, not just the least squares regression line. To draw a line with intercept a and slope b, we type abline(a, b). Below we experiment with some additional settings for plotting lines and points. The lwd = 3 command causes the width of the regression line to be increased by a factor of 3; this works for the plot() and lines() functions also. We can also use the pch option to create different plotting symbols.\n\nplot(lstat, medv)\nabline(lm.fit, lwd = 3)\nabline(lm.fit, lwd = 3, col = \"red\")\n\n\n\n\n\n\n\nplot(lstat, medv, col = \"red\")\n\n\n\n\n\n\n\nplot(lstat, medv, pch = 20)\n\n\n\n\n\n\n\nplot(lstat, medv, pch = \"+\")\n\n\n\n\n\n\n\nplot(1:20, 1:20, pch = 1:20)\n\n\n\n\n\n\n\n\nNext we examine some diagnostic plots, several of which were discussed in Section 3.3.3. Four diagnostic plots are automatically produced by applying the plot() function directly to the output from lm(). In general, this command will produce one plot at a time, and hitting Enter will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the par() and mfrow() functions, which tell R to split the display screen into separate panels so that multiple plots can be viewed simultaneously. For example, par(mfrow = c(2, 2)) divides the plotting region into a \\(2 \\times 2\\) grid of panels.\n\npar(mfrow = c(2, 2))\nplot(lm.fit)\n\n\n\n\n\n\n\n\nAlternatively, we can compute the residuals from a linear regression fit using the residuals() function. The function rstudent() will return the studentized residuals, and we can use this function to plot the residuals against the fitted values.\n\nplot(predict(lm.fit), residuals(lm.fit))\n\n\n\n\n\n\n\nplot(predict(lm.fit), rstudent(lm.fit))\n\n\n\n\n\n\n\n\nOn the basis of the residual plots, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the hatvalues() function.\n\nplot(hatvalues(lm.fit))\n\n\n\n\n\n\n\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\nThe which.max() function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic.\n\n\n\nIn order to fit a multiple linear regression model using least squares, we again use the lm() function. The syntax lm(y ~ x1 + x2 + x3) is used to fit a model with three predictors, x1, x2, and x3. The summary() function now outputs the regression coefficients for all the predictors.\n\nlm.fit &lt;- lm(medv ~ lstat + age, data = Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\nThe Boston data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:\n\nlm.fit &lt;- lm(medv ~ ., data = Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1304  -2.7673  -0.5814   1.9414  26.2526 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.617270   4.936039   8.431 3.79e-16 ***\ncrim         -0.121389   0.033000  -3.678 0.000261 ***\nzn            0.046963   0.013879   3.384 0.000772 ***\nindus         0.013468   0.062145   0.217 0.828520    \nchas          2.839993   0.870007   3.264 0.001173 ** \nnox         -18.758022   3.851355  -4.870 1.50e-06 ***\nrm            3.658119   0.420246   8.705  &lt; 2e-16 ***\nage           0.003611   0.013329   0.271 0.786595    \ndis          -1.490754   0.201623  -7.394 6.17e-13 ***\nrad           0.289405   0.066908   4.325 1.84e-05 ***\ntax          -0.012682   0.003801  -3.337 0.000912 ***\nptratio      -0.937533   0.132206  -7.091 4.63e-12 ***\nlstat        -0.552019   0.050659 -10.897  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.798 on 493 degrees of freedom\nMultiple R-squared:  0.7343,    Adjusted R-squared:  0.7278 \nF-statistic: 113.5 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\n\nWe can access the individual components of a summary object by name (type ?summary.lm to see what is available). Hence summary(lm.fit)$r.sq gives us the \\(R^2\\), and summary(lm.fit)$sigma gives us the RSE. The vif() function, part of the car package, can be used to compute variance inflation factors. Most VIF’s are low to moderate for this data. The car package is not part of the base R installation so it must be downloaded the first time you use it via the install.packages() function in R.\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.767486 2.298459 3.987181 1.071168 4.369093 1.912532 3.088232 3.954037 \n     rad      tax  ptratio    lstat \n7.445301 9.002158 1.797060 2.870777 \n\n\nWhat if we would like to perform a regression using all of the variables but one? For example, in the above regression output, age has a high \\(p\\)-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except age.\n\nlm.fit1 &lt;- lm(medv ~ . - age, data = Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1851  -2.7330  -0.6116   1.8555  26.3838 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.525128   4.919684   8.441 3.52e-16 ***\ncrim         -0.121426   0.032969  -3.683 0.000256 ***\nzn            0.046512   0.013766   3.379 0.000785 ***\nindus         0.013451   0.062086   0.217 0.828577    \nchas          2.852773   0.867912   3.287 0.001085 ** \nnox         -18.485070   3.713714  -4.978 8.91e-07 ***\nrm            3.681070   0.411230   8.951  &lt; 2e-16 ***\ndis          -1.506777   0.192570  -7.825 3.12e-14 ***\nrad           0.287940   0.066627   4.322 1.87e-05 ***\ntax          -0.012653   0.003796  -3.333 0.000923 ***\nptratio      -0.934649   0.131653  -7.099 4.39e-12 ***\nlstat        -0.547409   0.047669 -11.483  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.794 on 494 degrees of freedom\nMultiple R-squared:  0.7343,    Adjusted R-squared:  0.7284 \nF-statistic: 124.1 on 11 and 494 DF,  p-value: &lt; 2.2e-16\n\n\nAlternatively, the update() function can be used.\n\nlm.fit1 &lt;- update(lm.fit, ~ . - age)\n\n\n\n\nIt is easy to include interaction terms in a linear model using the lm() function. The syntax lstat:age tells R to include an interaction term between lstat and age. The syntax lstat * age simultaneously includes lstat, age, and the interaction term lstat\\(\\times\\)age as predictors; it is a shorthand for lstat + age + lstat:age. %We can also pass in transformed versions of the predictors.\n\nsummary(lm(medv ~ lstat * age, data = Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nThe lm() function can also accommodate non-linear transformations of the predictors. For instance, given a predictor \\(X\\), we can create a predictor \\(X^2\\) using I(X^2). The function I() is needed since the ^ has a special meaning in a formula object; wrapping as we do allows the standard usage in R, which is to raise X to the power 2. We now perform a regression of medv onto lstat and lstat^2.\n\nlm.fit2 &lt;- lm(medv ~ lstat + I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\nThe near-zero \\(p\\)-value associated with the quadratic term suggests that it leads to an improved model. We use the anova() function to further quantify the extent to which the quadratic fit is superior to the linear fit.\n\nlm.fit &lt;- lm(medv ~ lstat)\nanova(lm.fit, lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere Model 1 represents the linear submodel containing only one predictor, lstat, while Model 2 corresponds to the larger quadratic model that has two predictors, lstat and lstat^2. The anova() function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Here the \\(F\\)-statistic is \\(135\\) and the associated \\(p\\)-value is virtually zero. This provides very clear evidence that the model containing the predictors lstat and lstat^2 is far superior to the model that only contains the predictor lstat. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between medv and lstat. If we type\n\npar(mfrow = c(2, 2))\nplot(lm.fit2)\n\n\n\n\n\n\n\n\nthen we see that when the lstat^2 term is included in the model, there is little discernible pattern in the residuals.\nIn order to create a cubic fit, we can include a predictor of the form I(X^3). However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the poly() function to create the polynomial within lm(). For example, the following command produces a fifth-order polynomial fit:\n\nlm.fit5 &lt;- lm(medv ~ poly(lstat, 5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\n\nThis suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant \\(p\\)-values in a regression fit.\nBy default, the poly() function orthogonalizes the predictors: this means that the features output by this function are not simply a sequence of powers of the argument. However, a linear model applied to the output of the poly() function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). In order to obtain the raw polynomials from the poly() function, the argument raw = TRUE must be used.\nOf course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.\n\nsummary(lm(medv ~ log(rm), data = Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nWe will now examine the Carseats data, which is part of the ISLR2 library. We will attempt to predict Sales (child car seat sales) in \\(400\\) locations based on a number of predictors.\n\nhead(Carseats)\n\n  Sales CompPrice Income Advertising Population Price ShelveLoc Age Education\n1  9.50       138     73          11        276   120       Bad  42        17\n2 11.22       111     48          16        260    83      Good  65        10\n3 10.06       113     35          10        269    80    Medium  59        12\n4  7.40       117    100           4        466    97    Medium  55        14\n5  4.15       141     64           3        340   128       Bad  38        13\n6 10.81       124    113          13        501    72       Bad  78        16\n  Urban  US\n1   Yes Yes\n2   Yes Yes\n3   Yes Yes\n4   Yes Yes\n5   Yes  No\n6    No Yes\n\n\nThe Carseats data includes qualitative predictors such as shelveloc, an indicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor shelveloc takes on three possible values: Bad, Medium, and Good. Given a qualitative variable such as shelveloc, R generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.\n\nlm.fit &lt;- lm(Sales ~ . + Income:Advertising + Price:Age, \n    data = Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\n\nThe contrasts() function returns the coding that R uses for the dummy variables.\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\nUse ?contrasts to learn about other contrasts, and how to set them.\nR has created a ShelveLocGood dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a ShelveLocMedium dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for ShelveLocGood in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And ShelveLocMedium has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location.\n\n\n\nAs we have seen, R comes with many useful functions, and still more functions are available by way of R libraries. However, we will often be interested in performing an operation for which no function is available. In this setting, we may want to write our own function. For instance, below we provide a simple function that reads in the ISLR2 and MASS libraries, called LoadLibraries(). Before we have created the function, R returns an error if we try to call it.\n\nLoadLibraries\n\nError: object 'LoadLibraries' not found\n\nLoadLibraries()\n\nError in LoadLibraries(): could not find function \"LoadLibraries\"\n\n\nWe now create the function. Note that the + symbols are printed by R and should not be typed in. The { symbol informs R that multiple commands are about to be input. Hitting Enter after typing { will cause R to print the + symbol. We can then input as many commands as we wish, hitting {Enter} after each one. Finally the } symbol informs R that no further commands will be entered.\n\nLoadLibraries &lt;- function() {\n library(ISLR2)\n library(MASS)\n print(\"The libraries have been loaded.\")\n}\n\nNow if we type in LoadLibraries, R will tell us what is in the function.\n\nLoadLibraries\n\nfunction () \n{\n    library(ISLR2)\n    library(MASS)\n    print(\"The libraries have been loaded.\")\n}\n\n\nIf we call the function, the libraries are loaded in and the print statement is output.\n\nLoadLibraries()\n\n[1] \"The libraries have been loaded.\""
  },
  {
    "objectID": "teaching/stat-learn/material/02-linear-regression-I.html#libraries",
    "href": "teaching/stat-learn/material/02-linear-regression-I.html#libraries",
    "title": "Linear Regression I",
    "section": "",
    "text": "The library() function is used to load libraries, or groups of functions and data sets that are not included in the base R distribution. Basic functions that perform least squares linear regression and other simple analyses come standard with the base distribution, but more exotic functions require additional libraries. Here we load the MASS package, which is a very large collection of data sets and functions. We also load the ISLR2 package, which includes the data sets associated with this book.\n\nlibrary(MASS)\nlibrary(ISLR2)\n\n\nAttaching package: 'ISLR2'\n\n\nThe following object is masked from 'package:MASS':\n\n    Boston\n\n\nIf you receive an error message when loading any of these libraries, it likely indicates that the corresponding library has not yet been installed on your system. Some libraries, such as MASS, come with R and do not need to be separately installed on your computer. However, other packages, such as ISLR2, must be downloaded the first time they are used. This can be done directly from within R. For example, on a Windows system, select the Install package option under the Packages tab. After you select any mirror site, a list of available packages will appear. Simply select the package you wish to install and R will automatically download the package. Alternatively, this can be done at the R command line via install.packages(\"ISLR2\"). This installation only needs to be done the first time you use a package. However, the library() function must be called within each R session."
  },
  {
    "objectID": "teaching/stat-learn/material/02-linear-regression-I.html#simple-linear-regression",
    "href": "teaching/stat-learn/material/02-linear-regression-I.html#simple-linear-regression",
    "title": "Linear Regression I",
    "section": "",
    "text": "The ISLR2 library contains the Boston data set, which records medv (median house value) for \\(506\\) census tracts in Boston. We will seek to predict medv using \\(12\\) predictors such as rmvar (average number of rooms per house), age (proportion of owner-occupied units built prior to 1940) and lstat (percent of households with low socioeconomic status).\n\nknitr::kable(head(Boston)) # note: you only write head(Boston) here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nlstat\nmedv\n\n\n\n\n0.00632\n18\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n4.98\n24.0\n\n\n0.02731\n0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n9.14\n21.6\n\n\n0.02729\n0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n4.03\n34.7\n\n\n0.03237\n0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n2.94\n33.4\n\n\n0.06905\n0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n5.33\n36.2\n\n\n0.02985\n0\n2.18\n0\n0.458\n6.430\n58.7\n6.0622\n3\n222\n18.7\n5.21\n28.7\n\n\n\n\n\nTo find out more about the data set, we can type ?Boston.\nWe will start by using the lm() function to fit a simple linear regression model, with medv as the response and lstat as the predictor. The basic syntax is lm(y ~ x, data), where y is the response, x is the predictor, and data is the data set in which these two variables are kept.\n\nlm.fit &lt;- lm(medv ~ lstat)\n\nError in eval(predvars, data, env): object 'medv' not found\n\n\nThe command causes an error because R does not know where to find the variables medv and lstat. The next line tells R that the variables are in Boston. If we attach Boston, the first line works fine because R now recognizes the variables.\n\nlm.fit &lt;- lm(medv ~ lstat, data = Boston)\nattach(Boston)\nlm.fit &lt;- lm(medv ~ lstat)\n\nIf we type lm.fit, some basic information about the model is output. For more detailed information, we use summary(lm.fit). This gives us \\(p\\)-values and standard errors for the coefficients, as well as the \\(R^2\\) statistic and \\(F\\)-statistic for the model.\n\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\nWe can use the names() function in order to find out what other pieces of information are stored in lm.fit. Although we can extract these quantities by name—e.g. lm.fit$coefficients—it is safer to use the extractor functions like coef() to access them.\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\n\nIn order to obtain a confidence interval for the coefficient estimates, we can use the confint() command.\n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\n\nThe predict() function can be used to produce confidence intervals and prediction intervals for the prediction of medv for a given value of lstat.\n\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))),\n    interval = \"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit, data.frame(lstat = (c(5, 10, 15))),\n    interval = \"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n\nFor instance, the 95 % confidence interval associated with a lstat value of 10 is \\((24.47, 25.63)\\), and the 95 % prediction interval is \\((12.828, 37.28)\\). As expected, the confidence and prediction intervals are centered around the same point (a predicted value of \\(25.05\\) for medv when lstat equals 10), but the latter are substantially wider.\nWe will now plot medv and lstat along with the least squares regression line using the plot() and abline() functions.\n\nplot(lstat, medv)\nabline(lm.fit)\n\n\n\n\n\n\n\n\nThere is some evidence for non-linearity in the relationship between lstat and medv. We will explore this issue later in this lab.\nThe abline() function can be used to draw any line, not just the least squares regression line. To draw a line with intercept a and slope b, we type abline(a, b). Below we experiment with some additional settings for plotting lines and points. The lwd = 3 command causes the width of the regression line to be increased by a factor of 3; this works for the plot() and lines() functions also. We can also use the pch option to create different plotting symbols.\n\nplot(lstat, medv)\nabline(lm.fit, lwd = 3)\nabline(lm.fit, lwd = 3, col = \"red\")\n\n\n\n\n\n\n\nplot(lstat, medv, col = \"red\")\n\n\n\n\n\n\n\nplot(lstat, medv, pch = 20)\n\n\n\n\n\n\n\nplot(lstat, medv, pch = \"+\")\n\n\n\n\n\n\n\nplot(1:20, 1:20, pch = 1:20)\n\n\n\n\n\n\n\n\nNext we examine some diagnostic plots, several of which were discussed in Section 3.3.3. Four diagnostic plots are automatically produced by applying the plot() function directly to the output from lm(). In general, this command will produce one plot at a time, and hitting Enter will generate the next plot. However, it is often convenient to view all four plots together. We can achieve this by using the par() and mfrow() functions, which tell R to split the display screen into separate panels so that multiple plots can be viewed simultaneously. For example, par(mfrow = c(2, 2)) divides the plotting region into a \\(2 \\times 2\\) grid of panels.\n\npar(mfrow = c(2, 2))\nplot(lm.fit)\n\n\n\n\n\n\n\n\nAlternatively, we can compute the residuals from a linear regression fit using the residuals() function. The function rstudent() will return the studentized residuals, and we can use this function to plot the residuals against the fitted values.\n\nplot(predict(lm.fit), residuals(lm.fit))\n\n\n\n\n\n\n\nplot(predict(lm.fit), rstudent(lm.fit))\n\n\n\n\n\n\n\n\nOn the basis of the residual plots, there is some evidence of non-linearity. Leverage statistics can be computed for any number of predictors using the hatvalues() function.\n\nplot(hatvalues(lm.fit))\n\n\n\n\n\n\n\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\nThe which.max() function identifies the index of the largest element of a vector. In this case, it tells us which observation has the largest leverage statistic."
  },
  {
    "objectID": "teaching/stat-learn/material/02-linear-regression-I.html#multiple-linear-regression",
    "href": "teaching/stat-learn/material/02-linear-regression-I.html#multiple-linear-regression",
    "title": "Linear Regression I",
    "section": "",
    "text": "In order to fit a multiple linear regression model using least squares, we again use the lm() function. The syntax lm(y ~ x1 + x2 + x3) is used to fit a model with three predictors, x1, x2, and x3. The summary() function now outputs the regression coefficients for all the predictors.\n\nlm.fit &lt;- lm(medv ~ lstat + age, data = Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\nThe Boston data set contains 12 variables, and so it would be cumbersome to have to type all of these in order to perform a regression using all of the predictors. Instead, we can use the following short-hand:\n\nlm.fit &lt;- lm(medv ~ ., data = Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1304  -2.7673  -0.5814   1.9414  26.2526 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.617270   4.936039   8.431 3.79e-16 ***\ncrim         -0.121389   0.033000  -3.678 0.000261 ***\nzn            0.046963   0.013879   3.384 0.000772 ***\nindus         0.013468   0.062145   0.217 0.828520    \nchas          2.839993   0.870007   3.264 0.001173 ** \nnox         -18.758022   3.851355  -4.870 1.50e-06 ***\nrm            3.658119   0.420246   8.705  &lt; 2e-16 ***\nage           0.003611   0.013329   0.271 0.786595    \ndis          -1.490754   0.201623  -7.394 6.17e-13 ***\nrad           0.289405   0.066908   4.325 1.84e-05 ***\ntax          -0.012682   0.003801  -3.337 0.000912 ***\nptratio      -0.937533   0.132206  -7.091 4.63e-12 ***\nlstat        -0.552019   0.050659 -10.897  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.798 on 493 degrees of freedom\nMultiple R-squared:  0.7343,    Adjusted R-squared:  0.7278 \nF-statistic: 113.5 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\n\nWe can access the individual components of a summary object by name (type ?summary.lm to see what is available). Hence summary(lm.fit)$r.sq gives us the \\(R^2\\), and summary(lm.fit)$sigma gives us the RSE. The vif() function, part of the car package, can be used to compute variance inflation factors. Most VIF’s are low to moderate for this data. The car package is not part of the base R installation so it must be downloaded the first time you use it via the install.packages() function in R.\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.767486 2.298459 3.987181 1.071168 4.369093 1.912532 3.088232 3.954037 \n     rad      tax  ptratio    lstat \n7.445301 9.002158 1.797060 2.870777 \n\n\nWhat if we would like to perform a regression using all of the variables but one? For example, in the above regression output, age has a high \\(p\\)-value. So we may wish to run a regression excluding this predictor. The following syntax results in a regression using all predictors except age.\n\nlm.fit1 &lt;- lm(medv ~ . - age, data = Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.1851  -2.7330  -0.6116   1.8555  26.3838 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.525128   4.919684   8.441 3.52e-16 ***\ncrim         -0.121426   0.032969  -3.683 0.000256 ***\nzn            0.046512   0.013766   3.379 0.000785 ***\nindus         0.013451   0.062086   0.217 0.828577    \nchas          2.852773   0.867912   3.287 0.001085 ** \nnox         -18.485070   3.713714  -4.978 8.91e-07 ***\nrm            3.681070   0.411230   8.951  &lt; 2e-16 ***\ndis          -1.506777   0.192570  -7.825 3.12e-14 ***\nrad           0.287940   0.066627   4.322 1.87e-05 ***\ntax          -0.012653   0.003796  -3.333 0.000923 ***\nptratio      -0.934649   0.131653  -7.099 4.39e-12 ***\nlstat        -0.547409   0.047669 -11.483  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.794 on 494 degrees of freedom\nMultiple R-squared:  0.7343,    Adjusted R-squared:  0.7284 \nF-statistic: 124.1 on 11 and 494 DF,  p-value: &lt; 2.2e-16\n\n\nAlternatively, the update() function can be used.\n\nlm.fit1 &lt;- update(lm.fit, ~ . - age)"
  },
  {
    "objectID": "teaching/stat-learn/material/02-linear-regression-I.html#interaction-terms",
    "href": "teaching/stat-learn/material/02-linear-regression-I.html#interaction-terms",
    "title": "Linear Regression I",
    "section": "",
    "text": "It is easy to include interaction terms in a linear model using the lm() function. The syntax lstat:age tells R to include an interaction term between lstat and age. The syntax lstat * age simultaneously includes lstat, age, and the interaction term lstat\\(\\times\\)age as predictors; it is a shorthand for lstat + age + lstat:age. %We can also pass in transformed versions of the predictors.\n\nsummary(lm(medv ~ lstat * age, data = Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "teaching/stat-learn/material/02-linear-regression-I.html#non-linear-transformations-of-the-predictors",
    "href": "teaching/stat-learn/material/02-linear-regression-I.html#non-linear-transformations-of-the-predictors",
    "title": "Linear Regression I",
    "section": "",
    "text": "The lm() function can also accommodate non-linear transformations of the predictors. For instance, given a predictor \\(X\\), we can create a predictor \\(X^2\\) using I(X^2). The function I() is needed since the ^ has a special meaning in a formula object; wrapping as we do allows the standard usage in R, which is to raise X to the power 2. We now perform a regression of medv onto lstat and lstat^2.\n\nlm.fit2 &lt;- lm(medv ~ lstat + I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\nThe near-zero \\(p\\)-value associated with the quadratic term suggests that it leads to an improved model. We use the anova() function to further quantify the extent to which the quadratic fit is superior to the linear fit.\n\nlm.fit &lt;- lm(medv ~ lstat)\nanova(lm.fit, lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere Model 1 represents the linear submodel containing only one predictor, lstat, while Model 2 corresponds to the larger quadratic model that has two predictors, lstat and lstat^2. The anova() function performs a hypothesis test comparing the two models. The null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the full model is superior. Here the \\(F\\)-statistic is \\(135\\) and the associated \\(p\\)-value is virtually zero. This provides very clear evidence that the model containing the predictors lstat and lstat^2 is far superior to the model that only contains the predictor lstat. This is not surprising, since earlier we saw evidence for non-linearity in the relationship between medv and lstat. If we type\n\npar(mfrow = c(2, 2))\nplot(lm.fit2)\n\n\n\n\n\n\n\n\nthen we see that when the lstat^2 term is included in the model, there is little discernible pattern in the residuals.\nIn order to create a cubic fit, we can include a predictor of the form I(X^3). However, this approach can start to get cumbersome for higher-order polynomials. A better approach involves using the poly() function to create the polynomial within lm(). For example, the following command produces a fifth-order polynomial fit:\n\nlm.fit5 &lt;- lm(medv ~ poly(lstat, 5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\n\nThis suggests that including additional polynomial terms, up to fifth order, leads to an improvement in the model fit! However, further investigation of the data reveals that no polynomial terms beyond fifth order have significant \\(p\\)-values in a regression fit.\nBy default, the poly() function orthogonalizes the predictors: this means that the features output by this function are not simply a sequence of powers of the argument. However, a linear model applied to the output of the poly() function will have the same fitted values as a linear model applied to the raw polynomials (although the coefficient estimates, standard errors, and p-values will differ). In order to obtain the raw polynomials from the poly() function, the argument raw = TRUE must be used.\nOf course, we are in no way restricted to using polynomial transformations of the predictors. Here we try a log transformation.\n\nsummary(lm(medv ~ log(rm), data = Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "teaching/stat-learn/material/02-linear-regression-I.html#qualitative-predictors",
    "href": "teaching/stat-learn/material/02-linear-regression-I.html#qualitative-predictors",
    "title": "Linear Regression I",
    "section": "",
    "text": "We will now examine the Carseats data, which is part of the ISLR2 library. We will attempt to predict Sales (child car seat sales) in \\(400\\) locations based on a number of predictors.\n\nhead(Carseats)\n\n  Sales CompPrice Income Advertising Population Price ShelveLoc Age Education\n1  9.50       138     73          11        276   120       Bad  42        17\n2 11.22       111     48          16        260    83      Good  65        10\n3 10.06       113     35          10        269    80    Medium  59        12\n4  7.40       117    100           4        466    97    Medium  55        14\n5  4.15       141     64           3        340   128       Bad  38        13\n6 10.81       124    113          13        501    72       Bad  78        16\n  Urban  US\n1   Yes Yes\n2   Yes Yes\n3   Yes Yes\n4   Yes Yes\n5   Yes  No\n6    No Yes\n\n\nThe Carseats data includes qualitative predictors such as shelveloc, an indicator of the quality of the shelving location—that is, the space within a store in which the car seat is displayed—at each location. The predictor shelveloc takes on three possible values: Bad, Medium, and Good. Given a qualitative variable such as shelveloc, R generates dummy variables automatically. Below we fit a multiple regression model that includes some interaction terms.\n\nlm.fit &lt;- lm(Sales ~ . + Income:Advertising + Price:Age, \n    data = Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\n\nThe contrasts() function returns the coding that R uses for the dummy variables.\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\nUse ?contrasts to learn about other contrasts, and how to set them.\nR has created a ShelveLocGood dummy variable that takes on a value of 1 if the shelving location is good, and 0 otherwise. It has also created a ShelveLocMedium dummy variable that equals 1 if the shelving location is medium, and 0 otherwise. A bad shelving location corresponds to a zero for each of the two dummy variables. The fact that the coefficient for ShelveLocGood in the regression output is positive indicates that a good shelving location is associated with high sales (relative to a bad location). And ShelveLocMedium has a smaller positive coefficient, indicating that a medium shelving location is associated with higher sales than a bad shelving location but lower sales than a good shelving location."
  },
  {
    "objectID": "teaching/stat-learn/material/02-linear-regression-I.html#writing-functions",
    "href": "teaching/stat-learn/material/02-linear-regression-I.html#writing-functions",
    "title": "Linear Regression I",
    "section": "",
    "text": "As we have seen, R comes with many useful functions, and still more functions are available by way of R libraries. However, we will often be interested in performing an operation for which no function is available. In this setting, we may want to write our own function. For instance, below we provide a simple function that reads in the ISLR2 and MASS libraries, called LoadLibraries(). Before we have created the function, R returns an error if we try to call it.\n\nLoadLibraries\n\nError: object 'LoadLibraries' not found\n\nLoadLibraries()\n\nError in LoadLibraries(): could not find function \"LoadLibraries\"\n\n\nWe now create the function. Note that the + symbols are printed by R and should not be typed in. The { symbol informs R that multiple commands are about to be input. Hitting Enter after typing { will cause R to print the + symbol. We can then input as many commands as we wish, hitting {Enter} after each one. Finally the } symbol informs R that no further commands will be entered.\n\nLoadLibraries &lt;- function() {\n library(ISLR2)\n library(MASS)\n print(\"The libraries have been loaded.\")\n}\n\nNow if we type in LoadLibraries, R will tell us what is in the function.\n\nLoadLibraries\n\nfunction () \n{\n    library(ISLR2)\n    library(MASS)\n    print(\"The libraries have been loaded.\")\n}\n\n\nIf we call the function, the libraries are loaded in and the print statement is output.\n\nLoadLibraries()\n\n[1] \"The libraries have been loaded.\""
  },
  {
    "objectID": "teaching/stat-learn/material/05-knn-reg-class.html",
    "href": "teaching/stat-learn/material/05-knn-reg-class.html",
    "title": "KNN classification + regression",
    "section": "",
    "text": "We are going to look at the probability version of the KNN classifier algorithm. We create a function called KNN_classifier⁠ for performing KNN classification using the following arguments:\n\n\\(x_0\\) as the new point at which we wish to predict \\(y\\)\n\\({\\bf x} = (x_1,x_2, \\dots, x_n)\\) as the vector of training \\(x\\)’s, where \\(x_i\\) is real-valued\n\\({\\bf y} = (y_1,y_2, \\dots, y_n)\\) as the vector of training \\(y\\)’s, where \\(y_i\\) is 0 or 1\n\\(K\\) as number of neighbors to use\n\\(\\hat{p}_{1}\\) as the estimated probability of \\(y_0=1\\) given \\(x_0\\)\n\nThe function calculates the Euclidean distance between \\(x_0\\) and each of the \\(x_i\\)’s in the training set \\((x_1, x_2, \\dots, x_n)\\). Then we order them from nearest to furthest away and computes the fraction of \\(y\\) values of the \\(y\\) values of the \\(K\\) nearest training points that are equal to 1 and return this proportion as an estimated probability of \\(y_0=1\\). We can transform \\(\\hat{p}_{1}\\) to a prediction of the \\(y\\) value at \\(x_0\\) by using a threshold on \\(\\hat{p}_{1}\\) and return\n\nKNN_classifier = function(x0, x, y, K) {\n  distances = abs(x - x0)  \n  o = order(distances)  \n  p1_hat = mean(y[o[1:K]])  \n  return(p1_hat)  \n}\n\n\n\nWe simulate training vector \\(\\bf{x}\\) from a uniform distribution on the interval \\([0,5]\\) and the true probability \\(p_1(x)\\) of \\(y=1\\) given \\(x\\) (true relationship between \\(x\\) and \\(y\\)) according to \\[p_1(x) = \\frac{\\exp(2*\\cos(x))}{(1 + \\exp(2*\\cos(x)))}\\] We simulate simulate training \\(y\\)’s as Bernoulli random variables with probabilities \\(p_1(x)\\).\n\nset.seed(1)  # set random number generator\nn = 20 \nx = 5*runif(n)  \np1 = function(x) { exp(2*cos(x))/(1 + exp(2*cos(x))) }  \ny = rbinom(n,1,p1(x)) \n\n\n\n\nplot(x,y,col=2,pch=20,cex=2)  # plot training data\nx_grid = seq(from=0, to=5, by=0.01)  # grid of x values \nlines(x_grid,p1(x_grid))  # plot true p1(x) values for the grid\n\n\n\n\n\n\n\n\n\n\n\n\nNow we run the KNN_classifier function to predict \\(y\\) at each point on the grid of \\(x\\) values. For that we need to define \\(K\\), that is number of nearest neighbors to use. We start with setting it equal to 1 but this can be changed later as an exercise. Further, we predict the \\(y\\) values for each \\(x\\) in the grid by thresholding the estimated probabilities to \\(\\leq 0.5\\) and \\(&gt;0.5\\).\n\nK = 1\np1_grid_hat = sapply(x_grid, function(x0) { KNN_classifier(x0, x, y, K) })\ny_grid_hat = round(p1_grid_hat &gt; 0.5)  \n\nNext we add the predicted values to our plot:\n\nplot(x,y,col=2,pch=20,cex=2)  # plot training data\ntitle(paste(\"K =\",K))\nlines(x_grid,p1(x_grid))  # plot true p1(x) values \nlines(x_grid,p1_grid_hat,col=4)  # plot estimated probabilities of y=1 \nlines(x_grid,y_grid_hat,col=4)  # plot predicted y values for each x0\n\n\n\n\n\n\n\n\n\n\n\nThe training error rate is given by \\[\\textrm{training error} = \\frac{1}{n}\\sum_{i=1}^n  I(\\hat{y_i} \\neq y_i)\\] So we first run KNN classifier (probability version) at each \\(x\\) in the training set, then we predict the \\(y\\) values for each \\(x\\) in the training set (prediction version of KNN), and finally compute the compute the training error rate which is the on average misclassification rate.\n\np1_hat = sapply(x, function(x0) { KNN_classifier(x0, x, y, K) }) \ny_hat = round(p1_hat &gt; 0.5)  \ntrain_error = mean(y_hat != y)  \nprint(paste0(\"Training error rate (K = \",K,\") = \",train_error))\n\n[1] \"Training error rate (K = 1) = 0\"\n\n\nNow we compute the test error rate. We again simulate a large number of samples as test set, namely 10000. We simulate test \\(x\\)’s and test \\(y\\)’s and run the KNN classifier at each \\(x\\) in the test set. We then predict the \\(y\\) values for each \\(x\\) in the test set and compute the test error rate.\n\nn_test = 10000 \nx_test = 5*runif(n_test)  \ny_test = rbinom(n_test,1,p1(x_test))  \np1_test_hat = sapply(x_test, function(x0) { KNN_classifier(x0, x, y, K) })  \ny_test_hat = round(p1_test_hat &gt; 0.5) \ntest_error = mean(y_test_hat != y_test) \nprint(paste0(\"Test error rate (K = \",K,\"): \",test_error))\n\n[1] \"Test error rate (K = 1): 0.2869\"\n\n\nHow can we tell if the above is a good test error rate? We compute the test error rate for the Bayes optimal classifier.\n\n# Bayes optimal classifier\n# use the true p1(x) to make the best possible predictions on the training set\ny_hat_optimal = p1(x) &gt; 0.5  \n# compute the training error rate for the Bayes optimal classifier\ntrain_error_optimal = mean(y_hat_optimal != y)  \nprint(paste0(\"Training error rate (Optimal): \",train_error_optimal))\n\n[1] \"Training error rate (Optimal): 0.15\"\n\n# use the true p1(x) to make the best possible predictions on the test set\ny_test_hat_optimal = round(p1(x_test) &gt; 0.5)\n# compute the test error rate for the Bayes optimal classifier\ntest_error_optimal = mean(y_test_hat_optimal != y_test) \nprint(paste0(\"Test error rate (Optimal): \",test_error_optimal))\n\n[1] \"Test error rate (Optimal): 0.2495\""
  },
  {
    "objectID": "teaching/stat-learn/material/05-knn-reg-class.html#simulate-training-data",
    "href": "teaching/stat-learn/material/05-knn-reg-class.html#simulate-training-data",
    "title": "KNN classification + regression",
    "section": "",
    "text": "We simulate training vector \\(\\bf{x}\\) from a uniform distribution on the interval \\([0,5]\\) and the true probability \\(p_1(x)\\) of \\(y=1\\) given \\(x\\) (true relationship between \\(x\\) and \\(y\\)) according to \\[p_1(x) = \\frac{\\exp(2*\\cos(x))}{(1 + \\exp(2*\\cos(x)))}\\] We simulate simulate training \\(y\\)’s as Bernoulli random variables with probabilities \\(p_1(x)\\).\n\nset.seed(1)  # set random number generator\nn = 20 \nx = 5*runif(n)  \np1 = function(x) { exp(2*cos(x))/(1 + exp(2*cos(x))) }  \ny = rbinom(n,1,p1(x)) \n\n\n\n\nplot(x,y,col=2,pch=20,cex=2)  # plot training data\nx_grid = seq(from=0, to=5, by=0.01)  # grid of x values \nlines(x_grid,p1(x_grid))  # plot true p1(x) values for the grid"
  },
  {
    "objectID": "teaching/stat-learn/material/05-knn-reg-class.html#predicting-classes",
    "href": "teaching/stat-learn/material/05-knn-reg-class.html#predicting-classes",
    "title": "KNN classification + regression",
    "section": "",
    "text": "Now we run the KNN_classifier function to predict \\(y\\) at each point on the grid of \\(x\\) values. For that we need to define \\(K\\), that is number of nearest neighbors to use. We start with setting it equal to 1 but this can be changed later as an exercise. Further, we predict the \\(y\\) values for each \\(x\\) in the grid by thresholding the estimated probabilities to \\(\\leq 0.5\\) and \\(&gt;0.5\\).\n\nK = 1\np1_grid_hat = sapply(x_grid, function(x0) { KNN_classifier(x0, x, y, K) })\ny_grid_hat = round(p1_grid_hat &gt; 0.5)  \n\nNext we add the predicted values to our plot:\n\nplot(x,y,col=2,pch=20,cex=2)  # plot training data\ntitle(paste(\"K =\",K))\nlines(x_grid,p1(x_grid))  # plot true p1(x) values \nlines(x_grid,p1_grid_hat,col=4)  # plot estimated probabilities of y=1 \nlines(x_grid,y_grid_hat,col=4)  # plot predicted y values for each x0"
  },
  {
    "objectID": "teaching/stat-learn/material/05-knn-reg-class.html#error-rates",
    "href": "teaching/stat-learn/material/05-knn-reg-class.html#error-rates",
    "title": "KNN classification + regression",
    "section": "",
    "text": "The training error rate is given by \\[\\textrm{training error} = \\frac{1}{n}\\sum_{i=1}^n  I(\\hat{y_i} \\neq y_i)\\] So we first run KNN classifier (probability version) at each \\(x\\) in the training set, then we predict the \\(y\\) values for each \\(x\\) in the training set (prediction version of KNN), and finally compute the compute the training error rate which is the on average misclassification rate.\n\np1_hat = sapply(x, function(x0) { KNN_classifier(x0, x, y, K) }) \ny_hat = round(p1_hat &gt; 0.5)  \ntrain_error = mean(y_hat != y)  \nprint(paste0(\"Training error rate (K = \",K,\") = \",train_error))\n\n[1] \"Training error rate (K = 1) = 0\"\n\n\nNow we compute the test error rate. We again simulate a large number of samples as test set, namely 10000. We simulate test \\(x\\)’s and test \\(y\\)’s and run the KNN classifier at each \\(x\\) in the test set. We then predict the \\(y\\) values for each \\(x\\) in the test set and compute the test error rate.\n\nn_test = 10000 \nx_test = 5*runif(n_test)  \ny_test = rbinom(n_test,1,p1(x_test))  \np1_test_hat = sapply(x_test, function(x0) { KNN_classifier(x0, x, y, K) })  \ny_test_hat = round(p1_test_hat &gt; 0.5) \ntest_error = mean(y_test_hat != y_test) \nprint(paste0(\"Test error rate (K = \",K,\"): \",test_error))\n\n[1] \"Test error rate (K = 1): 0.2869\"\n\n\nHow can we tell if the above is a good test error rate? We compute the test error rate for the Bayes optimal classifier.\n\n# Bayes optimal classifier\n# use the true p1(x) to make the best possible predictions on the training set\ny_hat_optimal = p1(x) &gt; 0.5  \n# compute the training error rate for the Bayes optimal classifier\ntrain_error_optimal = mean(y_hat_optimal != y)  \nprint(paste0(\"Training error rate (Optimal): \",train_error_optimal))\n\n[1] \"Training error rate (Optimal): 0.15\"\n\n# use the true p1(x) to make the best possible predictions on the test set\ny_test_hat_optimal = round(p1(x_test) &gt; 0.5)\n# compute the test error rate for the Bayes optimal classifier\ntest_error_optimal = mean(y_test_hat_optimal != y_test) \nprint(paste0(\"Test error rate (Optimal): \",test_error_optimal))\n\n[1] \"Test error rate (Optimal): 0.2495\""
  },
  {
    "objectID": "teaching/stat-learn/material/05-knn-reg-class.html#simulate-training-data-1",
    "href": "teaching/stat-learn/material/05-knn-reg-class.html#simulate-training-data-1",
    "title": "KNN classification + regression",
    "section": "Simulate training data",
    "text": "Simulate training data\nWe simulate training vector \\(\\bf{x}\\) from a uniform distribution on the interval \\([0,5]\\) and simulate training vector \\(\\bf{y}\\) by assuming \\[y = f(x) + \\varepsilon\\] where \\(f(x) = \\cos(x)\\) and \\(\\varepsilon \\sim N(0, \\sigma^2)\\) and \\(\\sigma = 0.3\\).\n\nset.seed(1)  # set random number generator\nn = 20  # number of samples\nx = 5*runif(n)  \nsigma = 0.3  \nf = function(x) { cos(x) }  \ny = f(x) + sigma*rnorm(n)  \n\n\nPlot of the training data\n\nplot(x,y,col=2,pch=20,cex=2)  # plot training data\nx_grid = seq(from=0, to=5, by=0.01)  # grid of x values for plotting f(x) values\nlines(x_grid,f(x_grid))  # plot true f(x) values for the grid"
  },
  {
    "objectID": "teaching/stat-learn/material/05-knn-reg-class.html#predicting-values",
    "href": "teaching/stat-learn/material/05-knn-reg-class.html#predicting-values",
    "title": "KNN classification + regression",
    "section": "Predicting values",
    "text": "Predicting values\nNow we run the KNN function to predict \\(y\\) at each point on the grid of \\(x\\) values. For that we need to define \\(K\\), that is number of nearest neighbors to use. We start with setting it equal to 1 but this can be changed later as an exercise.\n\nK = 1 \ny_grid_hat = sapply(x_grid, function(x0) { KNN(x0, x, y, K) })\n\nNext we add the predicted values to our plot:\n\nplot(x,y,col=2,pch=20,cex=2)  # plot training data\ntitle(paste(\"K =\",K))\nlines(x_grid,f(x_grid))  # plot true f(x) values\nlines(x_grid,y_grid_hat,col=4)  # plot predicted y values \n\n\n\n\n\n\n\n\nWhat happens to predicted curve when you change the value of \\(K\\)?"
  },
  {
    "objectID": "teaching/stat-learn/material/05-knn-reg-class.html#bias-variance-trade-off",
    "href": "teaching/stat-learn/material/05-knn-reg-class.html#bias-variance-trade-off",
    "title": "KNN classification + regression",
    "section": "Bias-Variance Trade-Off",
    "text": "Bias-Variance Trade-Off\nWe are going to run through some code in order to illustrate the trade-off between bias and variance. We set \\(x_0\\) to 1.5, which is the point we wish to estimate \\(y\\) at.\nWe simulate 10000 data sets to approximate expectations over the \\(Y\\)’s (given fixed \\(x\\)). We initialize two vectors of zeros to hold predicted and true \\(y\\) values at \\(x_0\\). Then for each of the 10000 datasets simulated, we repeat the above syntax for prediction. For the first 5 datasets simulated, we plot out the results to see what is happening.\n\nK = 1  \nx0 = 1.5  \nn_datasets = 10000  \ny0_hat = rep(0,n_datasets)  \ny0 = rep(0,n_datasets) \nfor (i in 1:n_datasets) {\n  y = f(x) + sigma*rnorm(n) \n  y0[i] = f(x0) + sigma*rnorm(1) \n  y0_hat[i] = KNN(x0, x, y, K)  \n  if (i &lt;= 5) {\n    plot(x,y,col=2,pch=20,cex=2,ylim=c(-1.5,1.5))  \n    lines(x_grid,f(x_grid))  \n    y_grid_hat = sapply(x_grid, function(x0) { KNN(x0, x, y, K) })\n    lines(x_grid,y_grid_hat,col=4)  \n    points(x0,y0_hat[i],pch=20,cex=4,col=4)  # plot predicted value of y at x0\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext we calculate the bias and variance of the KNN predictions at \\(x_0\\).We also compute the variance of the noise at \\(x_0\\) in order to be able to get the test MSE both using the bias variance representation \\[\\textrm{test MSE} = \\textrm{bias}^2 + \\textrm{variance} + \\textrm{noise}\\] and the direct formula: \\[\\mathop{\\mathbb{E}} \\left(y_0- \\hat{f}(x_0)\\right)^2\\]\n\nbias = mean(y0_hat) - f(x0)  # bias of KNN predictions at x0\nvariance = var(y0_hat)  # variance of KNN predictions at x0\nnoise = sigma^2  # variance of the noise at x0\n\nbias^2 + variance + noise \n\n[1] 0.2086705\n\nmean((y0 - y0_hat)^2) \n\n[1] 0.2097131\n\n\nWhy do you think the two values differ?\nLet’s visualize and explain the bias-variance trade-off using the syntax above. We show how the bias, variance and test MSE is influenced by the choice of \\(K\\), and include three plots should be included showing the following:\n\nbias vs. \\(K\\) (or flexibility)\nvariance vs. \\(K\\) (or flexibility)\ntest MSE versus \\(K\\) (or flexibility)\n\nWe plot of MSE, bias^2 and variance against number of neighbors \\(K\\):\n\nK = 1:5\ny0_hat_matrix = matrix(0, nrow = n_datasets, ncol = length(K))\n\n\nstatsout &lt;- function(K) {\n  y0_hat_K = rep(0, n_datasets)\n  \n  for (i in 1:n_datasets) {\n    y = f(x) + sigma * rnorm(n)\n    y0[i] = f(x0) + sigma * rnorm(1)\n    y0_hat_K[i] = KNN(x0, x, y, K)\n  }\n  \n  bias_K = mean(y0_hat_K) - f(x0)\n  variance_K = var(y0_hat_K)\n  noise_K = sigma^2\n  MSE_K = bias_K^2 + variance_K + noise_K \n  \n  return(c(bias_K^2, variance_K, MSE_K))\n}\n\ntoplot = sapply(K, statsout)\n\n# Plot the bias-variance trade-off\nplot(K, toplot[1, ], type = \"l\", lty=1, xlab = \"K\", \n     ylab = \"Bias^2\", ylim = c(min(toplot), max(toplot)))\nlines(K, toplot[2, ], lty=2)\nlines(K, toplot[3, ],lty=3)\n# Add a legend\nlegend(\"topleft\", legend=c(\"Bias^2\", \"Variance\", \"MSE\"),lty=1:3, cex=0.8)"
  },
  {
    "objectID": "teaching/stat-learn/index.html",
    "href": "teaching/stat-learn/index.html",
    "title": "Statistical Learning",
    "section": "",
    "text": "Schedule\n\n\n\n\ndate\nslides\nhandout\npractical\ndata\n\n\n\n\n1: Introduction: What is Statistical Learning?\n22.10.2024\n\n\n\n.zip\n\n\n2: Linear Regression I\n29.10.2024\n\n\n\n\n\n\n3: Linear Regression II\n05.11.2024\n\n\n\n.zip\n\n\n4: Classification I\n12.11.2024\n\n\n\n\n\n\n5: Classification II\n19.11.2024\n\n\n \n\n\n\n6: Model Validation\n26.11.2024\n\n\n \n\n\n\n7: Model Selection & Regularization\n02.12.2024"
  },
  {
    "objectID": "teaching/tidyverse/material/nobel-laureates.html",
    "href": "teaching/tidyverse/material/nobel-laureates.html",
    "title": "Nobel Laureates",
    "section": "",
    "text": "In January 2017, Buzzfeed published an article on why Nobel laureates show immigration is so important for American science. You can read the article here. In the article they show that while most living Nobel laureates in the sciences are based in the US, many of them were born in other countries. This is one reason why scientific leaders say that immigration is vital for progress. In this lab we will work with the data from this article to recreate some of their visualizations as well as explore new questions."
  },
  {
    "objectID": "teaching/tidyverse/material/nobel-laureates.html#packages",
    "href": "teaching/tidyverse/material/nobel-laureates.html#packages",
    "title": "Nobel Laureates",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for much of the data wrangling. This package is already installed for you. You can load them by running the following in your Console:\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "teaching/tidyverse/material/nobel-laureates.html#data",
    "href": "teaching/tidyverse/material/nobel-laureates.html#data",
    "title": "Nobel Laureates",
    "section": "Data",
    "text": "Data\nThe dataset for this assignment can be found as a CSV (comma separated values) file. You can read it in using the following.\n\nnobel &lt;- read_csv(\"data/nobel.csv\")\n\nThe variable descriptions are as follows:\n\nid: ID number\nfirstname: First name of laureate\nsurname: Surname\nyear: Year prize won\ncategory: Category of prize\naffiliation: Affiliation of laureate\ncity: City of laureate in prize year\ncountry: Country of laureate in prize year\nborn_date: Birth date of laureate\ndied_date: Death date of laureate\ngender: Gender of laureate\nborn_city: City where laureate was born\nborn_country: Country where laureate was born\nborn_country_code: Code of country where laureate was born\ndied_city: City where laureate died\ndied_country: Country where laureate died\ndied_country_code: Code of country where laureate died\noverall_motivation: Overall motivation for recognition\nshare: Number of other winners award is shared with\nmotivation: Motivation for recognition\n\nIn a few cases the name of the city/country changed after laureate was given (e.g. in 1975 Bosnia and Herzegovina was called the Socialist Federative Republic of Yugoslavia). In these cases the variables below reflect a different name than their counterparts without the suffix `_original`.\n\nborn_country_original: Original country where laureate was born\nborn_city_original: Original city where laureate was born\ndied_country_original: Original country where laureate died\ndied_city_original: Original city where laureate died\ncity_original: Original city where laureate lived at the time of winning the award\ncountry_original: Original country where laureate lived at the time of winning the award"
  },
  {
    "objectID": "teaching/tidyverse/material/nobel-laureates.html#get-to-know-your-data",
    "href": "teaching/tidyverse/material/nobel-laureates.html#get-to-know-your-data",
    "title": "Nobel Laureates",
    "section": "Get to know your data",
    "text": "Get to know your data\n\nHow many observations and how many variables are in the dataset? Use inline code to answer this question. What does each row represent?\n\nThere are some observations in this dataset that we will exclude from our analysis to match the Buzzfeed results.\n\nCreate a new data frame called nobel_living that filters for\n\n\nlaureates for whom country is available\nlaureates who are people as opposed to organizations (organizations are denoted with \"org\" as their gender)\nlaureates who are still alive (their died_date is NA)\n\nConfirm that once you have filtered for these characteristics you are left with a data frame with 228 observations, once again using inline code."
  },
  {
    "objectID": "teaching/tidyverse/material/nobel-laureates.html#most-living-nobel-laureates-were-based-in-the-us-when-they-won-their-prizes",
    "href": "teaching/tidyverse/material/nobel-laureates.html#most-living-nobel-laureates-were-based-in-the-us-when-they-won-their-prizes",
    "title": "Nobel Laureates",
    "section": "Most living Nobel laureates were based in the US when they won their prizes",
    "text": "Most living Nobel laureates were based in the US when they won their prizes\n… says the Buzzfeed article. Let’s see if that’s true.\nFirst, we’ll create a new variable to identify whether the laureate was in the US when they won their prize. We’ll use the mutate() function for this. The following pipeline mutates the nobel_living data frame by adding a new variable called country_us. We use an if statement to create this variable. The first argument in the if_else() function we’re using to write this if statement is the condition we’re testing for. If country is equal to \"USA\", we set country_us to \"USA\". If not, we set the country_us to \"Other\".\nNote: we can achieve the same result using the fct_other() function we’ve seen before (i.e. with country_us = fct_other(country, \"USA\")). We decided to use the if_else() here to show you one example of an if statement in R.\n\nnobel_living &lt;- nobel_living %&gt;%\n  mutate(\n    country_us = if_else(country == \"USA\", \"USA\", \"Other\")\n  )\n\nNext, we will limit our analysis to only the following categories: Physics, Medicine, Chemistry, and Economics.\n\nnobel_living_science &lt;- nobel_living %&gt;%\n  filter(category %in% c(\"Physics\", \"Medicine\", \"Chemistry\", \"Economics\"))\n\nFor the next exercise work with the nobel_living_science data frame you created above.\n\nCreate a faceted bar plot visualizing the relationship between the category of prize and whether the laureate was in the US when they won the nobel prize. Interpret your visualization, and say a few words about whether the Buzzfeed headline is supported by the data.\n\nYour visualization should be faceted by category.\nFor each facet you should have two bars, one for winners in the US and one for Other.\nFlip the coordinates so the bars are horizontal, not vertical."
  },
  {
    "objectID": "teaching/tidyverse/material/nobel-laureates.html#but-of-those-us-based-nobel-laureates-many-were-born-in-other-countries",
    "href": "teaching/tidyverse/material/nobel-laureates.html#but-of-those-us-based-nobel-laureates-many-were-born-in-other-countries",
    "title": "Nobel Laureates",
    "section": "But of those US-based Nobel laureates, many were born in other countries",
    "text": "But of those US-based Nobel laureates, many were born in other countries\nHint: You should be able to cheat borrow from code you used earlier to create the country_us variable.\n\nCreate a new variable called born_country_us that has the value \"USA\" if the laureate is born in the US, and \"Other\" otherwise. How many of the winners are born in the US?\nAdd a second variable to your visualization from Exercise 3 based on whether the laureate was born in the US or not. Based on your visualization, do the data appear to support Buzzfeed’s claim? Explain your reasoning in 1-2 sentences.\n\nYour final visualization should contain a facet for each category.\nWithin each facet, there should be a bar for whether the laureate won the award in the US or not.\nEach bar should have segments for whether the laureate was born in the US or not."
  },
  {
    "objectID": "teaching/tidyverse/material/nobel-laureates.html#heres-where-those-immigrant-nobelists-were-born",
    "href": "teaching/tidyverse/material/nobel-laureates.html#heres-where-those-immigrant-nobelists-were-born",
    "title": "Nobel Laureates",
    "section": "Here’s where those immigrant Nobelists were born",
    "text": "Here’s where those immigrant Nobelists were born\nNote: your bar plot won’t exactly match the one from the Buzzfeed article. This is likely because the data has been updated since the article was published.\n\nIn a single pipeline, filter for laureates who won their prize in the US, but were born outside of the US, and then create a frequency table (with the count() function) for their birth country (born_country) and arrange the resulting data frame in descending order of number of observations for each country. Which country is the most common?"
  },
  {
    "objectID": "teaching/tidyverse/material/uoe-art.html",
    "href": "teaching/tidyverse/material/uoe-art.html",
    "title": "University of Edinburgh Art Collection",
    "section": "",
    "text": "The University of Edinburgh Art Collection “supports the world-leading research and teaching that happens within the University. Comprised of an astonishing range of objects and ideas spanning two millennia and a multitude of artistic forms, the collection reflects not only the long and rich trajectory of the University, but also major national and international shifts in art history.”\nIn this practical we’ll scrape data on all art pieces in the Edinburgh College of Art collection."
  },
  {
    "objectID": "teaching/tidyverse/material/uoe-art.html#r-scripts-vs.-quarto-documents",
    "href": "teaching/tidyverse/material/uoe-art.html#r-scripts-vs.-quarto-documents",
    "title": "University of Edinburgh Art Collection",
    "section": "R scripts vs. Quarto documents",
    "text": "R scripts vs. Quarto documents\nToday you’ll be using both R scripts and R Markdown documents:\n\nuse R scripts in the web scraping stage and ultimately save the scraped data as a csv.\nuse an Quarto document in the web analysis stage, where we start off by reading in the csv file we wrote out in the scraping stage."
  },
  {
    "objectID": "teaching/tidyverse/material/uoe-art.html#packages",
    "href": "teaching/tidyverse/material/uoe-art.html#packages",
    "title": "University of Edinburgh Art Collection",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for much of the data wrangling and visualisation, the robotstxt package to check if we’re allowed to scrape the data, the rvest package for data scraping.\n\nlibrary(tidyverse) \nlibrary(robotstxt)\nlibrary(rvest)"
  },
  {
    "objectID": "teaching/tidyverse/material/uoe-art.html#data",
    "href": "teaching/tidyverse/material/uoe-art.html#data",
    "title": "University of Edinburgh Art Collection",
    "section": "Data",
    "text": "Data\nThis assignment does not come with any prepared datasets. Instead you’ll be scraping the data! But before doing so, let’s check that a bot has permissions to access pages on this domain.\n\npaths_allowed(\"https://collections.ed.ac.uk/art)\")\n\n\n collections.ed.ac.uk                      \n\n\n[1] TRUE"
  },
  {
    "objectID": "teaching/tidyverse/material/uoe-art.html#scraping-a-single-page",
    "href": "teaching/tidyverse/material/uoe-art.html#scraping-a-single-page",
    "title": "University of Edinburgh Art Collection",
    "section": "Scraping a single page",
    "text": "Scraping a single page\nWe will start off by scraping data on the first 10 pieces in the collection from here.\nFirst, we define a new object called first_url, which is the link above. Then, we read the page at this url with the read_html() function from the rvest package. The code for this is already provided in 01-scrape-page-one.R.\n\n# set url\nfirst_url &lt;- \"https://collections.ed.ac.uk/art/search/*:*/Collection:%22edinburgh+college+of+art%7C%7C%7CEdinburgh+College+of+Art%22?offset=0\"\n\n# read html page\npage &lt;- read_html(first_url)\n\nFor the ten pieces on this page we will extract title, artist, and link information, and put these three variables in a data frame.\n\nTitles\nLet’s start with titles. We make use of the SelectorGadget to identify the tags for the relevant nodes:\n\n\n\n\n\n\n\n\n\n\npage %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%\n  html_node(\"h3 a\")\n\n{xml_nodeset (10)}\n [1] &lt;a href=\"./record/20567?highlight=*:*\"&gt;Tomb of Ferry de Gros, Lord of Dy ...\n [2] &lt;a href=\"./record/22534?highlight=*:*\"&gt;Tomb                              ...\n [3] &lt;a href=\"./record/21218?highlight=*:*\"&gt;Portrait of a Woman               ...\n [4] &lt;a href=\"./record/21881?highlight=*:*\"&gt;Untitled                          ...\n [5] &lt;a href=\"./record/20907?highlight=*:*\"&gt;Standing Male Nude                ...\n [6] &lt;a href=\"./record/20972?highlight=*:*\"&gt;Seated Male Nude with Blanket     ...\n [7] &lt;a href=\"./record/99439?highlight=*:*\"&gt;Untitled - Woman and Man          ...\n [8] &lt;a href=\"./record/50508?highlight=*:*\"&gt;Unknown                           ...\n [9] &lt;a href=\"./record/21054?highlight=*:*\"&gt;Striding Male Nude                ...\n[10] &lt;a href=\"./record/21038?highlight=*:*\"&gt;Standing Male Nude                ...\n\n\nThen we extract the text with html_text():\n\npage %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%\n  html_node(\"h3 a\") %&gt;%\n  html_text()\n\n [1] \"Tomb of Ferry de Gros, Lord of Dyeghem Nieunland                                    \"                           \n [2] \"Tomb                                    \"                                                                       \n [3] \"Portrait of a Woman                                                                            (1955)\"          \n [4] \"Untitled                                                                            (Unknown)\"                  \n [5] \"Standing Male Nude                                                                            (JAN 1958)\"       \n [6] \"Seated Male Nude with Blanket                                                                            (1959)\"\n [7] \"Untitled - Woman and Man                                                                            (1981)\"     \n [8] \"Unknown                                                                            (1957)\"                      \n [9] \"Striding Male Nude                                                                            (1950)\"           \n[10] \"Standing Male Nude                                    \"                                                         \n\n\nAnd get rid of all the spurious white space in the text with str_squish(), which reduces repeated whitespace inside a string.\nTake a look at the help for str_squish() to find out more about how it works and how it’s different from str_trim().\n\npage %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%\n  html_node(\"h3 a\") %&gt;%\n  html_text() %&gt;% \n  str_squish()\n\n [1] \"Tomb of Ferry de Gros, Lord of Dyeghem Nieunland\"\n [2] \"Tomb\"                                            \n [3] \"Portrait of a Woman (1955)\"                      \n [4] \"Untitled (Unknown)\"                              \n [5] \"Standing Male Nude (JAN 1958)\"                   \n [6] \"Seated Male Nude with Blanket (1959)\"            \n [7] \"Untitled - Woman and Man (1981)\"                 \n [8] \"Unknown (1957)\"                                  \n [9] \"Striding Male Nude (1950)\"                       \n[10] \"Standing Male Nude\"                              \n\n\nAnd finally save the resulting data as a vector of length 10:\n\ntitles &lt;- page %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%\n  html_node(\"h3 a\") %&gt;%\n  html_text() %&gt;%\n  str_squish()\n\n\n\nLinks\nThe same nodes that contain the text for the titles also contains information on the links to individual art piece pages for each title. We can extract this information using a new function from the rvest package, html_attr(), which extracts attributes.\nA mini HTML lesson! The following is how we define hyperlinked text in HTML:\n&lt;a href=\"https://www.google.com\"&gt;Search on Google&lt;/a&gt;\nAnd this is how the text would look like on a webpage: Search on Google.\nHere the text is Search on Google and the href attribute contains the url of the website you’d go to if you click on the hyperlinked text: https://www.google.com.\nThe moral of the story is: the link is stored in the href attribute.\n\npage %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%   # same nodes\n  html_node(\"h3 a\") %&gt;%         # as before\n  html_attr(\"href\")             # but get href attribute instead of text\n\n [1] \"./record/20567?highlight=*:*\" \"./record/22534?highlight=*:*\"\n [3] \"./record/21218?highlight=*:*\" \"./record/21881?highlight=*:*\"\n [5] \"./record/20907?highlight=*:*\" \"./record/20972?highlight=*:*\"\n [7] \"./record/99439?highlight=*:*\" \"./record/50508?highlight=*:*\"\n [9] \"./record/21054?highlight=*:*\" \"./record/21038?highlight=*:*\"\n\n\nThese don’t really look like URLs as we know then though. They’re relative links.\nSee the help for str_replace() to find out how it works. Remember that the first argument is passed in from the pipeline, so you just need to define the pattern and replacement arguments.\n\nClick on one of art piece titles in your browser and take note of the url of the webpage it takes you to. Think about how that url compares to what we scraped above? How is it different? Using str_replace(), fix the URLs. You’ll note something special happening in the pattern to replace. We want to replace the ., but we have it as \\\\.. This is because the period . is a special character and so we need to escape it first with backslashes, \\\\s.\n\n\n\nArtists\n\nFill in the blanks to scrape artist names.\n\n\n\nPut it altogether\n\nFill in the blanks to organize everything in a tibble.\n\n\n\nScrape the next page\n\nClick on the next page, and grab its url. Fill in the blank in to define a new object: second_url. Copy-paste code from top of the R script to scrape the new set of art pieces, and save the resulting data frame as second_ten."
  },
  {
    "objectID": "teaching/tidyverse/material/uoe-art.html#functions",
    "href": "teaching/tidyverse/material/uoe-art.html#functions",
    "title": "University of Edinburgh Art Collection",
    "section": "Functions",
    "text": "Functions\nYou’ve been using R functions, now it’s time to write your own!\nLet’s start simple. Here is a function that takes in an argument x, and adds 2 to it.\n\nadd_two &lt;- function(x){\n  x + 2\n}\n\nLet’s test it:\n\nadd_two(3)\n\n[1] 5\n\nadd_two(10)\n\n[1] 12\n\n\nThe skeleton for defining functions in R is as follows:\n\nfunction_name &lt;- function(input){\n  # do something with the input(s)\n  # return something\n}\n\nThen, a function for scraping a page should look something like:\nReminder: Function names should be short but evocative verbs.\n\nfunction_name &lt;- function(url){\n  # read page at url\n  # extract title, link, artist info for n pieces on page\n  # return a n x 3 tibble\n}\n\n\nFill in the blanks using code you already developed in the previous exercises. Name the function scrape_page.\n\nTest out your new function by running the following in the console. Does the output look right? Discuss with teammates whether you’re getting the same results as before.\n\nscrape_page(first_url)\nscrape_page(second_url)"
  },
  {
    "objectID": "teaching/tidyverse/material/uoe-art.html#iteration",
    "href": "teaching/tidyverse/material/uoe-art.html#iteration",
    "title": "University of Edinburgh Art Collection",
    "section": "Iteration",
    "text": "Iteration\nWe went from manually scraping individual pages to writing a function to do the same. Next, we will work on making our workflow a little more efficient by using R to iterate over all pages that contain information on the art collection.\nThat means we give develop a list of URLs (of pages that each have 10 art pieces), and write some code that applies the scrape_page() function to each page, and combines the resulting data frames from each page into a single data frame with 3289 rows and 3 columns.\n\nList of URLs\nClick through the first few of the pages in the art collection and observe their URLs to confirm the following pattern:\n[sometext]offset=0     # Pieces 1-10\n[sometext]offset=10    # Pieces 11-20\n[sometext]offset=20    # Pieces 21-30\n[sometext]offset=30    # Pieces 31-40\n...\n[sometext]offset=3280  # Pieces 3281-3289\nWe can construct these URLs in R by pasting together two pieces: (1) a common (root) text for the beginning of the URL, and (2) numbers starting at 0, increasing by 10, all the way up to 3289. Two new functions are helpful for accomplishing this: glue() for pasting two pieces of text and seq() for generating a sequence of numbers.\n\nFill in the blanks to construct the list of URLs.\n\n\n\nMapping\nFinally, we’re ready to iterate over the list of URLs we constructed. We will do this by mapping the function we developed over the list of URLs. There are a series of mapping functions in R and they each take the following form:\nmap([x], [function to apply to each element of x])\nIn our case x is the list of URLs we constructed and the function to apply to each element of x is the function we developed earlier, scrape_page. And as a result we want a data frame, so we use map_dfr function:\n\nmap_dfr(urls, scrape_page)\n\n\nFill in the blanks to scrape all pages, and to create a new data frame called uoe_art.\n\n\n\nWrite out data\n\nFinally write out the data frame you constructed into the data folder so that you can use it in the analysis section."
  },
  {
    "objectID": "teaching/tidyverse/material/uoe-art.html#analysis",
    "href": "teaching/tidyverse/material/uoe-art.html#analysis",
    "title": "University of Edinburgh Art Collection",
    "section": "Analysis",
    "text": "Analysis\nFor the rest of the exercises you can work in Quarto/R Markdown.\nNow that we have a tidy dataset that we can analyze, let’s do that!\nWe’ll start with some data cleaning, to clean up the dates that appear at the end of some title text in parentheses. Some of these are years, others are more specific dates, some art pieces have no date information whatsoever, and others have some non-date information in parentheses. This should be interesting to clean up!\nFirst thing we’ll try is to separate the title column into two: one for the actual title and the other for the date if it exists. In human speak, we need to\n“separate the title column at the first occurrence of ( and put the contents on one side of the ( into a column called title and the contents on the other side into a column called date”\nLuckily, there’s a function that does just this: separate()!\nAnd once we have completed separating the single title column into title and date, we need to do further clean-up in the date column to get rid of extraneous )s with str_remove(), capture year information, and save the data as a numeric variable.\n\nFill in the blanks in to implement the data wrangling we described above. Note that this will result in some warnings when you run the code, and that’s OK! Read the warnings, and explain what they mean, and why we are ok with leaving them in given that our objective is to just capture year where it’s convenient to do so.\nPrint out a summary of the data frame using the skim() function. How many pieces have artist info missing? How many have year info missing?\nMake a histogram of years. Use a reasonable binwidth. Do you see anything out of the ordinary?\nFind which piece has the out of the ordinary year and go to its page on the art collection website to find the correct year for it. Can you tell why our code didn’t capture the correct year information? Correct the error in the data frame and visualize the data again.\n\nHint: You’ll want to use mutate() and if_else() or case_when() to implement the correction.\n\nWho is the most commonly featured artist in the collection? Do you know them? Any guess as to why the university has so many pieces from them?\nFinal question! How many art pieces have the word “child” in their title? Try to figure it out, and ask for help if you’re stuck.\n\nHint: str_subset() can be helful here. You should consider how you might capture titles where the word appears as “child” and “Child”.\nSource: https://collections.ed.ac.uk/art/about"
  },
  {
    "objectID": "teaching/tidyverse/material/plastic-waste.html",
    "href": "teaching/tidyverse/material/plastic-waste.html",
    "title": "Global plastic waste",
    "section": "",
    "text": "Plastic pollution is a major and growing problem, negatively affecting oceans and wildlife health. Our World in Data has a lot of great data at various levels including globally, per country, and over time. For this lab we focus on data from 2010.\nAdditionally, National Geographic ran a data visualization communication contest on plastic waste as seen here."
  },
  {
    "objectID": "teaching/tidyverse/material/plastic-waste.html#packages",
    "href": "teaching/tidyverse/material/plastic-waste.html#packages",
    "title": "Global plastic waste",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for this analysis.\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "teaching/tidyverse/material/plastic-waste.html#data",
    "href": "teaching/tidyverse/material/plastic-waste.html#data",
    "title": "Global plastic waste",
    "section": "Data",
    "text": "Data\nThe dataset for this assignment can be found as a csv file. You can read it in using the following (make sure you save the data in your working directory).\n\nplastic_waste &lt;- read_csv(\"data/plastic-waste.csv\")\n\nThe variable descriptions are as follows:\n\ncode: 3 Letter country code\nentity: Country name\ncontinent: Continent name\nyear: Year\ngdp_per_cap: GDP per capita constant 2011 international $, rate\nplastic_waste_per_cap: Amount of plastic waste per capita in kg/day\nmismanaged_plastic_waste_per_cap: Amount of mismanaged plastic waste per capita in kg/day\nmismanaged_plastic_waste: Tonnes of mismanaged plastic waste\ncoastal_pop: Number of individuals living on/near coast\ntotal_pop: Total population according to Gapminder"
  },
  {
    "objectID": "teaching/tidyverse/material/nobels-csv.html",
    "href": "teaching/tidyverse/material/nobels-csv.html",
    "title": "Nobel winners",
    "section": "",
    "text": "library(tidyverse)\n\nLet’s first load the data:\n\nnobel &lt;- ___(___)\n\nThen let’s split the data into two:\n\n# stem laureates\n___ &lt;- nobel %&gt;%\n  filter(___)\n\n# non-steam laureates\n___ &lt;- nobel %&gt;%\n  filter(___)\n\nAnd finally write out the data:\n\n# add code for writing out the two data frames here"
  },
  {
    "objectID": "teaching/tidyverse/material/brexit.html",
    "href": "teaching/tidyverse/material/brexit.html",
    "title": "Brexit",
    "section": "",
    "text": "library(tidyverse)\n\nIn September 2019, YouGov survey asked 1,639 GB adults the following question:\n\nIn hindsight, do you think Britain was right/wrong to vote to leave EU?\n\nRight to leave\n\nWrong to leave\n\nDon’t know\n\n\nThe data from the survey is in data/brexit.csv.\n\nbrexit &lt;- read_csv(\"data11/brexit.csv\")\n\nIn the course video we made the following visualisation.\n\nbrexit &lt;- brexit %&gt;%\n  mutate(\n    region = fct_relevel(region, \"london\", \"rest_of_south\", \"midlands_wales\", \"north\", \"scot\"),\n    region = fct_recode(region, London = \"london\", `Rest of South` = \"rest_of_south\", `Midlands / Wales` = \"midlands_wales\", North = \"north\", Scotland = \"scot\")\n  )\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  ) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\",\n    \"Right\" = \"#67a9cf\",\n    \"Don't know\" = \"gray\"\n  )) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn this application exercise we tell different stories with the same data.\n\nExercise 1 - Free scales\nAdd scales = \"free_x\" as an argument to the facet_wrap() function. How does the visualisation change? How is the story this visualisation telling different than the story the original plot tells?\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region,\n    nrow = 1, labeller = label_wrap_gen(width = 12),\n    # ___\n  ) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  ) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\",\n    \"Right\" = \"#67a9cf\",\n    \"Don't know\" = \"gray\"\n  )) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nExercise 2 - Comparing proportions across facets\nFirst, calculate the proportion of wrong, right, and don’t know answers in each category and then plot these proportions (rather than the counts) and then improve axis labeling. How is the story this visualisation telling different than the story the original plot tells? Hint: You’ll need the scales package to improve axis labeling, which means you’ll need to load it on top of the document as well.\n\n\nExercise 3 - Comparing proportions across bars\nRecreate the same visualisation from the previous exercise, this time dodging the bars for opinion proportions for each region, rather than faceting by region and then improve the legend. How is the story this visualisation telling different than the story the previous plot tells?"
  },
  {
    "objectID": "teaching/tidyverse/material/countries-of-the-world.html",
    "href": "teaching/tidyverse/material/countries-of-the-world.html",
    "title": "Countries of the world",
    "section": "",
    "text": "In order to complete this assignment you will need a Chrome browser with the Selector Gadget extension installed.\nThis website lists the names of 250 countries, as well as their flag, capital, population and size in square kilometres. Our goal could be to read this information into R for each country so that we can potentially analyse it further.\nBefore we start, we should load the required packages (we will also need the tidyverse package this time) and read the website with the function read_html() and assign it to an R object.\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(DT)\n\npage &lt;- read_html(\"https://scrapethissite.com/pages/simple/\")"
  },
  {
    "objectID": "teaching/tidyverse/material/countries-of-the-world.html#country-names",
    "href": "teaching/tidyverse/material/countries-of-the-world.html#country-names",
    "title": "Countries of the world",
    "section": "Country names",
    "text": "Country names\nUse the Selector Gadget to identify the CSS selectors needed to extract country names.\n\ncountry &lt;- page %&gt;%\n  html_elements(\".country-name\") %&gt;%\n  html_text(trim = TRUE) \n\nhead(country)\n\n[1] \"Andorra\"              \"United Arab Emirates\" \"Afghanistan\"         \n[4] \"Antigua and Barbuda\"  \"Anguilla\"             \"Albania\""
  },
  {
    "objectID": "teaching/tidyverse/material/countries-of-the-world.html#capitals-population-and-area",
    "href": "teaching/tidyverse/material/countries-of-the-world.html#capitals-population-and-area",
    "title": "Countries of the world",
    "section": "Capitals, population and area",
    "text": "Capitals, population and area\nLet us now turn to the further information for each country. Again use the selector gadget to identify the CSS selector needed which in this case is .country-info:\n\npage %&gt;%\n  html_elements(\".country-info\") %&gt;%\n  html_text(trim = TRUE) %&gt;% \n  head(n = 10)\n\n [1] \"Capital: Andorra la VellaPopulation: 84000Area (km2): 468.0\"   \n [2] \"Capital: Abu DhabiPopulation: 4975593Area (km2): 82880.0\"      \n [3] \"Capital: KabulPopulation: 29121286Area (km2): 647500.0\"        \n [4] \"Capital: St. John'sPopulation: 86754Area (km2): 443.0\"         \n [5] \"Capital: The ValleyPopulation: 13254Area (km2): 102.0\"         \n [6] \"Capital: TiranaPopulation: 2986952Area (km2): 28748.0\"         \n [7] \"Capital: YerevanPopulation: 2968000Area (km2): 29800.0\"        \n [8] \"Capital: LuandaPopulation: 13068161Area (km2): 1246700.0\"      \n [9] \"Capital: NonePopulation: 0Area (km2): 1.4E7\"                   \n[10] \"Capital: Buenos AiresPopulation: 41343201Area (km2): 2766890.0\"\n\n\nSo we get the names of the capitals, but also the population and the size of the country. The selector was not specific enough and we have to tell html_elements() more precisely which of these we are interested in. These CSS selectors differ between the three countries’ information:\n\nThe selector country-capital gives us the capital of the countries:\n\n\ncapital &lt;- page %&gt;%\n  html_elements(\".country-capital\") %&gt;%\n  html_text(trim = TRUE) \n\nhead(capital)\n\n[1] \"Andorra la Vella\" \"Abu Dhabi\"        \"Kabul\"            \"St. John's\"      \n[5] \"The Valley\"       \"Tirana\"          \n\n\n\nThe selector country-population gives us the population of the countries:\n\n\npopulation &lt;-  page %&gt;%\n  html_elements(\".country-population\") %&gt;%\n  html_text() %&gt;% \n  as.numeric()\nhead(population)\n\n[1]    84000  4975593 29121286    86754    13254  2986952\n\n\n\nThe selector country-area gives us the area of the countries:\n\n\narea &lt;-  page %&gt;%\n  html_elements(\".country-area\") %&gt;%\n  html_text() %&gt;% \n  as.numeric()\nhead(area)\n\n[1]    468  82880 647500    443    102  28748\n\n\nNote that we need to tell R to interpret the “text” read from the HTML code as numbers using the function as.numeric()."
  },
  {
    "objectID": "teaching/tidyverse/material/countries-of-the-world.html#merge-into-one-tibble",
    "href": "teaching/tidyverse/material/countries-of-the-world.html#merge-into-one-tibble",
    "title": "Countries of the world",
    "section": "Merge into one tibble",
    "text": "Merge into one tibble\nWe could already continue working with this, but for many applications it is more practical if we combine the data in a vertical form:\n\ncountries &lt;- tibble(\n  country = country,\n  capital = capital,\n  population = population,\n  area = area\n)\ncountries\n\n# A tibble: 250 × 4\n   country              capital          population     area\n   &lt;chr&gt;                &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;\n 1 Andorra              Andorra la Vella      84000      468\n 2 United Arab Emirates Abu Dhabi           4975593    82880\n 3 Afghanistan          Kabul              29121286   647500\n 4 Antigua and Barbuda  St. John's            86754      443\n 5 Anguilla             The Valley            13254      102\n 6 Albania              Tirana              2986952    28748\n 7 Armenia              Yerevan             2968000    29800\n 8 Angola               Luanda             13068161  1246700\n 9 Antarctica           None                      0 14000000\n10 Argentina            Buenos Aires       41343201  2766890\n# ℹ 240 more rows"
  },
  {
    "objectID": "teaching/tidyverse/material/countries-of-the-world.html#all-in-one-step",
    "href": "teaching/tidyverse/material/countries-of-the-world.html#all-in-one-step",
    "title": "Countries of the world",
    "section": "All in one step",
    "text": "All in one step\nIf we are sure that we do not need the individual vectors, we can also perform the reading of the data and the creation of the tibble in a single step. Below you can see how the complete scraping process can be completed in relatively few lines.\n\npage &lt;- \"https://scrapethissite.com/pages/simple/\" %&gt;%\n  read_html()\n\ncountries_2 &lt;- tibble(\n  Land = page %&gt;%\n    html_elements(css = \".country-name\") %&gt;% \n    html_text(trim = TRUE),\n  capital = page %&gt;% \n    html_elements(css = \".country-capital\") %&gt;% \n    html_text(),\n  population = page %&gt;% \n    html_elements(css = \".country-population\") %&gt;% \n    html_text() %&gt;% \n    as.numeric(),\n  area = page %&gt;% \n    html_elements(css = \".country-area\") %&gt;% \n    html_text() %&gt;% \n    as.numeric()\n)\n\ncountries_2\n\n# A tibble: 250 × 4\n   Land                 capital          population     area\n   &lt;chr&gt;                &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;\n 1 Andorra              Andorra la Vella      84000      468\n 2 United Arab Emirates Abu Dhabi           4975593    82880\n 3 Afghanistan          Kabul              29121286   647500\n 4 Antigua and Barbuda  St. John's            86754      443\n 5 Anguilla             The Valley            13254      102\n 6 Albania              Tirana              2986952    28748\n 7 Armenia              Yerevan             2968000    29800\n 8 Angola               Luanda             13068161  1246700\n 9 Antarctica           None                      0 14000000\n10 Argentina            Buenos Aires       41343201  2766890\n# ℹ 240 more rows"
  },
  {
    "objectID": "teaching/tidyverse/material/nyc-flights.html",
    "href": "teaching/tidyverse/material/nyc-flights.html",
    "title": "NYC flights",
    "section": "",
    "text": "For the below exercies we use the tidyverse package for much of the data wrangling and visualisation and the data lives in the nycflights13 package. This is data a dataset of flights departing from New York City (NYC) airports in the year 2013.\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\nThe data sets available in the package can be viewed using the following syntax:\nYou will need one or combinations of these to solve the following exercises. These tables are organized as the figure below shows."
  },
  {
    "objectID": "teaching/tidyverse/material/nyc-flights.html#packages-and-data",
    "href": "teaching/tidyverse/material/nyc-flights.html#packages-and-data",
    "title": "NYC flights",
    "section": "",
    "text": "For the below exercies we use the tidyverse package for much of the data wrangling and visualisation and the data lives in the nycflights13 package. This is data a dataset of flights departing from New York City (NYC) airports in the year 2013.\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\nThe data sets available in the package can be viewed using the following syntax:\nYou will need one or combinations of these to solve the following exercises. These tables are organized as the figure below shows."
  },
  {
    "objectID": "teaching/tidyverse/material/nyc-flights.html#familiarizing-ourselves-with-the-dataset",
    "href": "teaching/tidyverse/material/nyc-flights.html#familiarizing-ourselves-with-the-dataset",
    "title": "NYC flights",
    "section": "Familiarizing ourselves with the dataset",
    "text": "Familiarizing ourselves with the dataset\n\nWhat variables are included in the flights dataset? How many rows are there?\nWhat variables are included in the airports dataset? How many rows are there?\nWhich variables are included in the airlines dataset? How many rows are there?"
  },
  {
    "objectID": "teaching/tidyverse/material/nyc-flights.html#focusing-on-atlanta",
    "href": "teaching/tidyverse/material/nyc-flights.html#focusing-on-atlanta",
    "title": "NYC flights",
    "section": "Focusing on Atlanta",
    "text": "Focusing on Atlanta\n\nLet’s focus on flights from NYC area airports to Atlanta GA (FAA code ATL). Create a new object atlanta that includes only these flights. Hint: use filter()). How many flights to Atlanta were there in 2013?"
  },
  {
    "objectID": "teaching/tidyverse/material/nyc-flights.html#seasonality",
    "href": "teaching/tidyverse/material/nyc-flights.html#seasonality",
    "title": "NYC flights",
    "section": "Seasonality",
    "text": "Seasonality\n\nIs there a difference in the number of flights per month?\nSummarize the number of flights for each month and provide a sorted list with the months with the most flights first. Hint: use group_by() in combination with summarize())."
  },
  {
    "objectID": "teaching/tidyverse/material/nyc-flights.html#use-filter",
    "href": "teaching/tidyverse/material/nyc-flights.html#use-filter",
    "title": "NYC flights",
    "section": "Use filter()",
    "text": "Use filter()\n\nFind all flights that\n\n\nHad an arrival delay of two or more hours.\nFlew to Houston (IAH or HOU)\nWere operated by United, American, or Delta. Hint: In the flights dataset, the column carrier indicates the airline, but it uses two-character carrier codes. You can find the carrier codes for the airlines in the airlines dataset. Since the carrier code dataset only has 16 rows, and the names of the airlines in that dataset are not exactly “United”, “American”, or “Delta”, it is easiest to manually look up their carrier codes in that data.\nDeparted in summer (July, August, and September). Hint: the summer flights are those that departed in months 7 (July), 8 (August), and 9 (September).\nArrived more than two hours late, but didn’t leave late. Hint: Flights that arrived more than two hours late, but didn’t leave late will have an arrival delay of more than 120 minutes (arr_delay &gt; 120) and a non-positive departure delay (dep_delay &lt;=0)\nWere delayed by at least an hour, but made up over 30 minutes in flight. Hint: If a flight was delayed by at least an hour, then dep_delay &gt;= 60. If the flight didn’t make up any time in the air, then its arrival would be delayed by the same amount as its departure, meaning dep_delay == arr_delay, or alternatively, dep_delay - arr_delay == 0. If it makes up over 30 minutes in the air, then the arrival delay must be at least 30 minutes less than the departure delay, which is stated as dep_delay - arr_delay &gt; 30.\nDeparted between midnight and 6 am (inclusive). Hint: In dep_time, midnight is represented by 2400, not 0. You can verify this by checking the minimum and maximum of dep_time."
  },
  {
    "objectID": "teaching/tidyverse/material/nyc-flights.html#arrange-rows-with-arrange",
    "href": "teaching/tidyverse/material/nyc-flights.html#arrange-rows-with-arrange",
    "title": "NYC flights",
    "section": "Arrange rows with arrange()",
    "text": "Arrange rows with arrange()\n\nHow could you use arrange() to sort all missing values to the start? Hint: use is.na()) and add an indicator of whether the column has a missing value, the flights will first be sorted by desc(is.na(dep_time)). Since desc(is.na(dep_time)) is either TRUE when dep_time is missing, or FALSE, when it is not, the rows with missing values of dep_time will come first, since TRUE &gt; FALSE.\nSort flights to find the most delayed flights. Find the flights that left earliest.\nSort flights to find the fastest flights."
  },
  {
    "objectID": "teaching/tidyverse/material/nyc-flights.html#seelct-variables-with-select",
    "href": "teaching/tidyverse/material/nyc-flights.html#seelct-variables-with-select",
    "title": "NYC flights",
    "section": "Seelct variables with select()",
    "text": "Seelct variables with select()\n\nWhat does the one_of() function do? Why might it be helpful in conjunction with this vector?"
  },
  {
    "objectID": "teaching/tidyverse/material/nyc-flights.html#add-new-variables-with-mutate",
    "href": "teaching/tidyverse/material/nyc-flights.html#add-new-variables-with-mutate",
    "title": "NYC flights",
    "section": "Add new variables with mutate()",
    "text": "Add new variables with mutate()\n\nCompare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it?\nCome up with another approach that will give you the same output as not_cancelled %&gt;% count(dest) and not_cancelled %&gt;% count(tailnum, wt = distance) (without using count()).\nLook at the number of cancelled flights per day. Is there a pattern? Create a plot to visualize your answers.\nFor each destination, compute the total minutes of delay. For each flight, compute the proportion of the total delay for its destination.\nDelays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag() to explore how the delay of a flight is related to the delay of the immediately preceding flight. Use a plot to visualize this."
  },
  {
    "objectID": "teaching/tidyverse/material/nyc-flights.html#more-viz",
    "href": "teaching/tidyverse/material/nyc-flights.html#more-viz",
    "title": "NYC flights",
    "section": "More Viz",
    "text": "More Viz\n\nVisualize the distribution of on time departure rate across the three airports using a segmented bar plot. Hint: Remove NA’s and suppose that a flight that is delayed for less than 5 minutes is basically “on time”."
  },
  {
    "objectID": "teaching/tidyverse/material/nyc-flights.html#advanced-exercises",
    "href": "teaching/tidyverse/material/nyc-flights.html#advanced-exercises",
    "title": "NYC flights",
    "section": "Advanced Exercises:",
    "text": "Advanced Exercises:\n\nImagine you wanted to draw (approximately) the route each plane flies from its origin to its destination. What variables would you need? What tables from the package you loaded would you need to combine?\nThis plots the approximate flight paths of the first 100 flights in the flights dataset. Try reproducing it. Hint: you can create a layer of map borders using borders(state).\n\n\n\n\n\n\n\n\n\n\n\nWe know that some days of the year are “special”, and fewer people than usual fly on them. Since it is US data for 2013 we will consider: New Years Day, Independence Day, Thanksgiving Day, Christmas Day.\n\nHow might you represent that data as a data frame? What would be the primary keys of that table? How would it connect to the existing tables?\nWe can add a table of special dates, similar to the following table.\n\nspecial_days &lt;- tribble(\n  ~year, ~month, ~day, ~holiday,\n  2013, 01, 01, \"New Years Day\",\n  2013, 07, 04, \"Independence Day\",\n  2013, 11, 29, \"Thanksgiving Day\",\n  2013, 12, 25, \"Christmas Day\"\n)\n\nThe primary key of the table would be the (year, month, day) columns. The (year, month, day) columns could be used to join special_days with other tables.\n\nCreate a visualization fo your own to illustrate if indeed fewer people than usual fly on the above special days.\nCompute the average delay by destination, then join on the airports data frame so you can show the spatial distribution of delays. Here’s an easy way to draw a map of the United States (can you understand why we choose semi-join?):\n\n\nairports %&gt;%\n  semi_join(flights, c(\"faa\" = \"dest\")) %&gt;%\n  ggplot(aes(lon, lat)) +\n  borders(\"state\") +\n  geom_point() +\n  coord_quickmap() + \n  theme_void()\n\n\n\n\n\n\n\n\nHint: You might want to use the size or color of the points to display the average delay for each airport.\n\nWhat weather conditions make it more likely to see a delay? Use the variable precip (precipitation) from the weather dataset to answer this.\nWhat happened on June 13, 2013? Reproduce the following plot which displays the spatial pattern of delays, and then use Google to cross-reference with the weather. Hint: use library(viridis) to get the same colors."
  },
  {
    "objectID": "teaching/tidyverse/material/hotels-datawrangling.html",
    "href": "teaching/tidyverse/material/hotels-datawrangling.html",
    "title": "Hotel bookings - data wrangling",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)\n# From TidyTuesday: https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-02-11/readme.md\nhotels &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\nThe data is also available as a csv file which you can import directly."
  },
  {
    "objectID": "teaching/tidyverse/material/hotels-datawrangling.html#exercises",
    "href": "teaching/tidyverse/material/hotels-datawrangling.html#exercises",
    "title": "Hotel bookings - data wrangling",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\nWarm up! Take a look at an overview of the data with the skim() function.\nNote: I already gave you the answer to this exercise. You just need to knit the document and view the output. A definition of all variables is given in the Data dictionary section at the end, though you don’t need to familiarize yourself with all variables in order to work through these exercises.\n\nskim(hotels)\n\n\nData summary\n\n\nName\nhotels\n\n\nNumber of rows\n119390\n\n\nNumber of columns\n32\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n13\n\n\nDate\n1\n\n\nnumeric\n18\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nhotel\n0\n1\n10\n12\n0\n2\n0\n\n\narrival_date_month\n0\n1\n3\n9\n0\n12\n0\n\n\nmeal\n0\n1\n2\n9\n0\n5\n0\n\n\ncountry\n0\n1\n2\n4\n0\n178\n0\n\n\nmarket_segment\n0\n1\n6\n13\n0\n8\n0\n\n\ndistribution_channel\n0\n1\n3\n9\n0\n5\n0\n\n\nreserved_room_type\n0\n1\n1\n1\n0\n10\n0\n\n\nassigned_room_type\n0\n1\n1\n1\n0\n12\n0\n\n\ndeposit_type\n0\n1\n10\n10\n0\n3\n0\n\n\nagent\n0\n1\n1\n4\n0\n334\n0\n\n\ncompany\n0\n1\n1\n4\n0\n353\n0\n\n\ncustomer_type\n0\n1\n5\n15\n0\n4\n0\n\n\nreservation_status\n0\n1\n7\n9\n0\n3\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nreservation_status_date\n0\n1\n2014-10-17\n2017-09-14\n2016-08-07\n926\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nis_canceled\n0\n1\n0.37\n0.48\n0.00\n0.00\n0.00\n1\n1\n▇▁▁▁▅\n\n\nlead_time\n0\n1\n104.01\n106.86\n0.00\n18.00\n69.00\n160\n737\n▇▂▁▁▁\n\n\narrival_date_year\n0\n1\n2016.16\n0.71\n2015.00\n2016.00\n2016.00\n2017\n2017\n▃▁▇▁▆\n\n\narrival_date_week_number\n0\n1\n27.17\n13.61\n1.00\n16.00\n28.00\n38\n53\n▅▇▇▇▅\n\n\narrival_date_day_of_month\n0\n1\n15.80\n8.78\n1.00\n8.00\n16.00\n23\n31\n▇▇▇▇▆\n\n\nstays_in_weekend_nights\n0\n1\n0.93\n1.00\n0.00\n0.00\n1.00\n2\n19\n▇▁▁▁▁\n\n\nstays_in_week_nights\n0\n1\n2.50\n1.91\n0.00\n1.00\n2.00\n3\n50\n▇▁▁▁▁\n\n\nadults\n0\n1\n1.86\n0.58\n0.00\n2.00\n2.00\n2\n55\n▇▁▁▁▁\n\n\nchildren\n4\n1\n0.10\n0.40\n0.00\n0.00\n0.00\n0\n10\n▇▁▁▁▁\n\n\nbabies\n0\n1\n0.01\n0.10\n0.00\n0.00\n0.00\n0\n10\n▇▁▁▁▁\n\n\nis_repeated_guest\n0\n1\n0.03\n0.18\n0.00\n0.00\n0.00\n0\n1\n▇▁▁▁▁\n\n\nprevious_cancellations\n0\n1\n0.09\n0.84\n0.00\n0.00\n0.00\n0\n26\n▇▁▁▁▁\n\n\nprevious_bookings_not_canceled\n0\n1\n0.14\n1.50\n0.00\n0.00\n0.00\n0\n72\n▇▁▁▁▁\n\n\nbooking_changes\n0\n1\n0.22\n0.65\n0.00\n0.00\n0.00\n0\n21\n▇▁▁▁▁\n\n\ndays_in_waiting_list\n0\n1\n2.32\n17.59\n0.00\n0.00\n0.00\n0\n391\n▇▁▁▁▁\n\n\nadr\n0\n1\n101.83\n50.54\n-6.38\n69.29\n94.58\n126\n5400\n▇▁▁▁▁\n\n\nrequired_car_parking_spaces\n0\n1\n0.06\n0.25\n0.00\n0.00\n0.00\n0\n8\n▇▁▁▁▁\n\n\ntotal_of_special_requests\n0\n1\n0.57\n0.79\n0.00\n0.00\n0.00\n1\n5\n▇▁▁▁▁\n\n\n\n\n\n\n\nExercise 2.\nAre people traveling on a whim? Let’s see…\nFill in the blanks for filtering for hotel bookings where the guest is not from the US (country code \"USA\") and the lead_time is less than 1 day.\n\nhotels %&gt;%\n  filter(\n    country ____ \"USA\", \n    lead_time ____ ____\n    )\n\n\n\nExercise 3.\nHow many bookings involve at least 1 child or baby?\nIn the following chunk, replace\n\n[AT LEAST] with the logical operator for “at least” (in two places)\n[OR] with the logical operator for “or”\n\nNote: You will need to set eval=TRUE when you have an answer you want to try out in the qmd file.\n\nhotels %&gt;%\n  filter(\n    children [AT LEAST] 1 [OR] babies [AT LEAST] 1\n    )\n\n\n\nExercise 4.\nDo you think it’s more likely to find bookings with children or babies in city hotels or resort hotels? Test your intuition. Using filter() determine the number of bookings in resort hotels that have more than 1 child or baby in the room? Then, do the same for city hotels, and compare the numbers of rows in the resulting filtered data frames.\n\n# add code here\n# pay attention to correctness and code style\n\n\n# add code here\n# pay attention to correctness and code style\n\n\n\nExercise 5.\nCreate a frequency table of the number of adults in a booking. Display the results in descending order so the most common observation is on top. What is the most common number of adults in bookings in this dataset? Are there any surprising results?\n\n# add code here\n# pay attention to correctness and code style\n\n\n\nExercise 6.\nRepeat Exercise 5, once for canceled bookings (is_canceled coded as 1) and once for not canceled bookings (is_canceled coded as 0). What does this reveal about the surprising results you spotted in the previous exercise?\n\n# add code here\n# pay attention to correctness and code style\n\n\n\nExercise 7.\nCalculate minimum, mean, median, and maximum average daily rate (adr) grouped by hotel type so that you can get these statistics separately for resort and city hotels. Which type of hotel is higher, on average?\n\n# add code here\n# pay attention to correctness and code style\n\n\n\nExercise 8.\nWe observe two unusual values in the summary statistics above – a negative minimum, and a very high maximum). What types of hotels are these? Locate these observations in the dataset and find out the arrival date (year and month) as well as how many people (adults, children, and babies) stayed in the room. You can investigate the data in the viewer to locate these values, but preferably you should identify them in a reproducible way with some code.\nHint: For example, you can filter for the given adr amounts and select the relevant columns.\n\n# add code here\n# pay attention to correctness and code style"
  },
  {
    "objectID": "teaching/tidyverse/material/hotels-datawrangling.html#data-dictionary",
    "href": "teaching/tidyverse/material/hotels-datawrangling.html#data-dictionary",
    "title": "Hotel bookings - data wrangling",
    "section": "Data dictionary",
    "text": "Data dictionary\nBelow is the full data dictionary. Note that it is long (there are lots of variables in the data), but we will be using a limited set of the variables for our analysis.\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nhotel\ncharacter\nHotel (H1 = Resort Hotel or H2 = City Hotel)\n\n\nis_canceled\ndouble\nValue indicating if the booking was canceled (1) or not (0)\n\n\nlead_time\ndouble\nNumber of days that elapsed between the entering date of the booking into the PMS and the arrival date\n\n\narrival_date_year\ndouble\nYear of arrival date\n\n\narrival_date_month\ncharacter\nMonth of arrival date\n\n\narrival_date_week_number\ndouble\nWeek number of year for arrival date\n\n\narrival_date_day_of_month\ndouble\nDay of arrival date\n\n\nstays_in_weekend_nights\ndouble\nNumber of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel\n\n\nstays_in_week_nights\ndouble\nNumber of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel\n\n\nadults\ndouble\nNumber of adults\n\n\nchildren\ndouble\nNumber of children\n\n\nbabies\ndouble\nNumber of babies\n\n\nmeal\ncharacter\nType of meal booked. Categories are presented in standard hospitality meal packages:  Undefined/SC – no meal package;BB – Bed & Breakfast;  HB – Half board (breakfast and one other meal – usually dinner);  FB – Full board (breakfast, lunch and dinner)\n\n\ncountry\ncharacter\nCountry of origin. Categories are represented in the ISO 3155–3:2013 format\n\n\nmarket_segment\ncharacter\nMarket segment designation. In categories, the term “TA” means “Travel Agents” and “TO” means “Tour Operators”\n\n\ndistribution_channel\ncharacter\nBooking distribution channel. The term “TA” means “Travel Agents” and “TO” means “Tour Operators”\n\n\nis_repeated_guest\ndouble\nValue indicating if the booking name was from a repeated guest (1) or not (0)\n\n\nprevious_cancellations\ndouble\nNumber of previous bookings that were cancelled by the customer prior to the current booking\n\n\nprevious_bookings_not_canceled\ndouble\nNumber of previous bookings not cancelled by the customer prior to the current booking\n\n\nreserved_room_type\ncharacter\nCode of room type reserved. Code is presented instead of designation for anonymity reasons\n\n\nassigned_room_type\ncharacter\nCode for the type of room assigned to the booking. Sometimes the assigned room type differs from the reserved room type due to hotel operation reasons (e.g. overbooking) or by customer request. Code is presented instead of designation for anonymity reasons\n\n\nbooking_changes\ndouble\nNumber of changes/amendments made to the booking from the moment the booking was entered on the PMS until the moment of check-in or cancellation\n\n\ndeposit_type\ncharacter\nIndication on if the customer made a deposit to guarantee the booking. This variable can assume three categories:No Deposit – no deposit was made;Non Refund – a deposit was made in the value of the total stay cost;Refundable – a deposit was made with a value under the total cost of stay.\n\n\nagent\ncharacter\nID of the travel agency that made the booking\n\n\ncompany\ncharacter\nID of the company/entity that made the booking or responsible for paying the booking. ID is presented instead of designation for anonymity reasons\n\n\ndays_in_waiting_list\ndouble\nNumber of days the booking was in the waiting list before it was confirmed to the customer\n\n\ncustomer_type\ncharacter\nType of booking, assuming one of four categories:Contract - when the booking has an allotment or other type of contract associated to it;Group – when the booking is associated to a group;Transient – when the booking is not part of a group or contract, and is not associated to other transient booking;Transient-party – when the booking is transient, but is associated to at least other transient booking\n\n\nadr\ndouble\nAverage Daily Rate as defined by dividing the sum of all lodging transactions by the total number of staying nights\n\n\nrequired_car_parking_spaces\ndouble\nNumber of car parking spaces required by the customer\n\n\ntotal_of_special_requests\ndouble\nNumber of special requests made by the customer (e.g. twin bed or high floor)\n\n\nreservation_status\ncharacter\nReservation last status, assuming one of three categories:Canceled – booking was canceled by the customer;Check-Out – customer has checked in but already departed;No-Show – customer did not check-in and did inform the hotel of the reason why\n\n\nreservation_status_date\ndouble\nDate at which the last status was set. This variable can be used in conjunction with the ReservationStatus to understand when was the booking canceled or when did the customer checked-out of the hotel"
  },
  {
    "objectID": "teaching/tidyverse/index.html",
    "href": "teaching/tidyverse/index.html",
    "title": "Data Science with Tidyverse",
    "section": "",
    "text": "Make sure to install and load Tidyverse:\n\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n\nSchedule\n\n\n\n\ndate\nslides\npractical\ndata\n\n\n\n\n1: Meet the toolkit\n12.04.2024\n\n\n\n\n\n2: Data visualization and ggplot\n19.04.2024\n\n\n\n\n\n3: Visualizing numerical and categorical data\n26.04.2024\n\n\n.zip\n\n\n4: Grammar of data wrangling I\n10.05.2024\n\n \n.zip\n\n\n5: Grammar of data wrangling II\n17.05.2024\n\n\n.zip\n\n\n6: Tidying Data\n23.05.2024\n\n\n.zip\n\n\n     More Practicals\n07.06.2024\n\n \n.zip\n\n\n7: Data Types and Data Classes\n14.06.2024\n\n  \n.zip\n\n\n8: Importing and Recoding Data\n21.06.2024\n\n  \n.zip\n\n\n9: Web Scraping\n05.07.2024\n\n\n\n\n\n10: Functions and Iteration\n12.07.2024\n\n\n\n\n\n11: Effective Visualization\n19.07.2024\n\n\n.zip"
  },
  {
    "objectID": "teaching/math-ss/index.html",
    "href": "teaching/math-ss/index.html",
    "title": "Mathematics for Social Scientists",
    "section": "",
    "text": "Schedule\n\n\n\n\ndate\nslides\nhandout\n\n\n\n\n1: Preliminaries\n22.10.2024\n\n\n\n\n2: Algebra Review, Modular Arithmetic & Boolean Algebra\n29.10.2024\n\n\n\n\n3: Functions & Relations\n05.11.2024\n\n\n\n\n4: Sequences & Series, Limits & Continuity\n12.11.2024\n\n\n\n\n5: Calculus Fundamentals: Differentiation\n19.11.2024\n\n\n\n\n6: Calculus Fundamentals: The Integral\n26.11.2024\n\n\n\n\n7:Extrema in One Dimension\n02.12.2024"
  },
  {
    "objectID": "project/index.html",
    "href": "project/index.html",
    "title": "Projects",
    "section": "",
    "text": "Statistical Entropy Analysis of Network Data\n\n\nIn this project, a general framework for using statistical entropies to capture interdependencies among node and tie variables in multivariate networks is developed.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultigraph Representation of Network Data\n\n\nThe exploratory and confirmatory statistical analysis of multivariate social networks represented as multigraphs.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork of Interconnected Convoys\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Co-Evolution of Network Structure and Individual Outcomes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGender Dependent Structures in Charachter Networks\n\n\nUsing network analysis to analyze gender representation in popular cinema.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNEXUS1492 - Reconstructing Archaeological Networks\n\n\nReconstructing Archaeological Networks And Their Transformations Across The Historical Divide.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "project/movienetworks/index.html",
    "href": "project/movienetworks/index.html",
    "title": "Gender Dependent Structures in Charachter Networks",
    "section": "",
    "text": "Since 2019, I’m sporadically working on a project with Pete Jones which aims to apply existing and novel quantitative methods (e.g. relational event models, entropy based natural language processing and multigraph representations), to study the gendered inequalities in popular cinema. See Pete’s website for more information and some awesome packages such as charinet which he has developed on the topic."
  },
  {
    "objectID": "project/movienetworks/index.html#project-summary",
    "href": "project/movienetworks/index.html#project-summary",
    "title": "Gender Dependent Structures in Charachter Networks",
    "section": "",
    "text": "Since 2019, I’m sporadically working on a project with Pete Jones which aims to apply existing and novel quantitative methods (e.g. relational event models, entropy based natural language processing and multigraph representations), to study the gendered inequalities in popular cinema. See Pete’s website for more information and some awesome packages such as charinet which he has developed on the topic."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "",
    "section": "",
    "text": "Hello World\n\n\n\ntest\n\n\n\nThis is a test entry…\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#general",
    "href": "index.html#general",
    "title": "Termeh Shafie",
    "section": "General",
    "text": "General\n\n09.09.2024: Netropy 0.2.0\nThe new version of netropy is published on CRAN.\n\n\n05.06.2024: Inaugural Professorial Lecture 🤓\nI’ll be giving my inaugural professorial lecture. which is open for anyone who wants join. Expect a lot of big words, personal anecdotes, some mildly amusing jokes, and maybe even a technical hiccup or two 🤓📚✨  Join afterward for finger food and drinks 🥂  📅 Date: June 5th 📍 Venue: Data Theatre ZT1204, University of Konstanz\n\n\n20.02.2024: Multigraphr 0.2.0\nThe new version of multigraphr is published on CRAN. No user facing changes, just improved performance thanks to David Schoch.\n\n\n10.08.2023: Maternity Leave 🐣\nOn parental leave until end of the year.\n\n\n27.07.2023: Professor of Computational Social Science and Data Science\nToday I start my new position as a Professor (Professorin mit Schwerpunkt Lehre) at the Department of Politics and Public Administration, University of Konstanz."
  },
  {
    "objectID": "index.html#recent-publications",
    "href": "index.html#recent-publications",
    "title": "Termeh Shafie",
    "section": "Recent Publications",
    "text": "Recent Publications\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nThe interplay of structural features and observed dissimilarities among centrality indices\n\n\nDec 6, 2023\n\n\n\n\nGoodness of fit tests for random multigraph models\n\n\nJul 21, 2022\n\n\n\n\n\nNo matching items\n\n\n\nall publications"
  },
  {
    "objectID": "index.html#latest-talks",
    "href": "index.html#latest-talks",
    "title": "Termeh Shafie",
    "section": "Latest Talks",
    "text": "Latest Talks\n\n\n\n\n\n\n\n\n\n\n“The Interplay of Structural Features and Observed Dissimilarities Among Centrality Indices”\n\n\nThe XLIV Social Networks Conference of INSNA\n\n\n\n\n\nJun 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Analysis & Modeling of Multivariate Networks\n\n\nInaugural Lecture, University of Konstanz\n\n\n\n\n\nJun 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n“Statistical Analysis of Multivariate Egocentric Networks”\n\n\nThe XLII Social Networks Conference of INSNA\n\n\n\n\n\nJul 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n“Statistical Entropy Analysis of Network Data”\n\n\nThe Women in Network Science (WiNS) seminar\n\n\n\n\n\nMay 5, 2022\n\n\n\n\n\n\nNo matching items\n\n\n\nmore talks"
  },
  {
    "objectID": "index.html#r-packages",
    "href": "index.html#r-packages",
    "title": "Termeh Shafie",
    "section": "R packages",
    "text": "R packages\n\n\n\n\n\n\n\n\n\nLatest published version 0.2.0 on CRAN is up to date.\n\n\n\n\n\n\n\n\n\n\n\nLatest published version 0.2.0 on CRAN is up to date.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#github-activity",
    "href": "index.html#github-activity",
    "title": "Termeh Shafie",
    "section": "GitHub Activity",
    "text": "GitHub Activity"
  },
  {
    "objectID": "index.html#social-media",
    "href": "index.html#social-media",
    "title": "Termeh Shafie",
    "section": "Social Media",
    "text": "Social Media\n\n \n\n    Toots"
  },
  {
    "objectID": "talks/SUNBELT2024/index.html",
    "href": "talks/SUNBELT2024/index.html",
    "title": "“The Interplay of Structural Features and Observed Dissimilarities Among Centrality Indices”",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "talks/EUSN2019/index.html",
    "href": "talks/EUSN2019/index.html",
    "title": "“Gender Dependent Structures of Dialogue Networks in Films”",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "talks/SUNBELT2022/index.html",
    "href": "talks/SUNBELT2022/index.html",
    "title": "“Statistical Analysis of Multivariate Egocentric Networks”",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "talks/NETWORKS2021/index.html",
    "href": "talks/NETWORKS2021/index.html",
    "title": "“Goodness of Fit Tests for Random Multigraph Models”",
    "section": "",
    "text": "Slides\n\n\nVideo"
  },
  {
    "objectID": "talks/WiNS2022/index.html",
    "href": "talks/WiNS2022/index.html",
    "title": "“Statistical Entropy Analysis of Network Data”",
    "section": "",
    "text": "Slides\n\n\nExamples\n\n\nVideo"
  },
  {
    "objectID": "publications/ergm_framework_arch/index.html",
    "href": "publications/ergm_framework_arch/index.html",
    "title": "A Framework for Reconstructing Archaeological Networks using Exponential Random Graph Models",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/ergm_framework_arch/index.html#abstract",
    "href": "publications/ergm_framework_arch/index.html#abstract",
    "title": "A Framework for Reconstructing Archaeological Networks using Exponential Random Graph Models",
    "section": "Abstract",
    "text": "Abstract\nReconstructing ties between archaeological contexts may contribute to explain and describe a variety of past social phenomena. Several models have been formulated to infer the structure of such archaeological networks. The underlying propositions about mechanisms regulating the formation of ties in the past are often articulated on a dyadic basis and therefore rarely account for dependencies among ties. Here, we present a general framework in which we combine exponential random graph models with archaeological substantiations of mechanisms that may be responsible for network formation to account for tie dependence. We use data collected over a set of sites in the Caribbean during the period AD 100 - 400 to illustrate the steps to obtain a network reconstruction.\nJournal of Archaeological Method and Theory 27, 192-219"
  },
  {
    "objectID": "publications/multigraph_approach/index.html",
    "href": "publications/multigraph_approach/index.html",
    "title": "A Multigraph Approach to Social Network Analysis",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/multigraph_approach/index.html#abstract",
    "href": "publications/multigraph_approach/index.html#abstract",
    "title": "A Multigraph Approach to Social Network Analysis",
    "section": "Abstract",
    "text": "Abstract\nMultigraphs are graphs where multiple edges and edge loops are permitted. The main purpose of this article is to show the versatility of a multigraph approach when analysing social networks. Multigraph data structures are described and it is exemplified how they naturally occur in many contexts but also how they can be constructed by different kinds of aggregation in graphs. Special attention is given to a random multigraph model based on independent edge assignments to sites of vertex pairs and some useful measures of the local and global structure under this model are presented. Further, it is shown how some general measures of simplicity and complexity of multigraphs are easily handled under the present model.\n‘Journal of Social Structure 16, 1-22’"
  },
  {
    "objectID": "publications/sna_chapter/index.html",
    "href": "publications/sna_chapter/index.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "book chapter"
  },
  {
    "objectID": "publications/sna_chapter/index.html#abstract",
    "href": "publications/sna_chapter/index.html#abstract",
    "title": "Social Network Analysis",
    "section": "Abstract",
    "text": "Abstract\nSocial networks comprise a set of nodes or actors (representing, e.g., individuals, groups, organisations) that are pairwise connected by edges or ties (representing, e.g., relationships, interactions, communication). The social systems arising exhibit patterns of interest, and social network analysis is the study of how and why these patterns emerge, sustain, and evolve. Of primary interest is thus to understand and describe the social processes that support the observed structure. These processes are founded in theories about network representation and theories about observed social phenomena. The benefit from network conceptualisation is thus obtained by outlining the association and distinction between these theories. This entry serves as an introduction to fundamental network concepts and analytical approaches, their potential for studying social phenomena, and a description of why they are central to theoretical constructs. This entry also provides a short introduction to statistical network modelling for cross-sectional and longitudinal network data.\nSAGE Research Methods Foundations"
  },
  {
    "objectID": "publications/complexity_multigraphs/index.html",
    "href": "publications/complexity_multigraphs/index.html",
    "title": "Complexity of Families of Multigraphs",
    "section": "",
    "text": "pdf"
  },
  {
    "objectID": "publications/complexity_multigraphs/index.html#abstract",
    "href": "publications/complexity_multigraphs/index.html#abstract",
    "title": "Complexity of Families of Multigraphs",
    "section": "Abstract",
    "text": "Abstract\nThis article describes families of finite multigraphs with labeled or unlabeled edges and vertices. It shows how size and complexity vary for different types of equivalence classes of graphs defined by ignoring only edge labels or ignoring both edge and vertex labels. Complexity is quantified by the distribution of edge multiplicities, and different complexity measures are discussed. Basic occupancy models for multigraphs are used to illustrate different graph distributions on isomorphism and complexity. The loss of information caused by ignoring edge and vertex labels is quantified by entropy and joint information that provide tools for studying properties of and relations between different graph families.\nIn 2012 JSM Proceedings: Papers Presented at the Joint Statistical Meetings, San Diego, California, July 28-August 2, 2012, and Other ASA-sponsored Conferences, American Statistical Association, 2012"
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "The interplay of structural features and observed dissimilarities among centrality indices\n\n\nThe association of network topology with dissimilarities of indices is assessed\n\n\n\n\n\n\n\n\n\n\n\n\n\nGoodness of fit tests for random multigraph models\n\n\nGoodness of fit tests for different probability models for random multigraphs.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiplexity Analysis of Networks using Multigraph Representations\n\n\nA method for performing multiplexity analysis in social networks with several node covariates is presented.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSocial Network Analysis\n\n\nA review chapter on social network analysis aimed towards undergraduate students.\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Framework for Reconstructing Archaeological Networks using Exponential Random Graph Models\n\n\nWe present a general framework in which we combine exponential random graph models with archaeological substantiations of mechanisms for network formation.\n\n\n\n\n\n\n\n\n\n\n\n\n\nReconstructing Archaeological Networks with Structural Holes\n\n\nWe consider exponential random graph models, and show how they can be applied to reconstruct networks coherent with Burt’s arguments on closure and structural holes.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Multigraphs and Aggregated Triads with Fixed Degrees\n\n\nNew combinatorial results are given for the global probability distribution of edge multiplicities and its marginal local distributions of loops and edges.\n\n\n\n\n\n\n\n\n\n\n\n\n\nHypergraph Representations: A Study of Carib Attacks on Colonial Forces, 1509-1700\n\n\nWe perform a centrality analysis of a directed hypergraph representing attacks by indigenous peoples from the Lesser Antilles on European colonial settlements, 1509-1700.\n\n\n\n\n\n\n\n\n\n\n\n\n\nInvestigating Human Geographic Origins using Dual-Isotope (87Sr/86Sr, d18O) Assignment approaches\n\n\nWe develop and test more standardized and quantitative approaches to geographic assignment of individual origins using multivariate isotopic data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Local and Global Properties of Multigraphs\n\n\nThe local and global structures of undirected multigraphs under two random multigraph models are analyzed and compared.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNation Building and Social Signaling in Southern Ontario AD 1350-1650\n\n\nSocial network analysis is used to demonstrates the signaling practices reflecting regional patterns.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariate Entropy Analysis of Network Data\n\n\nWe show how it is possible to systematically check for tendencies in data, such as independencies or conditional independencies, using multivariate entropies.\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Protection for Online Social Networks and P-Stability for Graphs\n\n\nWe consider different approaches for data privacy in online social networks and for developing graph protection.\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Multigraph Approach to Social Network Analysis\n\n\nThe theoretical background for analyzing multivariate social networks using multigraph representations is introduced.\n\n\n\n\n\n\n\n\n\n\n\n\n\nComplexity of Families of Multigraphs\n\n\nComplexity measured for multigraphs are specified and their applicability is discussed.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/aggregated_triads/index.html",
    "href": "publications/aggregated_triads/index.html",
    "title": "Random Multigraphs and Aggregated Triads with Fixed Degrees",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/aggregated_triads/index.html#abstract",
    "href": "publications/aggregated_triads/index.html#abstract",
    "title": "Random Multigraphs and Aggregated Triads with Fixed Degrees",
    "section": "Abstract",
    "text": "Abstract\nRandom multigraphs with fixed degrees are obtained by the configuration model or by so called random stub matching. New combinatorial results are given for the global probability distribution of edge multiplicities and its marginal local distributions of loops and edges. The number of multigraphs on triads is determined for arbitrary degrees, and aggregated triads are shown to be useful for analyzing regular and almost regular multigraphs. Relationships between entropy and complexity are given and numerically illustrated for multigraphs with different number of vertices and specified average and variance for the degrees.\nNetwork Science 6(2), 232-250"
  },
  {
    "objectID": "publications/interplay_str_cent/index.html",
    "href": "publications/interplay_str_cent/index.html",
    "title": "The interplay of structural features and observed dissimilarities among centrality indices",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/interplay_str_cent/index.html#abstract",
    "href": "publications/interplay_str_cent/index.html#abstract",
    "title": "The interplay of structural features and observed dissimilarities among centrality indices",
    "section": "Abstract",
    "text": "Abstract\nAn abundance of centrality indices has been proposed which capture the importance of nodes in a network based on different structural features. While there remains a persistent belief that similarities in outcomes of indices is contingent on their technical definitions, a growing body of research shows that structural features affect observed similarities more than technicalities. We conduct a series of experiments on artificial networks to trace the influence of specific structural features on the similarity of indices which confirm previous results in the literature. Our analysis on 1163 real-world networks, however, shows that little of the observations on synthetic networks convincingly carry over to empirical settings. Our findings suggest that although it seems clear that (dis)similarities among centralities depend on structural properties of the network, using correlation type analyses do not seem to be a promising approach to uncover such connections.\nSocial Networks"
  },
  {
    "objectID": "publications/reconstructing_arch_nets/index.html",
    "href": "publications/reconstructing_arch_nets/index.html",
    "title": "Reconstructing Archaeological Networks with Structural Holes",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/reconstructing_arch_nets/index.html#abstract",
    "href": "publications/reconstructing_arch_nets/index.html#abstract",
    "title": "Reconstructing Archaeological Networks with Structural Holes",
    "section": "Abstract",
    "text": "Abstract\nModel-based reconstruction is an approach to infer network structures where they cannot be observed. For archaeological networks, several models based on assumptions concerning distance among sites, site size, or costs and benefits have been proposed to infer missing ties. Since these assumptions are formulated at a dyadic level, they do not provide means to express dependencies among ties and therefore include less plausible network scenarios. In this paper we investigate the use of network models that explicitly incorporate tie dependence. In particular, we consider exponential random graph models, and show how they can be applied to reconstruct networks coherent with Burt’s arguments on closure and structural holes (Burt 2001). The approach is illustrated on data from the Middle Bronze Age in the Aegean. authors:\nJournal of Archaeological Method and Theory 25(1), 226-253"
  },
  {
    "objectID": "publications/multivariate_entropy_analysis/index.html",
    "href": "publications/multivariate_entropy_analysis/index.html",
    "title": "Multivariate Entropy Analysis of Network Data",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/multivariate_entropy_analysis/index.html#abstract",
    "href": "publications/multivariate_entropy_analysis/index.html#abstract",
    "title": "Multivariate Entropy Analysis of Network Data",
    "section": "Abstract",
    "text": "Abstract\nMultigraphs with numerical or qualitative attributes defined on vertices and edges can benefit from systematic methods based on multivariate entropies for describing and analysing the interdependencies that are present between vertex and edge attributes. This is here illustrated by application of these tools to a subset of data on the social relations among Renaissance Florentine families collected by John Padgett. Using multivariate entropies we show how it is possible to systematically check for tendencies in data that can be described as independencies or conditional independencies, or as dependencies allowing certain combinations of variables to predict other variables. We also show how different structural models can be tested by divergence measures obtained from the multivariate entropies.\nBulletin of Sociological Methodology/Bulletin de Méthodologie Sociologique 120(1), 45-63"
  },
  {
    "objectID": "publications/multiplexity/index.html",
    "href": "publications/multiplexity/index.html",
    "title": "Multiplexity Analysis of Networks using Multigraph Representations",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/multiplexity/index.html#abstract",
    "href": "publications/multiplexity/index.html#abstract",
    "title": "Multiplexity Analysis of Networks using Multigraph Representations",
    "section": "Abstract",
    "text": "Abstract\nMultivariate networks comprising several compositional and structural variables can be represented as multigraphs by various forms of aggregations based on vertex attributes. We propose a framework to perform exploratory and confirmatory multiplexity analysis of aggregated multigraphs in order to find relevant associations between vertex and edge attributes. The exploration is performed by comparing frequencies of the different edges within and between aggregated vertex categories, while the confirmatory analysis is performed using derived complexity or multiplexity statistics under different random multigraph models. These statistics are defined by the distribution of edge multiplicities and provide information on the covariation and dependencies of different edges given vertex attributes. The presented approach highlights the need to further analyse and model structural dependencies with respect to edge entrainment. We illustrate the approach by applying it on a well known multivariate network dataset which has previously been analysed in the context of multiplexity.\npublication: Statistical Methods & Applications 30, 1425–1444"
  },
  {
    "objectID": "publications/hypergraph/index.html",
    "href": "publications/hypergraph/index.html",
    "title": "Hypergraph Representations: A Study of Carib Attacks on Colonial Forces, 1509-1700",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/hypergraph/index.html#abstract",
    "href": "publications/hypergraph/index.html#abstract",
    "title": "Hypergraph Representations: A Study of Carib Attacks on Colonial Forces, 1509-1700",
    "section": "Abstract",
    "text": "Abstract\nNetwork data consisting of recorded historical events can be represented as hyper-graphs where the ties or events can connect any number of nodes or event related attributes. In this paper, we perform a centrality analysis of a directed hypergraph representing attacks by indigenous peoples from the Lesser Antilles on European colonial settlements, 1509-1700. The results of central attacks with respect to at- tacked colonial force, member of attack alliances, and year and location of attack are discussed and compared to a non-relational exploratory analysis of the data. This comparison points to the importance of a mixed methods approach to enhance the analysis and to obtain a complementary understanding of a network study.\npublication: ‘Journal of Historical Network Research 1(1), 52-70’"
  },
  {
    "objectID": "publications/gof_multigraph/index.html",
    "href": "publications/gof_multigraph/index.html",
    "title": "Goodness of fit tests for random multigraph models",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/gof_multigraph/index.html#abstract",
    "href": "publications/gof_multigraph/index.html#abstract",
    "title": "Goodness of fit tests for random multigraph models",
    "section": "Abstract",
    "text": "Abstract\nGoodness of fit tests for two probabilistic multigraph models are presented. The first model is random stub matching given fixed degrees (RSM) so that edge assignments to vertex pair sites are dependent, and the second is independent edge assignments (IEA) according to a common probability distribution. Tests are performed using goodness of fit measures between the edge multiplicity sequence of an observed multigraph, and the expected one according to a simple or composite hypothesis. Test statistics of Pearson type and of likelihood ratio type are used, and the expected values of the Pearson statistic under the different models are derived. Test performances based on simulations indicate that even for small number of edges, the null distributions of both statistics are well approximated by their asymptotic χ2-distribution. The non-null distributions of the test statistics can be well approximated by proposed adjusted χ2-distributions used for power approximations. The influence of RSM on both test statistics is substantial for small number of edges and implies a shift of their distributions towards smaller values compared to what holds true for the null distributions under IEA. Two applications on social networks are included to illustrate how the tests can guide in the analysis of social structure.\nJournal of Applied Statistics"
  },
  {
    "objectID": "publications/global_local_multigraphs/index.html",
    "href": "publications/global_local_multigraphs/index.html",
    "title": "Analyzing Local and Global Properties of Multigraphs",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/global_local_multigraphs/index.html#abstract",
    "href": "publications/global_local_multigraphs/index.html#abstract",
    "title": "Analyzing Local and Global Properties of Multigraphs",
    "section": "Abstract",
    "text": "Abstract\nThe local structure of undirected multigraphs under two random multigraph models is analyzed and compared. The first model generates multigraphs by randomly coupling pairs of stubs according to a fixed degree sequence so that edge assignments to vertex pair sites are dependent. The second model is a simplification that ignores the dependency between the edge assignments. It is investigated when this ignorance is justified so that the simplified model can be used as an approximation, thus facilitating the structural analysis of network data with multiple relations and loops. The comparison is based on the local properties of multigraphs given by marginal distribution of edge multiplicities and some local properties that are aggregations of global properties.\nJournal of Mathematical Sociology 40(4), 239-264"
  },
  {
    "objectID": "publications/data_protection/index.html",
    "href": "publications/data_protection/index.html",
    "title": "Data Protection for Online Social Networks and P-Stability for Graphs",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/data_protection/index.html#abstract",
    "href": "publications/data_protection/index.html#abstract",
    "title": "Data Protection for Online Social Networks and P-Stability for Graphs",
    "section": "Abstract",
    "text": "Abstract\nGraphs can be used as a model for online social networks. In this framework, vertices represent individuals and edges relationships between individuals. In recent years, different approaches have been considered to offer data privacy to online social networks and for developing graph protection. Perturbative approaches are formally defined in terms of perturbation and modification of graphs. In this paper, we discuss the concept of P -stability on graphs and its relation to data privacy. The concept of P-stability is rooted in the number of graphs given a fixed degree sequence. In this paper, we show that for any graph there exists a class of P-stable graphs. This result implies that there is a fully polynomial randomized approximation for graph masking for the graphs in the class. In order to further refine the classification of a given graph, we introduce the concept of natural class of a graph. It is based on a class of scale-free networks.\nIEEE Transactions on Emerging Topics in Computing 4(3), 374-381"
  },
  {
    "objectID": "publications/isotopes/index.html",
    "href": "publications/isotopes/index.html",
    "title": "Investigating Human Geographic Origins using Dual-Isotope (87Sr/86Sr, d18O) Assignment approaches",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/isotopes/index.html#abstract",
    "href": "publications/isotopes/index.html#abstract",
    "title": "Investigating Human Geographic Origins using Dual-Isotope (87Sr/86Sr, d18O) Assignment approaches",
    "section": "Abstract",
    "text": "Abstract\nSubstantial progress in the application of multiple isotope analyses has greatly improved the ability to identify nonlocal individuals amongst archaeological populations over the past decades. More recently the development of large scale models of spatial isotopic variation (isoscapes) has contributed to improved geographic assignments of human and animal origins. Persistent challenges remain, however, in the accurate identification of individual geographic origins from skeletal isotope data in studies of human (and animal) migration and provenance. In an attempt to develop and test more standardized and quantitative approaches to geographic assignment of individual origins using isotopic data two methods, combining 87Sr/86Sr and d18O isoscapes, are examined for the Circum-Caribbean region 1) an Interval approach using a defined range of fixed isotopic variation per location and 2) a Likelihood assignment approach using univariate and bivariate probability density functions. These two methods are tested with enamel isotope data from a modern sample of known origin from Caracas, Venezuela and further explored with two archaeological samples of unknown origin recovered from Cuba and Trinidad. The results emphasize both the potential and limitation of the different approaches. Validation tests on the known origin sample exclude most areas of the Circum-Caribbean region and correctly highlight Caracas as a possible place of origin with both approaches. The positive validation results clearly demonstrate the overall efficacy of a dual-isotope approach to geoprovenance. The accuracy and precision of geographic assignments may be further improved by better understanding of the relationships between environmental and biological isotope variation; continued development and refinement of relevant isoscapes; and the eventual incorporation of a broader array of isotope proxy data.\nPLoS ONE 12(2), e0172562"
  },
  {
    "objectID": "publications/ontario_sna/index.html",
    "href": "publications/ontario_sna/index.html",
    "title": "Nation Building and Social Signaling in Southern Ontario AD 1350-1650",
    "section": "",
    "text": "journal"
  },
  {
    "objectID": "publications/ontario_sna/index.html#abstract",
    "href": "publications/ontario_sna/index.html#abstract",
    "title": "Nation Building and Social Signaling in Southern Ontario AD 1350-1650",
    "section": "Abstract",
    "text": "Abstract\nPottery is a mainstay of archaeological analysis worldwide. Often, high proportions of the pottery recovered from a given site are decorated in some manner. In northern Iroquoia, late pre-contact pottery and early contact decoration commonly occur on collars—thick bands of clay that encircle a pot and extend several centimeters down from the lip. These decorations constitute signals that conveyed information about a pot’s user(s). In southern Ontario the period A.D. 1350 to 1650 witnessed substantial changes in socio-political and settlement systems that included population movement, coalescence of formerly separate communities into large villages and towns, waxing and waning of regional strife, the formation of nations, and finally the development of three confederacies that each occupied distinct, constricted areas. Social network analysis demonstrates that signaling practices changed to reflect these regional patterns. Networks become more consolidated through time ultimately resulting in a “small world” network with small degrees of separation between sites reflecting the integration of communities within and between the three confederacies.\nPLoS ONE 11(5), e0156178"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "“The Interplay of Structural Features and Observed Dissimilarities Among Centrality Indices”\n\n\nThe XLIV Social Networks Conference of INSNA\n\n\n\nJun 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Analysis & Modeling of Multivariate Networks\n\n\nInaugural Lecture, University of Konstanz\n\n\n\nJun 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Analyzing Social Structure using Multigraph Representations”\n\n\nPresentation @ Centre Marc Bloch, Berlin\n\n\n\nNov 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Statistical Analysis of Multivariate Egocentric Networks”\n\n\nThe XLII Social Networks Conference of INSNA\n\n\n\nJul 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Statistical Entropy Analysis of Network Data”\n\n\nThe Women in Network Science (WiNS) seminar\n\n\n\nMay 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiplexity Analysis of Networks using Multigraph Representations\n\n\nThe 5th European Conference on Social Networks\n\n\n\nSep 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Goodness of Fit Tests for Random Multigraph Models”\n\n\nNetwork 2021 - A Joint Sunbelt and NetSci Conference\n\n\n\nJul 6, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Gender Dependent Structures of Dialogue Networks in Films”\n\n\nThe 4th European Conference on Social Networks\n\n\n\nSep 9, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/Inaugural-KN/index.html",
    "href": "talks/Inaugural-KN/index.html",
    "title": "Statistical Analysis & Modeling of Multivariate Networks",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "talks/CMB2022/index.html",
    "href": "talks/CMB2022/index.html",
    "title": "“Analyzing Social Structure using Multigraph Representations”",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "talks/EUSN2021/index.html",
    "href": "talks/EUSN2021/index.html",
    "title": "Multiplexity Analysis of Networks using Multigraph Representations",
    "section": "",
    "text": "Slides"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Termeh Shafie",
    "section": "",
    "text": "👋 Hi, I’m Termeh\nI’m a statistician with more than a decade of experience in developing methods and models to analyze multivariate social networks. I’m also the developer and maintainer of two R packages on the topic.\nInterdisciplinary applications are close to heart and I have had many such collaborations in different branches of computational social science and digital humanities. You can find information on some of the methodological and empirical projects that I have worked on or currently work on here.\nA bit of random trivia about me: my Erdős number is 3 and I have an irrational obsession with Pokémon."
  },
  {
    "objectID": "about/index.html#education",
    "href": "about/index.html#education",
    "title": "Termeh Shafie",
    "section": "Education",
    "text": "Education\n\nPhD in Statistics | 2013 | Stockholm University | Stockholm, Sweden\nLicentiate of Philosophy in Statistic | 2008 | Umeå University | Umeå Sweden\nMaster of Social Science (major in Statistics) | 2006 | Umeå University | Umeå Sweden\nMaster of Science in Public Administration and Economics | 2006 | Umeå University | Umeå Sweden"
  },
  {
    "objectID": "about/index.html#experience",
    "href": "about/index.html#experience",
    "title": "Termeh Shafie",
    "section": "Experience",
    "text": "Experience\n\nProfessor of Computational Social Science and Data Science (W2) | July 2023 – | Department of Politics and Public Administration | Center for Data and Methods | University of Konstanz\nInterim Professor | Apr 2023 – July 2023 | Department of Politics and Public Administration | University of Konstanz\nSenior Researcher | Jul 2022 – Mar 2023 | Department of Computational Social Science GESIS – Leibniz Institute for the Social Sciences\nLecturer in Social Statistics | Jul 2018 – Apr 2022 | Department of Social Statistics | The University of Manchester\nPostdoctoral Researcher | Nov 2017 – Jun 2018 | Department of Humanities, Social and Political Sciences | ETH Zürich\nPostdoctoral Researcher | Sep 2013 – Oct 2017 | Department of Computer and Information Science | University of Konstanz"
  },
  {
    "objectID": "project/seand/index.html",
    "href": "project/seand/index.html",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "",
    "text": "In multivariate statistics, there is an abundance of different measures of centrality and spread, many of which cannot be applied on variables measured on nominal or ordinal scale. Since network data in majority comprises such variables, alternative measures for analyzing spread, flatness and association is needed. This is also of particular relevance given the special feature of interdependent observations in networks.\nMultivariate entropy analysis is a general statistical method for analyzing and finding dependence structure in data consisting of repeated observations of variables with a common domain and with discrete finite range spaces. Only nominal scale is required for each variable, so only the size of the variable’s range space is important but not its actual values. Variables on ordinal or numerical scales, even continuous numerical scales, can be used, but they should be aggregated so that their ranges match the number of available repeated observations. By investigating the frequencies of occurrences of joint variable outcomes, complicated dependence structures, partial independence and conditional independence as well as redundancies and functional dependence can be found.\nSince 2015, I am working with Ove Frank and Krzysztof Nowicki on a project in which we build a systematic framework for using statistical entropy tools to analyze network data.\nThe proposed framework is implemented in the R package ‘netropy’ and a description of various functions implemented in the package are given in the following. More details are provided in the package vignettes and the references listed."
  },
  {
    "objectID": "project/seand/index.html#project-summary",
    "href": "project/seand/index.html#project-summary",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "",
    "text": "In multivariate statistics, there is an abundance of different measures of centrality and spread, many of which cannot be applied on variables measured on nominal or ordinal scale. Since network data in majority comprises such variables, alternative measures for analyzing spread, flatness and association is needed. This is also of particular relevance given the special feature of interdependent observations in networks.\nMultivariate entropy analysis is a general statistical method for analyzing and finding dependence structure in data consisting of repeated observations of variables with a common domain and with discrete finite range spaces. Only nominal scale is required for each variable, so only the size of the variable’s range space is important but not its actual values. Variables on ordinal or numerical scales, even continuous numerical scales, can be used, but they should be aggregated so that their ranges match the number of available repeated observations. By investigating the frequencies of occurrences of joint variable outcomes, complicated dependence structures, partial independence and conditional independence as well as redundancies and functional dependence can be found.\nSince 2015, I am working with Ove Frank and Krzysztof Nowicki on a project in which we build a systematic framework for using statistical entropy tools to analyze network data.\nThe proposed framework is implemented in the R package ‘netropy’ and a description of various functions implemented in the package are given in the following. More details are provided in the package vignettes and the references listed."
  },
  {
    "objectID": "project/seand/index.html#r-package-netropy",
    "href": "project/seand/index.html#r-package-netropy",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "R package netropy",
    "text": "R package netropy\n\n\nPackage overview\n  \nThis package introduces these entropy tools in the context of network data. Brief description of various functions implemented in the package are given in the following but more details are provided in the package vignettes and the references listed.\n\n\nInstallation\nYou can install the released version of netropy from CRAN with:\ninstall.packages(\"netropy\")\nThe development version from GitHub with:\n# install.packages(\"devtools\")\ndevtools::install_github(\"termehs/netropy\")\nTo load the package:\n\nlibrary('netropy')\n\n\n\nLoading internal data\nThe different entropy tools are explained and illustrated by exploring data from a network study of a corporate law firm, which has previously been analysed by several authors (link). The data set is included in the package as a list with objects representing adjacency matrices for each of the three networks advice (directed), friendship (directed) and co-work (undirected), together with a data frame comprising 8 attributes on each of the 71 lawyers.\nTo load the data, extract each object and assign the correct names to them:\n\ndata(lawdata) \nadj.advice &lt;- lawdata[[1]]\nadj.friend &lt;- lawdata[[2]]\nadj.cowork &lt;-lawdata[[3]]\ndf.att &lt;- lawdata[[4]]"
  },
  {
    "objectID": "project/seand/index.html#variable-domains-and-data-editing",
    "href": "project/seand/index.html#variable-domains-and-data-editing",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "Variable domains and data editing",
    "text": "Variable domains and data editing\nA requirement for the applicability of these entropy tools is the specification of discrete variables with finite range spaces on the same domain: either node attributes/vertex variables, edges/dyad variables or triad variables. These can be either observed or transformed as shown in the following using the above example data set.\nWe have 8 vertex variables with 71 observations, two of which (years and age) are numerical and needs categorization based on their cumulative distributions. This categorization is in details described in the vignette “variable domains and data editing”. Here we just show the new dataframe created (note that variable senior is omitted as it only comprises unique values and that we edit all variable to start from 0):\n\natt.var &lt;-\n  data.frame(\n    status   = df.att$status-1,\n    gender   = df.att$gender,\n    office   = df.att$office-1,\n    years    = ifelse(df.att$years &lt;= 3,0,\n                      ifelse(df.att$years &lt;= 13,1,2)),\n    age      = ifelse(df.att$age &lt;= 35,0,\n                      ifelse(df.att$age &lt;= 45,1,2)),\n    practice = df.att$practice,\n    lawschool= df.att$lawschool-1\n    )\nhead(att.var)\n\n  status gender office years age practice lawschool\n1      0      1      0     2   2        1         0\n2      0      1      0     2   2        0         0\n3      0      1      1     1   2        1         0\n4      0      1      0     2   2        0         2\n5      0      1      1     2   2        1         1\n6      0      1      1     2   2        1         0\n\n\nThese vertex variables can be transformed into dyad variables by using the function get_dyad_var(). Observed node attributes in the dataframe att_var are then transformed into pairs of individual attributes. For example, status with binary outcomes is transformed into dyads having 4 possible outcomes (0,0), (0,1), (1,0), (1,1):\n\ndyad.status    &lt;- get_dyad_var(att.var$status, type = 'att')\ndyad.gender    &lt;- get_dyad_var(att.var$gender, type = 'att')\ndyad.office    &lt;- get_dyad_var(att.var$office, type = 'att')\ndyad.years     &lt;- get_dyad_var(att.var$years, type = 'att')\ndyad.age       &lt;- get_dyad_var(att.var$age, type = 'att')\ndyad.practice  &lt;- get_dyad_var(att.var$practice, type = 'att')\ndyad.lawschool &lt;- get_dyad_var(att.var$lawschool, type = 'att')\n\nSimilarly, dyad variables can be created based on observed ties. For the undirected edges, we use indicator variables read directly from the adjacency matrix for the dyad in question, while for the directed ones (advice and friendship) we have pairs of indicators representing sending and receiving ties with 4 possible outcomes :\n\ndyad.cwk    &lt;- get_dyad_var(adj.cowork, type = 'tie')\ndyad.adv    &lt;- get_dyad_var(adj.advice, type = 'tie')\ndyad.frn    &lt;- get_dyad_var(adj.friend, type = 'tie')\n\nAll 10 dyad variables are merged into one data frame for subsequent entropy analysis:\n\ndyad.var &lt;-\n  data.frame(cbind(status   = dyad.status$var,\n                  gender    = dyad.gender$var,\n                  office    = dyad.office$var,\n                  years     = dyad.years$var,\n                  age       = dyad.age$var,\n                  practice  = dyad.practice$var,\n                  lawschool = dyad.lawschool$var,\n                  cowork    = dyad.cwk$var,\n                  advice    = dyad.adv$var,\n                  friend    = dyad.frn$var)\n                  )\nhead(dyad.var)\n\n  status gender office years age practice lawschool cowork advice friend\n1      3      3      0     8   8        1         0      0      3      2\n2      3      3      3     5   8        3         0      0      0      0\n3      3      3      3     5   8        2         0      0      1      0\n4      3      3      0     8   8        1         6      0      1      2\n5      3      3      0     8   8        0         6      0      1      1\n6      3      3      1     7   8        1         6      0      1      1\n\n\nA similar function get_triad_var() is implemented for transforming vertex variables and different relation types into triad variables. This is described in more detail in the vignette “variable domains and data editing”."
  },
  {
    "objectID": "project/seand/index.html#univariate-bivariate-and-trivariate-entropies",
    "href": "project/seand/index.html#univariate-bivariate-and-trivariate-entropies",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "Univariate, bivariate and trivariate entropies",
    "text": "Univariate, bivariate and trivariate entropies\nThe function entropy_bivar() computes the bivariate entropies of all pairs of variables in the dataframe. The output is given as an upper triangular matrix with cells giving the bivariate entropies of row and column variables. The diagonal thus gives the univariate entropies for each variable in the dataframe:\n\nH2 &lt;- entropy_bivar(dyad.var)\nH2\n\n          status gender office years   age practice lawschool cowork advice\nstatus     1.493  2.868  3.640 3.370 3.912    3.453     4.363  2.092  2.687\ngender        NA  1.547  3.758 3.939 4.274    3.506     4.439  2.158  2.785\noffice        NA     NA  2.239 4.828 4.901    4.154     5.058  2.792  3.388\nyears         NA     NA     NA 2.671 4.857    4.582     5.422  3.268  3.868\nage           NA     NA     NA    NA 2.801    4.743     5.347  3.411  4.028\npractice      NA     NA     NA    NA    NA    1.962     4.880  2.530  3.127\nlawschool     NA     NA     NA    NA    NA       NA     2.953  3.567  4.186\ncowork        NA     NA     NA    NA    NA       NA        NA  0.615  1.687\nadvice        NA     NA     NA    NA    NA       NA        NA     NA  1.248\nfriend        NA     NA     NA    NA    NA       NA        NA     NA     NA\n          friend\nstatus     2.324\ngender     2.415\noffice     3.044\nyears      3.483\nage        3.637\npractice   2.831\nlawschool  3.812\ncowork     1.456\nadvice     1.953\nfriend     0.881\n\n\nBivariate entropies can be used to detect redundant variables that should be omitted from the dataframe for further analysis. This occurs when the univariate entropy for a variable is equal to the bivariate entropies for pairs including that variable. As seen above, the dataframe dyad.var has no redundant variables. This can also be checked using the function redundancy() which yields a binary matrix as output indicating which row and column variables are hold the same information:\n\nredundancy(dyad.var)\n\nNULL\n\n\nMore examples of using the function redundancy() is given in the vignette “univariate bivariate and trivariate entropies”.\nTrivariate entropies can be computed using the function entropy_trivar() which returns a dataframe with the first three columns representing possible triples of variables V1,V2, and V3 from the dataframe in question, and their entropies H(V1,V2,V3) as the fourth column. We illustrated this on the dataframe dyad.var:\n\nH3 &lt;- entropy_trivar(dyad.var)\nhead(H3, 10) # view first 10 rows of dataframe\n\n       V1     V2        V3 H(V1,V2,V3)\n1  status gender    office       4.938\n2  status gender     years       4.609\n3  status gender       age       5.129\n4  status gender  practice       4.810\n5  status gender lawschool       5.664\n6  status gender    cowork       3.464\n7  status gender    advice       4.048\n8  status gender    friend       3.685\n9  status office     years       5.321\n10 status office       age       5.721"
  },
  {
    "objectID": "project/seand/index.html#joint-entropy-and-association-graphs",
    "href": "project/seand/index.html#joint-entropy-and-association-graphs",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "Joint entropy and association graphs",
    "text": "Joint entropy and association graphs\nJoint entropies is a non-negative measure of association among pairs of variables. It is equal to 0 if and only if two variables are completely independent of each other.\nThe function joint_entropy() computes the joint entropies between all pairs of variables in a given dataframe and returns a list consisting of the upper triangular joint entropy matrix (univariate entropies in the diagonal) and a dataframe giving the frequency distributions of unique joint entropy values. A function argument specifies the precision given in number of decimals for which the frequency distribution of unique entropy values is created (default is 3). Applying the function on the dataframe dyad.var with two decimals:\n\nJ &lt;- joint_entropy(dyad.var, 2)\nJ$matrix\n\n          status gender office years  age practice lawschool cowork advice\nstatus      1.49   0.17   0.09  0.79 0.38     0.00      0.08   0.02   0.05\ngender        NA   1.55   0.03  0.28 0.07     0.00      0.06   0.00   0.01\noffice        NA     NA   2.24  0.08 0.14     0.05      0.13   0.06   0.10\nyears         NA     NA     NA  2.67 0.61     0.05      0.20   0.02   0.05\nage           NA     NA     NA    NA 2.80     0.02      0.41   0.01   0.02\npractice      NA     NA     NA    NA   NA     1.96      0.04   0.05   0.08\nlawschool     NA     NA     NA    NA   NA       NA      2.95   0.00   0.01\ncowork        NA     NA     NA    NA   NA       NA        NA   0.62   0.18\nadvice        NA     NA     NA    NA   NA       NA        NA     NA   1.25\nfriend        NA     NA     NA    NA   NA       NA        NA     NA     NA\n          friend\nstatus      0.05\ngender      0.01\noffice      0.08\nyears       0.07\nage         0.05\npractice    0.01\nlawschool   0.02\ncowork      0.04\nadvice      0.18\nfriend      0.88\n\nJ$freq\n\n      j  #(J = j) #(J &gt;= j)\n1  0.79         1         1\n2  0.61         1         2\n3  0.41         1         3\n4  0.38         1         4\n5  0.28         1         5\n6   0.2         1         6\n7  0.18         2         8\n8  0.17         1         9\n9  0.14         1        10\n10 0.13         1        11\n11  0.1         1        12\n12 0.09         1        13\n13 0.08         4        17\n14 0.07         2        19\n15 0.06         2        21\n16 0.05         7        28\n17 0.04         2        30\n18 0.03         1        31\n19 0.02         5        36\n20 0.01         5        41\n21    0         4        45\n\n\nAs seen, the strongest association is between the variables status and years with joint entropy values of 0.79. We have independence (joint entropy value of 0) between two pairs of variables: (status,practice), (practise,gender), (cowork,gender),and (cowork,lawschool).\nThese results can be illustrated in a association graph using the function assoc_graph() which returns a ggraph object in which nodes represent variables and links represent strength of association (thicker links indicate stronger dependence). To use the function we need to load the ggraph library and to determine a threshold which the graph drawn is based on. We set it to 0.15 so that we only visualize the strongest associations\n\nlibrary(ggraph)\nassoc_graph(dyad.var, 0.15)\n\n\n\n\n\n\n\n\nGiven this threshold, we see isolated and disconnected nodes representing independent variables. We note strong dependence between the three dyadic variables status,years and age, but also a somewhat strong dependence among the three variables lawschool, years and age, and the three variables status, years and gender. The association graph can also be interpreted as a tendency for relations cowork and friend to be independent conditionally on relation advice, that is, any dependence between dyad variables cowork and friend is explained by advice.\nA threshold that gives a graph with reasonably many small independent or conditionally independent subsets of variables can be considered to represent a multivariate model for further testing.\nMore details and examples of joint entropies and association graphs are given in the vignette “joint entropies and association graphs”."
  },
  {
    "objectID": "project/seand/index.html#prediction-power-based-on-expected-conditional-entropies",
    "href": "project/seand/index.html#prediction-power-based-on-expected-conditional-entropies",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "Prediction power based on expected conditional entropies",
    "text": "Prediction power based on expected conditional entropies\nThe function prediction_power() computes prediction power when pairs of variables in a given dataframe are used to predict a third variable from the same dataframe. The variable to be predicted and the dataframe in which this variable also is part of is given as input arguments, and the output is an upper triangular matrix giving the expected conditional entropies of pairs of row and column variables (denoted \\(X\\) and \\(Y\\)) of the matrix, i.e. EH(Z|X,Y) where \\(Z\\) is the variable to be predicted. The diagonal gives EH(Z|X) , that is when only one variable as a predictor. Note that NA’s are in the row and column representing the variable being predicted.\nAssume we are interested in predicting variable status (that is whether a lawyer in the data set is an associate or partner). This is done by running the following syntax\n\nprediction_power('status', dyad.var)\n\n          status gender office years   age practice lawschool cowork advice\nstatus        NA     NA     NA    NA    NA       NA        NA     NA     NA\ngender        NA  1.375  1.180 0.670 0.855    1.304     1.225  1.306  1.263\noffice        NA     NA  2.147 0.493 0.820    1.374     1.245  1.373  1.325\nyears         NA     NA     NA 2.265 0.573    0.682     0.554  0.691  0.667\nage           NA     NA     NA    NA 1.877    1.089     0.958  1.087  1.052\npractice      NA     NA     NA    NA    NA    2.446     1.388  1.459  1.410\nlawschool     NA     NA     NA    NA    NA       NA     3.335  1.390  1.337\ncowork        NA     NA     NA    NA    NA       NA        NA  2.419  1.400\nadvice        NA     NA     NA    NA    NA       NA        NA     NA  2.781\nfriend        NA     NA     NA    NA    NA       NA        NA     NA     NA\n          friend\nstatus        NA\ngender     1.270\noffice     1.334\nyears      0.684\nage        1.058\npractice   1.427\nlawschool  1.350\ncowork     1.411\nadvice     1.407\nfriend     3.408\n\n\nFor better readability, the powers of different predictors can be conveniently compared by using prediction plots that display a color matrix with rows for \\(X\\) and columns for \\(Y\\) with darker colors in the cells when we have higher prediction power for \\(Z\\).\nMore details and examples of expected conditional entropies and prediction power are given in the package vignette."
  },
  {
    "objectID": "project/seand/index.html#divergence-tests-of-goodness-of-fit",
    "href": "project/seand/index.html#divergence-tests-of-goodness-of-fit",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "Divergence Tests of Goodness of Fit",
    "text": "Divergence Tests of Goodness of Fit\nOccurring cliques in association graphs represent connected components of dependent variables, and by comparing the graphs for different thresholds, specific structural models of multivariate dependence can be suggested and tested. The function div_gof() allows such hypothesis tests for pairwise independence of \\(X\\) and \\(Y\\): \\(X \\bot Y\\), and pairwise independence conditional a third variable \\(Z\\): \\(X\\bot Y|Z\\).\nTo test friend\\(\\bot\\) cowork\\(|\\)advice, that is whether dyad variable friend is independent of cowork given advice we use the function as shown below:\n\ndiv_gof(dat = dyad.var, var1 = \"friend\", var2 = \"cowork\", var_cond = \"advice\")\n\n     D df(D)\n1 0.94    12\n\n\nNot specifying argument var_cond would instead test friend\\(\\bot\\)cowork without any conditioning."
  },
  {
    "objectID": "project/seand/index.html#references",
    "href": "project/seand/index.html#references",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "References",
    "text": "References\nParts of the theoretical background is provided in the package vignettes, but for more details, consult the following literature:\n\nFrank, O., & Shafie, T. (2016). Multivariate entropy analysis of network data. Bulletin of Sociological Methodology/Bulletin de Méthodologie Sociologique, 129(1), 45-63. link"
  },
  {
    "objectID": "project/rmm/index.html",
    "href": "project/rmm/index.html",
    "title": "Multigraph Representation of Network Data",
    "section": "",
    "text": "Multigraphs are network representations in which multiple edges and edge loops (self edges) are permitted. Multigraph data structure can be observed directly but the possibility to obtain multigraphs using blocking, aggregation and scaling makes them widely applicable.\nFor the past decade, I’ve been working on a statistical framework for analyzing network data using this representation. I have developed different multigraph multigraph models and derived several statistics under these models that can be used to analyse local and global network properties in order to convey important social phenomena and processes. The latest contribution in this framework is formal goodness of fit tests for the developed probability models for random multigraphs.\nThe proposed framework is in full implemented in the R package ‘multigraphr’ and a description of various functions implemented in the package are given in the following. More details are provided in the package vignetteand the references listed."
  },
  {
    "objectID": "project/rmm/index.html#project-summary",
    "href": "project/rmm/index.html#project-summary",
    "title": "Multigraph Representation of Network Data",
    "section": "",
    "text": "Multigraphs are network representations in which multiple edges and edge loops (self edges) are permitted. Multigraph data structure can be observed directly but the possibility to obtain multigraphs using blocking, aggregation and scaling makes them widely applicable.\nFor the past decade, I’ve been working on a statistical framework for analyzing network data using this representation. I have developed different multigraph multigraph models and derived several statistics under these models that can be used to analyse local and global network properties in order to convey important social phenomena and processes. The latest contribution in this framework is formal goodness of fit tests for the developed probability models for random multigraphs.\nThe proposed framework is in full implemented in the R package ‘multigraphr’ and a description of various functions implemented in the package are given in the following. More details are provided in the package vignetteand the references listed."
  },
  {
    "objectID": "project/rmm/index.html#r-package-multigraphr",
    "href": "project/rmm/index.html#r-package-multigraphr",
    "title": "Multigraph Representation of Network Data",
    "section": "R package multigraphr",
    "text": "R package multigraphr\n\nPackage overview\n  \nThis package introduces the multigraph framework for analyzing social network data. Brief description of various functions implemented in the package are given in the following but more details are provided in the package vignettes and the references listed.\n\n\nInstallation\nYou can install the released version of multigraphr from CRAN with:\ninstall.packages(\"multigraphr\")\nThe development version from GitHub with:\n# install.packages(\"devtools\")\ndevtools::install_github(\"termehs/multigraphr\")"
  },
  {
    "objectID": "project/rmm/index.html#multigraphs-and-applicability",
    "href": "project/rmm/index.html#multigraphs-and-applicability",
    "title": "Multigraph Representation of Network Data",
    "section": "Multigraphs and applicability",
    "text": "Multigraphs and applicability\nMultigraphs are network representations in which multiple edges and edge loops (self edges) are permitted. These data structures can be either directly observed or aggregated by classifying or cross-classifying node attributes into meta nodes. For the latter case, within group edges correspond to self-edges. See example below where the original graph with 15 nodes and 12 edges (left) is aggregated based on node categories into a small multigraph with 4 nodes (right).\n\nEdge aggregation can also be used to obtain multigraphs. Assume that we study a graph with three different types of relations over three periods of time: \nIf we aggregate over time periods, we obtain for each edge category a multigraph for the total time period of three days:\n\nFor more details on these kinds of aggregations, see Shafie (2015;2016)."
  },
  {
    "objectID": "project/rmm/index.html#multigraph-representation-of-network-data",
    "href": "project/rmm/index.html#multigraph-representation-of-network-data",
    "title": "Multigraph Representation of Network Data",
    "section": "Multigraph representation of network data",
    "text": "Multigraph representation of network data\nMultigraphs are represented by their edge multiplicity sequence M with elements M(i,j), denoting the number of edges at vertex pair sites (i,j) ordered according to (1,1) &lt; (1,2) &lt;···&lt; (1,n) &lt; (2,2) &lt; (2,3) &lt;···&lt; (n,n), where n is number of nodes. The number of vertex pair sites is given by r = n(n+1)/2.\n\nRandom multigraph models\nTwo probability models for generating undirected random multigraphs are implemented in the package together with several statistics under these two models. Moreover, functions for goodness of fit tests are available for the presented models.\nNote that some of the functions are only practical for small scale multigraphs.\nThe first model is obtained by random stub matching (RSM) given observed degree sequence of a multigraphs, so that edge assignments to vertex pair sites are dependent. The second is obtained by independent edge assignments (IEA) according to a common probability distribution. There are two ways in which an approximate IEA model can be obtained from an RSM model, thus facilitating the structural analysis. These two ways are\n\nindependent stub assignment (ISA)\nindependent edge assignment of stubs (IEAS)\n\n(Shafie, 2016).\n\n\nExample\n\nlibrary('multigraphr')\n\nConsider a small graph on 3 nodes and the following adjacency matrix:\n\nA &lt;-  matrix(c(1, 1, 0, \n               1, 2, 2, \n               0, 2, 0), \n             nrow = 3, ncol = 3)\nA\n\n     [,1] [,2] [,3]\n[1,]    1    1    0\n[2,]    1    2    2\n[3,]    0    2    0\n\n\nThe degree sequence of the multigraph has double counted diagonals (edge stubs for loops) and is given by\n\nD &lt;- get_degree_seq(adj = A, type = 'graph')\nD\n\n[1] 3 7 2\n\n\nso that number of edges in the multigraph is half the sum of the degree sequence which is equal to 6.\nThe RSM model given observed degree sequence shows the sample space consists of 7 possible multigraphs, as represented by their multiplicity sequence m.seq (each row correspond to the edge multiplicity sequence of a unique multigraph):\n\nrsm_1 &lt;- rsm_model(deg.seq = D)\nrsm_1$m.seq\n\n  M11 M12 M13 M22 M23 M33\n1   1   1   0   3   0   1\n2   1   1   0   2   2   0\n3   1   0   1   3   1   0\n4   0   3   0   2   0   1\n5   0   3   0   1   2   0\n6   0   2   1   2   1   0\n7   0   1   2   3   0   0\n\n\nwith probabilities associated with each multigraph, together with statistics ‘number of loops’, ‘number of multiple edges’ and ‘simple graphs or not’:\n\nrsm_1$prob.dists\n\n    prob.rsm loops multiedges simple\n1 0.03030303     5          1      0\n2 0.18181818     3          3      0\n3 0.06060606     4          2      0\n4 0.06060606     3          3      0\n5 0.24242424     1          5      0\n6 0.36363636     2          4      0\n7 0.06060606     3          3      0\n\n\nConsider using the IEA model to approximate the RSM model so that edge assignment probabilities are functions of observed degree sequence. Note that the sample space for multigraphs is much bigger than for the RSM model so the multiplicity sequences are not printed (they can be found using the function get_edgemultip_seq for very small multigraphs and their probabilities can be found using the multinomial distribution). The following shows the number of multigraphs under either of the IEA models:\n\nieas_1 &lt;-   iea_model(adj = A , type = 'graph',  model = 'IEAS', K = 0, apx = TRUE)\nieas_1$nr.multigraphs\n\n[1] 462"
  },
  {
    "objectID": "project/rmm/index.html#statistics-to-analyze-structural-properties",
    "href": "project/rmm/index.html#statistics-to-analyze-structural-properties",
    "title": "Multigraph Representation of Network Data",
    "section": "Statistics to analyze structural properties",
    "text": "Statistics to analyze structural properties\nThese statistics include number of loops (indicator of e.g. homophily) and number of multiple edges (indicator of e.g. multiplexity/interlocking), which are implemented in the package together with their probability distributions, moments and interval estimates under the different multigraph models.\n\nExample (cont’d)\nUnder the RSM model, the first two moments and interval estimates of the statistics M1 = ‘number of loops’ and M2 = ‘number of multiple edges’ are given by\n\nrsm_1$M\n\n             M1    M2\nExpected  2.273 3.727\nVariance  0.986 0.986\nUpper 95% 4.259 5.713\nLower 95% 0.287 1.741\n\n\nwhich are calculated using the numerically found probability distributions under RSM (no analytical solutions exist for these moments).\nUnder the IEA models (IEAS or ISA), moments of these statistics, together with the complexity statistic \\(R_k\\) representing the sequence of frequencies of edge sites with multiplicities 0,1,…,k, are found using derived formulas. Thus, there is no limit on multigraph size to use these. When the IEAS model is used to approximate the RSM model as shown above:\n\nieas_1$M\n\n              M1    M2\nObserved   3.000 3.000\nExpected   2.273 3.727\nVariance   1.412 1.412\nUpper 95%  4.649 6.104\nLower 95% -0.104 1.351\n\nieas_1$R\n\n             R0     R1     R2\nObserved  2.000  2.000  2.000\nExpected  2.674  1.588  1.030\nVariance  0.575  1.129  0.760\nUpper 95% 4.191  3.713  2.773\nLower 95% 1.156 -0.537 -0.713\n\n\nWhen the ISA model is used to approximate the RSM model (see above):\n\nisa_1 &lt;-   iea_model(adj = A , type = 'graph',  \n                     model = 'ISA', K = 0, apx = TRUE)\nisa_1$M\n\n             M1    M2\nObserved  3.000 3.000\nExpected  2.583 3.417\nVariance  1.471 1.471\nUpper 95% 5.009 5.842\nLower 95% 0.158 0.991\n\nisa_1$R\n\n             R0     R1     R2\nObserved  2.000  2.000  2.000\nExpected  2.599  1.703  1.018\nVariance  0.622  1.223  0.748\nUpper 95% 4.176  3.915  2.748\nLower 95% 1.021 -0.509 -0.711\n\n\nThe IEA models can also be used independent of the RSM model. For example, the IEAS model can be used where edge assignment probabilities are estimated using the observed edge multiplicities (maximum likelihood estimates):\n\nieas_2 &lt;-   iea_model(adj = A , type = 'graph',  \n                      model = 'IEAS', K = 0, apx = FALSE)\nieas_2$M\n\n             M1    M2\nObserved  3.000 3.000\nExpected  3.000 3.000\nVariance  1.500 1.500\nUpper 95% 5.449 5.449\nLower 95% 0.551 0.551\n\nieas_2$R\n\n             R0     R1     R2\nObserved  2.000  2.000  2.000\nExpected  2.845  1.331  1.060\nVariance  0.434  0.805  0.800\nUpper 95% 4.163  3.125  2.849\nLower 95% 1.528 -0.464 -0.729\n\n\nThe ISA model can also be used independent of the RSM model. Then, a sequence containing the stub assignment probabilities (for example based on prior belief) should be given as argument:\n\nisa_2 &lt;-   iea_model(adj = A , type = 'graph',  \n                     model = 'ISA', K = 0, apx = FALSE, p.seq = c(1/3, 1/3, 1/3))\nisa_2$M\n\n              M1    M2\nObserved   3.000 3.000\nExpected   2.000 4.000\nVariance   1.333 1.333\nUpper 95%  4.309 6.309\nLower 95% -0.309 1.691\n\nisa_2$R\n\n             R0     R1     R2\nObserved  2.000  2.000  2.000\nExpected  2.144  2.248  1.160\nVariance  0.632  1.487  0.710\nUpper 95% 3.734  4.687  2.845\nLower 95% 0.554 -0.190 -0.525\n\n\nThe interval estimates can then be visualized to detect discrepancies between observed and expected values thus indicating social mechanisms at play in the generation of edges, and to detect interval overlap and potential interdependence between different types of edges (see Shafie 2015,2016; Shafie & Schoch 2021)."
  },
  {
    "objectID": "project/rmm/index.html#goodness-of-fit-tests",
    "href": "project/rmm/index.html#goodness-of-fit-tests",
    "title": "Multigraph Representation of Network Data",
    "section": "Goodness of fit tests",
    "text": "Goodness of fit tests\nGoodness of fits tests of multigraph models using Pearson (S) and information divergence (A) test statistics under the random stub matching (RSM) and by independent edge assignments (IEA) model, where the latter is either independent edge assignments of stubs (IEAS) or independent stub assignment (ISA). The tests are performed using goodness-of-fit measures between the edge multiplicity sequence of a specified model or an observed multigraph, and the expected multiplicity sequence according to a simple or composite hypothesis."
  },
  {
    "objectID": "project/rmm/index.html#simulated-goodness-of-fit-tests",
    "href": "project/rmm/index.html#simulated-goodness-of-fit-tests",
    "title": "Multigraph Representation of Network Data",
    "section": "Simulated goodness of fit tests",
    "text": "Simulated goodness of fit tests\nProbability distributions of test statistics, summary of tests, moments of tests statistics, adjusted test statistics, critical values, significance level according to asymptotic distribution, and power of tests can be examined using gof_sim given a specified model from which we simulate observed values from, and a null or non-null hypothesis from which we calculate expected values from. This in order to investigate the behavior of the null and non-null distributions of the test statistics and their fit to to asymptotic chi-square distributions.\n\nExample\nSimulated goodness of fit tests for multigraphs with n=4 nodes and m=10 edges.\n(1) Testing a simple IEAS hypothesis with degree sequence (6,6,6,2) against a RSM model with degrees (8,8,2,2):\n\ngof1 &lt;- gof_sim(m = 10, model = 'IEAS', deg.mod = c(8,8,2,2), \n                hyp = 'IEAS', deg.hyp = c(6,6,6,2))\n\n(2) Testing a correctly specified simple IEAS hypothesis with degree sequence (14,2,2,2):\n\ngof2 &lt;- gof_sim(m = 10, model = 'IEAS', deg.mod = c(14,2,2,2), \n                hyp = 'IEAS', deg.hyp = c(14,2,2,2))\n\nThe non-null (gof1) and null (gof2) distributions of the test statistics together with their asymptotic chi2-distribution can be visualized using ggplot2:\n \n(3) Testing a composite IEAS hypothesis against a RSM model with degree sequence (14,2,2,2):\n\ngof3 &lt;- gof_sim(m = 10, model = 'RSM', deg.mod = c(14,2,2,2), \n                hyp = 'IEAS', deg.hyp = 0)\n\n(4) Testing a composite ISA hypothesis against a ISA model with degree sequence (14,2,2,2):\n\ngof4 &lt;- gof_sim(m = 10, model = 'ISA', deg.mod = c(14,2,2,2), \n                hyp = 'ISA', deg.hyp = 0)\n\nThe non-null (gof3) and null (gof4) distributions of the test statistics can then be visualized as shown above to check their fit to the asymptotic χ²-distribution."
  },
  {
    "objectID": "project/rmm/index.html#performing-the-goodness-of-fit-test-on-your-data",
    "href": "project/rmm/index.html#performing-the-goodness-of-fit-test-on-your-data",
    "title": "Multigraph Representation of Network Data",
    "section": "Performing the goodness of fit test on your data",
    "text": "Performing the goodness of fit test on your data\nUse function gof_test to test whether the observed data follows IEA approximations of the RSM model. The null hypotheses can be simple or composite, although the latter is not recommended for small multigraphs as it is difficult to detect a false composite hypothesis under an RSM model and under IEA models (this can be checked and verified using gof_sim to simulate these cases).\nNon-rejection of the null implies that the approximations fit the data, thus implying that above statistics under the IEA models can be used to further analyze the observed network. Consider the following multigraph from the well known Florentine family network with marital. This multigraphs is aggregated based on the three actor attributes wealth (W), number of priorates (P) and total number of ties (T) which are all dichotomized to reflect high or low economic, political and social influence (details on the aggregation can be found in Shafie, 2015):\n\nThe multiplicity sequence represented as an upper triangular matrix for this mutigrpah is given by\n\nflor_m &lt;- t(matrix(c (0, 0, 1, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 2, 0, 0, 1, 5,\n                      0, 0, 0, 0, 0, 0, 1, 1,\n                      0, 0, 0, 0, 0, 0, 1, 2,\n                      0, 0, 0, 0, 0, 0, 2, 1,\n                      0, 0, 0, 0, 0, 0, 0, 2,\n                      0, 0, 0, 0, 0, 0, 0, 1), nrow= 8, ncol=8))\n\nThe equivalence of adjacency matrix for the multigraph is given by\n\nflor_adj &lt;- flor_m+t(flor_m)\nflor_adj \n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    0    0    1    0    0    0    0    0\n[2,]    0    0    0    0    0    0    0    0\n[3,]    1    0    0    2    0    0    1    5\n[4,]    0    0    2    0    0    0    1    1\n[5,]    0    0    0    0    0    0    1    2\n[6,]    0    0    0    0    0    0    2    1\n[7,]    0    0    1    1    1    2    0    2\n[8,]    0    0    5    1    2    1    2    2\n\n\nwith the diagonal representing the loops double counted (Shafie, 2016). The function get_degree_seq can now be used to find the degree sequence for this multigraph:\n\nflor_d &lt;- get_degree_seq(adj = flor_adj, type = 'multigraph')\nflor_d\n\n[1]  1  0  9  4  3  3  7 13\n\n\nNow we test whether the observed network fits the IEAS or the ISA model. The \\(p\\)-values for testing whether there is a significant difference between observed and expected edge multiplicity values according to the two approximate IEA models are given in the output tables below. Note that the asymptotic χ²-distribution has \\(r-1 = (n(n+1)/2) - 1 =35\\) degrees of freedom.\n\nflor_ieas_test &lt;- gof_test(flor_adj, 'multigraph', 'IEAS', flor_d, 35)\nflor_ieas_test\n\n  Stat dof Stat(obs) p-value\n1    S  35    15.762   0.998\n2    A  35    18.905   0.988\n\n\n\nflor_isa_test &lt;- gof_test(flor_adj, 'multigraph', 'ISA', flor_d, 35)\nflor_isa_test \n\n  Stat dof Stat(obs) p-value\n1    S  35    16.572   0.997\n2    A  35    19.648   0.983\n\n\nThe results show that we have strong evidence for the null such that we fail to reject it. Thus, there is not a significant difference between the observed and the expected edge multiplicity sequence according on the two IEA models. Statistics derived under these models presented above can thus be used to analyze the structure of these multigraphs."
  },
  {
    "objectID": "project/rmm/index.html#references",
    "href": "project/rmm/index.html#references",
    "title": "Multigraph Representation of Network Data",
    "section": "References",
    "text": "References\n\nShafie, T. (2015). A multigraph approach to social network analysis. Journal of Social Structure, 16. Link\nShafie, T. (2016). Analyzing local and global properties of multigraphs. The Journal of Mathematical Sociology, 40(4), 239-264. Link\nFrank, O., Shafie, T., (2018). Random Multigraphs and Aggregated Triads with Fixed Degrees. Network Science, 6(2), 232-250. Link\nShafie, T., Schoch, D. (2021) Multiplexity analysis of networks using multigraph representations. Statistical Methods & Applications 30, 1425–1444. Link\nShafie, T. (2022). Goodness of fit tests for random multigraph models, Journal of Applied Statistics. Link"
  },
  {
    "objectID": "project/nexus/index.html",
    "href": "project/nexus/index.html",
    "title": "NEXUS1492 - Reconstructing Archaeological Networks",
    "section": "",
    "text": "Between 2013-2018, I worked in international ERC-Synergy research project NEXUS1492 investigates the impacts of colonial encounters in the Caribbean through the reconstruction and modelling of archaeological networks.\nYou can read more about the project and its output here."
  },
  {
    "objectID": "project/nexus/index.html#project-summary",
    "href": "project/nexus/index.html#project-summary",
    "title": "NEXUS1492 - Reconstructing Archaeological Networks",
    "section": "",
    "text": "Between 2013-2018, I worked in international ERC-Synergy research project NEXUS1492 investigates the impacts of colonial encounters in the Caribbean through the reconstruction and modelling of archaeological networks.\nYou can read more about the project and its output here."
  },
  {
    "objectID": "project/nexus/index.html#references",
    "href": "project/nexus/index.html#references",
    "title": "NEXUS1492 - Reconstructing Archaeological Networks",
    "section": "References",
    "text": "References\n\nShafie T., Schoch D., Mans J., Hofman C., Brandes U., (2017). Hypergraph Representations: A Study of Carib Attacks on Colonial Forces, 1509-1700. Journal of Historical Network Research,1(1), 52-70. Link\nLaffoon, J.E., Sonnemann, T.F., Shafie, T., Hofman, C.L., Brandes, U. and Davies, G.R., (2017). Investigating human geographic origins using dual-isotope (87Sr/86Sr, δ18O) assignment approaches. PloS one, 12(2), p.e0172562. Link\nAmati, V., Shafie, T., Brandes U., (2018) Reconstructing Archaeological Networks with Structural Holes. Journal of Archaeological Method and Theory volume 25, 226–253. Link\nAmati, V., Mol, A., Shafie, T., Hofman, C., Brandes U., (2020). A Framework for Reconstructing Archaeological Networks Using Exponential Random Graph Models. Journal of Archaeological Method and Theory volume 27, 192–219. Link"
  },
  {
    "objectID": "teaching/tidyverse/material/bechdel.html",
    "href": "teaching/tidyverse/material/bechdel.html",
    "title": "Bechdel",
    "section": "",
    "text": "In this mini analysis we work with the data used in the FiveThirtyEight story titled “The Dollar-And-Cents Case Against Hollywood’s Exclusion of Women”. We will together fill in the blanks denoted by ___."
  },
  {
    "objectID": "teaching/tidyverse/material/bechdel.html#data-and-packages",
    "href": "teaching/tidyverse/material/bechdel.html#data-and-packages",
    "title": "Bechdel",
    "section": "Data and packages",
    "text": "Data and packages\nWe start with loading the packages we’ll use.\n\nlibrary(fivethirtyeight)\nlibrary(tidyverse)\n\nThe dataset contains information on 1794 movies released between 1970 and 2013. However we’ll focus our analysis on movies released between 1990 and 2013.\n\nbechdel90_13 &lt;- bechdel %&gt;% \n  filter(between(year, 1990, 2013))\n\nThere are ___ such movies.\nThe financial variables we’ll focus on are the following:\n\nbudget_2013: Budget in 2013 inflation adjusted dollars\ndomgross_2013: Domestic gross (US) in 2013 inflation adjusted dollars\nintgross_2013: Total International (i.e., worldwide) gross in 2013 inflation adjusted dollars\n\nAnd we’ll also use the binary and clean_test variables for grouping."
  },
  {
    "objectID": "teaching/tidyverse/material/bechdel.html#analysis",
    "href": "teaching/tidyverse/material/bechdel.html#analysis",
    "title": "Bechdel",
    "section": "Analysis",
    "text": "Analysis\nLet’s take a look at how median budget and gross vary by whether the movie passed the Bechdel test, which is stored in the binary variable.\n\nbechdel90_13 %&gt;%\n  group_by(binary) %&gt;%\n  summarise(\n    med_budget = median(budget_2013),\n    med_domgross = median(domgross_2013, na.rm = TRUE),\n    med_intgross = median(intgross_2013, na.rm = TRUE)\n    )\n\n# A tibble: 2 × 4\n  binary med_budget med_domgross med_intgross\n  &lt;chr&gt;       &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 FAIL    48385984.    57318606.    104475669\n2 PASS    31070724     45330446.     80124349\n\n\nNext, let’s take a look at how median budget and gross vary by a more detailed indicator of the Bechdel test result. This information is stored in the clean_test variable, which takes on the following values:\n\nok = passes test\ndubious\nmen = women only talk about men\nnotalk = women don’t talk to each other\nnowomen = fewer than two women\n\n\nbechdel90_13 %&gt;%\n  #group_by(___) %&gt;%\n  summarise(\n    med_budget = median(budget_2013),\n    med_domgross = median(domgross_2013, na.rm = TRUE),\n    med_intgross = median(intgross_2013, na.rm = TRUE)\n    )\n\n# A tibble: 1 × 3\n  med_budget med_domgross med_intgross\n       &lt;int&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1   37878971     52270207     93523336\n\n\nIn order to evaluate how return on investment varies among movies that pass and fail the Bechdel test, we’ll first create a new variable called roi as the ratio of the gross to budget.\n\nbechdel90_13 &lt;- bechdel90_13 %&gt;%\n  mutate(roi = (intgross_2013 + domgross_2013) / budget_2013)\n\nLet’s see which movies have the highest return on investment.\n\nbechdel90_13 %&gt;%\n  arrange(desc(roi)) %&gt;% \n  select(title, roi, year)\n\n# A tibble: 1,615 × 3\n   title                     roi  year\n   &lt;chr&gt;                   &lt;dbl&gt; &lt;int&gt;\n 1 Paranormal Activity      671.  2007\n 2 The Blair Witch Project  648.  1999\n 3 El Mariachi              583.  1992\n 4 Clerks.                  258.  1994\n 5 In the Company of Men    231.  1997\n 6 Napoleon Dynamite        227.  2004\n 7 Once                     190.  2006\n 8 The Devil Inside         155.  2012\n 9 Primer                   142.  2004\n10 Fireproof                134.  2008\n# ℹ 1,605 more rows\n\n\nBelow is a visualization of the return on investment by test result, however it’s difficult to see the distributions due to a few extreme observations.\n\nggplot(data = bechdel90_13, \n       mapping = aes(x = clean_test, y = roi, color = binary)) +\n  geom_boxplot() +\n  labs(\n    title = \"Return on investment vs. Bechdel test result\",\n    x = \"Detailed Bechdel result\",\n    y = \"___\",\n    color = \"Binary Bechdel result\"\n    )\n\n\n\n\n\n\n\n\nWhat are those movies with very high returns on investment?\n\nbechdel90_13 %&gt;%\n  filter(roi &gt; 400) %&gt;%\n  select(title, budget_2013, domgross_2013, year)\n\n# A tibble: 3 × 4\n  title                   budget_2013 domgross_2013  year\n  &lt;chr&gt;                         &lt;int&gt;         &lt;dbl&gt; &lt;int&gt;\n1 Paranormal Activity          505595     121251476  2007\n2 The Blair Witch Project      839077     196538593  1999\n3 El Mariachi                   11622       3388636  1992\n\n\nZooming in on the movies with roi &lt; ___ provides a better view of how the medians across the categories compare:\n\nggplot(data = bechdel90_13, mapping = aes(x = clean_test, y = roi, color = binary)) +\n  geom_boxplot() +\n  labs(\n    title = \"Return on investment vs. Bechdel test result\",\n    subtitle = \"___\", # Something about zooming in to a certain level\n    x = \"Detailed Bechdel result\",\n    y = \"Return on investment\",\n    color = \"Binary Bechdel result\"\n    ) +\n  coord_cartesian(ylim = c(0, 15))"
  },
  {
    "objectID": "teaching/tidyverse/material/sales-excel.html",
    "href": "teaching/tidyverse/material/sales-excel.html",
    "title": "Sales",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl)\n\n\nRead in the Excel file called sales.xlsx from the data-raw/ folder such that it looks like the following.\n\n\n\n\n\n\n\n\n\n\n\nStretch goal: Manipulate the sales data such such that it looks like the following."
  },
  {
    "objectID": "teaching/tidyverse/material/data-type-class-exercises.html",
    "href": "teaching/tidyverse/material/data-type-class-exercises.html",
    "title": "Data Type and Data Classes: Exercises",
    "section": "",
    "text": "Double check that you do not have stored objects in your current session with the following command. This will list all objects that you have in your current R session.\n\nls()\n\n[1] \"pandoc_dir\"      \"quarto_bin_path\"\n\n\nIn case you have objects that you want to remove from the current session you can do so with the rm() function.  This command will remove all objects available in your current environment.\n\nrm(list = ls())\n\nThis command uses commands that we have not talked about yet. If you do not understand how it works now, you will do so after tomorrows lectures and exercises.\n\nCreate variables var1 and var2 and initialize them with two integers of choice.\nAdd the two variables and save them as a new variable named var3 and print the result.\nCheck the class, mode, and type for var1, var2, var3 and π (is found under the variable name pi in R)\nCreate two character variables containing a text of choice. Check the mode, class, and type of the first one.\n\nAdd var1 to it. What is the result and why?\n\n\n\n\nConvert var3 to an integer, cast an integer variable to double, cast a string to a double.\nReport floor and ceiling of π and round π to 3 decimal places.\nIs floor of π an integer?\nTreat \"3.56437\" string as number.\nDivide ∞ by - ∞\nPrint a truth table for OR (for three distinct logical values). Read about truth tables here.\nMultiply a logical TRUE by a logical FALSE. Rise the logical true to the 7-th power.\nCreate two character variables containing two verses of a chosen song.\n\n\nConcatenate the two variables,\n\nPaste the variables with ‘*’ as separator.\n\nFind if ‘and’ occurs in the second line,\n\nSubstitute a word for another,\n\nExtract substring starting at the 5th character and 5 characters long."
  },
  {
    "objectID": "teaching/tidyverse/material/data-type-class-exercises.html#exercise",
    "href": "teaching/tidyverse/material/data-type-class-exercises.html#exercise",
    "title": "Data Type and Data Classes: Exercises",
    "section": "",
    "text": "Double check that you do not have stored objects in your current session with the following command. This will list all objects that you have in your current R session.\n\nls()\n\n[1] \"pandoc_dir\"      \"quarto_bin_path\"\n\n\nIn case you have objects that you want to remove from the current session you can do so with the rm() function.  This command will remove all objects available in your current environment.\n\nrm(list = ls())\n\nThis command uses commands that we have not talked about yet. If you do not understand how it works now, you will do so after tomorrows lectures and exercises.\n\nCreate variables var1 and var2 and initialize them with two integers of choice.\nAdd the two variables and save them as a new variable named var3 and print the result.\nCheck the class, mode, and type for var1, var2, var3 and π (is found under the variable name pi in R)\nCreate two character variables containing a text of choice. Check the mode, class, and type of the first one.\n\nAdd var1 to it. What is the result and why?\n\n\n\n\nConvert var3 to an integer, cast an integer variable to double, cast a string to a double.\nReport floor and ceiling of π and round π to 3 decimal places.\nIs floor of π an integer?\nTreat \"3.56437\" string as number.\nDivide ∞ by - ∞\nPrint a truth table for OR (for three distinct logical values). Read about truth tables here.\nMultiply a logical TRUE by a logical FALSE. Rise the logical true to the 7-th power.\nCreate two character variables containing two verses of a chosen song.\n\n\nConcatenate the two variables,\n\nPaste the variables with ‘*’ as separator.\n\nFind if ‘and’ occurs in the second line,\n\nSubstitute a word for another,\n\nExtract substring starting at the 5th character and 5 characters long."
  },
  {
    "objectID": "teaching/tidyverse/material/hotels-forcats.html",
    "href": "teaching/tidyverse/material/hotels-forcats.html",
    "title": "Hotel bookings - factors",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)\n\nLoad the hotels data set we used in a previous practical. Render and view the following visualisation. How are the months ordered? What would be a better order? Then, reorder the months on the x-axis (levels of arrival_date_month) in a way that makes more sense. You will want to use a function from the forcats package, see https://forcats.tidyverse.org/reference/index.html for inspiration and help.\nStretch goal: If you finish the above task before time is up, change the y-axis label so the values are shown with dollar signs, e.g. $80 instead of 80. You will want to use a function from the scales package, see https://scales.r-lib.org/reference/index.html for inspiration and help.\n\nhotels %&gt;%\n  group_by(hotel, arrival_date_month) %&gt;%   # group by hotel type and arrival month\n  summarise(mean_adr = mean(adr)) %&gt;%       # calculate mean adr for each group\n  ggplot(aes(\n    x = arrival_date_month,                 # x-axis = arrival_date_month\n    y = mean_adr,                           # y-axis = mean_adr calculated above\n    group = hotel,                          # group lines by hotel type\n    color = hotel)                          # and color by hotel type\n    ) +\n  geom_line() +                             # use lines to represent data\n  theme_minimal() +                         # use a minimal theme\n  labs(\n    x = \"Arrival month\",                 # customize labels\n    y = \"Mean ADR (average daily rate)\",\n    title = \"Comparison of resort and city hotel prices across months\",\n    subtitle = \"Resort hotel prices soar in the summer while ciry hotel prices remain relatively constant throughout the year\",\n    color = \"Hotel type\"\n    )"
  },
  {
    "objectID": "teaching/tidyverse/material/legos.html",
    "href": "teaching/tidyverse/material/legos.html",
    "title": "Legos",
    "section": "",
    "text": "Here, we work with (simulated) data from Lego sales in 2018 for a sample of customers who bought Legos in the US."
  },
  {
    "objectID": "teaching/tidyverse/material/legos.html#data-and-packages",
    "href": "teaching/tidyverse/material/legos.html#data-and-packages",
    "title": "Legos",
    "section": "Data and Packages",
    "text": "Data and Packages\nWe’ll use the tidyverse package for much of the data wrangling and visualisation and the data is given to import.\n\nlibrary(tidyverse)\n\nThe following variables are available in the data set:\n\nfirst_name: First name of customer\nlast_name: Last name of customer\nage: Age of customer\nphone_number: Phone number of customer\nset_id: Set ID of lego set purchased\nnumber: Item number of lego set purchased\ntheme: Theme of lego set purchased\nsubtheme: Sub theme of lego set purchased\nyear: Year of purchase\nname: Name of lego set purchased\npieces: Number of pieces of legos in set purchased\nus_price: Price of set purchase in US Dollars\nimage_url: Image URL of lego set purchased\nquantity: Quantity of lego set(s) purchased"
  },
  {
    "objectID": "teaching/tidyverse/material/starwars.html",
    "href": "teaching/tidyverse/material/starwars.html",
    "title": "Visualizing Starwars characters",
    "section": "",
    "text": "Glimpse at the starwars data frame.\n\n\nglimpse(starwars)\n\nRows: 87\nColumns: 14\n$ name       &lt;chr&gt; \"Luke Skywalker\", \"C-3PO\", \"R2-D2\", \"Darth Vader\", \"Leia Or…\n$ height     &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 2…\n$ mass       &lt;dbl&gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77.…\n$ hair_color &lt;chr&gt; \"blond\", NA, NA, \"none\", \"brown\", \"brown, grey\", \"brown\", N…\n$ skin_color &lt;chr&gt; \"fair\", \"gold\", \"white, blue\", \"white\", \"light\", \"light\", \"…\n$ eye_color  &lt;chr&gt; \"blue\", \"yellow\", \"red\", \"yellow\", \"brown\", \"blue\", \"blue\",…\n$ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57.0, …\n$ sex        &lt;chr&gt; \"male\", \"none\", \"none\", \"male\", \"female\", \"male\", \"female\",…\n$ gender     &lt;chr&gt; \"masculine\", \"masculine\", \"masculine\", \"masculine\", \"femini…\n$ homeworld  &lt;chr&gt; \"Tatooine\", \"Tatooine\", \"Naboo\", \"Tatooine\", \"Alderaan\", \"T…\n$ species    &lt;chr&gt; \"Human\", \"Droid\", \"Droid\", \"Human\", \"Human\", \"Human\", \"Huma…\n$ films      &lt;list&gt; &lt;\"A New Hope\", \"The Empire Strikes Back\", \"Return of the J…\n$ vehicles   &lt;list&gt; &lt;\"Snowspeeder\", \"Imperial Speeder Bike\"&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, \"Imp…\n$ starships  &lt;list&gt; &lt;\"X-wing\", \"Imperial shuttle\"&gt;, &lt;&gt;, &lt;&gt;, \"TIE Advanced x1\",…\n\n\n\nModify the following plot to change the color of all points to \"pink\".\n\n\nggplot(starwars, \n       aes(x = height, y = mass, color = gender, size = birth_year)) +\n  geom_point(color = \"pink\")\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nAdd labels for title, x and y axes, and size of points. Uncomment to see the effect.\n\n\nggplot(starwars, \n       aes(x = height, y = mass, color = gender, size = birth_year)) +\n  geom_point(color = \"#30509C\") +\n  labs(\n    #title = \"___\",\n    #x = \"___\", \n    #y = \"___\",\n    #___\n    )\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nPick a single numerical variable and make a histogram of it. Select a reasonable binwidth for it.\n\n(A little bit of starter code is provided below, and the code chunk is set to not be evaluated with eval: false because the current code in there is not valid code and hence the document wouldn’t knit. Once you replace the code with valid code, set the chunk option to eval: true, or remove the eval option altogether since it’s set to true by default.)\n\nggplot(starwars, aes(___)) +\n  geom___\n\n\nPick a numerical variable and a categorical variable and make a visualization (you pick the type!) to visualization the relationship between the two variables. Along with your code and output, provide an interpretation of the visualization.\n\nInterpretation goes here…\n\nPick a single categorical variable from the data set and make a bar plot of its distribution.\n\n\nPick two categorical variables and make a visualization to visualize the relationship between the two variables. Along with your code and output, provide an interpretation of the visualization.\n\nInterpretation goes here…\n\nPick two numerical variables and two categorical variables and make a visualization that incorporates all of them and provide an interpretation with your answer.\n\n(This time no starter code is provided, you’re on your own!)\nInterpretation goes here…"
  },
  {
    "objectID": "teaching/tidyverse/material/type-coercion.html",
    "href": "teaching/tidyverse/material/type-coercion.html",
    "title": "Type coercion",
    "section": "",
    "text": "c(1, 1L, \"C\")\n\n\nc(1, 1L, \"C\")\n\n[1] \"1\" \"1\" \"C\"\n\n\n\n1\n\n[1] 1\n\n1L\n\n[1] 1\n\n\"C\"\n\n[1] \"C\"\n\n\n\n#typeof(c(1, 1L, \"C\"))\n\n\nc(1L / 0, \"A\")\n\n\nc(1L / 0, \"A\")\n\n[1] \"Inf\" \"A\"  \n\n\n\ntypeof(1L)\n\n[1] \"integer\"\n\ntypeof(0)\n\n[1] \"double\"\n\ntypeof(1L/0)\n\n[1] \"double\"\n\ntypeof(\"A\")\n\n[1] \"character\"\n\n\n\n#typeof(c(1L / 0, \"A\"))\n\n\nc(1:3, 5)\n\n\nc(1:3, 5)\n\n[1] 1 2 3 5\n\n\n\ntypeof(1:3)\n\n[1] \"integer\"\n\ntypeof(5)\n\n[1] \"double\"\n\n\n\n#typeof(c(1:3, 5))\n\n\nc(3, \"3+\")\n\n\nc(3, \"3+\")\n\n[1] \"3\"  \"3+\"\n\n\n\ntypeof(3)\n\n[1] \"double\"\n\ntypeof(\"3+\")\n\n[1] \"character\"\n\n\n\n#typeof(c(3, \"3+\"))\n\n\nc(NA, TRUE)\n\n\nc(NA, TRUE)\n\n[1]   NA TRUE\n\n\n\ntypeof(NA)\n\n[1] \"logical\"\n\ntypeof(TRUE)\n\n[1] \"logical\"\n\n\n\n#typeof(c(NA, TRUE))"
  },
  {
    "objectID": "teaching/tidyverse/material/la-quinta.html",
    "href": "teaching/tidyverse/material/la-quinta.html",
    "title": "La Quinta is Spanish for next to Denny’s",
    "section": "",
    "text": "Have you ever taken a road trip in the US and thought to yourself “I wonder what La Quinta means”. Well, the late comedian Mitch Hedberg thinks it’s Spanish for next to Denny’s.\nIf you’re not familiar with these two establishments, Denny’s is a casual diner chain that is open 24 hours and La Quinta Inn and Suites is a hotel chain.\nThese two establishments tend to be clustered together, or at least this observation is a joke made famous by Mitch Hedberg. In this lab we explore the validity of this joke and along the way learn some more data wrangling and tips for visualizing spatial data.\nThe inspiration for this comes from a blog post by John Reiser on his new jersey geographer blog. You can read that analysis here. Reiser’s blog post focuses on scraping data from Denny’s and La Quinta Inn and Suites websites using Python. Here, we focus on visualization and analysis of these data."
  },
  {
    "objectID": "teaching/tidyverse/material/la-quinta.html#packages",
    "href": "teaching/tidyverse/material/la-quinta.html#packages",
    "title": "La Quinta is Spanish for next to Denny’s",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for much of the data wrangling and visualisation and the data lives in the dsbox package. These packages are already installed for you. You can load them by running the following in your Console:\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "teaching/tidyverse/material/la-quinta.html#data",
    "href": "teaching/tidyverse/material/la-quinta.html#data",
    "title": "La Quinta is Spanish for next to Denny’s",
    "section": "Data",
    "text": "Data\nThe data sets we’ll use are called dennys and laquinta and are available for download. Note that these data were scraped from here and here, respectively. You can find information about the data sets here and here. To help with our analysis we will also use a data set on US states.\n\nlaquinta &lt;- read_csv(\"data/laquinta.csv\")\ndennys &lt;- read_csv(\"data/dennys.csv\")\nstates &lt;- read_csv(\"data/states.csv\")\n\nEach observation in the states dataset represents a state, including DC. Along with the name of the state we have the two-letter abbreviation and we have the geographic area of the state (in square miles)."
  },
  {
    "objectID": "teaching/tidyverse/material/college-majors.html",
    "href": "teaching/tidyverse/material/college-majors.html",
    "title": "What should I major in?",
    "section": "",
    "text": "The first step in the process of turning information into knowledge process is to summarize and describe the raw information - the data. In this assignment we explore data on college majors and earnings, specifically the data begin the FiveThirtyEight story “The Economic Guide To Picking A College Major”.\nThese data originally come from the American Community Survey (ACS) 2010-2012 Public Use Microdata Series. While this is outside the scope of this assignment, if you are curious about how raw data from the ACS were cleaned and prepared, see the code FiveThirtyEight authors used.\nWe should also note that there are many considerations that go into picking a major. Earnings potential and employment prospects are two of them, and they are important, but they don’t tell the whole story. Keep this in mind as you analyze the data."
  },
  {
    "objectID": "teaching/tidyverse/material/college-majors.html#packages",
    "href": "teaching/tidyverse/material/college-majors.html#packages",
    "title": "What should I major in?",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for much of the data wrangling and visualisation, the scales package for better formatting of labels on visualisations, and the data lives in the fivethirtyeight package. These packages are already installed for you. You can load them by running the following in your Console:\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(fivethirtyeight)"
  },
  {
    "objectID": "teaching/tidyverse/material/college-majors.html#data",
    "href": "teaching/tidyverse/material/college-majors.html#data",
    "title": "What should I major in?",
    "section": "Data",
    "text": "Data\nThe data can be found in the fivethirtyeight package, and it’s called college_recent_grads. Since the dataset is distributed with the package, we don’t need to load it separately; it becomes available to us when we load the package. You can find out more about the dataset by inspecting its documentation, which you can access by running ?college_recent_grads in the Console or using the Help menu in RStudio to search for college_recent_grads. You can also find this information here.\nYou can also take a quick peek at your data frame and view its dimensions with the glimpse function.\n\nglimpse(college_recent_grads)\n\nRows: 173\nColumns: 21\n$ rank                        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,…\n$ major_code                  &lt;int&gt; 2419, 2416, 2415, 2417, 2405, 2418, 6202, …\n$ major                       &lt;chr&gt; \"Petroleum Engineering\", \"Mining And Miner…\n$ major_category              &lt;chr&gt; \"Engineering\", \"Engineering\", \"Engineering…\n$ total                       &lt;int&gt; 2339, 756, 856, 1258, 32260, 2573, 3777, 1…\n$ sample_size                 &lt;int&gt; 36, 7, 3, 16, 289, 17, 51, 10, 1029, 631, …\n$ men                         &lt;int&gt; 2057, 679, 725, 1123, 21239, 2200, 2110, 8…\n$ women                       &lt;int&gt; 282, 77, 131, 135, 11021, 373, 1667, 960, …\n$ sharewomen                  &lt;dbl&gt; 0.1205643, 0.1018519, 0.1530374, 0.1073132…\n$ employed                    &lt;int&gt; 1976, 640, 648, 758, 25694, 1857, 2912, 15…\n$ employed_fulltime           &lt;int&gt; 1849, 556, 558, 1069, 23170, 2038, 2924, 1…\n$ employed_parttime           &lt;int&gt; 270, 170, 133, 150, 5180, 264, 296, 553, 1…\n$ employed_fulltime_yearround &lt;int&gt; 1207, 388, 340, 692, 16697, 1449, 2482, 82…\n$ unemployed                  &lt;int&gt; 37, 85, 16, 40, 1672, 400, 308, 33, 4650, …\n$ unemployment_rate           &lt;dbl&gt; 0.018380527, 0.117241379, 0.024096386, 0.0…\n$ p25th                       &lt;dbl&gt; 95000, 55000, 50000, 43000, 50000, 50000, …\n$ median                      &lt;dbl&gt; 110000, 75000, 73000, 70000, 65000, 65000,…\n$ p75th                       &lt;dbl&gt; 125000, 90000, 105000, 80000, 75000, 10200…\n$ college_jobs                &lt;int&gt; 1534, 350, 456, 529, 18314, 1142, 1768, 97…\n$ non_college_jobs            &lt;int&gt; 364, 257, 176, 102, 4440, 657, 314, 500, 1…\n$ low_wage_jobs               &lt;int&gt; 193, 50, 0, 0, 972, 244, 259, 220, 3253, 3…\n\n\nThe college_recent_grads data frame is a trove of information. Let’s think about some questions we might want to answer with these data:\n\nWhich major has the lowest unemployment rate?\nWhich major has the highest percentage of women?\nHow do the distributions of median income compare across major categories?\nDo women tend to choose majors with lower or higher earnings?\n\nIn the next section we aim to answer these questions."
  },
  {
    "objectID": "teaching/tidyverse/material/college-majors.html#which-major-has-the-lowest-unemployment-rate",
    "href": "teaching/tidyverse/material/college-majors.html#which-major-has-the-lowest-unemployment-rate",
    "title": "What should I major in?",
    "section": "Which major has the lowest unemployment rate?",
    "text": "Which major has the lowest unemployment rate?\nIn order to answer this question all we need to do is sort the data. We use the arrange function to do this, and sort it by the unemployment_rate variable. By default arrange sorts in ascending order, which is what we want here – we’re interested in the major with the lowest unemployment rate.\n\ncollege_recent_grads %&gt;%\n  arrange(unemployment_rate)\n\n# A tibble: 173 × 21\n    rank major_code major           major_category total sample_size   men women\n   &lt;int&gt;      &lt;int&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;int&gt;       &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1    53       4005 Mathematics An… Computers & M…   609           7   500   109\n 2    74       3801 Military Techn… Industrial Ar…   124           4   124     0\n 3    84       3602 Botany          Biology & Lif…  1329           9   626   703\n 4   113       1106 Soil Science    Agriculture &…   685           4   476   209\n 5   121       2301 Educational Ad… Education        804           5   280   524\n 6    15       2409 Engineering Me… Engineering     4321          30  3526   795\n 7    20       3201 Court Reporting Law & Public …  1148          14   877   271\n 8   120       2305 Mathematics Te… Education      14237         123  3872 10365\n 9     1       2419 Petroleum Engi… Engineering     2339          36  2057   282\n10    65       1100 General Agricu… Agriculture &… 10399         158  6053  4346\n# ℹ 163 more rows\n# ℹ 13 more variables: sharewomen &lt;dbl&gt;, employed &lt;int&gt;,\n#   employed_fulltime &lt;int&gt;, employed_parttime &lt;int&gt;,\n#   employed_fulltime_yearround &lt;int&gt;, unemployed &lt;int&gt;,\n#   unemployment_rate &lt;dbl&gt;, p25th &lt;dbl&gt;, median &lt;dbl&gt;, p75th &lt;dbl&gt;,\n#   college_jobs &lt;int&gt;, non_college_jobs &lt;int&gt;, low_wage_jobs &lt;int&gt;\n\n\nThis gives us what we wanted, but not in an ideal form. First, the name of the major barely fits on the page. Second, some of the variables are not that useful (e.g. major_code, major_category) and some we might want front and center are not easily viewed (e.g. unemployment_rate).\nWe can use the select function to choose which variables to display, and in which order:\n\ncollege_recent_grads %&gt;%\n  arrange(unemployment_rate) %&gt;%\n  select(rank, major, unemployment_rate)\n\n# A tibble: 173 × 3\n    rank major                                      unemployment_rate\n   &lt;int&gt; &lt;chr&gt;                                                  &lt;dbl&gt;\n 1    53 Mathematics And Computer Science                     0      \n 2    74 Military Technologies                                0      \n 3    84 Botany                                               0      \n 4   113 Soil Science                                         0      \n 5   121 Educational Administration And Supervision           0      \n 6    15 Engineering Mechanics Physics And Science            0.00633\n 7    20 Court Reporting                                      0.0117 \n 8   120 Mathematics Teacher Education                        0.0162 \n 9     1 Petroleum Engineering                                0.0184 \n10    65 General Agriculture                                  0.0196 \n# ℹ 163 more rows\n\n\nOk, this is looking better, but do we really need to display all those decimal places in the unemployment variable? Not really!\nWe can use the percent() function to clean up the display a bit.\n\ncollege_recent_grads %&gt;%\n  arrange(unemployment_rate) %&gt;%\n  select(rank, major, unemployment_rate) %&gt;%\n  mutate(unemployment_rate = percent(unemployment_rate))\n\n# A tibble: 173 × 3\n    rank major                                      unemployment_rate\n   &lt;int&gt; &lt;chr&gt;                                      &lt;chr&gt;            \n 1    53 Mathematics And Computer Science           0.00000%         \n 2    74 Military Technologies                      0.00000%         \n 3    84 Botany                                     0.00000%         \n 4   113 Soil Science                               0.00000%         \n 5   121 Educational Administration And Supervision 0.00000%         \n 6    15 Engineering Mechanics Physics And Science  0.63343%         \n 7    20 Court Reporting                            1.16897%         \n 8   120 Mathematics Teacher Education              1.62028%         \n 9     1 Petroleum Engineering                      1.83805%         \n10    65 General Agriculture                        1.96425%         \n# ℹ 163 more rows"
  },
  {
    "objectID": "teaching/tidyverse/material/college-majors.html#which-major-has-the-highest-percentage-of-women",
    "href": "teaching/tidyverse/material/college-majors.html#which-major-has-the-highest-percentage-of-women",
    "title": "What should I major in?",
    "section": "Which major has the highest percentage of women?",
    "text": "Which major has the highest percentage of women?\nTo answer such a question we need to arrange the data in descending order. For example, if earlier we were interested in the major with the highest unemployment rate, we would use the following:\n\nThe `desc` function specifies that we want `unemployment_rate` in descending order.\n\n\ncollege_recent_grads %&gt;%\n  arrange(desc(unemployment_rate)) %&gt;%\n  select(rank, major, unemployment_rate)\n\n# A tibble: 173 × 3\n    rank major                                      unemployment_rate\n   &lt;int&gt; &lt;chr&gt;                                                  &lt;dbl&gt;\n 1     6 Nuclear Engineering                                    0.177\n 2    90 Public Administration                                  0.159\n 3    85 Computer Networking And Telecommunications             0.152\n 4   171 Clinical Psychology                                    0.149\n 5    30 Public Policy                                          0.128\n 6   106 Communication Technologies                             0.120\n 7     2 Mining And Mineral Engineering                         0.117\n 8    54 Computer Programming And Data Processing               0.114\n 9    80 Geography                                              0.113\n10    59 Architecture                                           0.113\n# ℹ 163 more rows\n\n\n\nUsing what you’ve learned so far, arrange the data in descending order with respect to proportion of women in a major, and display only the major, the total number of people with major, and proportion of women. Show only the top 3 majors by adding top_n(3) at the end of the pipeline."
  },
  {
    "objectID": "teaching/tidyverse/material/college-majors.html#how-do-the-distributions-of-median-income-compare-across-major-categories",
    "href": "teaching/tidyverse/material/college-majors.html#how-do-the-distributions-of-median-income-compare-across-major-categories",
    "title": "What should I major in?",
    "section": "How do the distributions of median income compare across major categories?",
    "text": "How do the distributions of median income compare across major categories?\nNote: A percentile is a measure used in statistics indicating the value below which a given percentage of observations in a group of observations fall. For example, the 20th percentile is the value below which 20% of the observations may be found. (Source: Wikipedia\nThere are three types of incomes reported in this data frame: p25th, median, and p75th. These correspond to the 25th, 50th, and 75th percentiles of the income distribution of sampled individuals for a given major.\n\nWhy do we often choose the median, rather than the mean, to describe the typical income of a group of people?\n\nThe question we want to answer “How do the distributions of median income compare across major categories?”. We need to do a few things to answer this question: First, we need to group the data by major_category. Then, we need a way to summarize the distributions of median income within these groups. This decision will depend on the shapes of these distributions. So first, we need to visualize the data.\nWe use the ggplot() function to do this. The first argument is the data frame, and the next argument gives the mapping of the variables of the data to the aesthetic elements of the plot.\nLet’s start simple and take a look at the distribution of all median incomes, without considering the major categories.\n\nggplot(data = college_recent_grads, mapping = aes(x = median)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\nAlong with the plot, we get a message:\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nThis is telling us that we might want to reconsider the binwidth we chose for our histogram – or more accurately, the binwidth we didn’t specify. It’s good practice to always think in the context of the data and try out a few binwidths before settling on a binwidth. You might ask yourself: “What would be a meaningful difference in median incomes?” $1 is obviously too little, $10000 might be too high.\n\nTry binwidths of $1000 and $5000 and choose one. Explain your reasoning for your choice. Note that the binwidth is an argument for the geom_histogram function. So to specify a binwidth of $1000, you would use geom_histogram(binwidth = 1000).\n\nWe can also calculate summary statistics for this distribution using the summarise function:\n\ncollege_recent_grads %&gt;%\n  summarise(min = min(median), max = max(median),\n            mean = mean(median), med = median(median),\n            sd = sd(median), \n            q1 = quantile(median, probs = 0.25),\n            q3 = quantile(median, probs = 0.75))\n\n# A tibble: 1 × 7\n    min    max   mean   med     sd    q1    q3\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 22000 110000 40151. 36000 11470. 33000 45000\n\n\n\nBased on the shape of the histogram you created in the previous exercise, determine which of these summary statistics is useful for describing the distribution. Write up your description (remember shape, center, spread, any unusual observations) and include the summary statistic output as well.\nPlot the distribution of median income using a histogram, faceted by major_category. Use the binwidth you chose in the earlier exercise.\n\nNow that we’ve seen the shapes of the distributions of median incomes for each major category, we should have a better idea for which summary statistic to use to quantify the typical median income.\n\nWhich major category has the highest typical (you’ll need to decide what this means) median income? Use the partial code below, filling it in with the appropriate statistic and function. Also note that we are looking for the highest statistic, so make sure to arrange in the correct direction.\n\n\ncollege_recent_grads %&gt;%\n  group_by(major_category) %&gt;%\n  summarise(___ = ___(median)) %&gt;%\n  arrange(___)\n\n\nWhich major category is the least popular in this sample? To answer this question we use a new function called count, which first groups the data and then counts the number of observations in each category (see below). Add to the pipeline appropriately to arrange the results so that the major with the lowest observations is on top.\n\n\ncollege_recent_grads %&gt;%\n  count(major_category)\n\n# A tibble: 16 × 2\n   major_category                          n\n   &lt;chr&gt;                               &lt;int&gt;\n 1 Agriculture & Natural Resources        10\n 2 Arts                                    8\n 3 Biology & Life Science                 14\n 4 Business                               13\n 5 Communications & Journalism             4\n 6 Computers & Mathematics                11\n 7 Education                              16\n 8 Engineering                            29\n 9 Health                                 12\n10 Humanities & Liberal Arts              15\n11 Industrial Arts & Consumer Services     7\n12 Interdisciplinary                       1\n13 Law & Public Policy                     5\n14 Physical Sciences                      10\n15 Psychology & Social Work                9\n16 Social Science                          9"
  },
  {
    "objectID": "teaching/tidyverse/material/college-majors.html#all-stem-fields-arent-the-same",
    "href": "teaching/tidyverse/material/college-majors.html#all-stem-fields-arent-the-same",
    "title": "What should I major in?",
    "section": "All STEM fields aren’t the same",
    "text": "All STEM fields aren’t the same\nOne of the sections of the FiveThirtyEight story is “All STEM fields aren’t the same”. Let’s see if this is true.\nFirst, let’s create a new vector called stem_categories that lists the major categories that are considered STEM fields.\n\nstem_categories &lt;- c(\"Biology & Life Science\",\n                     \"Computers & Mathematics\",\n                     \"Engineering\",\n                     \"Physical Sciences\")\n\nThen, we can use this to create a new variable in our data frame indicating whether a major is STEM or not.\n\ncollege_recent_grads &lt;- college_recent_grads %&gt;%\n  mutate(major_type = ifelse(major_category %in% stem_categories, \"stem\", \"not stem\"))\n\nLet’s unpack this: with mutate we create a new variable called major_type, which is defined as \"stem\" if the major_category is in the vector called stem_categories we created earlier, and as \"not stem\" otherwise.\n%in% is a logical operator. Other logical operators that are commonly used are\n\n\n\nOperator\nOperation\n\n\n\n\nx &lt; y\nless than\n\n\nx &gt; y\ngreater than\n\n\nx &lt;= y\nless than or equal to\n\n\nx &gt;= y\ngreater than or equal to\n\n\nx != y\nnot equal to\n\n\nx == y\nequal to\n\n\nx %in% y\ncontains\n\n\nx | y\nor\n\n\nx & y\nand\n\n\n!x\nnot\n\n\n\nWe can use the logical operators to also filter our data for STEM majors whose median earnings is less than median for all majors’ median earnings, which we found to be $36,000 earlier.\n\ncollege_recent_grads %&gt;%\n  filter(\n    major_type == \"stem\",\n    median &lt; 36000\n  )\n\n# A tibble: 10 × 22\n    rank major_code major        major_category  total sample_size    men  women\n   &lt;int&gt;      &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;           &lt;int&gt;       &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n 1    93       1301 Environment… Biology & Lif…  25965         225  10787  15178\n 2    98       5098 Multi-Disci… Physical Scie…  62052         427  27015  35037\n 3   102       3608 Physiology   Biology & Lif…  22060          99   8422  13638\n 4   106       2001 Communicati… Computers & M…  18035         208  11431   6604\n 5   109       3611 Neuroscience Biology & Lif…  13663          53   4944   8719\n 6   111       5002 Atmospheric… Physical Scie…   4043          32   2744   1299\n 7   123       3699 Miscellaneo… Biology & Lif…  10706          63   4747   5959\n 8   124       3600 Biology      Biology & Lif… 280709        1370 111762 168947\n 9   133       3604 Ecology      Biology & Lif…   9154          86   3878   5276\n10   169       3609 Zoology      Biology & Lif…   8409          47   3050   5359\n# ℹ 14 more variables: sharewomen &lt;dbl&gt;, employed &lt;int&gt;,\n#   employed_fulltime &lt;int&gt;, employed_parttime &lt;int&gt;,\n#   employed_fulltime_yearround &lt;int&gt;, unemployed &lt;int&gt;,\n#   unemployment_rate &lt;dbl&gt;, p25th &lt;dbl&gt;, median &lt;dbl&gt;, p75th &lt;dbl&gt;,\n#   college_jobs &lt;int&gt;, non_college_jobs &lt;int&gt;, low_wage_jobs &lt;int&gt;,\n#   major_type &lt;chr&gt;\n\n\n\nWhich STEM majors have median salaries equal to or less than the median for all majors’ median earnings? Your output should only show the major name and median, 25th percentile, and 75th percentile earning for that major as and should be sorted such that the major with the highest median earning is on top."
  },
  {
    "objectID": "teaching/tidyverse/material/college-majors.html#what-types-of-majors-do-women-tend-to-major-in",
    "href": "teaching/tidyverse/material/college-majors.html#what-types-of-majors-do-women-tend-to-major-in",
    "title": "What should I major in?",
    "section": "What types of majors do women tend to major in?",
    "text": "What types of majors do women tend to major in?\n\nCreate a scatterplot of median income vs. proportion of women in that major, coloured by whether the major is in a STEM field or not. Describe the association between these three variables."
  },
  {
    "objectID": "teaching/tidyverse/material/college-majors.html#further-exploration",
    "href": "teaching/tidyverse/material/college-majors.html#further-exploration",
    "title": "What should I major in?",
    "section": "Further exploration",
    "text": "Further exploration\n\nAsk a question of interest to you, and answer it using summary statistic(s) and/or visualization(s)."
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nSubtitle\n\n\n\n\n\n\nData Science with Tidyverse\n\n\nMSc course\n\n\n\n\nMathematics for Social Scientists\n\n\nMSc course\n\n\n\n\nStatistical Learning\n\n\nMSc course\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/stat-learn/material/03-linear-regression-II.html",
    "href": "teaching/stat-learn/material/03-linear-regression-II.html",
    "title": "Linear Regression II",
    "section": "",
    "text": "Our goal is to run a few models of different complexity and evaluate the performance based on a test-train split of the data. Can you understand why we do not use the full data to test the different models?\nWe will start by creating a function that allows us to assess model performance:\n\nrmse = function(actual, predicted) {\n  sqrt(mean((actual - predicted) ^ 2))\n}\n\nWe will also use a function that gives the complexity of the linear model as number of independent variables defined.\n\nget_complexity = function(model) {\n  length(coef(model)) - 1\n}\n\nCan you understand why we take -1 in the above function?\n\n\nWe will use the Advertisement data from ISLR2 which is in a compressed folder for download. Make sure you have set the correct working directory for reading the data. You can also load the data from the ISLR2 package. The Advertising data set consists of the sales of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: TV, radio, and newspaper.\n\nlibrary(readr) # install if not in your library\nAdvertising = read_csv(\"03-data/Advertising.csv\") \nknitr::kable(head(Advertising,10)) # look at 10 first rows of data\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n230.1\n37.8\n69.2\n22.1\n\n\n44.5\n39.3\n45.1\n10.4\n\n\n17.2\n45.9\n69.3\n9.3\n\n\n151.5\n41.3\n58.5\n18.5\n\n\n180.8\n10.8\n58.4\n12.9\n\n\n8.7\n48.9\n75.0\n7.2\n\n\n57.5\n32.8\n23.5\n11.8\n\n\n120.2\n19.6\n11.6\n13.2\n\n\n8.6\n2.1\n1.0\n4.8\n\n\n199.8\n2.6\n21.2\n10.6\n\n\n\n\n\nExplore the data by doing some plots. Example shown below.\n\nplot(Sales ~ TV, data = Advertising, col = \"dodgerblue\", pch = 20, cex = 1.5,\n     main = \"Sales vs Television Advertising\")\n\n\n\n\n\n\n\n\nWe can also explore the data by using a correlation matrix plot:\n\npairs(Advertising)\n\n\n\n\n\n\n\n\nWhat are the main patterns you detect?\n\n\n\nWe will now split the data in two. One part of the datasets will be used to fit (train) a model, which we will call the training data. The remainder of the original data will be used to assess how well the model is predicting, which we will call the test data.\nWe use the sample() function to obtain a random sample of the rows of the original data. We then use those row numbers (and remaining row numbers) to split the data accordingly. Notice we used the set.seed() function to reproduce the same random split each time we perform this analysis.\n\nset.seed(1)\nnum_obs = nrow(Advertising)\n\ntrain_index = sample(num_obs, size = trunc(0.50 * num_obs))\ntrain_data = Advertising[train_index, ]\ntest_data = Advertising[-train_index, ]\n\n\n\n\nWe start by fitting the simplest linear model, a model with no predictors:\n\nfit_0 = lm(Sales ~ 1, data = train_data)\nget_complexity(fit_0)\n\n[1] 0\n\n\nAs seen, the complexity of this model is 0. We compute the Test and Train RMSE for this model, via the function specified above and a direct formula:\n\n# train RMSE\nrmse(actual = train_data$Sales, predicted = predict(fit_0, train_data))\n\n[1] 5.297333\n\nsqrt(mean((train_data$Sales - predict(fit_0, train_data)) ^ 2))\n\n[1] 5.297333\n\n# test RMSE\nrmse(actual = test_data$Sales, predicted = predict(fit_0, test_data))\n\n[1] 5.112073\n\nsqrt(mean((test_data$Sales - predict(fit_0, test_data)) ^ 2)) \n\n[1] 5.112073\n\n\nLet’s make the computations of RMSE even more easy by using the below function:\n\n# train RMSE\nget_rmse = function(model, data, response) {\n  rmse(actual = subset(data, select = response, drop = TRUE),\n       predicted = predict(model, data))\n}\n\nCan you figure out how to use the above function?\nWe will fit 5 models, compute the train and test MSE for each model to evaluate it and visualize how it relates to the complexity (i.e. model size in terms of parameters) of the model. Furthermore, we will interpret the results in terms of underfitting or overfitting. We will conclude with dertemrineing which model to choose. The five models to fit are listed below:\n\nSales ~ Radio + Newspaper + TV\nSales ~ Radio * Newspaper * TV\nSales ~ Radio * Newspaper * TV + I(TV ^ 2)\nSales ~ Radio * Newspaper * TV + I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2)\nSales ~ Radio * Newspaper * TV + I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2)\n\nNote that the specification first*second indicates the cross of first and second. This is the same as first + second + first:second.\nWe fit all five models and print the output by creating a list of the model fits. We then obtain train RMSE, test RMSE, and model complexity for each. Finally, plot the results. The train RMSE can be seen in blue, while the test RMSE is given in orange.\n\nfit_1 = lm(Sales ~ Radio + Newspaper + TV, data = train_data)\nfit_2 = lm(Sales ~ Radio * Newspaper * TV, data = train_data)\nfit_3 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2), data = train_data)\nfit_4 = lm(Sales ~ Radio * Newspaper * TV + \n             I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2), data = train_data)\nfit_5 = lm(Sales ~ Radio * Newspaper * TV +\n             I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2), data = train_data)\n\nmodel_list = list(fit_1, fit_2, fit_3, fit_4, fit_5)\n#model_list #uncomment if you wish to see this list object\n\ntrain_rmse = sapply(model_list, get_rmse, data = train_data, response = \"Sales\")\ntest_rmse = sapply(model_list, get_rmse, data = test_data, response = \"Sales\")\nmodel_complexity = sapply(model_list, get_complexity)\n\ntrain_rmse = sapply(model_list, get_rmse, data = train_data, response = \"Sales\")\ntest_rmse = sapply(model_list, get_rmse, data = test_data, response = \"Sales\")\nmodel_complexity = sapply(model_list, get_complexity)\n\nplot(model_complexity, train_rmse, type = \"b\", \n     ylim = c(min(c(train_rmse, test_rmse)) - 0.02, \n              max(c(train_rmse, test_rmse)) + 0.02), \n     col = \"blue\", \n     lwd = 2,\n     xlab = \"Complexity (Model Size)\",\n     ylab = \"RMSE\")\nlines(model_complexity, test_rmse, type = \"b\", col = \"red\", lwd = 2)\ngrid()\n\n\n\n\n\n\n\n\nConclusion:\n\nUnderfitting models: In general High Train RMSE, High Test RMSE. Seen in fit_1 and fit_2.\nOverfitting models: In general Low Train RMSE, High Test RMSE. Seen in fit_4 and fit_5.\n\nWe see that the Test RMSE is smallest for fit_3, and it is the model we believe will perform the best on future data not used to train the model.\n\n\n\n\nWrite functions for the some of the other loss functions we covered in the lecture (e.g. MAE, MAPE). Apply them to asses the same five models above. Do you see similar or deviant results in terms of overfitting/underfitting?\nReplicate the simulation example from lecture. Try other models based on different order polynomials."
  },
  {
    "objectID": "teaching/stat-learn/material/03-linear-regression-II.html#data",
    "href": "teaching/stat-learn/material/03-linear-regression-II.html#data",
    "title": "Linear Regression II",
    "section": "",
    "text": "We will use the Advertisement data from ISLR2 which is in a compressed folder for download. Make sure you have set the correct working directory for reading the data. You can also load the data from the ISLR2 package. The Advertising data set consists of the sales of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: TV, radio, and newspaper.\n\nlibrary(readr) # install if not in your library\nAdvertising = read_csv(\"03-data/Advertising.csv\") \nknitr::kable(head(Advertising,10)) # look at 10 first rows of data\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n230.1\n37.8\n69.2\n22.1\n\n\n44.5\n39.3\n45.1\n10.4\n\n\n17.2\n45.9\n69.3\n9.3\n\n\n151.5\n41.3\n58.5\n18.5\n\n\n180.8\n10.8\n58.4\n12.9\n\n\n8.7\n48.9\n75.0\n7.2\n\n\n57.5\n32.8\n23.5\n11.8\n\n\n120.2\n19.6\n11.6\n13.2\n\n\n8.6\n2.1\n1.0\n4.8\n\n\n199.8\n2.6\n21.2\n10.6\n\n\n\n\n\nExplore the data by doing some plots. Example shown below.\n\nplot(Sales ~ TV, data = Advertising, col = \"dodgerblue\", pch = 20, cex = 1.5,\n     main = \"Sales vs Television Advertising\")\n\n\n\n\n\n\n\n\nWe can also explore the data by using a correlation matrix plot:\n\npairs(Advertising)\n\n\n\n\n\n\n\n\nWhat are the main patterns you detect?"
  },
  {
    "objectID": "teaching/stat-learn/material/03-linear-regression-II.html#test-train-split",
    "href": "teaching/stat-learn/material/03-linear-regression-II.html#test-train-split",
    "title": "Linear Regression II",
    "section": "",
    "text": "We will now split the data in two. One part of the datasets will be used to fit (train) a model, which we will call the training data. The remainder of the original data will be used to assess how well the model is predicting, which we will call the test data.\nWe use the sample() function to obtain a random sample of the rows of the original data. We then use those row numbers (and remaining row numbers) to split the data accordingly. Notice we used the set.seed() function to reproduce the same random split each time we perform this analysis.\n\nset.seed(1)\nnum_obs = nrow(Advertising)\n\ntrain_index = sample(num_obs, size = trunc(0.50 * num_obs))\ntrain_data = Advertising[train_index, ]\ntest_data = Advertising[-train_index, ]"
  },
  {
    "objectID": "teaching/stat-learn/material/03-linear-regression-II.html#the-model",
    "href": "teaching/stat-learn/material/03-linear-regression-II.html#the-model",
    "title": "Linear Regression II",
    "section": "",
    "text": "We start by fitting the simplest linear model, a model with no predictors:\n\nfit_0 = lm(Sales ~ 1, data = train_data)\nget_complexity(fit_0)\n\n[1] 0\n\n\nAs seen, the complexity of this model is 0. We compute the Test and Train RMSE for this model, via the function specified above and a direct formula:\n\n# train RMSE\nrmse(actual = train_data$Sales, predicted = predict(fit_0, train_data))\n\n[1] 5.297333\n\nsqrt(mean((train_data$Sales - predict(fit_0, train_data)) ^ 2))\n\n[1] 5.297333\n\n# test RMSE\nrmse(actual = test_data$Sales, predicted = predict(fit_0, test_data))\n\n[1] 5.112073\n\nsqrt(mean((test_data$Sales - predict(fit_0, test_data)) ^ 2)) \n\n[1] 5.112073\n\n\nLet’s make the computations of RMSE even more easy by using the below function:\n\n# train RMSE\nget_rmse = function(model, data, response) {\n  rmse(actual = subset(data, select = response, drop = TRUE),\n       predicted = predict(model, data))\n}\n\nCan you figure out how to use the above function?\nWe will fit 5 models, compute the train and test MSE for each model to evaluate it and visualize how it relates to the complexity (i.e. model size in terms of parameters) of the model. Furthermore, we will interpret the results in terms of underfitting or overfitting. We will conclude with dertemrineing which model to choose. The five models to fit are listed below:\n\nSales ~ Radio + Newspaper + TV\nSales ~ Radio * Newspaper * TV\nSales ~ Radio * Newspaper * TV + I(TV ^ 2)\nSales ~ Radio * Newspaper * TV + I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2)\nSales ~ Radio * Newspaper * TV + I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2)\n\nNote that the specification first*second indicates the cross of first and second. This is the same as first + second + first:second.\nWe fit all five models and print the output by creating a list of the model fits. We then obtain train RMSE, test RMSE, and model complexity for each. Finally, plot the results. The train RMSE can be seen in blue, while the test RMSE is given in orange.\n\nfit_1 = lm(Sales ~ Radio + Newspaper + TV, data = train_data)\nfit_2 = lm(Sales ~ Radio * Newspaper * TV, data = train_data)\nfit_3 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2), data = train_data)\nfit_4 = lm(Sales ~ Radio * Newspaper * TV + \n             I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2), data = train_data)\nfit_5 = lm(Sales ~ Radio * Newspaper * TV +\n             I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2), data = train_data)\n\nmodel_list = list(fit_1, fit_2, fit_3, fit_4, fit_5)\n#model_list #uncomment if you wish to see this list object\n\ntrain_rmse = sapply(model_list, get_rmse, data = train_data, response = \"Sales\")\ntest_rmse = sapply(model_list, get_rmse, data = test_data, response = \"Sales\")\nmodel_complexity = sapply(model_list, get_complexity)\n\ntrain_rmse = sapply(model_list, get_rmse, data = train_data, response = \"Sales\")\ntest_rmse = sapply(model_list, get_rmse, data = test_data, response = \"Sales\")\nmodel_complexity = sapply(model_list, get_complexity)\n\nplot(model_complexity, train_rmse, type = \"b\", \n     ylim = c(min(c(train_rmse, test_rmse)) - 0.02, \n              max(c(train_rmse, test_rmse)) + 0.02), \n     col = \"blue\", \n     lwd = 2,\n     xlab = \"Complexity (Model Size)\",\n     ylab = \"RMSE\")\nlines(model_complexity, test_rmse, type = \"b\", col = \"red\", lwd = 2)\ngrid()\n\n\n\n\n\n\n\n\nConclusion:\n\nUnderfitting models: In general High Train RMSE, High Test RMSE. Seen in fit_1 and fit_2.\nOverfitting models: In general Low Train RMSE, High Test RMSE. Seen in fit_4 and fit_5.\n\nWe see that the Test RMSE is smallest for fit_3, and it is the model we believe will perform the best on future data not used to train the model."
  },
  {
    "objectID": "teaching/stat-learn/material/03-linear-regression-II.html#exercises",
    "href": "teaching/stat-learn/material/03-linear-regression-II.html#exercises",
    "title": "Linear Regression II",
    "section": "",
    "text": "Write functions for the some of the other loss functions we covered in the lecture (e.g. MAE, MAPE). Apply them to asses the same five models above. Do you see similar or deviant results in terms of overfitting/underfitting?\nReplicate the simulation example from lecture. Try other models based on different order polynomials."
  },
  {
    "objectID": "teaching/stat-learn/material/01-R-you-ready.html",
    "href": "teaching/stat-learn/material/01-R-you-ready.html",
    "title": "R you ready?",
    "section": "",
    "text": "In this lab, we will introduce some simple R commands. The best way to learn a new language is to try out the commands. R can be downloaded from\nhttp://cran.r-project.org/\nWe recommend that you run R within an integrated development environment (IDE) such as RStudio, which can be freely downloaded from\nhttp://rstudio.com\nThe RStudio website also provides a cloud-based version of R, which does not require installing any software.\n\n\nR uses functions to perform operations. To run a function called funcname, we type funcname(input1, input2), where the inputs (or arguments) input1 and input2 tell R how to run the function. A function can have any number of inputs. For example, to create a vector of numbers, we use the function c() (for concatenate). Any numbers inside the parentheses are joined together. The following command instructs R to join together the numbers 1, 3, 2, and 5, and to save them as a vector named x. When we type x, it gives us back the vector.\n\nx &lt;- c(1, 3, 2, 5)\nx\n\n[1] 1 3 2 5\n\n\nNote that the &gt; is not part of the command; rather, it is printed by R to indicate that it is ready for another command to be entered. We can also save things using = rather than &lt;-:\n\nx = c(1, 6, 2)\nx\n\n[1] 1 6 2\n\ny = c(1, 4, 3)\n\nHitting the up arrow multiple times will display the previous commands, which can then be edited. This is useful since one often wishes to repeat a similar command. In addition, typing ?funcname will always cause R to open a new help file window with additional information about the function funcname().\nWe can tell R to add two sets of numbers together. It will then add the first number from x to the first number from y, and so on. However, x and y should be the same length. We can check their length using the length() function.\n\nlength(x)\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\nx + y\n\n[1]  2 10  5\n\n\nThe ls() function allows us to look at a list of all of the objects, such as data and functions, that we have saved so far. The rm() function can be used to delete any that we don’t want.\n\nls()\n\n[1] \"x\" \"y\"\n\nrm(x, y)\nls()\n\ncharacter(0)\n\n\nIt’s also possible to remove all objects at once:\n\nrm(list = ls())\n\nThe matrix() function can be used to create a matrix of numbers. Before we use the matrix() function, we can learn more about it:\n\n?matrix\n\nThe help file reveals that the matrix() function takes a number of inputs, but for now we focus on the first three: the data (the entries in the matrix), the number of rows, and the number of columns. First, we create a simple matrix.\n\nx &lt;- matrix(data = c(1, 2, 3, 4), nrow = 2, ncol = 2)\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nNote that we could just as well omit typing data=, nrow=, and ncol= in the matrix() command above: that is, we could just type\n\nx &lt;- matrix(c(1, 2, 3, 4), 2, 2)\n\nand this would have the same effect. However, it can sometimes be useful to specify the names of the arguments passed in, since otherwise R will assume that the function arguments are passed into the function in the same order that is given in the function’s help file. As this example illustrates, by default R creates matrices by successively filling in columns. Alternatively, the byrow = TRUE option can be used to populate the matrix in order of the rows.\n\nmatrix(c(1, 2, 3, 4), 2, 2, byrow = TRUE)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n\nNotice that in the above command we did not assign the matrix to a value such as x. In this case the matrix is printed to the screen but is not saved for future calculations. The sqrt() function returns the square root of each element of a vector or matrix. The command x^2 raises each element of x to the power 2; any powers are possible, including fractional or negative powers.\n\nsqrt(x)\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\n\nThe rnorm() function generates a vector of random normal variables, with first argument n the sample size. Each time we call this function, we will get a different answer. Here we create two correlated sets of numbers, x and y, and use the cor() function to compute the correlation between them.\n\nx &lt;- rnorm(50)\ny &lt;- x + rnorm(50, mean = 50, sd = .1)\ncor(x, y)\n\n[1] 0.997281\n\n\nBy default, rnorm() creates standard normal random variables with a mean of \\(0\\) and a standard deviation of \\(1\\). However, the mean and standard deviation can be altered using the mean and sd arguments, as illustrated above. Sometimes we want our code to reproduce the exact same set of random numbers; we can use the set.seed() function to do this. The set.seed() function takes an (arbitrary) integer argument.\n\nset.seed(1303)\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\n\nWe use set.seed() throughout the labs whenever we perform calculations involving random quantities. In general this should allow the user to reproduce our results. However, as new versions of R become available, small discrepancies may arise between this book and the output from R.\nThe mean() and var() functions can be used to compute the mean and variance of a vector of numbers. Applying sqrt() to the output of var() will give the standard deviation. Or we can simply use the sd() function.\n\nset.seed(3)\ny &lt;- rnorm(100)\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768\n\n\n\n\n\nThe plot() function is the primary way to plot data in R. For instance, plot(x, y) produces a scatterplot of the numbers in x versus the numbers in y. There are many additional options that can be passed in to the plot() function. For example, passing in the argument xlab will result in a label on the \\(x\\)-axis. To find out more information about the plot() function, type ?plot.\n\nx &lt;- rnorm(100)\ny &lt;- rnorm(100)\nplot(x, y)\n\n\n\n\n\n\n\nplot(x, y, xlab = \"this is the x-axis\",\n    ylab = \"this is the y-axis\",\n    main = \"Plot of X vs Y\")\n\n\n\n\n\n\n\n\nWe will often want to save the output of an R plot. The command that we use to do this will depend on the file type that we would like to create. For instance, to create a pdf, we use the pdf() function, and to create a jpeg, we use the jpeg() function.\n\npdf(\"Figure.pdf\")\nplot(x, y, col = \"gray\")\ndev.off()\n\nquartz_off_screen \n                2 \n\n\nThe function dev.off() indicates to R that we are done creating the plot. Alternatively, we can simply copy the plot window and paste it into an appropriate file type, such as a Word document.\nThe function seq() can be used to create a sequence of numbers. For instance, seq(a, b) makes a vector of integers between a and b. There are many other options: for instance, seq(0, 1, length = 10) makes a sequence of 10 numbers that are equally spaced between 0 and 1. Typing 3:11 is a shorthand for seq(3, 11) for integer arguments.\n\nx &lt;- seq(1, 10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx &lt;- 1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx &lt;- seq(-pi, pi, length = 50)\n\nWe will now create some more sophisticated plots. The contour() function produces a contour plot in order to represent three-dimensional data; it is like a topographical map. It takes three arguments:\n\nA vector of the x values (the first dimension),\nA vector of the y values (the second dimension), and\nA matrix whose elements correspond to the z value (the third dimension) for each pair of (x, y) coordinates.\n\nAs with the plot() function, there are many other inputs that can be used to fine-tune the output of the contour() function. To learn more about these, take a look at the help file by typing ?contour.\n\ny &lt;- x\nf &lt;- outer(x, y, function(x, y) cos(y) / (1 + x^2))\ncontour(x, y, f)\ncontour(x, y, f, nlevels = 45, add = T)\n\n\n\n\n\n\n\nfa &lt;- (f - t(f)) / 2\ncontour(x, y, fa, nlevels = 15)\n\n\n\n\n\n\n\n\nThe image() function works the same way as contour(), except that it produces a color-coded plot whose colors depend on the z value. This is known as a heatmap, and is sometimes used to plot temperature in weather forecasts. Alternatively, persp() can be used to produce a three-dimensional plot. The arguments theta and phi control the angles at which the plot is viewed.\n\nimage(x, y, fa)\n\n\n\n\n\n\n\npersp(x, y, fa)\n\n\n\n\n\n\n\npersp(x, y, fa, theta = 30)\n\n\n\n\n\n\n\npersp(x, y, fa, theta = 30, phi = 20)\n\n\n\n\n\n\n\npersp(x, y, fa, theta = 30, phi = 70)\n\n\n\n\n\n\n\npersp(x, y, fa, theta = 30, phi = 40)\n\n\n\n\n\n\n\n\n\n\n\nWe often wish to examine part of a set of data. Suppose that our data is stored in the matrix A.\n\nA &lt;- matrix(1:16, 4, 4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\n\nThen, typing\n\nA[2, 3]\n\n[1] 10\n\n\nwill select the element corresponding to the second row and the third column. The first number after the open-bracket symbol [ always refers to the row, and the second number always refers to the column. We can also select multiple rows and columns at a time, by providing vectors as the indices.\n\nA[c(1, 3), c(2, 4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3, 2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2, ]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[, 1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\n\nThe last two examples include either no index for the columns or no index for the rows. These indicate that R should include all columns or all rows, respectively. R treats a single row or column of a matrix as a vector.\n\nA[1, ]\n\n[1]  1  5  9 13\n\n\nThe use of a negative sign - in the index tells R to keep all rows or columns except those indicated in the index.\n\nA[-c(1, 3), ]\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1, 3), -c(1, 3, 4)]\n\n[1] 6 8\n\n\nThe dim() function outputs the number of rows followed by the number of columns of a given matrix.\n\ndim(A)\n\n[1] 4 4\n\n\n\n\n\nFor most analyses, the first step involves importing a data set into R. The read.table() function is one of the primary ways to do this. The help file contains details about how to use this function. We can use the function write.table() to export data.\nBefore attempting to load a data set, we must make sure that R knows to search for the data in the proper directory. For example, on a Windows system one could select the directory using the Change dir ... option under the File menu. However, the details of how to do this depend on the operating system (e.g. Windows, Mac, Unix) that is being used, and so we do not give further details here.\nWe begin by loading in the Auto data set. This data is part of the ISLR2 library, discussed in Chapter 3. To illustrate the read.table() function, we load it now from a text file, Auto.data, which you can find on the textbook website. The following command will load the Auto.data file into R and store it as an object called Auto, in a format referred to as a data frame. Once the data has been loaded, the View() function can be used to view it in a spreadsheet-like window. (This function can sometimes be a bit finicky. If you have trouble using it, then try the head() function instead.) The head() function can also be used to view the first few rows of the data.\n\nAuto &lt;- read.table(\"01-data/Auto.data\")\nknitr::kable(head(Auto)) # note: you only write head(Auto) here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\n\n\n18.0\n8\n307.0\n130.0\n3504.\n12.0\n70\n1\nchevrolet chevelle malibu\n\n\n15.0\n8\n350.0\n165.0\n3693.\n11.5\n70\n1\nbuick skylark 320\n\n\n18.0\n8\n318.0\n150.0\n3436.\n11.0\n70\n1\nplymouth satellite\n\n\n16.0\n8\n304.0\n150.0\n3433.\n12.0\n70\n1\namc rebel sst\n\n\n17.0\n8\n302.0\n140.0\n3449.\n10.5\n70\n1\nford torino\n\n\n\n\n\nNote that Auto.data is simply a text file, which you could alternatively open on your computer using a standard text editor (you can even change the format to .txt). It is often a good idea to view a data set using a text editor or other software such as Excel before loading it into R.\nThis particular data set has not been loaded correctly, because R has assumed that the variable names are part of the data and so has included them in the first row. The data set also includes a number of missing observations, indicated by a question mark ?. Missing values are a common occurrence in real data sets. Using the option header = T (or header = TRUE) in the read.table() function tells R that the first line of the file contains the variable names, and using the option na.strings tells R that any time it sees a particular character or set of characters (such as a question mark), it should be treated as a missing element of the data matrix.\n\nAuto &lt;- read.table(\"01-data/Auto.data\", header = T, na.strings = \"?\", stringsAsFactors = T)\n# View(Auto)\n\nThe stringsAsFactors = T argument tells R that any variable containing character strings should be interpreted as a qualitative variable, and that each distinct character string represents a distinct level for that qualitative variable. An easy way to load data from Excel into R is to save it as a csv (comma-separated values) file, and then use the read.csv() function.\n\nAuto &lt;- read.csv(\"01-data/Auto.csv\", na.strings = \"?\", stringsAsFactors = T)\ndim(Auto)\n\n[1] 397   9\n\nknitr::kable(Auto[1:4, ])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\n\n\n\n\n18\n8\n307\n130\n3504\n12.0\n70\n1\nchevrolet chevelle malibu\n\n\n15\n8\n350\n165\n3693\n11.5\n70\n1\nbuick skylark 320\n\n\n18\n8\n318\n150\n3436\n11.0\n70\n1\nplymouth satellite\n\n\n16\n8\n304\n150\n3433\n12.0\n70\n1\namc rebel sst\n\n\n\n\n\nThe dim() function tells us that the data has \\(397\\) observations, or rows, and nine variables, or columns. There are various ways to deal with the missing data. In this case, only five of the rows contain missing observations, and so we choose to use the na.omit() function to simply remove these rows.\n\nAuto &lt;- na.omit(Auto)\ndim(Auto)\n\n[1] 392   9\n\n\nOnce the data are loaded correctly, we can use names() to check the variable names.\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\nWe can use the plot() function to produce scatterplots of the quantitative variables. However, simply typing the variable names will produce an error message, because R does not know to look in the Auto data set for those variables.\nTo refer to a variable, we must type the data set and the variable name joined with a $ symbol. Alternatively, we can use the attach() function in order to tell R to make the variables in this data frame available by name.\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\n\nThe cylinders variable is stored as a numeric vector, so R has treated it as quantitative. However, since there are only a small number of possible values for cylinders, one may prefer to treat it as a qualitative variable. The as.factor() function converts quantitative variables into qualitative variables.\n\ncylinders &lt;- as.factor(cylinders)\n\nIf the variable plotted on the \\(x\\)-axis is qualitative, then boxplots will automatically be produced by the plot() function. As usual, a number of options can be specified in order to customize the plots.\n\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col = \"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col = \"red\", varwidth = T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col = \"red\", varwidth = T,\n    horizontal = T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col = \"red\", varwidth = T,\n    xlab = \"cylinders\", ylab = \"MPG\")\n\n\n\n\n\n\n\n\nThe hist() function can be used to plot a histogram. Note that col = 2 has the same effect as col = \"red\".\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg, col = 2)\n\n\n\n\n\n\n\nhist(mpg, col = 2, breaks = 15)\n\n\n\n\n\n\n\n\nThe pairs() function creates a scatterplot matrix, i.e. a scatterplot for every pair of variables. We can also produce scatterplots for just a subset of the variables.\n\npairs(Auto)\n\n\n\n\n\n\n\npairs(\n    ~ mpg + displacement + horsepower + weight + acceleration,\n    data = Auto\n  )\n\n\n\n\n\n\n\n\nIn conjunction with the plot() function, identify() provides a useful interactive method for identifying the value of a particular variable for points on a plot. We pass in three arguments to identify(): the \\(x\\)-axis variable, the \\(y\\)-axis variable, and the variable whose values we would like to see printed for each point. Then clicking one or more points in the plot and hitting Escape will cause R to print the values of the variable of interest. The numbers printed under the identify() function correspond to the rows for the selected points.\n\nplot(horsepower, mpg)\nidentify(horsepower, mpg, name)\n\n\n\n\n\n\n\n\ninteger(0)\n\n\nThe summary() function produces a numerical summary of each variable in a particular data set.\n\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.00   1st Qu.:4.000   1st Qu.:105.0   1st Qu.: 75.0   1st Qu.:2225  \n Median :22.75   Median :4.000   Median :151.0   Median : 93.5   Median :2804  \n Mean   :23.45   Mean   :5.472   Mean   :194.4   Mean   :104.5   Mean   :2978  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:275.8   3rd Qu.:126.0   3rd Qu.:3615  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                                               \n  acceleration        year           origin                      name    \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   amc matador       :  5  \n 1st Qu.:13.78   1st Qu.:73.00   1st Qu.:1.000   ford pinto        :  5  \n Median :15.50   Median :76.00   Median :1.000   toyota corolla    :  5  \n Mean   :15.54   Mean   :75.98   Mean   :1.577   amc gremlin       :  4  \n 3rd Qu.:17.02   3rd Qu.:79.00   3rd Qu.:2.000   amc hornet        :  4  \n Max.   :24.80   Max.   :82.00   Max.   :3.000   chevrolet chevette:  4  \n                                                 (Other)           :365  \n\n\nFor qualitative variables such as name, R will list the number of observations that fall in each category. We can also produce a summary of just a single variable.\n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.00   22.75   23.45   29.00   46.60 \n\n\nOnce we have finished using R, we type q() in order to shut it down, or quit. When exiting R, we have the option to save the current workspace so that all objects (such as data sets) that we have created in this R session will be available next time. Before exiting R, we may want to save a record of all of the commands that we typed in the most recent session; this can be accomplished using the savehistory() function. Next time we enter R, we can load that history using the loadhistory() function, if we wish."
  },
  {
    "objectID": "teaching/stat-learn/material/01-R-you-ready.html#basic-commands",
    "href": "teaching/stat-learn/material/01-R-you-ready.html#basic-commands",
    "title": "R you ready?",
    "section": "",
    "text": "R uses functions to perform operations. To run a function called funcname, we type funcname(input1, input2), where the inputs (or arguments) input1 and input2 tell R how to run the function. A function can have any number of inputs. For example, to create a vector of numbers, we use the function c() (for concatenate). Any numbers inside the parentheses are joined together. The following command instructs R to join together the numbers 1, 3, 2, and 5, and to save them as a vector named x. When we type x, it gives us back the vector.\n\nx &lt;- c(1, 3, 2, 5)\nx\n\n[1] 1 3 2 5\n\n\nNote that the &gt; is not part of the command; rather, it is printed by R to indicate that it is ready for another command to be entered. We can also save things using = rather than &lt;-:\n\nx = c(1, 6, 2)\nx\n\n[1] 1 6 2\n\ny = c(1, 4, 3)\n\nHitting the up arrow multiple times will display the previous commands, which can then be edited. This is useful since one often wishes to repeat a similar command. In addition, typing ?funcname will always cause R to open a new help file window with additional information about the function funcname().\nWe can tell R to add two sets of numbers together. It will then add the first number from x to the first number from y, and so on. However, x and y should be the same length. We can check their length using the length() function.\n\nlength(x)\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\nx + y\n\n[1]  2 10  5\n\n\nThe ls() function allows us to look at a list of all of the objects, such as data and functions, that we have saved so far. The rm() function can be used to delete any that we don’t want.\n\nls()\n\n[1] \"x\" \"y\"\n\nrm(x, y)\nls()\n\ncharacter(0)\n\n\nIt’s also possible to remove all objects at once:\n\nrm(list = ls())\n\nThe matrix() function can be used to create a matrix of numbers. Before we use the matrix() function, we can learn more about it:\n\n?matrix\n\nThe help file reveals that the matrix() function takes a number of inputs, but for now we focus on the first three: the data (the entries in the matrix), the number of rows, and the number of columns. First, we create a simple matrix.\n\nx &lt;- matrix(data = c(1, 2, 3, 4), nrow = 2, ncol = 2)\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nNote that we could just as well omit typing data=, nrow=, and ncol= in the matrix() command above: that is, we could just type\n\nx &lt;- matrix(c(1, 2, 3, 4), 2, 2)\n\nand this would have the same effect. However, it can sometimes be useful to specify the names of the arguments passed in, since otherwise R will assume that the function arguments are passed into the function in the same order that is given in the function’s help file. As this example illustrates, by default R creates matrices by successively filling in columns. Alternatively, the byrow = TRUE option can be used to populate the matrix in order of the rows.\n\nmatrix(c(1, 2, 3, 4), 2, 2, byrow = TRUE)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n\nNotice that in the above command we did not assign the matrix to a value such as x. In this case the matrix is printed to the screen but is not saved for future calculations. The sqrt() function returns the square root of each element of a vector or matrix. The command x^2 raises each element of x to the power 2; any powers are possible, including fractional or negative powers.\n\nsqrt(x)\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\n\nThe rnorm() function generates a vector of random normal variables, with first argument n the sample size. Each time we call this function, we will get a different answer. Here we create two correlated sets of numbers, x and y, and use the cor() function to compute the correlation between them.\n\nx &lt;- rnorm(50)\ny &lt;- x + rnorm(50, mean = 50, sd = .1)\ncor(x, y)\n\n[1] 0.997281\n\n\nBy default, rnorm() creates standard normal random variables with a mean of \\(0\\) and a standard deviation of \\(1\\). However, the mean and standard deviation can be altered using the mean and sd arguments, as illustrated above. Sometimes we want our code to reproduce the exact same set of random numbers; we can use the set.seed() function to do this. The set.seed() function takes an (arbitrary) integer argument.\n\nset.seed(1303)\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\n\nWe use set.seed() throughout the labs whenever we perform calculations involving random quantities. In general this should allow the user to reproduce our results. However, as new versions of R become available, small discrepancies may arise between this book and the output from R.\nThe mean() and var() functions can be used to compute the mean and variance of a vector of numbers. Applying sqrt() to the output of var() will give the standard deviation. Or we can simply use the sd() function.\n\nset.seed(3)\ny &lt;- rnorm(100)\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768"
  },
  {
    "objectID": "teaching/stat-learn/material/01-R-you-ready.html#graphics",
    "href": "teaching/stat-learn/material/01-R-you-ready.html#graphics",
    "title": "R you ready?",
    "section": "",
    "text": "The plot() function is the primary way to plot data in R. For instance, plot(x, y) produces a scatterplot of the numbers in x versus the numbers in y. There are many additional options that can be passed in to the plot() function. For example, passing in the argument xlab will result in a label on the \\(x\\)-axis. To find out more information about the plot() function, type ?plot.\n\nx &lt;- rnorm(100)\ny &lt;- rnorm(100)\nplot(x, y)\n\n\n\n\n\n\n\nplot(x, y, xlab = \"this is the x-axis\",\n    ylab = \"this is the y-axis\",\n    main = \"Plot of X vs Y\")\n\n\n\n\n\n\n\n\nWe will often want to save the output of an R plot. The command that we use to do this will depend on the file type that we would like to create. For instance, to create a pdf, we use the pdf() function, and to create a jpeg, we use the jpeg() function.\n\npdf(\"Figure.pdf\")\nplot(x, y, col = \"gray\")\ndev.off()\n\nquartz_off_screen \n                2 \n\n\nThe function dev.off() indicates to R that we are done creating the plot. Alternatively, we can simply copy the plot window and paste it into an appropriate file type, such as a Word document.\nThe function seq() can be used to create a sequence of numbers. For instance, seq(a, b) makes a vector of integers between a and b. There are many other options: for instance, seq(0, 1, length = 10) makes a sequence of 10 numbers that are equally spaced between 0 and 1. Typing 3:11 is a shorthand for seq(3, 11) for integer arguments.\n\nx &lt;- seq(1, 10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx &lt;- 1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx &lt;- seq(-pi, pi, length = 50)\n\nWe will now create some more sophisticated plots. The contour() function produces a contour plot in order to represent three-dimensional data; it is like a topographical map. It takes three arguments:\n\nA vector of the x values (the first dimension),\nA vector of the y values (the second dimension), and\nA matrix whose elements correspond to the z value (the third dimension) for each pair of (x, y) coordinates.\n\nAs with the plot() function, there are many other inputs that can be used to fine-tune the output of the contour() function. To learn more about these, take a look at the help file by typing ?contour.\n\ny &lt;- x\nf &lt;- outer(x, y, function(x, y) cos(y) / (1 + x^2))\ncontour(x, y, f)\ncontour(x, y, f, nlevels = 45, add = T)\n\n\n\n\n\n\n\nfa &lt;- (f - t(f)) / 2\ncontour(x, y, fa, nlevels = 15)\n\n\n\n\n\n\n\n\nThe image() function works the same way as contour(), except that it produces a color-coded plot whose colors depend on the z value. This is known as a heatmap, and is sometimes used to plot temperature in weather forecasts. Alternatively, persp() can be used to produce a three-dimensional plot. The arguments theta and phi control the angles at which the plot is viewed.\n\nimage(x, y, fa)\n\n\n\n\n\n\n\npersp(x, y, fa)\n\n\n\n\n\n\n\npersp(x, y, fa, theta = 30)\n\n\n\n\n\n\n\npersp(x, y, fa, theta = 30, phi = 20)\n\n\n\n\n\n\n\npersp(x, y, fa, theta = 30, phi = 70)\n\n\n\n\n\n\n\npersp(x, y, fa, theta = 30, phi = 40)"
  },
  {
    "objectID": "teaching/stat-learn/material/01-R-you-ready.html#indexing-data",
    "href": "teaching/stat-learn/material/01-R-you-ready.html#indexing-data",
    "title": "R you ready?",
    "section": "",
    "text": "We often wish to examine part of a set of data. Suppose that our data is stored in the matrix A.\n\nA &lt;- matrix(1:16, 4, 4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\n\nThen, typing\n\nA[2, 3]\n\n[1] 10\n\n\nwill select the element corresponding to the second row and the third column. The first number after the open-bracket symbol [ always refers to the row, and the second number always refers to the column. We can also select multiple rows and columns at a time, by providing vectors as the indices.\n\nA[c(1, 3), c(2, 4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3, 2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2, ]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[, 1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\n\nThe last two examples include either no index for the columns or no index for the rows. These indicate that R should include all columns or all rows, respectively. R treats a single row or column of a matrix as a vector.\n\nA[1, ]\n\n[1]  1  5  9 13\n\n\nThe use of a negative sign - in the index tells R to keep all rows or columns except those indicated in the index.\n\nA[-c(1, 3), ]\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1, 3), -c(1, 3, 4)]\n\n[1] 6 8\n\n\nThe dim() function outputs the number of rows followed by the number of columns of a given matrix.\n\ndim(A)\n\n[1] 4 4"
  },
  {
    "objectID": "teaching/stat-learn/material/01-R-you-ready.html#loading-data",
    "href": "teaching/stat-learn/material/01-R-you-ready.html#loading-data",
    "title": "R you ready?",
    "section": "",
    "text": "For most analyses, the first step involves importing a data set into R. The read.table() function is one of the primary ways to do this. The help file contains details about how to use this function. We can use the function write.table() to export data.\nBefore attempting to load a data set, we must make sure that R knows to search for the data in the proper directory. For example, on a Windows system one could select the directory using the Change dir ... option under the File menu. However, the details of how to do this depend on the operating system (e.g. Windows, Mac, Unix) that is being used, and so we do not give further details here.\nWe begin by loading in the Auto data set. This data is part of the ISLR2 library, discussed in Chapter 3. To illustrate the read.table() function, we load it now from a text file, Auto.data, which you can find on the textbook website. The following command will load the Auto.data file into R and store it as an object called Auto, in a format referred to as a data frame. Once the data has been loaded, the View() function can be used to view it in a spreadsheet-like window. (This function can sometimes be a bit finicky. If you have trouble using it, then try the head() function instead.) The head() function can also be used to view the first few rows of the data.\n\nAuto &lt;- read.table(\"01-data/Auto.data\")\nknitr::kable(head(Auto)) # note: you only write head(Auto) here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\n\n\n18.0\n8\n307.0\n130.0\n3504.\n12.0\n70\n1\nchevrolet chevelle malibu\n\n\n15.0\n8\n350.0\n165.0\n3693.\n11.5\n70\n1\nbuick skylark 320\n\n\n18.0\n8\n318.0\n150.0\n3436.\n11.0\n70\n1\nplymouth satellite\n\n\n16.0\n8\n304.0\n150.0\n3433.\n12.0\n70\n1\namc rebel sst\n\n\n17.0\n8\n302.0\n140.0\n3449.\n10.5\n70\n1\nford torino\n\n\n\n\n\nNote that Auto.data is simply a text file, which you could alternatively open on your computer using a standard text editor (you can even change the format to .txt). It is often a good idea to view a data set using a text editor or other software such as Excel before loading it into R.\nThis particular data set has not been loaded correctly, because R has assumed that the variable names are part of the data and so has included them in the first row. The data set also includes a number of missing observations, indicated by a question mark ?. Missing values are a common occurrence in real data sets. Using the option header = T (or header = TRUE) in the read.table() function tells R that the first line of the file contains the variable names, and using the option na.strings tells R that any time it sees a particular character or set of characters (such as a question mark), it should be treated as a missing element of the data matrix.\n\nAuto &lt;- read.table(\"01-data/Auto.data\", header = T, na.strings = \"?\", stringsAsFactors = T)\n# View(Auto)\n\nThe stringsAsFactors = T argument tells R that any variable containing character strings should be interpreted as a qualitative variable, and that each distinct character string represents a distinct level for that qualitative variable. An easy way to load data from Excel into R is to save it as a csv (comma-separated values) file, and then use the read.csv() function.\n\nAuto &lt;- read.csv(\"01-data/Auto.csv\", na.strings = \"?\", stringsAsFactors = T)\ndim(Auto)\n\n[1] 397   9\n\nknitr::kable(Auto[1:4, ])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\n\n\n\n\n18\n8\n307\n130\n3504\n12.0\n70\n1\nchevrolet chevelle malibu\n\n\n15\n8\n350\n165\n3693\n11.5\n70\n1\nbuick skylark 320\n\n\n18\n8\n318\n150\n3436\n11.0\n70\n1\nplymouth satellite\n\n\n16\n8\n304\n150\n3433\n12.0\n70\n1\namc rebel sst\n\n\n\n\n\nThe dim() function tells us that the data has \\(397\\) observations, or rows, and nine variables, or columns. There are various ways to deal with the missing data. In this case, only five of the rows contain missing observations, and so we choose to use the na.omit() function to simply remove these rows.\n\nAuto &lt;- na.omit(Auto)\ndim(Auto)\n\n[1] 392   9\n\n\nOnce the data are loaded correctly, we can use names() to check the variable names.\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\""
  },
  {
    "objectID": "teaching/stat-learn/material/01-R-you-ready.html#additional-graphical-and-numerical-summaries",
    "href": "teaching/stat-learn/material/01-R-you-ready.html#additional-graphical-and-numerical-summaries",
    "title": "R you ready?",
    "section": "",
    "text": "We can use the plot() function to produce scatterplots of the quantitative variables. However, simply typing the variable names will produce an error message, because R does not know to look in the Auto data set for those variables.\nTo refer to a variable, we must type the data set and the variable name joined with a $ symbol. Alternatively, we can use the attach() function in order to tell R to make the variables in this data frame available by name.\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\n\nThe cylinders variable is stored as a numeric vector, so R has treated it as quantitative. However, since there are only a small number of possible values for cylinders, one may prefer to treat it as a qualitative variable. The as.factor() function converts quantitative variables into qualitative variables.\n\ncylinders &lt;- as.factor(cylinders)\n\nIf the variable plotted on the \\(x\\)-axis is qualitative, then boxplots will automatically be produced by the plot() function. As usual, a number of options can be specified in order to customize the plots.\n\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col = \"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col = \"red\", varwidth = T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col = \"red\", varwidth = T,\n    horizontal = T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col = \"red\", varwidth = T,\n    xlab = \"cylinders\", ylab = \"MPG\")\n\n\n\n\n\n\n\n\nThe hist() function can be used to plot a histogram. Note that col = 2 has the same effect as col = \"red\".\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg, col = 2)\n\n\n\n\n\n\n\nhist(mpg, col = 2, breaks = 15)\n\n\n\n\n\n\n\n\nThe pairs() function creates a scatterplot matrix, i.e. a scatterplot for every pair of variables. We can also produce scatterplots for just a subset of the variables.\n\npairs(Auto)\n\n\n\n\n\n\n\npairs(\n    ~ mpg + displacement + horsepower + weight + acceleration,\n    data = Auto\n  )\n\n\n\n\n\n\n\n\nIn conjunction with the plot() function, identify() provides a useful interactive method for identifying the value of a particular variable for points on a plot. We pass in three arguments to identify(): the \\(x\\)-axis variable, the \\(y\\)-axis variable, and the variable whose values we would like to see printed for each point. Then clicking one or more points in the plot and hitting Escape will cause R to print the values of the variable of interest. The numbers printed under the identify() function correspond to the rows for the selected points.\n\nplot(horsepower, mpg)\nidentify(horsepower, mpg, name)\n\n\n\n\n\n\n\n\ninteger(0)\n\n\nThe summary() function produces a numerical summary of each variable in a particular data set.\n\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.00   1st Qu.:4.000   1st Qu.:105.0   1st Qu.: 75.0   1st Qu.:2225  \n Median :22.75   Median :4.000   Median :151.0   Median : 93.5   Median :2804  \n Mean   :23.45   Mean   :5.472   Mean   :194.4   Mean   :104.5   Mean   :2978  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:275.8   3rd Qu.:126.0   3rd Qu.:3615  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                                               \n  acceleration        year           origin                      name    \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   amc matador       :  5  \n 1st Qu.:13.78   1st Qu.:73.00   1st Qu.:1.000   ford pinto        :  5  \n Median :15.50   Median :76.00   Median :1.000   toyota corolla    :  5  \n Mean   :15.54   Mean   :75.98   Mean   :1.577   amc gremlin       :  4  \n 3rd Qu.:17.02   3rd Qu.:79.00   3rd Qu.:2.000   amc hornet        :  4  \n Max.   :24.80   Max.   :82.00   Max.   :3.000   chevrolet chevette:  4  \n                                                 (Other)           :365  \n\n\nFor qualitative variables such as name, R will list the number of observations that fall in each category. We can also produce a summary of just a single variable.\n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.00   22.75   23.45   29.00   46.60 \n\n\nOnce we have finished using R, we type q() in order to shut it down, or quit. When exiting R, we have the option to save the current workspace so that all objects (such as data sets) that we have created in this R session will be available next time. Before exiting R, we may want to save a record of all of the commands that we typed in the most recent session; this can be accomplished using the savehistory() function. Next time we enter R, we can load that history using the loadhistory() function, if we wish."
  },
  {
    "objectID": "teaching/stat-learn/material/04-classification-I.html",
    "href": "teaching/stat-learn/material/04-classification-I.html",
    "title": "Classification Methods",
    "section": "",
    "text": "We will begin by examining some numerical and graphical summaries of the Smarket data, which is part of the ISLR2 library. This data set consists of percentage returns for the S&P 500 stock index over \\(1,250\\) days, from the beginning of 2001 until the end of 2005. For each date, we have recorded the percentage returns for each of the five previous trading days, lagone through lagfive. We have also recorded volume (the number of shares traded on the previous day, in billions), Today (the percentage return on the date in question) and direction (whether the market was Up or Down on this date). Our goal is to predict direction (a qualitative response) using the other features.\n\nlibrary(ISLR2)\nnames(Smarket)\n\n[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n[7] \"Volume\"    \"Today\"     \"Direction\"\n\ndim(Smarket)\n\n[1] 1250    9\n\nsummary(Smarket)\n\n      Year           Lag1                Lag2                Lag3          \n Min.   :2001   Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.922000  \n 1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500   1st Qu.:-0.640000  \n Median :2003   Median : 0.039000   Median : 0.039000   Median : 0.038500  \n Mean   :2003   Mean   : 0.003834   Mean   : 0.003919   Mean   : 0.001716  \n 3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.596750  \n Max.   :2005   Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.733000  \n      Lag4                Lag5              Volume           Today          \n Min.   :-4.922000   Min.   :-4.92200   Min.   :0.3561   Min.   :-4.922000  \n 1st Qu.:-0.640000   1st Qu.:-0.64000   1st Qu.:1.2574   1st Qu.:-0.639500  \n Median : 0.038500   Median : 0.03850   Median :1.4229   Median : 0.038500  \n Mean   : 0.001636   Mean   : 0.00561   Mean   :1.4783   Mean   : 0.003138  \n 3rd Qu.: 0.596750   3rd Qu.: 0.59700   3rd Qu.:1.6417   3rd Qu.: 0.596750  \n Max.   : 5.733000   Max.   : 5.73300   Max.   :3.1525   Max.   : 5.733000  \n Direction \n Down:602  \n Up  :648  \n           \n           \n           \n           \n\npairs(Smarket)\n\n\n\n\n\n\n\n\nThe cor() function produces a matrix that contains all of the pairwise correlations among the predictors in a data set. The first command below gives an error message because the direction variable is qualitative.\n\ncor(Smarket)\n\nError in cor(Smarket): 'x' must be numeric\n\ncor(Smarket[, -9])\n\n             Year         Lag1         Lag2         Lag3         Lag4\nYear   1.00000000  0.029699649  0.030596422  0.033194581  0.035688718\nLag1   0.02969965  1.000000000 -0.026294328 -0.010803402 -0.002985911\nLag2   0.03059642 -0.026294328  1.000000000 -0.025896670 -0.010853533\nLag3   0.03319458 -0.010803402 -0.025896670  1.000000000 -0.024051036\nLag4   0.03568872 -0.002985911 -0.010853533 -0.024051036  1.000000000\nLag5   0.02978799 -0.005674606 -0.003557949 -0.018808338 -0.027083641\nVolume 0.53900647  0.040909908 -0.043383215 -0.041823686 -0.048414246\nToday  0.03009523 -0.026155045 -0.010250033 -0.002447647 -0.006899527\n               Lag5      Volume        Today\nYear    0.029787995  0.53900647  0.030095229\nLag1   -0.005674606  0.04090991 -0.026155045\nLag2   -0.003557949 -0.04338321 -0.010250033\nLag3   -0.018808338 -0.04182369 -0.002447647\nLag4   -0.027083641 -0.04841425 -0.006899527\nLag5    1.000000000 -0.02200231 -0.034860083\nVolume -0.022002315  1.00000000  0.014591823\nToday  -0.034860083  0.01459182  1.000000000\n\n\nAs one would expect, the correlations between the lag variables and today’s returns are close to zero. In other words, there appears to be little correlation between today’s returns and previous days’ returns. The only substantial correlation is between Year and volume. By plotting the data, which is ordered chronologically, we see that volume is increasing over time. In other words, the average number of shares traded daily increased from 2001 to 2005.\n\nattach(Smarket)\nplot(Volume)\n\n\n\n\n\n\n\n\n\n\n\nNext, we will fit a logistic regression model in order to predict direction using lagone through lagfive and volume. The glm() function can be used to fit many types of generalized linear models, including logistic regression. The syntax of the glm() function is similar to that of lm(), except that we must pass in the argument family = binomial in order to tell R to run a logistic regression rather than some other type of generalized linear model.\n\nglm.fits &lt;- glm(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket, family = binomial\n  )\nsummary(glm.fits)\n\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n    Volume, family = binomial, data = Smarket)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -0.126000   0.240736  -0.523    0.601\nLag1        -0.073074   0.050167  -1.457    0.145\nLag2        -0.042301   0.050086  -0.845    0.398\nLag3         0.011085   0.049939   0.222    0.824\nLag4         0.009359   0.049974   0.187    0.851\nLag5         0.010313   0.049511   0.208    0.835\nVolume       0.135441   0.158360   0.855    0.392\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1731.2  on 1249  degrees of freedom\nResidual deviance: 1727.6  on 1243  degrees of freedom\nAIC: 1741.6\n\nNumber of Fisher Scoring iterations: 3\n\n\nThe smallest \\(p\\)-value here is associated with lagone. The negative coefficient for this predictor suggests that if the market had a positive return yesterday, then it is less likely to go up today. However, at a value of \\(0.15\\), the \\(p\\)-value is still relatively large, and so there is no clear evidence of a real association between lagone and direction.\nThe predict() function can be used to predict the probability that the market will go up, given values of the predictors. The type = \"response\" option tells R to output probabilities of the form \\(P(Y=1|X)\\), as opposed to other information such as the logit. If no data set is supplied to the predict() function, then the probabilities are computed for the training data that was used to fit the logistic regression model. Here we have printed only the first ten probabilities. We know that these values correspond to the probability of the market going up, rather than down, because the contrasts() function indicates that R has created a dummy variable with a 1 for Up.\n\nglm.probs &lt;- predict(glm.fits, type = \"response\")\nglm.probs[1:10]\n\n        1         2         3         4         5         6         7         8 \n0.5070841 0.4814679 0.4811388 0.5152224 0.5107812 0.5069565 0.4926509 0.5092292 \n        9        10 \n0.5176135 0.4888378 \n\ncontrasts(Direction)\n\n     Up\nDown  0\nUp    1\n\n\nIn order to make a prediction as to whether the market will go up or down on a particular day, we must convert these predicted probabilities into class labels, Up or Down. The following two commands create a vector of class predictions based on whether the predicted probability of a market increase is greater than or less than \\(0.5\\).\n\nglm.pred &lt;- rep(\"Down\", 1250)\nglm.pred[glm.probs &gt; .5] = \"Up\"\n\nThe first command creates a vector of 1,250 Down elements. The second line transforms to Up all of the elements for which the predicted probability of a market increase exceeds \\(0.5\\). Given these predictions, the table() function can be used to produce a confusion matrix in order to determine how many observations were correctly or incorrectly classified.\n\ntable(glm.pred, Direction)\n\n        Direction\nglm.pred Down  Up\n    Down  145 141\n    Up    457 507\n\n(507 + 145) / 1250\n\n[1] 0.5216\n\nmean(glm.pred == Direction)\n\n[1] 0.5216\n\n\nThe diagonal elements of the confusion matrix indicate correct predictions, while the off-diagonals represent incorrect predictions. Hence our model correctly predicted that the market would go up on \\(507\\) days and that it would go down on \\(145\\) days, for a total of \\(507+145 = 652\\) correct predictions. The mean() function can be used to compute the fraction of days for which the prediction was correct. In this case, logistic regression correctly predicted the movement of the market \\(52.2\\) % of the time.\nAt first glance, it appears that the logistic regression model is working a little better than random guessing. However, this result is misleading because we trained and tested the model on the same set of \\(1,250\\) observations. In other words, \\(100\\%-52.2\\%=47.8\\%\\), is the training error rate. As we have seen previously, the training error rate is often overly optimistic—it tends to underestimate the test error rate. In order to better assess the accuracy of the logistic regression model in this setting, we can fit the model using part of the data, and then examine how well it predicts the held out data.\nTo implement this strategy, we will first create a vector corresponding to the observations from 2001 through 2004. We will then use this vector to create a held out data set of observations from 2005.\n\ntrain &lt;- (Year &lt; 2005)\nSmarket.2005 &lt;- Smarket[!train, ]\ndim(Smarket.2005)\n\n[1] 252   9\n\nDirection.2005 &lt;- Direction[!train]\n\nThe object train is a vector of \\(1{,}250\\) elements, corresponding to the observations in our data set. The elements of the vector that correspond to observations that occurred before 2005 are set to TRUE, whereas those that correspond to observations in 2005 are set to FALSE. The object train is a Boolean vector, since its elements are TRUE and FALSE. Boolean vectors can be used to obtain a subset of the rows or columns of a matrix. For instance, the command Smarket[train, ] would pick out a submatrix of the stock market data set, corresponding only to the dates before 2005, since those are the ones for which the elements of train are TRUE. The ! symbol can be used to reverse all of the elements of a Boolean vector. That is, !train is a vector similar to train, except that the elements that are TRUE in train get swapped to FALSE in !train, and the elements that are FALSE in train get swapped to TRUE in !train. Therefore, Smarket[!train, ] yields a submatrix of the stock market data containing only the observations for which train is FALSE—that is, the observations with dates in 2005. The output above indicates that there are 252 such observations.\nWe now fit a logistic regression model using only the subset of the observations that correspond to dates before 2005, using the subset argument. We then obtain predicted probabilities of the stock market going up for each of the days in our test set—that is, for the days in 2005.\n\nglm.fits &lt;- glm(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket, family = binomial, subset = train\n  )\nglm.probs &lt;- predict(glm.fits, Smarket.2005,\n    type = \"response\")\n\nNotice that we have trained and tested our model on two completely separate data sets: training was performed using only the dates before 2005, and testing was performed using only the dates in 2005. Finally, we compute the predictions for 2005 and compare them to the actual movements of the market over that time period.\n\nglm.pred &lt;- rep(\"Down\", 252)\nglm.pred[glm.probs &gt; .5] &lt;- \"Up\"\ntable(glm.pred, Direction.2005)\n\n        Direction.2005\nglm.pred Down Up\n    Down   77 97\n    Up     34 44\n\nmean(glm.pred == Direction.2005)\n\n[1] 0.4801587\n\nmean(glm.pred != Direction.2005)\n\n[1] 0.5198413\n\n\nThe != notation means not equal to, and so the last command computes the test set error rate. The results are rather disappointing: the test error rate is \\(52\\) %, which is worse than random guessing! Of course this result is not all that surprising, given that one would not generally expect to be able to use previous days’ returns to predict future market performance. (After all, if it were possible to do so, then the authors of this book would be out striking it rich rather than writing a statistics textbook.)\nWe recall that the logistic regression model had very underwhelming \\(p\\)-values associated with all of the predictors, and that the smallest \\(p\\)-value, though not very small, corresponded to lagone. Perhaps by removing the variables that appear not to be helpful in predicting direction, we can obtain a more effective model. After all, using predictors that have no relationship with the response tends to cause a deterioration in the test error rate (since such predictors cause an increase in variance without a corresponding decrease in bias), and so removing such predictors may in turn yield an improvement. Below we have refit the logistic regression using just lagone and lagtwo, which seemed to have the highest predictive power in the original logistic regression model.\n\nglm.fits &lt;- glm(Direction ~ Lag1 + Lag2, data = Smarket,\n    family = binomial, subset = train)\nglm.probs &lt;- predict(glm.fits, Smarket.2005,\n    type = \"response\")\nglm.pred &lt;- rep(\"Down\", 252)\nglm.pred[glm.probs &gt; .5] &lt;- \"Up\"\ntable(glm.pred, Direction.2005)\n\n        Direction.2005\nglm.pred Down  Up\n    Down   35  35\n    Up     76 106\n\nmean(glm.pred == Direction.2005)\n\n[1] 0.5595238\n\n106 / (106 + 76)\n\n[1] 0.5824176\n\n\nNow the results appear to be a little better: \\(56\\%\\) of the daily movements have been correctly predicted. It is worth noting that in this case, a much simpler strategy of predicting that the market will increase every day will also be correct \\(56\\%\\) of the time! Hence, in terms of overall error rate, the logistic regression method is no better than the naive approach. However, the confusion matrix shows that on days when logistic regression predicts an increase in the market, it has a \\(58\\%\\) accuracy rate. This suggests a possible trading strategy of buying on days when the model predicts an increasing market, and avoiding trades on days when a decrease is predicted. Of course one would need to investigate more carefully whether this small improvement was real or just due to random chance.\nSuppose that we want to predict the returns associated with particular values of lagone and lagtwo. In particular, we want to predict direction on a day when lagone and lagtwo equal 1.2 and~1.1, respectively, and on a day when they equal 1.5 and $-$0.8. We do this using the predict() function.\n\npredict(glm.fits,\n    newdata =\n      data.frame(Lag1 = c(1.2, 1.5),  Lag2 = c(1.1, -0.8)),\n    type = \"response\"\n  )\n\n        1         2 \n0.4791462 0.4960939 \n\n\n\n\n\nCreate a new DirectionNum column to store your variable as numeric (0/1)\nFit a linear model (Y ~ lag1 + lag2) using that new column and compare the estimates with the logistic model.\n\n\n\n\n\nROC curves are a great tool to assess our classification performance. The pROC package gives us an easy way to create one. Please make sure you have it installed!\nA ROC curve plots the sensitivity and specificity of our classifier.\nRemember:\nsensitivity = TP / TP + FN\nspecificity = TN / TN + FP\n\n#install.packages('pROC')\nlibrary(pROC)\n\nType 'citation(\"pROC\")' for a citation.\n\n\n\nAttaching package: 'pROC'\n\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\n\nThis is a more explicit way to subset, again dividing via the train object.\n\nSmarket_train = subset(Smarket, train ) # pre 2005\nSmarket_test = subset(Smarket, !train ) # post 2005\n\nFor the pROC function roc() we need our data stored in numeric form, so let’s fit our model and assign new columns to our df.\n\nglm.fits = glm(Direction ~ Lag1 + Lag2,\n               data = Smarket_train,\n               family = binomial(link = \"logit\"))\n\nglm.probs = predict(glm.fits, Smarket_test, type = \"response\")\nglm.pred &lt;- rep(0, 252) \nglm.pred[glm.probs &gt; .5] &lt;- 1 # 0/1 instead of up/down\n\nSmarket_test$Preds = glm.pred\n\n# $Direction uses named factors, stored as 1/2, so we take -1 \n\nSmarket_test$DirectionNum = as.numeric(Smarket_test$Direction) - 1 \n\nWe create a roc_object by using the true values and predictions of our test set\n\nroc_obj = roc(Smarket_test$DirectionNum, Smarket_test$Preds)\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\nplot(roc_obj)\n\n\n\n\n\n\n\n\nThis looks terrible! Both visually and performance wise. As we can see our curve is extremely close to the diagonal which indicates a completely random guess (50:50). This is in line with our previous assessment which gave us \\(56\\%\\).\nWe can spice up the curve a bit by adding CIs, or add some colour, but pROC is a very base-R friendly package so feel free to shop around for some more “modern” implementations.\n\nroc_obj = roc(Smarket_test$DirectionNum, Smarket_test$Preds)\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\nplot(roc_obj, print.auc =TRUE)\nplot(ci.se(roc_obj), type=\"shape\", col=\"lightblue\")\n\nWarning in plot.ci.se(ci.se(roc_obj), type = \"shape\", col = \"lightblue\"): Low\ndefinition shape.\n\n\n\n\n\n\n\n\nplot(sens.ci, type=\"bars\")\n\nError: object 'sens.ci' not found\n\n\n\n\n\nFinally, we fit a Poisson regression model to the Bikeshare data set, which measures the number of bike rentals (bikers) per hour in Washington, DC. The data can be found in the ISLR2 library.\n\nattach(Bikeshare)\ndim(Bikeshare)\n\n[1] 8645   15\n\nnames(Bikeshare)\n\n [1] \"season\"     \"mnth\"       \"day\"        \"hr\"         \"holiday\"   \n [6] \"weekday\"    \"workingday\" \"weathersit\" \"temp\"       \"atemp\"     \n[11] \"hum\"        \"windspeed\"  \"casual\"     \"registered\" \"bikers\"    \n\n\nWe begin by fitting a least squares linear regression model to the data.\n\nmod.lm &lt;- lm(\n    bikers ~ mnth + hr + workingday + temp + weathersit,\n    data = Bikeshare\n  )\nsummary(mod.lm)\n\n\nCall:\nlm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, \n    data = Bikeshare)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-299.00  -45.70   -6.23   41.08  425.29 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                -68.632      5.307 -12.932  &lt; 2e-16 ***\nmnthFeb                      6.845      4.287   1.597 0.110398    \nmnthMarch                   16.551      4.301   3.848 0.000120 ***\nmnthApril                   41.425      4.972   8.331  &lt; 2e-16 ***\nmnthMay                     72.557      5.641  12.862  &lt; 2e-16 ***\nmnthJune                    67.819      6.544  10.364  &lt; 2e-16 ***\nmnthJuly                    45.324      7.081   6.401 1.63e-10 ***\nmnthAug                     53.243      6.640   8.019 1.21e-15 ***\nmnthSept                    66.678      5.925  11.254  &lt; 2e-16 ***\nmnthOct                     75.834      4.950  15.319  &lt; 2e-16 ***\nmnthNov                     60.310      4.610  13.083  &lt; 2e-16 ***\nmnthDec                     46.458      4.271  10.878  &lt; 2e-16 ***\nhr1                        -14.579      5.699  -2.558 0.010536 *  \nhr2                        -21.579      5.733  -3.764 0.000168 ***\nhr3                        -31.141      5.778  -5.389 7.26e-08 ***\nhr4                        -36.908      5.802  -6.361 2.11e-10 ***\nhr5                        -24.135      5.737  -4.207 2.61e-05 ***\nhr6                         20.600      5.704   3.612 0.000306 ***\nhr7                        120.093      5.693  21.095  &lt; 2e-16 ***\nhr8                        223.662      5.690  39.310  &lt; 2e-16 ***\nhr9                        120.582      5.693  21.182  &lt; 2e-16 ***\nhr10                        83.801      5.705  14.689  &lt; 2e-16 ***\nhr11                       105.423      5.722  18.424  &lt; 2e-16 ***\nhr12                       137.284      5.740  23.916  &lt; 2e-16 ***\nhr13                       136.036      5.760  23.617  &lt; 2e-16 ***\nhr14                       126.636      5.776  21.923  &lt; 2e-16 ***\nhr15                       132.087      5.780  22.852  &lt; 2e-16 ***\nhr16                       178.521      5.772  30.927  &lt; 2e-16 ***\nhr17                       296.267      5.749  51.537  &lt; 2e-16 ***\nhr18                       269.441      5.736  46.976  &lt; 2e-16 ***\nhr19                       186.256      5.714  32.596  &lt; 2e-16 ***\nhr20                       125.549      5.704  22.012  &lt; 2e-16 ***\nhr21                        87.554      5.693  15.378  &lt; 2e-16 ***\nhr22                        59.123      5.689  10.392  &lt; 2e-16 ***\nhr23                        26.838      5.688   4.719 2.41e-06 ***\nworkingday                   1.270      1.784   0.711 0.476810    \ntemp                       157.209     10.261  15.321  &lt; 2e-16 ***\nweathersitcloudy/misty     -12.890      1.964  -6.562 5.60e-11 ***\nweathersitlight rain/snow  -66.494      2.965 -22.425  &lt; 2e-16 ***\nweathersitheavy rain/snow -109.745     76.667  -1.431 0.152341    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 76.5 on 8605 degrees of freedom\nMultiple R-squared:  0.6745,    Adjusted R-squared:  0.6731 \nF-statistic: 457.3 on 39 and 8605 DF,  p-value: &lt; 2.2e-16\n\n\nDue to space constraints, we truncate the output of summary(mod.lm). In mod.lm, the first level of hr (0) and mnth (Jan) are treated as the baseline values, and so no coefficient estimates are provided for them: implicitly, their coefficient estimates are zero, and all other levels are measured relative to these baselines. For example, the Feb coefficient of \\(6.845\\) signifies that, holding all other variables constant, there are on average about 7 more riders in February than in January. Similarly there are about 16.5 more riders in March than in January.\nThe results seen in Section 4.6.1 used a slightly different coding of the variables hr and mnth, as follows:\n\ncontrasts(Bikeshare$hr) = contr.sum(24)\ncontrasts(Bikeshare$mnth) = contr.sum(12)\nmod.lm2 &lt;- lm(\n    bikers ~ mnth + hr + workingday + temp + weathersit,\n    data = Bikeshare\n  )\nsummary(mod.lm2)\n\n\nCall:\nlm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, \n    data = Bikeshare)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-299.00  -45.70   -6.23   41.08  425.29 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 73.5974     5.1322  14.340  &lt; 2e-16 ***\nmnth1                      -46.0871     4.0855 -11.281  &lt; 2e-16 ***\nmnth2                      -39.2419     3.5391 -11.088  &lt; 2e-16 ***\nmnth3                      -29.5357     3.1552  -9.361  &lt; 2e-16 ***\nmnth4                       -4.6622     2.7406  -1.701  0.08895 .  \nmnth5                       26.4700     2.8508   9.285  &lt; 2e-16 ***\nmnth6                       21.7317     3.4651   6.272 3.75e-10 ***\nmnth7                       -0.7626     3.9084  -0.195  0.84530    \nmnth8                        7.1560     3.5347   2.024  0.04295 *  \nmnth9                       20.5912     3.0456   6.761 1.46e-11 ***\nmnth10                      29.7472     2.6995  11.019  &lt; 2e-16 ***\nmnth11                      14.2229     2.8604   4.972 6.74e-07 ***\nhr1                        -96.1420     3.9554 -24.307  &lt; 2e-16 ***\nhr2                       -110.7213     3.9662 -27.916  &lt; 2e-16 ***\nhr3                       -117.7212     4.0165 -29.310  &lt; 2e-16 ***\nhr4                       -127.2828     4.0808 -31.191  &lt; 2e-16 ***\nhr5                       -133.0495     4.1168 -32.319  &lt; 2e-16 ***\nhr6                       -120.2775     4.0370 -29.794  &lt; 2e-16 ***\nhr7                        -75.5424     3.9916 -18.925  &lt; 2e-16 ***\nhr8                         23.9511     3.9686   6.035 1.65e-09 ***\nhr9                        127.5199     3.9500  32.284  &lt; 2e-16 ***\nhr10                        24.4399     3.9360   6.209 5.57e-10 ***\nhr11                       -12.3407     3.9361  -3.135  0.00172 ** \nhr12                         9.2814     3.9447   2.353  0.01865 *  \nhr13                        41.1417     3.9571  10.397  &lt; 2e-16 ***\nhr14                        39.8939     3.9750  10.036  &lt; 2e-16 ***\nhr15                        30.4940     3.9910   7.641 2.39e-14 ***\nhr16                        35.9445     3.9949   8.998  &lt; 2e-16 ***\nhr17                        82.3786     3.9883  20.655  &lt; 2e-16 ***\nhr18                       200.1249     3.9638  50.488  &lt; 2e-16 ***\nhr19                       173.2989     3.9561  43.806  &lt; 2e-16 ***\nhr20                        90.1138     3.9400  22.872  &lt; 2e-16 ***\nhr21                        29.4071     3.9362   7.471 8.74e-14 ***\nhr22                        -8.5883     3.9332  -2.184  0.02902 *  \nhr23                       -37.0194     3.9344  -9.409  &lt; 2e-16 ***\nworkingday                   1.2696     1.7845   0.711  0.47681    \ntemp                       157.2094    10.2612  15.321  &lt; 2e-16 ***\nweathersitcloudy/misty     -12.8903     1.9643  -6.562 5.60e-11 ***\nweathersitlight rain/snow  -66.4944     2.9652 -22.425  &lt; 2e-16 ***\nweathersitheavy rain/snow -109.7446    76.6674  -1.431  0.15234    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 76.5 on 8605 degrees of freedom\nMultiple R-squared:  0.6745,    Adjusted R-squared:  0.6731 \nF-statistic: 457.3 on 39 and 8605 DF,  p-value: &lt; 2.2e-16\n\n\nWhat is the difference between the two codings? In mod.lm2, a coefficient estimate is reported for all but the last level of hr and mnth. Importantly, in mod.lm2, the coefficient estimate for the last level of mnth is not zero: instead, it equals the negative of the sum of the coefficient estimates for all of the other levels. Similarly, in mod.lm2, the coefficient estimate for the last level of hr is the negative of the sum of the coefficient estimates for all of the other levels. This means that the coefficients of hr and mnth in mod.lm2 will always sum to zero, and can be interpreted as the difference from the mean level. For example, the coefficient for January of \\(-46.087\\) indicates that, holding all other variables constant, there are typically 46 fewer riders in January relative to the yearly average.\nIt is important to realize that the choice of coding really does not matter, provided that we interpret the model output correctly in light of the coding used. For example, we see that the predictions from the linear model are the same regardless of coding:\n\nsum((predict(mod.lm) - predict(mod.lm2))^2)\n\n[1] 1.573305e-18\n\n\nThe sum of squared differences is zero. We can also see this using the all.equal() function:\n\nall.equal(predict(mod.lm), predict(mod.lm2))\n\n[1] TRUE\n\n\nTo reproduce the left-hand side of Figure 4.13, we must first obtain the coefficient estimates associated with mnth. The coefficients for January through November can be obtained directly from the mod.lm2 object. The coefficient for December must be explicitly computed as the negative sum of all the other months.\n\ncoef.months &lt;- c(coef(mod.lm2)[2:12],\n    -sum(coef(mod.lm2)[2:12]))\n\nTo make the plot, we manually label the \\(x\\)-axis with the names of the months.\n\nplot(coef.months, xlab = \"Month\", ylab = \"Coefficient\",\n    xaxt = \"n\", col = \"blue\", pch = 19, type = \"o\")\naxis(side = 1, at = 1:12, labels = c(\"J\", \"F\", \"M\", \"A\",\n    \"M\", \"J\", \"J\", \"A\", \"S\", \"O\", \"N\", \"D\"))\n\n\n\n\n\n\n\n\nReproducing the right-hand side of Figure 4.13 follows a similar process.\n\ncoef.hours &lt;- c(coef(mod.lm2)[13:35],\n    -sum(coef(mod.lm2)[13:35]))\nplot(coef.hours, xlab = \"Hour\", ylab = \"Coefficient\",\n    col = \"blue\", pch = 19, type = \"o\")\n\n\n\n\n\n\n\n\nNow, we consider instead fitting a Poisson regression model to the Bikeshare data. Very little changes, except that we now use the function glm() with the argument family = poisson to specify that we wish to fit a Poisson regression model:\n\nmod.pois &lt;- glm(\n    bikers ~ mnth + hr + workingday + temp + weathersit,\n    data = Bikeshare, family = poisson\n  )\nsummary(mod.pois)\n\n\nCall:\nglm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, \n    family = poisson, data = Bikeshare)\n\nCoefficients:\n                           Estimate Std. Error  z value Pr(&gt;|z|)    \n(Intercept)                4.118245   0.006021  683.964  &lt; 2e-16 ***\nmnth1                     -0.670170   0.005907 -113.445  &lt; 2e-16 ***\nmnth2                     -0.444124   0.004860  -91.379  &lt; 2e-16 ***\nmnth3                     -0.293733   0.004144  -70.886  &lt; 2e-16 ***\nmnth4                      0.021523   0.003125    6.888 5.66e-12 ***\nmnth5                      0.240471   0.002916   82.462  &lt; 2e-16 ***\nmnth6                      0.223235   0.003554   62.818  &lt; 2e-16 ***\nmnth7                      0.103617   0.004125   25.121  &lt; 2e-16 ***\nmnth8                      0.151171   0.003662   41.281  &lt; 2e-16 ***\nmnth9                      0.233493   0.003102   75.281  &lt; 2e-16 ***\nmnth10                     0.267573   0.002785   96.091  &lt; 2e-16 ***\nmnth11                     0.150264   0.003180   47.248  &lt; 2e-16 ***\nhr1                       -0.754386   0.007879  -95.744  &lt; 2e-16 ***\nhr2                       -1.225979   0.009953 -123.173  &lt; 2e-16 ***\nhr3                       -1.563147   0.011869 -131.702  &lt; 2e-16 ***\nhr4                       -2.198304   0.016424 -133.846  &lt; 2e-16 ***\nhr5                       -2.830484   0.022538 -125.586  &lt; 2e-16 ***\nhr6                       -1.814657   0.013464 -134.775  &lt; 2e-16 ***\nhr7                       -0.429888   0.006896  -62.341  &lt; 2e-16 ***\nhr8                        0.575181   0.004406  130.544  &lt; 2e-16 ***\nhr9                        1.076927   0.003563  302.220  &lt; 2e-16 ***\nhr10                       0.581769   0.004286  135.727  &lt; 2e-16 ***\nhr11                       0.336852   0.004720   71.372  &lt; 2e-16 ***\nhr12                       0.494121   0.004392  112.494  &lt; 2e-16 ***\nhr13                       0.679642   0.004069  167.040  &lt; 2e-16 ***\nhr14                       0.673565   0.004089  164.722  &lt; 2e-16 ***\nhr15                       0.624910   0.004178  149.570  &lt; 2e-16 ***\nhr16                       0.653763   0.004132  158.205  &lt; 2e-16 ***\nhr17                       0.874301   0.003784  231.040  &lt; 2e-16 ***\nhr18                       1.294635   0.003254  397.848  &lt; 2e-16 ***\nhr19                       1.212281   0.003321  365.084  &lt; 2e-16 ***\nhr20                       0.914022   0.003700  247.065  &lt; 2e-16 ***\nhr21                       0.616201   0.004191  147.045  &lt; 2e-16 ***\nhr22                       0.364181   0.004659   78.173  &lt; 2e-16 ***\nhr23                       0.117493   0.005225   22.488  &lt; 2e-16 ***\nworkingday                 0.014665   0.001955    7.502 6.27e-14 ***\ntemp                       0.785292   0.011475   68.434  &lt; 2e-16 ***\nweathersitcloudy/misty    -0.075231   0.002179  -34.528  &lt; 2e-16 ***\nweathersitlight rain/snow -0.575800   0.004058 -141.905  &lt; 2e-16 ***\nweathersitheavy rain/snow -0.926287   0.166782   -5.554 2.79e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1052921  on 8644  degrees of freedom\nResidual deviance:  228041  on 8605  degrees of freedom\nAIC: 281159\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe can plot the coefficients associated with mnth and hr, in order to reproduce Figure 4.15:\n\ncoef.mnth &lt;- c(coef(mod.pois)[2:12],\n    -sum(coef(mod.pois)[2:12]))\nplot(coef.mnth, xlab = \"Month\", ylab = \"Coefficient\",\n     xaxt = \"n\", col = \"blue\", pch = 19, type = \"o\")\naxis(side = 1, at = 1:12, labels = c(\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \"J\", \"A\", \"S\", \"O\", \"N\", \"D\"))\n\n\n\n\n\n\n\ncoef.hours &lt;- c(coef(mod.pois)[13:35],\n     -sum(coef(mod.pois)[13:35]))\nplot(coef.hours, xlab = \"Hour\", ylab = \"Coefficient\",\n    col = \"blue\", pch = 19, type = \"o\")\n\n\n\n\n\n\n\n\nWe can once again use the predict() function to obtain the fitted values (predictions) from this Poisson regression model. However, we must use the argument type = \"response\" to specify that we want R to output \\(\\exp(\\hat\\beta_0 + \\hat\\beta_1 X_1 + \\ldots +\\hat\\beta_p X_p)\\) rather than \\(\\hat\\beta_0 + \\hat\\beta_1 X_1 + \\ldots + \\hat\\beta_p X_p\\), which it will output by default.\n\nplot(predict(mod.lm2), predict(mod.pois, type = \"response\"))\nabline(0, 1, col = 2, lwd = 3)\n\n\n\n\n\n\n\n\nThe predictions from the Poisson regression model are correlated with those from the linear model; however, the former are non-negative. As a result the Poisson regression predictions tend to be larger than those from the linear model for either very low or very high levels of ridership.\nIn this section, we used the glm() function with the argument family = poisson in order to perform Poisson regression. Earlier in this lab we used the glm() function with family = binomial to perform logistic regression. Other choices for the family argument can be used to fit other types of GLMs. For instance, family = Gamma fits a gamma regression model."
  },
  {
    "objectID": "teaching/stat-learn/material/04-classification-I.html#the-stock-market-data",
    "href": "teaching/stat-learn/material/04-classification-I.html#the-stock-market-data",
    "title": "Classification Methods",
    "section": "",
    "text": "We will begin by examining some numerical and graphical summaries of the Smarket data, which is part of the ISLR2 library. This data set consists of percentage returns for the S&P 500 stock index over \\(1,250\\) days, from the beginning of 2001 until the end of 2005. For each date, we have recorded the percentage returns for each of the five previous trading days, lagone through lagfive. We have also recorded volume (the number of shares traded on the previous day, in billions), Today (the percentage return on the date in question) and direction (whether the market was Up or Down on this date). Our goal is to predict direction (a qualitative response) using the other features.\n\nlibrary(ISLR2)\nnames(Smarket)\n\n[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n[7] \"Volume\"    \"Today\"     \"Direction\"\n\ndim(Smarket)\n\n[1] 1250    9\n\nsummary(Smarket)\n\n      Year           Lag1                Lag2                Lag3          \n Min.   :2001   Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.922000  \n 1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500   1st Qu.:-0.640000  \n Median :2003   Median : 0.039000   Median : 0.039000   Median : 0.038500  \n Mean   :2003   Mean   : 0.003834   Mean   : 0.003919   Mean   : 0.001716  \n 3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.596750  \n Max.   :2005   Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.733000  \n      Lag4                Lag5              Volume           Today          \n Min.   :-4.922000   Min.   :-4.92200   Min.   :0.3561   Min.   :-4.922000  \n 1st Qu.:-0.640000   1st Qu.:-0.64000   1st Qu.:1.2574   1st Qu.:-0.639500  \n Median : 0.038500   Median : 0.03850   Median :1.4229   Median : 0.038500  \n Mean   : 0.001636   Mean   : 0.00561   Mean   :1.4783   Mean   : 0.003138  \n 3rd Qu.: 0.596750   3rd Qu.: 0.59700   3rd Qu.:1.6417   3rd Qu.: 0.596750  \n Max.   : 5.733000   Max.   : 5.73300   Max.   :3.1525   Max.   : 5.733000  \n Direction \n Down:602  \n Up  :648  \n           \n           \n           \n           \n\npairs(Smarket)\n\n\n\n\n\n\n\n\nThe cor() function produces a matrix that contains all of the pairwise correlations among the predictors in a data set. The first command below gives an error message because the direction variable is qualitative.\n\ncor(Smarket)\n\nError in cor(Smarket): 'x' must be numeric\n\ncor(Smarket[, -9])\n\n             Year         Lag1         Lag2         Lag3         Lag4\nYear   1.00000000  0.029699649  0.030596422  0.033194581  0.035688718\nLag1   0.02969965  1.000000000 -0.026294328 -0.010803402 -0.002985911\nLag2   0.03059642 -0.026294328  1.000000000 -0.025896670 -0.010853533\nLag3   0.03319458 -0.010803402 -0.025896670  1.000000000 -0.024051036\nLag4   0.03568872 -0.002985911 -0.010853533 -0.024051036  1.000000000\nLag5   0.02978799 -0.005674606 -0.003557949 -0.018808338 -0.027083641\nVolume 0.53900647  0.040909908 -0.043383215 -0.041823686 -0.048414246\nToday  0.03009523 -0.026155045 -0.010250033 -0.002447647 -0.006899527\n               Lag5      Volume        Today\nYear    0.029787995  0.53900647  0.030095229\nLag1   -0.005674606  0.04090991 -0.026155045\nLag2   -0.003557949 -0.04338321 -0.010250033\nLag3   -0.018808338 -0.04182369 -0.002447647\nLag4   -0.027083641 -0.04841425 -0.006899527\nLag5    1.000000000 -0.02200231 -0.034860083\nVolume -0.022002315  1.00000000  0.014591823\nToday  -0.034860083  0.01459182  1.000000000\n\n\nAs one would expect, the correlations between the lag variables and today’s returns are close to zero. In other words, there appears to be little correlation between today’s returns and previous days’ returns. The only substantial correlation is between Year and volume. By plotting the data, which is ordered chronologically, we see that volume is increasing over time. In other words, the average number of shares traded daily increased from 2001 to 2005.\n\nattach(Smarket)\nplot(Volume)"
  },
  {
    "objectID": "teaching/stat-learn/material/04-classification-I.html#logistic-regression",
    "href": "teaching/stat-learn/material/04-classification-I.html#logistic-regression",
    "title": "Classification Methods",
    "section": "",
    "text": "Next, we will fit a logistic regression model in order to predict direction using lagone through lagfive and volume. The glm() function can be used to fit many types of generalized linear models, including logistic regression. The syntax of the glm() function is similar to that of lm(), except that we must pass in the argument family = binomial in order to tell R to run a logistic regression rather than some other type of generalized linear model.\n\nglm.fits &lt;- glm(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket, family = binomial\n  )\nsummary(glm.fits)\n\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n    Volume, family = binomial, data = Smarket)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -0.126000   0.240736  -0.523    0.601\nLag1        -0.073074   0.050167  -1.457    0.145\nLag2        -0.042301   0.050086  -0.845    0.398\nLag3         0.011085   0.049939   0.222    0.824\nLag4         0.009359   0.049974   0.187    0.851\nLag5         0.010313   0.049511   0.208    0.835\nVolume       0.135441   0.158360   0.855    0.392\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1731.2  on 1249  degrees of freedom\nResidual deviance: 1727.6  on 1243  degrees of freedom\nAIC: 1741.6\n\nNumber of Fisher Scoring iterations: 3\n\n\nThe smallest \\(p\\)-value here is associated with lagone. The negative coefficient for this predictor suggests that if the market had a positive return yesterday, then it is less likely to go up today. However, at a value of \\(0.15\\), the \\(p\\)-value is still relatively large, and so there is no clear evidence of a real association between lagone and direction.\nThe predict() function can be used to predict the probability that the market will go up, given values of the predictors. The type = \"response\" option tells R to output probabilities of the form \\(P(Y=1|X)\\), as opposed to other information such as the logit. If no data set is supplied to the predict() function, then the probabilities are computed for the training data that was used to fit the logistic regression model. Here we have printed only the first ten probabilities. We know that these values correspond to the probability of the market going up, rather than down, because the contrasts() function indicates that R has created a dummy variable with a 1 for Up.\n\nglm.probs &lt;- predict(glm.fits, type = \"response\")\nglm.probs[1:10]\n\n        1         2         3         4         5         6         7         8 \n0.5070841 0.4814679 0.4811388 0.5152224 0.5107812 0.5069565 0.4926509 0.5092292 \n        9        10 \n0.5176135 0.4888378 \n\ncontrasts(Direction)\n\n     Up\nDown  0\nUp    1\n\n\nIn order to make a prediction as to whether the market will go up or down on a particular day, we must convert these predicted probabilities into class labels, Up or Down. The following two commands create a vector of class predictions based on whether the predicted probability of a market increase is greater than or less than \\(0.5\\).\n\nglm.pred &lt;- rep(\"Down\", 1250)\nglm.pred[glm.probs &gt; .5] = \"Up\"\n\nThe first command creates a vector of 1,250 Down elements. The second line transforms to Up all of the elements for which the predicted probability of a market increase exceeds \\(0.5\\). Given these predictions, the table() function can be used to produce a confusion matrix in order to determine how many observations were correctly or incorrectly classified.\n\ntable(glm.pred, Direction)\n\n        Direction\nglm.pred Down  Up\n    Down  145 141\n    Up    457 507\n\n(507 + 145) / 1250\n\n[1] 0.5216\n\nmean(glm.pred == Direction)\n\n[1] 0.5216\n\n\nThe diagonal elements of the confusion matrix indicate correct predictions, while the off-diagonals represent incorrect predictions. Hence our model correctly predicted that the market would go up on \\(507\\) days and that it would go down on \\(145\\) days, for a total of \\(507+145 = 652\\) correct predictions. The mean() function can be used to compute the fraction of days for which the prediction was correct. In this case, logistic regression correctly predicted the movement of the market \\(52.2\\) % of the time.\nAt first glance, it appears that the logistic regression model is working a little better than random guessing. However, this result is misleading because we trained and tested the model on the same set of \\(1,250\\) observations. In other words, \\(100\\%-52.2\\%=47.8\\%\\), is the training error rate. As we have seen previously, the training error rate is often overly optimistic—it tends to underestimate the test error rate. In order to better assess the accuracy of the logistic regression model in this setting, we can fit the model using part of the data, and then examine how well it predicts the held out data.\nTo implement this strategy, we will first create a vector corresponding to the observations from 2001 through 2004. We will then use this vector to create a held out data set of observations from 2005.\n\ntrain &lt;- (Year &lt; 2005)\nSmarket.2005 &lt;- Smarket[!train, ]\ndim(Smarket.2005)\n\n[1] 252   9\n\nDirection.2005 &lt;- Direction[!train]\n\nThe object train is a vector of \\(1{,}250\\) elements, corresponding to the observations in our data set. The elements of the vector that correspond to observations that occurred before 2005 are set to TRUE, whereas those that correspond to observations in 2005 are set to FALSE. The object train is a Boolean vector, since its elements are TRUE and FALSE. Boolean vectors can be used to obtain a subset of the rows or columns of a matrix. For instance, the command Smarket[train, ] would pick out a submatrix of the stock market data set, corresponding only to the dates before 2005, since those are the ones for which the elements of train are TRUE. The ! symbol can be used to reverse all of the elements of a Boolean vector. That is, !train is a vector similar to train, except that the elements that are TRUE in train get swapped to FALSE in !train, and the elements that are FALSE in train get swapped to TRUE in !train. Therefore, Smarket[!train, ] yields a submatrix of the stock market data containing only the observations for which train is FALSE—that is, the observations with dates in 2005. The output above indicates that there are 252 such observations.\nWe now fit a logistic regression model using only the subset of the observations that correspond to dates before 2005, using the subset argument. We then obtain predicted probabilities of the stock market going up for each of the days in our test set—that is, for the days in 2005.\n\nglm.fits &lt;- glm(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket, family = binomial, subset = train\n  )\nglm.probs &lt;- predict(glm.fits, Smarket.2005,\n    type = \"response\")\n\nNotice that we have trained and tested our model on two completely separate data sets: training was performed using only the dates before 2005, and testing was performed using only the dates in 2005. Finally, we compute the predictions for 2005 and compare them to the actual movements of the market over that time period.\n\nglm.pred &lt;- rep(\"Down\", 252)\nglm.pred[glm.probs &gt; .5] &lt;- \"Up\"\ntable(glm.pred, Direction.2005)\n\n        Direction.2005\nglm.pred Down Up\n    Down   77 97\n    Up     34 44\n\nmean(glm.pred == Direction.2005)\n\n[1] 0.4801587\n\nmean(glm.pred != Direction.2005)\n\n[1] 0.5198413\n\n\nThe != notation means not equal to, and so the last command computes the test set error rate. The results are rather disappointing: the test error rate is \\(52\\) %, which is worse than random guessing! Of course this result is not all that surprising, given that one would not generally expect to be able to use previous days’ returns to predict future market performance. (After all, if it were possible to do so, then the authors of this book would be out striking it rich rather than writing a statistics textbook.)\nWe recall that the logistic regression model had very underwhelming \\(p\\)-values associated with all of the predictors, and that the smallest \\(p\\)-value, though not very small, corresponded to lagone. Perhaps by removing the variables that appear not to be helpful in predicting direction, we can obtain a more effective model. After all, using predictors that have no relationship with the response tends to cause a deterioration in the test error rate (since such predictors cause an increase in variance without a corresponding decrease in bias), and so removing such predictors may in turn yield an improvement. Below we have refit the logistic regression using just lagone and lagtwo, which seemed to have the highest predictive power in the original logistic regression model.\n\nglm.fits &lt;- glm(Direction ~ Lag1 + Lag2, data = Smarket,\n    family = binomial, subset = train)\nglm.probs &lt;- predict(glm.fits, Smarket.2005,\n    type = \"response\")\nglm.pred &lt;- rep(\"Down\", 252)\nglm.pred[glm.probs &gt; .5] &lt;- \"Up\"\ntable(glm.pred, Direction.2005)\n\n        Direction.2005\nglm.pred Down  Up\n    Down   35  35\n    Up     76 106\n\nmean(glm.pred == Direction.2005)\n\n[1] 0.5595238\n\n106 / (106 + 76)\n\n[1] 0.5824176\n\n\nNow the results appear to be a little better: \\(56\\%\\) of the daily movements have been correctly predicted. It is worth noting that in this case, a much simpler strategy of predicting that the market will increase every day will also be correct \\(56\\%\\) of the time! Hence, in terms of overall error rate, the logistic regression method is no better than the naive approach. However, the confusion matrix shows that on days when logistic regression predicts an increase in the market, it has a \\(58\\%\\) accuracy rate. This suggests a possible trading strategy of buying on days when the model predicts an increasing market, and avoiding trades on days when a decrease is predicted. Of course one would need to investigate more carefully whether this small improvement was real or just due to random chance.\nSuppose that we want to predict the returns associated with particular values of lagone and lagtwo. In particular, we want to predict direction on a day when lagone and lagtwo equal 1.2 and~1.1, respectively, and on a day when they equal 1.5 and $-$0.8. We do this using the predict() function.\n\npredict(glm.fits,\n    newdata =\n      data.frame(Lag1 = c(1.2, 1.5),  Lag2 = c(1.1, -0.8)),\n    type = \"response\"\n  )\n\n        1         2 \n0.4791462 0.4960939 \n\n\n\n\n\nCreate a new DirectionNum column to store your variable as numeric (0/1)\nFit a linear model (Y ~ lag1 + lag2) using that new column and compare the estimates with the logistic model."
  },
  {
    "objectID": "teaching/stat-learn/material/04-classification-I.html#roc-curves",
    "href": "teaching/stat-learn/material/04-classification-I.html#roc-curves",
    "title": "Classification Methods",
    "section": "",
    "text": "ROC curves are a great tool to assess our classification performance. The pROC package gives us an easy way to create one. Please make sure you have it installed!\nA ROC curve plots the sensitivity and specificity of our classifier.\nRemember:\nsensitivity = TP / TP + FN\nspecificity = TN / TN + FP\n\n#install.packages('pROC')\nlibrary(pROC)\n\nType 'citation(\"pROC\")' for a citation.\n\n\n\nAttaching package: 'pROC'\n\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\n\nThis is a more explicit way to subset, again dividing via the train object.\n\nSmarket_train = subset(Smarket, train ) # pre 2005\nSmarket_test = subset(Smarket, !train ) # post 2005\n\nFor the pROC function roc() we need our data stored in numeric form, so let’s fit our model and assign new columns to our df.\n\nglm.fits = glm(Direction ~ Lag1 + Lag2,\n               data = Smarket_train,\n               family = binomial(link = \"logit\"))\n\nglm.probs = predict(glm.fits, Smarket_test, type = \"response\")\nglm.pred &lt;- rep(0, 252) \nglm.pred[glm.probs &gt; .5] &lt;- 1 # 0/1 instead of up/down\n\nSmarket_test$Preds = glm.pred\n\n# $Direction uses named factors, stored as 1/2, so we take -1 \n\nSmarket_test$DirectionNum = as.numeric(Smarket_test$Direction) - 1 \n\nWe create a roc_object by using the true values and predictions of our test set\n\nroc_obj = roc(Smarket_test$DirectionNum, Smarket_test$Preds)\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\nplot(roc_obj)\n\n\n\n\n\n\n\n\nThis looks terrible! Both visually and performance wise. As we can see our curve is extremely close to the diagonal which indicates a completely random guess (50:50). This is in line with our previous assessment which gave us \\(56\\%\\).\nWe can spice up the curve a bit by adding CIs, or add some colour, but pROC is a very base-R friendly package so feel free to shop around for some more “modern” implementations.\n\nroc_obj = roc(Smarket_test$DirectionNum, Smarket_test$Preds)\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\nplot(roc_obj, print.auc =TRUE)\nplot(ci.se(roc_obj), type=\"shape\", col=\"lightblue\")\n\nWarning in plot.ci.se(ci.se(roc_obj), type = \"shape\", col = \"lightblue\"): Low\ndefinition shape.\n\n\n\n\n\n\n\n\nplot(sens.ci, type=\"bars\")\n\nError: object 'sens.ci' not found"
  },
  {
    "objectID": "teaching/stat-learn/material/04-classification-I.html#poisson-regression",
    "href": "teaching/stat-learn/material/04-classification-I.html#poisson-regression",
    "title": "Classification Methods",
    "section": "",
    "text": "Finally, we fit a Poisson regression model to the Bikeshare data set, which measures the number of bike rentals (bikers) per hour in Washington, DC. The data can be found in the ISLR2 library.\n\nattach(Bikeshare)\ndim(Bikeshare)\n\n[1] 8645   15\n\nnames(Bikeshare)\n\n [1] \"season\"     \"mnth\"       \"day\"        \"hr\"         \"holiday\"   \n [6] \"weekday\"    \"workingday\" \"weathersit\" \"temp\"       \"atemp\"     \n[11] \"hum\"        \"windspeed\"  \"casual\"     \"registered\" \"bikers\"    \n\n\nWe begin by fitting a least squares linear regression model to the data.\n\nmod.lm &lt;- lm(\n    bikers ~ mnth + hr + workingday + temp + weathersit,\n    data = Bikeshare\n  )\nsummary(mod.lm)\n\n\nCall:\nlm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, \n    data = Bikeshare)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-299.00  -45.70   -6.23   41.08  425.29 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                -68.632      5.307 -12.932  &lt; 2e-16 ***\nmnthFeb                      6.845      4.287   1.597 0.110398    \nmnthMarch                   16.551      4.301   3.848 0.000120 ***\nmnthApril                   41.425      4.972   8.331  &lt; 2e-16 ***\nmnthMay                     72.557      5.641  12.862  &lt; 2e-16 ***\nmnthJune                    67.819      6.544  10.364  &lt; 2e-16 ***\nmnthJuly                    45.324      7.081   6.401 1.63e-10 ***\nmnthAug                     53.243      6.640   8.019 1.21e-15 ***\nmnthSept                    66.678      5.925  11.254  &lt; 2e-16 ***\nmnthOct                     75.834      4.950  15.319  &lt; 2e-16 ***\nmnthNov                     60.310      4.610  13.083  &lt; 2e-16 ***\nmnthDec                     46.458      4.271  10.878  &lt; 2e-16 ***\nhr1                        -14.579      5.699  -2.558 0.010536 *  \nhr2                        -21.579      5.733  -3.764 0.000168 ***\nhr3                        -31.141      5.778  -5.389 7.26e-08 ***\nhr4                        -36.908      5.802  -6.361 2.11e-10 ***\nhr5                        -24.135      5.737  -4.207 2.61e-05 ***\nhr6                         20.600      5.704   3.612 0.000306 ***\nhr7                        120.093      5.693  21.095  &lt; 2e-16 ***\nhr8                        223.662      5.690  39.310  &lt; 2e-16 ***\nhr9                        120.582      5.693  21.182  &lt; 2e-16 ***\nhr10                        83.801      5.705  14.689  &lt; 2e-16 ***\nhr11                       105.423      5.722  18.424  &lt; 2e-16 ***\nhr12                       137.284      5.740  23.916  &lt; 2e-16 ***\nhr13                       136.036      5.760  23.617  &lt; 2e-16 ***\nhr14                       126.636      5.776  21.923  &lt; 2e-16 ***\nhr15                       132.087      5.780  22.852  &lt; 2e-16 ***\nhr16                       178.521      5.772  30.927  &lt; 2e-16 ***\nhr17                       296.267      5.749  51.537  &lt; 2e-16 ***\nhr18                       269.441      5.736  46.976  &lt; 2e-16 ***\nhr19                       186.256      5.714  32.596  &lt; 2e-16 ***\nhr20                       125.549      5.704  22.012  &lt; 2e-16 ***\nhr21                        87.554      5.693  15.378  &lt; 2e-16 ***\nhr22                        59.123      5.689  10.392  &lt; 2e-16 ***\nhr23                        26.838      5.688   4.719 2.41e-06 ***\nworkingday                   1.270      1.784   0.711 0.476810    \ntemp                       157.209     10.261  15.321  &lt; 2e-16 ***\nweathersitcloudy/misty     -12.890      1.964  -6.562 5.60e-11 ***\nweathersitlight rain/snow  -66.494      2.965 -22.425  &lt; 2e-16 ***\nweathersitheavy rain/snow -109.745     76.667  -1.431 0.152341    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 76.5 on 8605 degrees of freedom\nMultiple R-squared:  0.6745,    Adjusted R-squared:  0.6731 \nF-statistic: 457.3 on 39 and 8605 DF,  p-value: &lt; 2.2e-16\n\n\nDue to space constraints, we truncate the output of summary(mod.lm). In mod.lm, the first level of hr (0) and mnth (Jan) are treated as the baseline values, and so no coefficient estimates are provided for them: implicitly, their coefficient estimates are zero, and all other levels are measured relative to these baselines. For example, the Feb coefficient of \\(6.845\\) signifies that, holding all other variables constant, there are on average about 7 more riders in February than in January. Similarly there are about 16.5 more riders in March than in January.\nThe results seen in Section 4.6.1 used a slightly different coding of the variables hr and mnth, as follows:\n\ncontrasts(Bikeshare$hr) = contr.sum(24)\ncontrasts(Bikeshare$mnth) = contr.sum(12)\nmod.lm2 &lt;- lm(\n    bikers ~ mnth + hr + workingday + temp + weathersit,\n    data = Bikeshare\n  )\nsummary(mod.lm2)\n\n\nCall:\nlm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, \n    data = Bikeshare)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-299.00  -45.70   -6.23   41.08  425.29 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 73.5974     5.1322  14.340  &lt; 2e-16 ***\nmnth1                      -46.0871     4.0855 -11.281  &lt; 2e-16 ***\nmnth2                      -39.2419     3.5391 -11.088  &lt; 2e-16 ***\nmnth3                      -29.5357     3.1552  -9.361  &lt; 2e-16 ***\nmnth4                       -4.6622     2.7406  -1.701  0.08895 .  \nmnth5                       26.4700     2.8508   9.285  &lt; 2e-16 ***\nmnth6                       21.7317     3.4651   6.272 3.75e-10 ***\nmnth7                       -0.7626     3.9084  -0.195  0.84530    \nmnth8                        7.1560     3.5347   2.024  0.04295 *  \nmnth9                       20.5912     3.0456   6.761 1.46e-11 ***\nmnth10                      29.7472     2.6995  11.019  &lt; 2e-16 ***\nmnth11                      14.2229     2.8604   4.972 6.74e-07 ***\nhr1                        -96.1420     3.9554 -24.307  &lt; 2e-16 ***\nhr2                       -110.7213     3.9662 -27.916  &lt; 2e-16 ***\nhr3                       -117.7212     4.0165 -29.310  &lt; 2e-16 ***\nhr4                       -127.2828     4.0808 -31.191  &lt; 2e-16 ***\nhr5                       -133.0495     4.1168 -32.319  &lt; 2e-16 ***\nhr6                       -120.2775     4.0370 -29.794  &lt; 2e-16 ***\nhr7                        -75.5424     3.9916 -18.925  &lt; 2e-16 ***\nhr8                         23.9511     3.9686   6.035 1.65e-09 ***\nhr9                        127.5199     3.9500  32.284  &lt; 2e-16 ***\nhr10                        24.4399     3.9360   6.209 5.57e-10 ***\nhr11                       -12.3407     3.9361  -3.135  0.00172 ** \nhr12                         9.2814     3.9447   2.353  0.01865 *  \nhr13                        41.1417     3.9571  10.397  &lt; 2e-16 ***\nhr14                        39.8939     3.9750  10.036  &lt; 2e-16 ***\nhr15                        30.4940     3.9910   7.641 2.39e-14 ***\nhr16                        35.9445     3.9949   8.998  &lt; 2e-16 ***\nhr17                        82.3786     3.9883  20.655  &lt; 2e-16 ***\nhr18                       200.1249     3.9638  50.488  &lt; 2e-16 ***\nhr19                       173.2989     3.9561  43.806  &lt; 2e-16 ***\nhr20                        90.1138     3.9400  22.872  &lt; 2e-16 ***\nhr21                        29.4071     3.9362   7.471 8.74e-14 ***\nhr22                        -8.5883     3.9332  -2.184  0.02902 *  \nhr23                       -37.0194     3.9344  -9.409  &lt; 2e-16 ***\nworkingday                   1.2696     1.7845   0.711  0.47681    \ntemp                       157.2094    10.2612  15.321  &lt; 2e-16 ***\nweathersitcloudy/misty     -12.8903     1.9643  -6.562 5.60e-11 ***\nweathersitlight rain/snow  -66.4944     2.9652 -22.425  &lt; 2e-16 ***\nweathersitheavy rain/snow -109.7446    76.6674  -1.431  0.15234    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 76.5 on 8605 degrees of freedom\nMultiple R-squared:  0.6745,    Adjusted R-squared:  0.6731 \nF-statistic: 457.3 on 39 and 8605 DF,  p-value: &lt; 2.2e-16\n\n\nWhat is the difference between the two codings? In mod.lm2, a coefficient estimate is reported for all but the last level of hr and mnth. Importantly, in mod.lm2, the coefficient estimate for the last level of mnth is not zero: instead, it equals the negative of the sum of the coefficient estimates for all of the other levels. Similarly, in mod.lm2, the coefficient estimate for the last level of hr is the negative of the sum of the coefficient estimates for all of the other levels. This means that the coefficients of hr and mnth in mod.lm2 will always sum to zero, and can be interpreted as the difference from the mean level. For example, the coefficient for January of \\(-46.087\\) indicates that, holding all other variables constant, there are typically 46 fewer riders in January relative to the yearly average.\nIt is important to realize that the choice of coding really does not matter, provided that we interpret the model output correctly in light of the coding used. For example, we see that the predictions from the linear model are the same regardless of coding:\n\nsum((predict(mod.lm) - predict(mod.lm2))^2)\n\n[1] 1.573305e-18\n\n\nThe sum of squared differences is zero. We can also see this using the all.equal() function:\n\nall.equal(predict(mod.lm), predict(mod.lm2))\n\n[1] TRUE\n\n\nTo reproduce the left-hand side of Figure 4.13, we must first obtain the coefficient estimates associated with mnth. The coefficients for January through November can be obtained directly from the mod.lm2 object. The coefficient for December must be explicitly computed as the negative sum of all the other months.\n\ncoef.months &lt;- c(coef(mod.lm2)[2:12],\n    -sum(coef(mod.lm2)[2:12]))\n\nTo make the plot, we manually label the \\(x\\)-axis with the names of the months.\n\nplot(coef.months, xlab = \"Month\", ylab = \"Coefficient\",\n    xaxt = \"n\", col = \"blue\", pch = 19, type = \"o\")\naxis(side = 1, at = 1:12, labels = c(\"J\", \"F\", \"M\", \"A\",\n    \"M\", \"J\", \"J\", \"A\", \"S\", \"O\", \"N\", \"D\"))\n\n\n\n\n\n\n\n\nReproducing the right-hand side of Figure 4.13 follows a similar process.\n\ncoef.hours &lt;- c(coef(mod.lm2)[13:35],\n    -sum(coef(mod.lm2)[13:35]))\nplot(coef.hours, xlab = \"Hour\", ylab = \"Coefficient\",\n    col = \"blue\", pch = 19, type = \"o\")\n\n\n\n\n\n\n\n\nNow, we consider instead fitting a Poisson regression model to the Bikeshare data. Very little changes, except that we now use the function glm() with the argument family = poisson to specify that we wish to fit a Poisson regression model:\n\nmod.pois &lt;- glm(\n    bikers ~ mnth + hr + workingday + temp + weathersit,\n    data = Bikeshare, family = poisson\n  )\nsummary(mod.pois)\n\n\nCall:\nglm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, \n    family = poisson, data = Bikeshare)\n\nCoefficients:\n                           Estimate Std. Error  z value Pr(&gt;|z|)    \n(Intercept)                4.118245   0.006021  683.964  &lt; 2e-16 ***\nmnth1                     -0.670170   0.005907 -113.445  &lt; 2e-16 ***\nmnth2                     -0.444124   0.004860  -91.379  &lt; 2e-16 ***\nmnth3                     -0.293733   0.004144  -70.886  &lt; 2e-16 ***\nmnth4                      0.021523   0.003125    6.888 5.66e-12 ***\nmnth5                      0.240471   0.002916   82.462  &lt; 2e-16 ***\nmnth6                      0.223235   0.003554   62.818  &lt; 2e-16 ***\nmnth7                      0.103617   0.004125   25.121  &lt; 2e-16 ***\nmnth8                      0.151171   0.003662   41.281  &lt; 2e-16 ***\nmnth9                      0.233493   0.003102   75.281  &lt; 2e-16 ***\nmnth10                     0.267573   0.002785   96.091  &lt; 2e-16 ***\nmnth11                     0.150264   0.003180   47.248  &lt; 2e-16 ***\nhr1                       -0.754386   0.007879  -95.744  &lt; 2e-16 ***\nhr2                       -1.225979   0.009953 -123.173  &lt; 2e-16 ***\nhr3                       -1.563147   0.011869 -131.702  &lt; 2e-16 ***\nhr4                       -2.198304   0.016424 -133.846  &lt; 2e-16 ***\nhr5                       -2.830484   0.022538 -125.586  &lt; 2e-16 ***\nhr6                       -1.814657   0.013464 -134.775  &lt; 2e-16 ***\nhr7                       -0.429888   0.006896  -62.341  &lt; 2e-16 ***\nhr8                        0.575181   0.004406  130.544  &lt; 2e-16 ***\nhr9                        1.076927   0.003563  302.220  &lt; 2e-16 ***\nhr10                       0.581769   0.004286  135.727  &lt; 2e-16 ***\nhr11                       0.336852   0.004720   71.372  &lt; 2e-16 ***\nhr12                       0.494121   0.004392  112.494  &lt; 2e-16 ***\nhr13                       0.679642   0.004069  167.040  &lt; 2e-16 ***\nhr14                       0.673565   0.004089  164.722  &lt; 2e-16 ***\nhr15                       0.624910   0.004178  149.570  &lt; 2e-16 ***\nhr16                       0.653763   0.004132  158.205  &lt; 2e-16 ***\nhr17                       0.874301   0.003784  231.040  &lt; 2e-16 ***\nhr18                       1.294635   0.003254  397.848  &lt; 2e-16 ***\nhr19                       1.212281   0.003321  365.084  &lt; 2e-16 ***\nhr20                       0.914022   0.003700  247.065  &lt; 2e-16 ***\nhr21                       0.616201   0.004191  147.045  &lt; 2e-16 ***\nhr22                       0.364181   0.004659   78.173  &lt; 2e-16 ***\nhr23                       0.117493   0.005225   22.488  &lt; 2e-16 ***\nworkingday                 0.014665   0.001955    7.502 6.27e-14 ***\ntemp                       0.785292   0.011475   68.434  &lt; 2e-16 ***\nweathersitcloudy/misty    -0.075231   0.002179  -34.528  &lt; 2e-16 ***\nweathersitlight rain/snow -0.575800   0.004058 -141.905  &lt; 2e-16 ***\nweathersitheavy rain/snow -0.926287   0.166782   -5.554 2.79e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1052921  on 8644  degrees of freedom\nResidual deviance:  228041  on 8605  degrees of freedom\nAIC: 281159\n\nNumber of Fisher Scoring iterations: 5\n\n\nWe can plot the coefficients associated with mnth and hr, in order to reproduce Figure 4.15:\n\ncoef.mnth &lt;- c(coef(mod.pois)[2:12],\n    -sum(coef(mod.pois)[2:12]))\nplot(coef.mnth, xlab = \"Month\", ylab = \"Coefficient\",\n     xaxt = \"n\", col = \"blue\", pch = 19, type = \"o\")\naxis(side = 1, at = 1:12, labels = c(\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \"J\", \"A\", \"S\", \"O\", \"N\", \"D\"))\n\n\n\n\n\n\n\ncoef.hours &lt;- c(coef(mod.pois)[13:35],\n     -sum(coef(mod.pois)[13:35]))\nplot(coef.hours, xlab = \"Hour\", ylab = \"Coefficient\",\n    col = \"blue\", pch = 19, type = \"o\")\n\n\n\n\n\n\n\n\nWe can once again use the predict() function to obtain the fitted values (predictions) from this Poisson regression model. However, we must use the argument type = \"response\" to specify that we want R to output \\(\\exp(\\hat\\beta_0 + \\hat\\beta_1 X_1 + \\ldots +\\hat\\beta_p X_p)\\) rather than \\(\\hat\\beta_0 + \\hat\\beta_1 X_1 + \\ldots + \\hat\\beta_p X_p\\), which it will output by default.\n\nplot(predict(mod.lm2), predict(mod.pois, type = \"response\"))\nabline(0, 1, col = 2, lwd = 3)\n\n\n\n\n\n\n\n\nThe predictions from the Poisson regression model are correlated with those from the linear model; however, the former are non-negative. As a result the Poisson regression predictions tend to be larger than those from the linear model for either very low or very high levels of ridership.\nIn this section, we used the glm() function with the argument family = poisson in order to perform Poisson regression. Earlier in this lab we used the glm() function with family = binomial to perform logistic regression. Other choices for the family argument can be used to fit other types of GLMs. For instance, family = Gamma fits a gamma regression model."
  },
  {
    "objectID": "teaching/stat-learn/material/05_Class-II.html",
    "href": "teaching/stat-learn/material/05_Class-II.html",
    "title": "Classification Methods II",
    "section": "",
    "text": "To continue our dive into classification methods we will take a loook at the Insurance dataset stored as “Caravan”.\nThis data set includes 85 predictors that measure demographic characteristics for 5,822 individuals. The response variable is Purchase, which indicates whether or not a given individual purchases a caravan insurance policy. In this data set, only 6% of people purchased caravan insurance.\n\nlibrary(ISLR2)\nlibrary(class)\nattach(Caravan)\ndim(Caravan)\n\n[1] 5822   86\n\n\n\nsummary(Caravan$Purchase)\n\n  No  Yes \n5474  348 \n\n348 / 5474\n\n[1] 0.06357326\n\n\nBecause the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale. For instance, imagine a data set that contains two variables, salary and age (measured in dollars and years, respectively). As far as KNN is concerned, a difference of $1,000 in salary is enormous compared to a difference of 50 years in age. Consequently, salary will drive the KNN classification results, and age will have almost no effect. This is contrary to our intuition that a salary difference of $1,000 is quite small compared to an age difference of 50 years. Furthermore, the importance of scale to the KNN classifier leads to another issue: if we measured salary in Japanese yen, or if we measured age in minutes, then we’d get quite different classification results from what we get if these two variables are measured in dollars and years.\nA good way to handle this problem is to standardize the data so that all variables are given a mean of zero and a standard deviation of one. Then all variables will be on a comparable scale. The scale() function does just this. In standardizing the data, we exclude column 86, because that is the qualitative Purchase variable.\n\nstandardized.X &lt;- scale(Caravan[, -86])\ncat(\"Unstandardized:\\n\")\n\nUnstandardized:\n\nvar(Caravan[, 1])\n\n[1] 165.0378\n\nvar(Caravan[, 2])\n\n[1] 0.1647078\n\ncat(\"Standardized:\\n\")\n\nStandardized:\n\nvar(standardized.X[, 1])\n\n[1] 1\n\nvar(standardized.X[, 2])\n\n[1] 1\n\n\nNow every column of standardized.X has a standard deviation of one and a mean of zero.\nWe now split the observations into a test set, containing the first 1,000 observations, and a training set, containing the remaining observations. We fit a KNN model on the training data using K=1, and evaluate its performance on the test data.\n\ntest &lt;- 1:1000\ntrain.X &lt;- standardized.X[-test, ]\ntest.X &lt;- standardized.X[test, ]\ntrain.Y &lt;- Purchase[-test]\ntest.Y &lt;- Purchase[test]\nset.seed(1)\nknn.pred &lt;- knn(train.X, test.X, train.Y, k = 1)\nmean(test.Y != knn.pred)\n\n[1] 0.118\n\n\n\ntable(knn.pred, test.Y)\n\n        test.Y\nknn.pred  No Yes\n     No  873  50\n     Yes  68   9\n\n9 / (68 + 9)\n\n[1] 0.1168831\n\n\nThis looks like a nice result - we have high accuracy. But do we care about how many people are not buying insurance in this example?\n\n\nTune your hyperparameter k and compare to our previous result. Also Compare to random guessing (50:50)\n\n\n\n\nNow we will perform LDA on the Smarket data. In R, we fit an LDA model using the lda() function, which is part of the MASS library. Notice that the syntax for the lda() function is identical to that of lm(), and to that of glm() except for the absence of the family option. We fit the model using only the observations before 2005.\n\nattach(Smarket)\ntrain &lt;- (Year &lt; 2005)\nSmarket.2005 &lt;- Smarket[!train, ]\ndim(Smarket.2005)\n\n[1] 252   9\n\nDirection.2005 &lt;- Direction[!train]\n\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:ISLR2':\n\n    Boston\n\nlda.fit &lt;- lda(Direction ~ Lag1 + Lag2, data = Smarket,\n    subset = train)\nlda.fit\n\nCall:\nlda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\nCoefficients of linear discriminants:\n            LD1\nLag1 -0.6420190\nLag2 -0.5135293\n\n\n\nplot(lda.fit)\n\n\n\n\n\n\n\n\nThe LDA output indicates that \\(\\hat\\pi_1=0.492\\) and \\(\\hat\\pi_2=0.508\\); in other words, \\(49.2\\) % of the training observations correspond to days during which the market went down. It also provides the group means; these are the average of each predictor within each class, and are used by LDA as estimates of \\(\\mu_k\\). These suggest that there is a tendency for the previous 2~days’ returns to be negative on days when the market increases, and a tendency for the previous days’ returns to be positive on days when the market declines. The coefficients of linear discriminants output provides the linear combination of lagone and lagtwo that are used to form the LDA decision rule. In other words, these are the multipliers of the elements of \\(X=x\\) in (4.24). If $-0.642 \\(`lagone`\\) - 0.514 $lagtwo is large, then the LDA classifier will predict a market increase, and if it is small, then the LDA classifier will predict a market decline.\nThe plot() function produces plots of the linear discriminants, obtained by computing $-0.642 \\(`lagone`\\) - 0.514 $lagtwo for each of the training observations. The Up and Down observations are displayed separately.\nThe predict() function returns a list with three elements. The first element, class, contains LDA’s predictions about the movement of the market. The second element, posterior, is a matrix whose \\(k\\)th column contains the posterior probability that the corresponding observation belongs to the \\(k\\)th class, computed from (4.15). Finally, x contains the linear discriminants, described earlier.\n\nlda.pred &lt;- predict(lda.fit, Smarket.2005)\nnames(lda.pred)\n\n[1] \"class\"     \"posterior\" \"x\"        \n\n\nAs we observed in Section 4.5, the LDA and logistic regression predictions are almost identical.\n\nlda.class &lt;- lda.pred$class\ntable(lda.class, Direction.2005)\n\n         Direction.2005\nlda.class Down  Up\n     Down   35  35\n     Up     76 106\n\nmean(lda.class == Direction.2005)\n\n[1] 0.5595238\n\n\nApplying a \\(50\\) % threshold to the posterior probabilities allows us to recreate the predictions contained in lda.pred$class.\n\nsum(lda.pred$posterior[, 1] &gt;= .5)\n\n[1] 70\n\nsum(lda.pred$posterior[, 1] &lt; .5)\n\n[1] 182\n\n\nNotice that the posterior probability output by the model corresponds to the probability that the market will decrease:\n\nlda.pred$posterior[1:20, 1]\n\n      999      1000      1001      1002      1003      1004      1005      1006 \n0.4901792 0.4792185 0.4668185 0.4740011 0.4927877 0.4938562 0.4951016 0.4872861 \n     1007      1008      1009      1010      1011      1012      1013      1014 \n0.4907013 0.4844026 0.4906963 0.5119988 0.4895152 0.4706761 0.4744593 0.4799583 \n     1015      1016      1017      1018 \n0.4935775 0.5030894 0.4978806 0.4886331 \n\nlda.class[1:20]\n\n [1] Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Down Up   Up   Up  \n[16] Up   Up   Down Up   Up  \nLevels: Down Up\n\n\nIf we wanted to use a posterior probability threshold other than \\(50\\) % in order to make predictions, then we could easily do so. For instance, suppose that we wish to predict a market decrease only if we are very certain that the market will indeed decrease on that day—say, if the posterior probability is at least \\(90\\) %.\n\nsum(lda.pred$posterior[, 1] &gt; .9)\n\n[1] 0\n\n\nNo days in 2005 meet that threshold! In fact, the greatest posterior probability of decrease in all of 2005 was \\(52.02\\)\n\n\n\nWe will now fit a QDA model to the Smarket data. QDA is implemented in R using the qda() function, which is also part of the MASS library. The syntax is identical to that of lda().\n\nqda.fit &lt;- qda(Direction ~ Lag1 + Lag2, data = Smarket,\n    subset = train)\nqda.fit\n\nCall:\nqda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\n\nThe output contains the group means. But it does not contain the coefficients of the linear discriminants, because the QDA classifier involves a quadratic, rather than a linear, function of the predictors. The predict() function works in exactly the same fashion as for LDA.\n\nqda.class &lt;- predict(qda.fit, Smarket.2005)$class\ntable(qda.class, Direction.2005)\n\n         Direction.2005\nqda.class Down  Up\n     Down   30  20\n     Up     81 121\n\nmean(qda.class == Direction.2005)\n\n[1] 0.5992063\n\n\nInterestingly, the QDA predictions are accurate almost \\(60\\) % of the time, even though the 2005 data was not used to fit the model. This level of accuracy is quite impressive for stock market data, which is known to be quite hard to model accurately. This suggests that the quadratic form assumed by QDA may capture the true relationship more accurately than the linear forms assumed by LDA and logistic regression. However, we recommend evaluating this method’s performance on a larger test set before betting that this approach will consistently beat the market!\n\n\n\nNext, we fit a naive Bayes model to the Smarket data. Naive Bayes is implemented in R using the naiveBayes() function, which is part of the e1071 library. The syntax is identical to that of lda() and qda(). By default, this implementation of the naive Bayes classifier models each quantitative feature using a Gaussian distribution. However, a kernel density method can also be used to estimate the distributions.\n\n# install.packages(\"e1071\")\nlibrary(e1071)\nnb.fit &lt;- naiveBayes(Direction ~ Lag1 + Lag2, data = Smarket,\n    subset = train)\nnb.fit\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n    Down       Up \n0.491984 0.508016 \n\nConditional probabilities:\n      Lag1\nY             [,1]     [,2]\n  Down  0.04279022 1.227446\n  Up   -0.03954635 1.231668\n\n      Lag2\nY             [,1]     [,2]\n  Down  0.03389409 1.239191\n  Up   -0.03132544 1.220765\n\n\nThe output contains the estimated mean and standard deviation for each variable in each class. For example, the mean for lagone is \\(0.0428\\) for\nDirection=Down, and the standard deviation is \\(1.23\\). We can easily verify this:\n\nmean(Lag1[train][Direction[train] == \"Down\"])\n\n[1] 0.04279022\n\nsd(Lag1[train][Direction[train] == \"Down\"])\n\n[1] 1.227446\n\n\nThe predict() function is straightforward.\n\nnb.class &lt;- predict(nb.fit, Smarket.2005)\ntable(nb.class, Direction.2005)\n\n        Direction.2005\nnb.class Down  Up\n    Down   28  20\n    Up     83 121\n\nmean(nb.class == Direction.2005)\n\n[1] 0.5912698\n\n\nNaive Bayes performs very well on this data, with accurate predictions over \\(59\\%\\) of the time. This is slightly worse than QDA, but much better than LDA.\nThe predict() function can also generate estimates of the probability that each observation belongs to a particular class. %\n\nnb.preds &lt;- predict(nb.fit, Smarket.2005, type = \"raw\")\nnb.preds[1:5, ]\n\n          Down        Up\n[1,] 0.4873164 0.5126836\n[2,] 0.4762492 0.5237508\n[3,] 0.4653377 0.5346623\n[4,] 0.4748652 0.5251348\n[5,] 0.4901890 0.5098110"
  },
  {
    "objectID": "teaching/stat-learn/material/05_Class-II.html#caravan-data-and-knn",
    "href": "teaching/stat-learn/material/05_Class-II.html#caravan-data-and-knn",
    "title": "Classification Methods II",
    "section": "",
    "text": "To continue our dive into classification methods we will take a loook at the Insurance dataset stored as “Caravan”.\nThis data set includes 85 predictors that measure demographic characteristics for 5,822 individuals. The response variable is Purchase, which indicates whether or not a given individual purchases a caravan insurance policy. In this data set, only 6% of people purchased caravan insurance.\n\nlibrary(ISLR2)\nlibrary(class)\nattach(Caravan)\ndim(Caravan)\n\n[1] 5822   86\n\n\n\nsummary(Caravan$Purchase)\n\n  No  Yes \n5474  348 \n\n348 / 5474\n\n[1] 0.06357326\n\n\nBecause the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale. For instance, imagine a data set that contains two variables, salary and age (measured in dollars and years, respectively). As far as KNN is concerned, a difference of $1,000 in salary is enormous compared to a difference of 50 years in age. Consequently, salary will drive the KNN classification results, and age will have almost no effect. This is contrary to our intuition that a salary difference of $1,000 is quite small compared to an age difference of 50 years. Furthermore, the importance of scale to the KNN classifier leads to another issue: if we measured salary in Japanese yen, or if we measured age in minutes, then we’d get quite different classification results from what we get if these two variables are measured in dollars and years.\nA good way to handle this problem is to standardize the data so that all variables are given a mean of zero and a standard deviation of one. Then all variables will be on a comparable scale. The scale() function does just this. In standardizing the data, we exclude column 86, because that is the qualitative Purchase variable.\n\nstandardized.X &lt;- scale(Caravan[, -86])\ncat(\"Unstandardized:\\n\")\n\nUnstandardized:\n\nvar(Caravan[, 1])\n\n[1] 165.0378\n\nvar(Caravan[, 2])\n\n[1] 0.1647078\n\ncat(\"Standardized:\\n\")\n\nStandardized:\n\nvar(standardized.X[, 1])\n\n[1] 1\n\nvar(standardized.X[, 2])\n\n[1] 1\n\n\nNow every column of standardized.X has a standard deviation of one and a mean of zero.\nWe now split the observations into a test set, containing the first 1,000 observations, and a training set, containing the remaining observations. We fit a KNN model on the training data using K=1, and evaluate its performance on the test data.\n\ntest &lt;- 1:1000\ntrain.X &lt;- standardized.X[-test, ]\ntest.X &lt;- standardized.X[test, ]\ntrain.Y &lt;- Purchase[-test]\ntest.Y &lt;- Purchase[test]\nset.seed(1)\nknn.pred &lt;- knn(train.X, test.X, train.Y, k = 1)\nmean(test.Y != knn.pred)\n\n[1] 0.118\n\n\n\ntable(knn.pred, test.Y)\n\n        test.Y\nknn.pred  No Yes\n     No  873  50\n     Yes  68   9\n\n9 / (68 + 9)\n\n[1] 0.1168831\n\n\nThis looks like a nice result - we have high accuracy. But do we care about how many people are not buying insurance in this example?\n\n\nTune your hyperparameter k and compare to our previous result. Also Compare to random guessing (50:50)"
  },
  {
    "objectID": "teaching/stat-learn/material/05_Class-II.html#lda-with-the-stocks-data",
    "href": "teaching/stat-learn/material/05_Class-II.html#lda-with-the-stocks-data",
    "title": "Classification Methods II",
    "section": "",
    "text": "Now we will perform LDA on the Smarket data. In R, we fit an LDA model using the lda() function, which is part of the MASS library. Notice that the syntax for the lda() function is identical to that of lm(), and to that of glm() except for the absence of the family option. We fit the model using only the observations before 2005.\n\nattach(Smarket)\ntrain &lt;- (Year &lt; 2005)\nSmarket.2005 &lt;- Smarket[!train, ]\ndim(Smarket.2005)\n\n[1] 252   9\n\nDirection.2005 &lt;- Direction[!train]\n\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:ISLR2':\n\n    Boston\n\nlda.fit &lt;- lda(Direction ~ Lag1 + Lag2, data = Smarket,\n    subset = train)\nlda.fit\n\nCall:\nlda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\nCoefficients of linear discriminants:\n            LD1\nLag1 -0.6420190\nLag2 -0.5135293\n\n\n\nplot(lda.fit)\n\n\n\n\n\n\n\n\nThe LDA output indicates that \\(\\hat\\pi_1=0.492\\) and \\(\\hat\\pi_2=0.508\\); in other words, \\(49.2\\) % of the training observations correspond to days during which the market went down. It also provides the group means; these are the average of each predictor within each class, and are used by LDA as estimates of \\(\\mu_k\\). These suggest that there is a tendency for the previous 2~days’ returns to be negative on days when the market increases, and a tendency for the previous days’ returns to be positive on days when the market declines. The coefficients of linear discriminants output provides the linear combination of lagone and lagtwo that are used to form the LDA decision rule. In other words, these are the multipliers of the elements of \\(X=x\\) in (4.24). If $-0.642 \\(`lagone`\\) - 0.514 $lagtwo is large, then the LDA classifier will predict a market increase, and if it is small, then the LDA classifier will predict a market decline.\nThe plot() function produces plots of the linear discriminants, obtained by computing $-0.642 \\(`lagone`\\) - 0.514 $lagtwo for each of the training observations. The Up and Down observations are displayed separately.\nThe predict() function returns a list with three elements. The first element, class, contains LDA’s predictions about the movement of the market. The second element, posterior, is a matrix whose \\(k\\)th column contains the posterior probability that the corresponding observation belongs to the \\(k\\)th class, computed from (4.15). Finally, x contains the linear discriminants, described earlier.\n\nlda.pred &lt;- predict(lda.fit, Smarket.2005)\nnames(lda.pred)\n\n[1] \"class\"     \"posterior\" \"x\"        \n\n\nAs we observed in Section 4.5, the LDA and logistic regression predictions are almost identical.\n\nlda.class &lt;- lda.pred$class\ntable(lda.class, Direction.2005)\n\n         Direction.2005\nlda.class Down  Up\n     Down   35  35\n     Up     76 106\n\nmean(lda.class == Direction.2005)\n\n[1] 0.5595238\n\n\nApplying a \\(50\\) % threshold to the posterior probabilities allows us to recreate the predictions contained in lda.pred$class.\n\nsum(lda.pred$posterior[, 1] &gt;= .5)\n\n[1] 70\n\nsum(lda.pred$posterior[, 1] &lt; .5)\n\n[1] 182\n\n\nNotice that the posterior probability output by the model corresponds to the probability that the market will decrease:\n\nlda.pred$posterior[1:20, 1]\n\n      999      1000      1001      1002      1003      1004      1005      1006 \n0.4901792 0.4792185 0.4668185 0.4740011 0.4927877 0.4938562 0.4951016 0.4872861 \n     1007      1008      1009      1010      1011      1012      1013      1014 \n0.4907013 0.4844026 0.4906963 0.5119988 0.4895152 0.4706761 0.4744593 0.4799583 \n     1015      1016      1017      1018 \n0.4935775 0.5030894 0.4978806 0.4886331 \n\nlda.class[1:20]\n\n [1] Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Down Up   Up   Up  \n[16] Up   Up   Down Up   Up  \nLevels: Down Up\n\n\nIf we wanted to use a posterior probability threshold other than \\(50\\) % in order to make predictions, then we could easily do so. For instance, suppose that we wish to predict a market decrease only if we are very certain that the market will indeed decrease on that day—say, if the posterior probability is at least \\(90\\) %.\n\nsum(lda.pred$posterior[, 1] &gt; .9)\n\n[1] 0\n\n\nNo days in 2005 meet that threshold! In fact, the greatest posterior probability of decrease in all of 2005 was \\(52.02\\)"
  },
  {
    "objectID": "teaching/stat-learn/material/05_Class-II.html#quadratic-discriminant-analysis",
    "href": "teaching/stat-learn/material/05_Class-II.html#quadratic-discriminant-analysis",
    "title": "Classification Methods II",
    "section": "",
    "text": "We will now fit a QDA model to the Smarket data. QDA is implemented in R using the qda() function, which is also part of the MASS library. The syntax is identical to that of lda().\n\nqda.fit &lt;- qda(Direction ~ Lag1 + Lag2, data = Smarket,\n    subset = train)\nqda.fit\n\nCall:\nqda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\n\nThe output contains the group means. But it does not contain the coefficients of the linear discriminants, because the QDA classifier involves a quadratic, rather than a linear, function of the predictors. The predict() function works in exactly the same fashion as for LDA.\n\nqda.class &lt;- predict(qda.fit, Smarket.2005)$class\ntable(qda.class, Direction.2005)\n\n         Direction.2005\nqda.class Down  Up\n     Down   30  20\n     Up     81 121\n\nmean(qda.class == Direction.2005)\n\n[1] 0.5992063\n\n\nInterestingly, the QDA predictions are accurate almost \\(60\\) % of the time, even though the 2005 data was not used to fit the model. This level of accuracy is quite impressive for stock market data, which is known to be quite hard to model accurately. This suggests that the quadratic form assumed by QDA may capture the true relationship more accurately than the linear forms assumed by LDA and logistic regression. However, we recommend evaluating this method’s performance on a larger test set before betting that this approach will consistently beat the market!"
  },
  {
    "objectID": "teaching/stat-learn/material/05_Class-II.html#naive-bayes",
    "href": "teaching/stat-learn/material/05_Class-II.html#naive-bayes",
    "title": "Classification Methods II",
    "section": "",
    "text": "Next, we fit a naive Bayes model to the Smarket data. Naive Bayes is implemented in R using the naiveBayes() function, which is part of the e1071 library. The syntax is identical to that of lda() and qda(). By default, this implementation of the naive Bayes classifier models each quantitative feature using a Gaussian distribution. However, a kernel density method can also be used to estimate the distributions.\n\n# install.packages(\"e1071\")\nlibrary(e1071)\nnb.fit &lt;- naiveBayes(Direction ~ Lag1 + Lag2, data = Smarket,\n    subset = train)\nnb.fit\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n    Down       Up \n0.491984 0.508016 \n\nConditional probabilities:\n      Lag1\nY             [,1]     [,2]\n  Down  0.04279022 1.227446\n  Up   -0.03954635 1.231668\n\n      Lag2\nY             [,1]     [,2]\n  Down  0.03389409 1.239191\n  Up   -0.03132544 1.220765\n\n\nThe output contains the estimated mean and standard deviation for each variable in each class. For example, the mean for lagone is \\(0.0428\\) for\nDirection=Down, and the standard deviation is \\(1.23\\). We can easily verify this:\n\nmean(Lag1[train][Direction[train] == \"Down\"])\n\n[1] 0.04279022\n\nsd(Lag1[train][Direction[train] == \"Down\"])\n\n[1] 1.227446\n\n\nThe predict() function is straightforward.\n\nnb.class &lt;- predict(nb.fit, Smarket.2005)\ntable(nb.class, Direction.2005)\n\n        Direction.2005\nnb.class Down  Up\n    Down   28  20\n    Up     83 121\n\nmean(nb.class == Direction.2005)\n\n[1] 0.5912698\n\n\nNaive Bayes performs very well on this data, with accurate predictions over \\(59\\%\\) of the time. This is slightly worse than QDA, but much better than LDA.\nThe predict() function can also generate estimates of the probability that each observation belongs to a particular class. %\n\nnb.preds &lt;- predict(nb.fit, Smarket.2005, type = \"raw\")\nnb.preds[1:5, ]\n\n          Down        Up\n[1,] 0.4873164 0.5126836\n[2,] 0.4762492 0.5237508\n[3,] 0.4653377 0.5346623\n[4,] 0.4748652 0.5251348\n[5,] 0.4901890 0.5098110"
  }
]