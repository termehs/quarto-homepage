[
  {
    "objectID": "rpackage/multigraphr/index.html",
    "href": "rpackage/multigraphr/index.html",
    "title": "multigraphr",
    "section": "",
    "text": "Termeh Shafie"
  },
  {
    "objectID": "rpackage/index.html",
    "href": "rpackage/index.html",
    "title": "R packages",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights-ws.html",
    "href": "teaching/tidyverse-I/material/nyc-flights-ws.html",
    "title": "NYC flights",
    "section": "",
    "text": "For the below exercies we use the tidyverse package for much of the data wrangling and visualisation and the data lives in the nycflights13 package. This is data a dataset of flights departing from New York City (NYC) airports in the year 2013.\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\nThe data sets available in the package can be viewed using the following syntax:\nYou will need one or combinations of these to solve the following exercises. These tables are organized as the figure below shows."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights-ws.html#packages-and-data",
    "href": "teaching/tidyverse-I/material/nyc-flights-ws.html#packages-and-data",
    "title": "NYC flights",
    "section": "",
    "text": "For the below exercies we use the tidyverse package for much of the data wrangling and visualisation and the data lives in the nycflights13 package. This is data a dataset of flights departing from New York City (NYC) airports in the year 2013.\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\nThe data sets available in the package can be viewed using the following syntax:\nYou will need one or combinations of these to solve the following exercises. These tables are organized as the figure below shows."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights-ws.html#familiarizing-ourselves-with-the-dataset",
    "href": "teaching/tidyverse-I/material/nyc-flights-ws.html#familiarizing-ourselves-with-the-dataset",
    "title": "NYC flights",
    "section": "Familiarizing ourselves with the dataset",
    "text": "Familiarizing ourselves with the dataset\n\nWhat variables are included in the flights dataset? How many rows are there?\nWhat variables are included in the airports dataset? How many rows are there?\nWhich variables are included in the airlines dataset? How many rows are there?"
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights-ws.html#focusing-on-atlanta",
    "href": "teaching/tidyverse-I/material/nyc-flights-ws.html#focusing-on-atlanta",
    "title": "NYC flights",
    "section": "Focusing on Atlanta",
    "text": "Focusing on Atlanta\n\nLet’s focus on flights from NYC area airports to Atlanta GA (FAA code ATL). Create a new object atlanta that includes only these flights. Hint: use filter()). How many flights to Atlanta were there in 2013?"
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights-ws.html#seasonality",
    "href": "teaching/tidyverse-I/material/nyc-flights-ws.html#seasonality",
    "title": "NYC flights",
    "section": "Seasonality",
    "text": "Seasonality\n\nIs there a difference in the number of flights per month?\nSummarize the number of flights for each month and provide a sorted list with the months with the most flights first. Hint: use group_by() in combination with summarize())."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights-ws.html#use-filter",
    "href": "teaching/tidyverse-I/material/nyc-flights-ws.html#use-filter",
    "title": "NYC flights",
    "section": "Use filter()",
    "text": "Use filter()\n\nFind all flights that\n\n\nHad an arrival delay of two or more hours.\nFlew to Houston (IAH or HOU)\nWere operated by United, American, or Delta. Hint: In the flights dataset, the column carrier indicates the airline, but it uses two-character carrier codes. You can find the carrier codes for the airlines in the airlines dataset. Since the carrier code dataset only has 16 rows, and the names of the airlines in that dataset are not exactly “United”, “American”, or “Delta”, it is easiest to manually look up their carrier codes in that data.\nDeparted in summer (July, August, and September). Hint: the summer flights are those that departed in months 7 (July), 8 (August), and 9 (September).\nArrived more than two hours late, but didn’t leave late. Hint: Flights that arrived more than two hours late, but didn’t leave late will have an arrival delay of more than 120 minutes (arr_delay &gt; 120) and a non-positive departure delay (dep_delay &lt;=0)\nWere delayed by at least an hour, but made up over 30 minutes in flight. Hint: If a flight was delayed by at least an hour, then dep_delay &gt;= 60. If the flight didn’t make up any time in the air, then its arrival would be delayed by the same amount as its departure, meaning dep_delay == arr_delay, or alternatively, dep_delay - arr_delay == 0. If it makes up over 30 minutes in the air, then the arrival delay must be at least 30 minutes less than the departure delay, which is stated as dep_delay - arr_delay &gt; 30.\nDeparted between midnight and 6 am (inclusive). Hint: In dep_time, midnight is represented by 2400, not 0. You can verify this by checking the minimum and maximum of dep_time."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights-ws.html#arrange-rows-with-arrange",
    "href": "teaching/tidyverse-I/material/nyc-flights-ws.html#arrange-rows-with-arrange",
    "title": "NYC flights",
    "section": "Arrange rows with arrange()",
    "text": "Arrange rows with arrange()\n\nHow could you use arrange() to sort all missing values to the start? Hint: use is.na()) and add an indicator of whether the column has a missing value, the flights will first be sorted by desc(is.na(dep_time)). Since desc(is.na(dep_time)) is either TRUE when dep_time is missing, or FALSE, when it is not, the rows with missing values of dep_time will come first, since TRUE &gt; FALSE.\nSort flights to find the most delayed flights. Find the flights that left earliest.\nSort flights to find the fastest flights."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights-ws.html#seelct-variables-with-select",
    "href": "teaching/tidyverse-I/material/nyc-flights-ws.html#seelct-variables-with-select",
    "title": "NYC flights",
    "section": "Seelct variables with select()",
    "text": "Seelct variables with select()\n\nWhat does the one_of() function do? Why might it be helpful in conjunction with this vector?"
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights-ws.html#add-new-variables-with-mutate",
    "href": "teaching/tidyverse-I/material/nyc-flights-ws.html#add-new-variables-with-mutate",
    "title": "NYC flights",
    "section": "Add new variables with mutate()",
    "text": "Add new variables with mutate()\n\nCompare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it?\nCome up with another approach that will give you the same output as not_cancelled %&gt;% count(dest) and not_cancelled %&gt;% count(tailnum, wt = distance) (without using count()).\nLook at the number of cancelled flights per day. Is there a pattern? Create a plot to visualize your answers.\nFor each destination, compute the total minutes of delay. For each flight, compute the proportion of the total delay for its destination.\nDelays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag() to explore how the delay of a flight is related to the delay of the immediately preceding flight. Use a plot to visualize this."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights-ws.html#more-viz",
    "href": "teaching/tidyverse-I/material/nyc-flights-ws.html#more-viz",
    "title": "NYC flights",
    "section": "More Viz",
    "text": "More Viz\n\nVisualize the distribution of on time departure rate across the three airports using a segmented bar plot. Hint: Remove NA’s and suppose that a flight that is delayed for less than 5 minutes is basically “on time”."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights-ws.html#advanced-exercises",
    "href": "teaching/tidyverse-I/material/nyc-flights-ws.html#advanced-exercises",
    "title": "NYC flights",
    "section": "Advanced Exercises:",
    "text": "Advanced Exercises:\n\nImagine you wanted to draw (approximately) the route each plane flies from its origin to its destination. What variables would you need? What tables from the package you loaded would you need to combine?\nThis plots the approximate flight paths of the first 100 flights in the flights dataset. Try reproducing it. Hint: you can create a layer of map borders using borders(state).\nWe know that some days of the year are “special”, and fewer people than usual fly on them. Since it is US data for 2013 we will consider: New Years Day, Independence Day, Thanksgiving Day, Christmas Day.\n\nHow might you represent that data as a data frame? What would be the primary keys of that table? How would it connect to the existing tables?\nWe can add a table of special dates, similar to the following table.\n\nspecial_days &lt;- tribble(\n  ~year, ~month, ~day, ~holiday,\n  2013, 01, 01, \"New Years Day\",\n  2013, 07, 04, \"Independence Day\",\n  2013, 11, 29, \"Thanksgiving Day\",\n  2013, 12, 25, \"Christmas Day\"\n)\n\nThe primary key of the table would be the (year, month, day) columns. The (year, month, day) columns could be used to join special_days with other tables.\n\nCreate a visualization fo your own to illustrate if indeed fewer people than usual fly on the above special days.\nCompute the average delay by destination, then join on the airports data frame so you can show the spatial distribution of delays. Here’s an easy way to draw a map of the United States (can you understand why we choose semi-join?):\n\n\nairports %&gt;%\n  semi_join(flights, c(\"faa\" = \"dest\")) %&gt;%\n  ggplot(aes(lon, lat)) +\n  borders(\"state\") +\n  geom_point() +\n  coord_quickmap() + \n  theme_void()\n\n\n\n\n\n\n\n\nHint: You might want to use the size or color of the points to display the average delay for each airport.\n\nWhat weather conditions make it more likely to see a delay? Use the variable precip (precipitation) from the weather dataset to answer this.\nWhat happened on June 13, 2013? Reproduce the following plot which displays the spatial pattern of delays, and then use Google to cross-reference with the weather. Hint: use library(viridis) to get the same colors."
  },
  {
    "objectID": "teaching/tidyverse-I/material/la-quinta.html",
    "href": "teaching/tidyverse-I/material/la-quinta.html",
    "title": "La Quinta is Spanish for next to Denny’s",
    "section": "",
    "text": "Have you ever taken a road trip in the US and thought to yourself “I wonder what La Quinta means”. Well, the late comedian Mitch Hedberg thinks it’s Spanish for next to Denny’s.\nIf you’re not familiar with these two establishments, Denny’s is a casual diner chain that is open 24 hours and La Quinta Inn and Suites is a hotel chain.\nThese two establishments tend to be clustered together, or at least this observation is a joke made famous by Mitch Hedberg. In this lab we explore the validity of this joke and along the way learn some more data wrangling and tips for visualizing spatial data.\nThe inspiration for this comes from a blog post by John Reiser on his new jersey geographer blog. You can read that analysis here. Reiser’s blog post focuses on scraping data from Denny’s and La Quinta Inn and Suites websites using Python. Here, we focus on visualization and analysis of these data."
  },
  {
    "objectID": "teaching/tidyverse-I/material/la-quinta.html#packages",
    "href": "teaching/tidyverse-I/material/la-quinta.html#packages",
    "title": "La Quinta is Spanish for next to Denny’s",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for much of the data wrangling and visualisation and the data lives in the dsbox package. These packages are already installed for you. You can load them by running the following in your Console:\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "teaching/tidyverse-I/material/la-quinta.html#data",
    "href": "teaching/tidyverse-I/material/la-quinta.html#data",
    "title": "La Quinta is Spanish for next to Denny’s",
    "section": "Data",
    "text": "Data\nThe data sets we’ll use are called dennys and laquinta and are available for download. Note that these data were scraped from here and here, respectively. You can find information about the data sets here and here. To help with our analysis we will also use a data set on US states.\n\nlaquinta &lt;- read_csv(\"data/laquinta.csv\")\ndennys &lt;- read_csv(\"data/dennys.csv\")\nstates &lt;- read_csv(\"data/states.csv\")\n\nEach observation in the states dataset represents a state, including DC. Along with the name of the state we have the two-letter abbreviation and we have the geographic area of the state (in square miles)."
  },
  {
    "objectID": "teaching/tidyverse-I/material/type-coercion.html",
    "href": "teaching/tidyverse-I/material/type-coercion.html",
    "title": "Type coercion",
    "section": "",
    "text": "c(1, 1L, \"C\")\n\n\nc(1, 1L, \"C\")\n\n[1] \"1\" \"1\" \"C\"\n\n\n\n1\n\n[1] 1\n\n1L\n\n[1] 1\n\n\"C\"\n\n[1] \"C\"\n\n\n\n#typeof(c(1, 1L, \"C\"))\n\n\nc(1L / 0, \"A\")\n\n\nc(1L / 0, \"A\")\n\n[1] \"Inf\" \"A\"  \n\n\n\ntypeof(1L)\n\n[1] \"integer\"\n\ntypeof(0)\n\n[1] \"double\"\n\ntypeof(1L/0)\n\n[1] \"double\"\n\ntypeof(\"A\")\n\n[1] \"character\"\n\n\n\n#typeof(c(1L / 0, \"A\"))\n\n\nc(1:3, 5)\n\n\nc(1:3, 5)\n\n[1] 1 2 3 5\n\n\n\ntypeof(1:3)\n\n[1] \"integer\"\n\ntypeof(5)\n\n[1] \"double\"\n\n\n\n#typeof(c(1:3, 5))\n\n\nc(3, \"3+\")\n\n\nc(3, \"3+\")\n\n[1] \"3\"  \"3+\"\n\n\n\ntypeof(3)\n\n[1] \"double\"\n\ntypeof(\"3+\")\n\n[1] \"character\"\n\n\n\n#typeof(c(3, \"3+\"))\n\n\nc(NA, TRUE)\n\n\nc(NA, TRUE)\n\n[1]   NA TRUE\n\n\n\ntypeof(NA)\n\n[1] \"logical\"\n\ntypeof(TRUE)\n\n[1] \"logical\"\n\n\n\n#typeof(c(NA, TRUE))"
  },
  {
    "objectID": "teaching/tidyverse-I/material/plastic-waste.html",
    "href": "teaching/tidyverse-I/material/plastic-waste.html",
    "title": "Global plastic waste",
    "section": "",
    "text": "Plastic pollution is a major and growing problem, negatively affecting oceans and wildlife health. Our World in Data has a lot of great data at various levels including globally, per country, and over time. For this lab we focus on data from 2010.\nAdditionally, National Geographic ran a data visualization communication contest on plastic waste as seen here."
  },
  {
    "objectID": "teaching/tidyverse-I/material/plastic-waste.html#packages",
    "href": "teaching/tidyverse-I/material/plastic-waste.html#packages",
    "title": "Global plastic waste",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for this analysis.\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "teaching/tidyverse-I/material/plastic-waste.html#data",
    "href": "teaching/tidyverse-I/material/plastic-waste.html#data",
    "title": "Global plastic waste",
    "section": "Data",
    "text": "Data\nThe dataset for this assignment can be found as a csv file. You can read it in using the following (make sure you save the data in your working directory).\n\nplastic_waste &lt;- read_csv(\"data-pw/plastic-waste.csv\")\n\nThe variable descriptions are as follows:\n\ncode: 3 Letter country code\nentity: Country name\ncontinent: Continent name\nyear: Year\ngdp_per_cap: GDP per capita constant 2011 international $, rate\nplastic_waste_per_cap: Amount of plastic waste per capita in kg/day\nmismanaged_plastic_waste_per_cap: Amount of mismanaged plastic waste per capita in kg/day\nmismanaged_plastic_waste: Tonnes of mismanaged plastic waste\ncoastal_pop: Number of individuals living on/near coast\ntotal_pop: Total population according to Gapminder"
  },
  {
    "objectID": "teaching/tidyverse-I/material/nobels-csv.html",
    "href": "teaching/tidyverse-I/material/nobels-csv.html",
    "title": "Nobel winners",
    "section": "",
    "text": "library(tidyverse)\n\nLet’s first load the data:\n\nnobel &lt;- ___(___)\n\nThen let’s split the data into two:\n\n# stem laureates\n___ &lt;- nobel %&gt;%\n  filter(___)\n\n# non-steam laureates\n___ &lt;- nobel %&gt;%\n  filter(___)\n\nAnd finally write out the data:\n\n# add code for writing out the two data frames here"
  },
  {
    "objectID": "teaching/tidyverse-I/material/brexit-ws.html",
    "href": "teaching/tidyverse-I/material/brexit-ws.html",
    "title": "Brexit",
    "section": "",
    "text": "library(tidyverse)\n\nIn September 2019, YouGov survey asked 1,639 GB adults the following question:\n\nIn hindsight, do you think Britain was right/wrong to vote to leave EU?\n\nRight to leave\n\nWrong to leave\n\nDon’t know\n\n\nThe data from the survey is in data/brexit.csv.\n\nbrexit &lt;- read_csv(\"data-brexit/brexit.csv\")\n\nIn the course video we made the following visualisation.\n\nbrexit &lt;- brexit %&gt;%\n  mutate(\n    region = fct_relevel(region, \"london\", \"rest_of_south\", \"midlands_wales\", \"north\", \"scot\"),\n    region = fct_recode(region, London = \"london\", `Rest of South` = \"rest_of_south\", `Midlands / Wales` = \"midlands_wales\", North = \"north\", Scotland = \"scot\")\n  )\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  ) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\",\n    \"Right\" = \"#67a9cf\",\n    \"Don't know\" = \"gray\"\n  )) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn this application exercise we tell different stories with the same data.\n\nExercise 1 - Free scales\nAdd scales = \"free_x\" as an argument to the facet_wrap() function. How does the visualisation change? How is the story this visualisation telling different than the story the original plot tells?\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region,\n    nrow = 1, labeller = label_wrap_gen(width = 12),\n    # ___\n  ) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  ) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\",\n    \"Right\" = \"#67a9cf\",\n    \"Don't know\" = \"gray\"\n  )) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nExercise 2 - Comparing proportions across facets\nFirst, calculate the proportion of wrong, right, and don’t know answers in each category and then plot these proportions (rather than the counts) and then improve axis labeling. How is the story this visualisation telling different than the story the original plot tells? Hint: You’ll need the scales package to improve axis labeling, which means you’ll need to load it on top of the document as well.\n\n# code goes here\n\n\n\nExercise 3 - Comparing proportions across bars\nRecreate the same visualisation from the previous exercise, this time dodging the bars for opinion proportions for each region, rather than faceting by region and then improve the legend. How is the story this visualisation telling different than the story the previous plot tells?\n\n# code goes here"
  },
  {
    "objectID": "teaching/tidyverse-I/material/hotels-forcats.html",
    "href": "teaching/tidyverse-I/material/hotels-forcats.html",
    "title": "Hotel bookings - factors",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)\n\nLoad the hotels data set we used in a previous practical. Render and view the following visualisation. How are the months ordered? What would be a better order? Then, reorder the months on the x-axis (levels of arrival_date_month) in a way that makes more sense. You will want to use a function from the forcats package, see https://forcats.tidyverse.org/reference/index.html for inspiration and help.\nStretch goal: If you finish the above task before time is up, change the y-axis label so the values are shown with dollar signs, e.g. $80 instead of 80. You will want to use a function from the scales package, see https://scales.r-lib.org/reference/index.html for inspiration and help.\n\nhotels %&gt;%\n  group_by(hotel, arrival_date_month) %&gt;%   # group by hotel type and arrival month\n  summarise(mean_adr = mean(adr)) %&gt;%       # calculate mean adr for each group\n  ggplot(aes(\n    x = arrival_date_month,                 # x-axis = arrival_date_month\n    y = mean_adr,                           # y-axis = mean_adr calculated above\n    group = hotel,                          # group lines by hotel type\n    color = hotel)                          # and color by hotel type\n    ) +\n  geom_line() +                             # use lines to represent data\n  theme_minimal() +                         # use a minimal theme\n  labs(\n    x = \"Arrival month\",                 # customize labels\n    y = \"Mean ADR (average daily rate)\",\n    title = \"Comparison of resort and city hotel prices across months\",\n    subtitle = \"Resort hotel prices soar in the summer while ciry hotel prices remain relatively constant throughout the year\",\n    color = \"Hotel type\"\n    )"
  },
  {
    "objectID": "teaching/tidyverse-I/material/bechdel-ws.html",
    "href": "teaching/tidyverse-I/material/bechdel-ws.html",
    "title": "Bechdel",
    "section": "",
    "text": "In this mini analysis we work with the data used in the FiveThirtyEight story titled “The Dollar-And-Cents Case Against Hollywood’s Exclusion of Women”. We will together fill in the blanks denoted by ___."
  },
  {
    "objectID": "teaching/tidyverse-I/material/bechdel-ws.html#data-and-packages",
    "href": "teaching/tidyverse-I/material/bechdel-ws.html#data-and-packages",
    "title": "Bechdel",
    "section": "Data and packages",
    "text": "Data and packages\nWe start with loading the packages we’ll use.\n\nlibrary(fivethirtyeight)\nlibrary(tidyverse)\n\nThe dataset contains information on 1794 movies released between 1970 and 2013. However we’ll focus our analysis on movies released between 1990 and 2013.\n\nbechdel90_13 &lt;- bechdel %&gt;% \n  filter(between(year, 1990, 2013))\n\nThere are ___ such movies.\nThe financial variables we’ll focus on are the following:\n\nbudget_2013: Budget in 2013 inflation adjusted dollars\ndomgross_2013: Domestic gross (US) in 2013 inflation adjusted dollars\nintgross_2013: Total International (i.e., worldwide) gross in 2013 inflation adjusted dollars\n\nAnd we’ll also use the binary and clean_test variables for grouping."
  },
  {
    "objectID": "teaching/tidyverse-I/material/bechdel-ws.html#analysis",
    "href": "teaching/tidyverse-I/material/bechdel-ws.html#analysis",
    "title": "Bechdel",
    "section": "Analysis",
    "text": "Analysis\nLet’s take a look at how median budget and gross vary by whether the movie passed the Bechdel test, which is stored in the binary variable.\n\nbechdel90_13 %&gt;%\n  group_by(binary) %&gt;%\n  summarise(\n    med_budget = median(budget_2013),\n    med_domgross = median(domgross_2013, na.rm = TRUE),\n    med_intgross = median(intgross_2013, na.rm = TRUE)\n    )\n\n# A tibble: 2 × 4\n  binary med_budget med_domgross med_intgross\n  &lt;chr&gt;       &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 FAIL    48385984.    57318606.    104475669\n2 PASS    31070724     45330446.     80124349\n\n\nNext, let’s take a look at how median budget and gross vary by a more detailed indicator of the Bechdel test result. This information is stored in the clean_test variable, which takes on the following values:\n\nok = passes test\ndubious\nmen = women only talk about men\nnotalk = women don’t talk to each other\nnowomen = fewer than two women\n\n\nbechdel90_13 %&gt;%\n  #group_by(___) %&gt;%\n  summarise(\n    med_budget = median(budget_2013),\n    med_domgross = median(domgross_2013, na.rm = TRUE),\n    med_intgross = median(intgross_2013, na.rm = TRUE)\n    )\n\n# A tibble: 1 × 3\n  med_budget med_domgross med_intgross\n       &lt;int&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1   37878971     52270207     93523336\n\n\nIn order to evaluate how return on investment varies among movies that pass and fail the Bechdel test, we’ll first create a new variable called roi as the ratio of the gross to budget.\n\nbechdel90_13 &lt;- bechdel90_13 %&gt;%\n  mutate(roi = (intgross_2013 + domgross_2013) / budget_2013)\n\nLet’s see which movies have the highest return on investment.\n\nbechdel90_13 %&gt;%\n  arrange(desc(roi)) %&gt;% \n  select(title, roi, year)\n\n# A tibble: 1,615 × 3\n   title                     roi  year\n   &lt;chr&gt;                   &lt;dbl&gt; &lt;int&gt;\n 1 Paranormal Activity      671.  2007\n 2 The Blair Witch Project  648.  1999\n 3 El Mariachi              583.  1992\n 4 Clerks.                  258.  1994\n 5 In the Company of Men    231.  1997\n 6 Napoleon Dynamite        227.  2004\n 7 Once                     190.  2006\n 8 The Devil Inside         155.  2012\n 9 Primer                   142.  2004\n10 Fireproof                134.  2008\n# ℹ 1,605 more rows\n\n\nBelow is a visualization of the return on investment by test result, however it’s difficult to see the distributions due to a few extreme observations.\n\nggplot(data = bechdel90_13, \n       mapping = aes(x = clean_test, y = roi, color = binary)) +\n  geom_boxplot() +\n  labs(\n    title = \"Return on investment vs. Bechdel test result\",\n    x = \"Detailed Bechdel result\",\n    y = \"___\",\n    color = \"Binary Bechdel result\"\n    )\n\n\n\n\n\n\n\n\nWhat are those movies with very high returns on investment?\n\nbechdel90_13 %&gt;%\n  filter(roi &gt; 400) %&gt;%\n  select(title, budget_2013, domgross_2013, year)\n\n# A tibble: 3 × 4\n  title                   budget_2013 domgross_2013  year\n  &lt;chr&gt;                         &lt;int&gt;         &lt;dbl&gt; &lt;int&gt;\n1 Paranormal Activity          505595     121251476  2007\n2 The Blair Witch Project      839077     196538593  1999\n3 El Mariachi                   11622       3388636  1992\n\n\nZooming in on the movies with roi &lt; ___ provides a better view of how the medians across the categories compare:\n\nggplot(data = bechdel90_13, mapping = aes(x = clean_test, y = roi, color = binary)) +\n  geom_boxplot() +\n  labs(\n    title = \"Return on investment vs. Bechdel test result\",\n    subtitle = \"___\", # Something about zooming in to a certain level\n    x = \"Detailed Bechdel result\",\n    y = \"Return on investment\",\n    color = \"Binary Bechdel result\"\n    ) +\n  coord_cartesian(ylim = c(0, 15))"
  },
  {
    "objectID": "teaching/tidyverse-I/material/data-type-class-exercises.html",
    "href": "teaching/tidyverse-I/material/data-type-class-exercises.html",
    "title": "Data Type and Data Classes: Exercises",
    "section": "",
    "text": "Double check that you do not have stored objects in your current session with the following command. This will list all objects that you have in your current R session.\n\nls()\n\ncharacter(0)\n\n\nIn case you have objects that you want to remove from the current session you can do so with the rm() function.  This command will remove all objects available in your current environment.\n\nrm(list = ls())\n\nThis command uses commands that we have not talked about yet. If you do not understand how it works now, you will do so after tomorrows lectures and exercises.\n\nCreate variables var1 and var2 and initialize them with two integers of choice.\nAdd the two variables and save them as a new variable named var3 and print the result.\nCheck the class, mode, and type for var1, var2, var3 and π (is found under the variable name pi in R)\nCreate two character variables containing a text of choice. Check the mode, class, and type of the first one.\n\nAdd var1 to it. What is the result and why?\n\n\n\n\nConvert var3 to an integer, cast an integer variable to double, cast a string to a double.\nReport floor and ceiling of π and round π to 3 decimal places.\nIs floor of π an integer?\nTreat \"3.56437\" string as number.\nDivide ∞ by - ∞\nPrint a truth table for OR (for three distinct logical values). Read about truth tables here.\nMultiply a logical TRUE by a logical FALSE. Rise the logical true to the 7-th power.\nCreate two character variables containing two verses of a chosen song.\n\n\nConcatenate the two variables,\n\nPaste the variables with ‘*’ as separator.\n\nFind if ‘and’ occurs in the second line,\n\nSubstitute a word for another,\n\nExtract substring starting at the 5th character and 5 characters long."
  },
  {
    "objectID": "teaching/tidyverse-I/material/data-type-class-exercises.html#exercise",
    "href": "teaching/tidyverse-I/material/data-type-class-exercises.html#exercise",
    "title": "Data Type and Data Classes: Exercises",
    "section": "",
    "text": "Double check that you do not have stored objects in your current session with the following command. This will list all objects that you have in your current R session.\n\nls()\n\ncharacter(0)\n\n\nIn case you have objects that you want to remove from the current session you can do so with the rm() function.  This command will remove all objects available in your current environment.\n\nrm(list = ls())\n\nThis command uses commands that we have not talked about yet. If you do not understand how it works now, you will do so after tomorrows lectures and exercises.\n\nCreate variables var1 and var2 and initialize them with two integers of choice.\nAdd the two variables and save them as a new variable named var3 and print the result.\nCheck the class, mode, and type for var1, var2, var3 and π (is found under the variable name pi in R)\nCreate two character variables containing a text of choice. Check the mode, class, and type of the first one.\n\nAdd var1 to it. What is the result and why?\n\n\n\n\nConvert var3 to an integer, cast an integer variable to double, cast a string to a double.\nReport floor and ceiling of π and round π to 3 decimal places.\nIs floor of π an integer?\nTreat \"3.56437\" string as number.\nDivide ∞ by - ∞\nPrint a truth table for OR (for three distinct logical values). Read about truth tables here.\nMultiply a logical TRUE by a logical FALSE. Rise the logical true to the 7-th power.\nCreate two character variables containing two verses of a chosen song.\n\n\nConcatenate the two variables,\n\nPaste the variables with ‘*’ as separator.\n\nFind if ‘and’ occurs in the second line,\n\nSubstitute a word for another,\n\nExtract substring starting at the 5th character and 5 characters long."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights.html",
    "href": "teaching/tidyverse-I/material/nyc-flights.html",
    "title": "NYC flights",
    "section": "",
    "text": "For the below exercies we use the tidyverse package for much of the data wrangling and visualisation and the data lives in the nycflights13 package. This is data a dataset of flights departing from New York City (NYC) airports in the year 2013.\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\nThe data sets available in the package can be viewed using the following syntax:\nYou will need one or combinations of these to solve the following exercises. These tables are organized as the figure below shows."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights.html#packages-and-data",
    "href": "teaching/tidyverse-I/material/nyc-flights.html#packages-and-data",
    "title": "NYC flights",
    "section": "",
    "text": "For the below exercies we use the tidyverse package for much of the data wrangling and visualisation and the data lives in the nycflights13 package. This is data a dataset of flights departing from New York City (NYC) airports in the year 2013.\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\nThe data sets available in the package can be viewed using the following syntax:\nYou will need one or combinations of these to solve the following exercises. These tables are organized as the figure below shows."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights.html#familiarizing-ourselves-with-the-dataset",
    "href": "teaching/tidyverse-I/material/nyc-flights.html#familiarizing-ourselves-with-the-dataset",
    "title": "NYC flights",
    "section": "Familiarizing ourselves with the dataset",
    "text": "Familiarizing ourselves with the dataset\n\nWhat variables are included in the flights dataset? How many rows are there?\nWhat variables are included in the airports dataset? How many rows are there?\nWhich variables are included in the airlines dataset? How many rows are there?"
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights.html#focusing-on-atlanta",
    "href": "teaching/tidyverse-I/material/nyc-flights.html#focusing-on-atlanta",
    "title": "NYC flights",
    "section": "Focusing on Atlanta",
    "text": "Focusing on Atlanta\n\nLet’s focus on flights from NYC area airports to Atlanta GA (FAA code ATL). Create a new object atlanta that includes only these flights. Hint: use filter()). How many flights to Atlanta were there in 2013?"
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights.html#seasonality",
    "href": "teaching/tidyverse-I/material/nyc-flights.html#seasonality",
    "title": "NYC flights",
    "section": "Seasonality",
    "text": "Seasonality\n\nIs there a difference in the number of flights per month?\nSummarize the number of flights for each month and provide a sorted list with the months with the most flights first. Hint: use group_by() in combination with summarize())."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights.html#use-filter",
    "href": "teaching/tidyverse-I/material/nyc-flights.html#use-filter",
    "title": "NYC flights",
    "section": "Use filter()",
    "text": "Use filter()\n\nFind all flights that\n\n\nHad an arrival delay of two or more hours.\nFlew to Houston (IAH or HOU)\nWere operated by United, American, or Delta. Hint: In the flights dataset, the column carrier indicates the airline, but it uses two-character carrier codes. You can find the carrier codes for the airlines in the airlines dataset. Since the carrier code dataset only has 16 rows, and the names of the airlines in that dataset are not exactly “United”, “American”, or “Delta”, it is easiest to manually look up their carrier codes in that data.\nDeparted in summer (July, August, and September). Hint: the summer flights are those that departed in months 7 (July), 8 (August), and 9 (September).\nArrived more than two hours late, but didn’t leave late. Hint: Flights that arrived more than two hours late, but didn’t leave late will have an arrival delay of more than 120 minutes (arr_delay &gt; 120) and a non-positive departure delay (dep_delay &lt;=0)\nWere delayed by at least an hour, but made up over 30 minutes in flight. Hint: If a flight was delayed by at least an hour, then dep_delay &gt;= 60. If the flight didn’t make up any time in the air, then its arrival would be delayed by the same amount as its departure, meaning dep_delay == arr_delay, or alternatively, dep_delay - arr_delay == 0. If it makes up over 30 minutes in the air, then the arrival delay must be at least 30 minutes less than the departure delay, which is stated as dep_delay - arr_delay &gt; 30.\nDeparted between midnight and 6 am (inclusive). Hint: In dep_time, midnight is represented by 2400, not 0. You can verify this by checking the minimum and maximum of dep_time."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights.html#arrange-rows-with-arrange",
    "href": "teaching/tidyverse-I/material/nyc-flights.html#arrange-rows-with-arrange",
    "title": "NYC flights",
    "section": "Arrange rows with arrange()",
    "text": "Arrange rows with arrange()\n\nHow could you use arrange() to sort all missing values to the start? Hint: use is.na()) and add an indicator of whether the column has a missing value, the flights will first be sorted by desc(is.na(dep_time)). Since desc(is.na(dep_time)) is either TRUE when dep_time is missing, or FALSE, when it is not, the rows with missing values of dep_time will come first, since TRUE &gt; FALSE.\nSort flights to find the most delayed flights. Find the flights that left earliest.\nSort flights to find the fastest flights."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights.html#seelct-variables-with-select",
    "href": "teaching/tidyverse-I/material/nyc-flights.html#seelct-variables-with-select",
    "title": "NYC flights",
    "section": "Seelct variables with select()",
    "text": "Seelct variables with select()\n\nWhat does the one_of() function do? Why might it be helpful in conjunction with this vector?"
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights.html#add-new-variables-with-mutate",
    "href": "teaching/tidyverse-I/material/nyc-flights.html#add-new-variables-with-mutate",
    "title": "NYC flights",
    "section": "Add new variables with mutate()",
    "text": "Add new variables with mutate()\n\nCompare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it?\nCome up with another approach that will give you the same output as not_cancelled %&gt;% count(dest) and not_cancelled %&gt;% count(tailnum, wt = distance) (without using count()).\nLook at the number of cancelled flights per day. Is there a pattern? Create a plot to visualize your answers.\nFor each destination, compute the total minutes of delay. For each flight, compute the proportion of the total delay for its destination.\nDelays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag() to explore how the delay of a flight is related to the delay of the immediately preceding flight. Use a plot to visualize this."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights.html#more-viz",
    "href": "teaching/tidyverse-I/material/nyc-flights.html#more-viz",
    "title": "NYC flights",
    "section": "More Viz",
    "text": "More Viz\n\nVisualize the distribution of on time departure rate across the three airports using a segmented bar plot. Hint: Remove NA’s and suppose that a flight that is delayed for less than 5 minutes is basically “on time”."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights.html#advanced-exercises",
    "href": "teaching/tidyverse-I/material/nyc-flights.html#advanced-exercises",
    "title": "NYC flights",
    "section": "Advanced Exercises:",
    "text": "Advanced Exercises:\n\nImagine you wanted to draw (approximately) the route each plane flies from its origin to its destination. What variables would you need? What tables from the package you loaded would you need to combine?\nThis plots the approximate flight paths of the first 100 flights in the flights dataset. Try reproducing it. Hint: you can create a layer of map borders using borders(state).\n\n\n\n\n\n\n\n\n\n\n\nWe know that some days of the year are “special”, and fewer people than usual fly on them. Since it is US data for 2013 we will consider: New Years Day, Independence Day, Thanksgiving Day, Christmas Day.\n\nHow might you represent that data as a data frame? What would be the primary keys of that table? How would it connect to the existing tables?\nWe can add a table of special dates, similar to the following table.\n\nspecial_days &lt;- tribble(\n  ~year, ~month, ~day, ~holiday,\n  2013, 01, 01, \"New Years Day\",\n  2013, 07, 04, \"Independence Day\",\n  2013, 11, 29, \"Thanksgiving Day\",\n  2013, 12, 25, \"Christmas Day\"\n)\n\nThe primary key of the table would be the (year, month, day) columns. The (year, month, day) columns could be used to join special_days with other tables.\n\nCreate a visualization fo your own to illustrate if indeed fewer people than usual fly on the above special days.\nCompute the average delay by destination, then join on the airports data frame so you can show the spatial distribution of delays. Here’s an easy way to draw a map of the United States (can you understand why we choose semi-join?):\n\n\nairports %&gt;%\n  semi_join(flights, c(\"faa\" = \"dest\")) %&gt;%\n  ggplot(aes(lon, lat)) +\n  borders(\"state\") +\n  geom_point() +\n  coord_quickmap() + \n  theme_void()\n\n\n\n\n\n\n\n\nHint: You might want to use the size or color of the points to display the average delay for each airport.\n\nWhat weather conditions make it more likely to see a delay? Use the variable precip (precipitation) from the weather dataset to answer this.\nWhat happened on June 13, 2013? Reproduce the following plot which displays the spatial pattern of delays, and then use Google to cross-reference with the weather. Hint: use library(viridis) to get the same colors."
  },
  {
    "objectID": "teaching/tidyverse-I/material/hotels-datawrangling.html",
    "href": "teaching/tidyverse-I/material/hotels-datawrangling.html",
    "title": "Hotel bookings - data wrangling",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)\n# From TidyTuesday: https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-02-11/readme.md\nhotels &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\nThe data is also available as a csv file which you can import directly."
  },
  {
    "objectID": "teaching/tidyverse-I/material/hotels-datawrangling.html#exercises",
    "href": "teaching/tidyverse-I/material/hotels-datawrangling.html#exercises",
    "title": "Hotel bookings - data wrangling",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\nWarm up! Take a look at an overview of the data with the skim() function.\nNote: I already gave you the answer to this exercise. You just need to knit the document and view the output. A definition of all variables is given in the Data dictionary section at the end, though you don’t need to familiarize yourself with all variables in order to work through these exercises.\n\nskim(hotels)\n\n\nData summary\n\n\nName\nhotels\n\n\nNumber of rows\n119390\n\n\nNumber of columns\n32\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n13\n\n\nDate\n1\n\n\nnumeric\n18\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nhotel\n0\n1\n10\n12\n0\n2\n0\n\n\narrival_date_month\n0\n1\n3\n9\n0\n12\n0\n\n\nmeal\n0\n1\n2\n9\n0\n5\n0\n\n\ncountry\n0\n1\n2\n4\n0\n178\n0\n\n\nmarket_segment\n0\n1\n6\n13\n0\n8\n0\n\n\ndistribution_channel\n0\n1\n3\n9\n0\n5\n0\n\n\nreserved_room_type\n0\n1\n1\n1\n0\n10\n0\n\n\nassigned_room_type\n0\n1\n1\n1\n0\n12\n0\n\n\ndeposit_type\n0\n1\n10\n10\n0\n3\n0\n\n\nagent\n0\n1\n1\n4\n0\n334\n0\n\n\ncompany\n0\n1\n1\n4\n0\n353\n0\n\n\ncustomer_type\n0\n1\n5\n15\n0\n4\n0\n\n\nreservation_status\n0\n1\n7\n9\n0\n3\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nreservation_status_date\n0\n1\n2014-10-17\n2017-09-14\n2016-08-07\n926\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nis_canceled\n0\n1\n0.37\n0.48\n0.00\n0.00\n0.00\n1\n1\n▇▁▁▁▅\n\n\nlead_time\n0\n1\n104.01\n106.86\n0.00\n18.00\n69.00\n160\n737\n▇▂▁▁▁\n\n\narrival_date_year\n0\n1\n2016.16\n0.71\n2015.00\n2016.00\n2016.00\n2017\n2017\n▃▁▇▁▆\n\n\narrival_date_week_number\n0\n1\n27.17\n13.61\n1.00\n16.00\n28.00\n38\n53\n▅▇▇▇▅\n\n\narrival_date_day_of_month\n0\n1\n15.80\n8.78\n1.00\n8.00\n16.00\n23\n31\n▇▇▇▇▆\n\n\nstays_in_weekend_nights\n0\n1\n0.93\n1.00\n0.00\n0.00\n1.00\n2\n19\n▇▁▁▁▁\n\n\nstays_in_week_nights\n0\n1\n2.50\n1.91\n0.00\n1.00\n2.00\n3\n50\n▇▁▁▁▁\n\n\nadults\n0\n1\n1.86\n0.58\n0.00\n2.00\n2.00\n2\n55\n▇▁▁▁▁\n\n\nchildren\n4\n1\n0.10\n0.40\n0.00\n0.00\n0.00\n0\n10\n▇▁▁▁▁\n\n\nbabies\n0\n1\n0.01\n0.10\n0.00\n0.00\n0.00\n0\n10\n▇▁▁▁▁\n\n\nis_repeated_guest\n0\n1\n0.03\n0.18\n0.00\n0.00\n0.00\n0\n1\n▇▁▁▁▁\n\n\nprevious_cancellations\n0\n1\n0.09\n0.84\n0.00\n0.00\n0.00\n0\n26\n▇▁▁▁▁\n\n\nprevious_bookings_not_canceled\n0\n1\n0.14\n1.50\n0.00\n0.00\n0.00\n0\n72\n▇▁▁▁▁\n\n\nbooking_changes\n0\n1\n0.22\n0.65\n0.00\n0.00\n0.00\n0\n21\n▇▁▁▁▁\n\n\ndays_in_waiting_list\n0\n1\n2.32\n17.59\n0.00\n0.00\n0.00\n0\n391\n▇▁▁▁▁\n\n\nadr\n0\n1\n101.83\n50.54\n-6.38\n69.29\n94.58\n126\n5400\n▇▁▁▁▁\n\n\nrequired_car_parking_spaces\n0\n1\n0.06\n0.25\n0.00\n0.00\n0.00\n0\n8\n▇▁▁▁▁\n\n\ntotal_of_special_requests\n0\n1\n0.57\n0.79\n0.00\n0.00\n0.00\n1\n5\n▇▁▁▁▁\n\n\n\n\n\n\n\nExercise 2.\nAre people traveling on a whim? Let’s see…\nFill in the blanks for filtering for hotel bookings where the guest is not from the US (country code \"USA\") and the lead_time is less than 1 day.\n\nhotels %&gt;%\n  filter(\n    country ____ \"USA\", \n    lead_time ____ ____\n    )\n\n\n\nExercise 3.\nHow many bookings involve at least 1 child or baby?\nIn the following chunk, replace\n\n[AT LEAST] with the logical operator for “at least” (in two places)\n[OR] with the logical operator for “or”\n\nNote: You will need to set eval=TRUE when you have an answer you want to try out in the qmd file.\n\nhotels %&gt;%\n  filter(\n    children [AT LEAST] 1 [OR] babies [AT LEAST] 1\n    )\n\n\n\nExercise 4.\nDo you think it’s more likely to find bookings with children or babies in city hotels or resort hotels? Test your intuition. Using filter() determine the number of bookings in resort hotels that have more than 1 child or baby in the room? Then, do the same for city hotels, and compare the numbers of rows in the resulting filtered data frames.\n\n# add code here\n# pay attention to correctness and code style\n\n\n# add code here\n# pay attention to correctness and code style\n\n\n\nExercise 5.\nCreate a frequency table of the number of adults in a booking. Display the results in descending order so the most common observation is on top. What is the most common number of adults in bookings in this dataset? Are there any surprising results?\n\n# add code here\n# pay attention to correctness and code style\n\n\n\nExercise 6.\nRepeat Exercise 5, once for canceled bookings (is_canceled coded as 1) and once for not canceled bookings (is_canceled coded as 0). What does this reveal about the surprising results you spotted in the previous exercise?\n\n# add code here\n# pay attention to correctness and code style\n\n\n\nExercise 7.\nCalculate minimum, mean, median, and maximum average daily rate (adr) grouped by hotel type so that you can get these statistics separately for resort and city hotels. Which type of hotel is higher, on average?\n\n# add code here\n# pay attention to correctness and code style\n\n\n\nExercise 8.\nWe observe two unusual values in the summary statistics above – a negative minimum, and a very high maximum). What types of hotels are these? Locate these observations in the dataset and find out the arrival date (year and month) as well as how many people (adults, children, and babies) stayed in the room. You can investigate the data in the viewer to locate these values, but preferably you should identify them in a reproducible way with some code.\nHint: For example, you can filter for the given adr amounts and select the relevant columns.\n\n# add code here\n# pay attention to correctness and code style"
  },
  {
    "objectID": "teaching/tidyverse-I/material/hotels-datawrangling.html#data-dictionary",
    "href": "teaching/tidyverse-I/material/hotels-datawrangling.html#data-dictionary",
    "title": "Hotel bookings - data wrangling",
    "section": "Data dictionary",
    "text": "Data dictionary\nBelow is the full data dictionary. Note that it is long (there are lots of variables in the data), but we will be using a limited set of the variables for our analysis.\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nhotel\ncharacter\nHotel (H1 = Resort Hotel or H2 = City Hotel)\n\n\nis_canceled\ndouble\nValue indicating if the booking was canceled (1) or not (0)\n\n\nlead_time\ndouble\nNumber of days that elapsed between the entering date of the booking into the PMS and the arrival date\n\n\narrival_date_year\ndouble\nYear of arrival date\n\n\narrival_date_month\ncharacter\nMonth of arrival date\n\n\narrival_date_week_number\ndouble\nWeek number of year for arrival date\n\n\narrival_date_day_of_month\ndouble\nDay of arrival date\n\n\nstays_in_weekend_nights\ndouble\nNumber of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel\n\n\nstays_in_week_nights\ndouble\nNumber of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel\n\n\nadults\ndouble\nNumber of adults\n\n\nchildren\ndouble\nNumber of children\n\n\nbabies\ndouble\nNumber of babies\n\n\nmeal\ncharacter\nType of meal booked. Categories are presented in standard hospitality meal packages:  Undefined/SC – no meal package;BB – Bed & Breakfast;  HB – Half board (breakfast and one other meal – usually dinner);  FB – Full board (breakfast, lunch and dinner)\n\n\ncountry\ncharacter\nCountry of origin. Categories are represented in the ISO 3155–3:2013 format\n\n\nmarket_segment\ncharacter\nMarket segment designation. In categories, the term “TA” means “Travel Agents” and “TO” means “Tour Operators”\n\n\ndistribution_channel\ncharacter\nBooking distribution channel. The term “TA” means “Travel Agents” and “TO” means “Tour Operators”\n\n\nis_repeated_guest\ndouble\nValue indicating if the booking name was from a repeated guest (1) or not (0)\n\n\nprevious_cancellations\ndouble\nNumber of previous bookings that were cancelled by the customer prior to the current booking\n\n\nprevious_bookings_not_canceled\ndouble\nNumber of previous bookings not cancelled by the customer prior to the current booking\n\n\nreserved_room_type\ncharacter\nCode of room type reserved. Code is presented instead of designation for anonymity reasons\n\n\nassigned_room_type\ncharacter\nCode for the type of room assigned to the booking. Sometimes the assigned room type differs from the reserved room type due to hotel operation reasons (e.g. overbooking) or by customer request. Code is presented instead of designation for anonymity reasons\n\n\nbooking_changes\ndouble\nNumber of changes/amendments made to the booking from the moment the booking was entered on the PMS until the moment of check-in or cancellation\n\n\ndeposit_type\ncharacter\nIndication on if the customer made a deposit to guarantee the booking. This variable can assume three categories:No Deposit – no deposit was made;Non Refund – a deposit was made in the value of the total stay cost;Refundable – a deposit was made with a value under the total cost of stay.\n\n\nagent\ncharacter\nID of the travel agency that made the booking\n\n\ncompany\ncharacter\nID of the company/entity that made the booking or responsible for paying the booking. ID is presented instead of designation for anonymity reasons\n\n\ndays_in_waiting_list\ndouble\nNumber of days the booking was in the waiting list before it was confirmed to the customer\n\n\ncustomer_type\ncharacter\nType of booking, assuming one of four categories:Contract - when the booking has an allotment or other type of contract associated to it;Group – when the booking is associated to a group;Transient – when the booking is not part of a group or contract, and is not associated to other transient booking;Transient-party – when the booking is transient, but is associated to at least other transient booking\n\n\nadr\ndouble\nAverage Daily Rate as defined by dividing the sum of all lodging transactions by the total number of staying nights\n\n\nrequired_car_parking_spaces\ndouble\nNumber of car parking spaces required by the customer\n\n\ntotal_of_special_requests\ndouble\nNumber of special requests made by the customer (e.g. twin bed or high floor)\n\n\nreservation_status\ncharacter\nReservation last status, assuming one of three categories:Canceled – booking was canceled by the customer;Check-Out – customer has checked in but already departed;No-Show – customer did not check-in and did inform the hotel of the reason why\n\n\nreservation_status_date\ndouble\nDate at which the last status was set. This variable can be used in conjunction with the ReservationStatus to understand when was the booking canceled or when did the customer checked-out of the hotel"
  },
  {
    "objectID": "teaching/tidyverse-I/index.html",
    "href": "teaching/tidyverse-I/index.html",
    "title": "Data Science with Tidyverse I",
    "section": "",
    "text": "Make sure to install and load Tidyverse:\n\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n\nSchedule\n\n\n\n\nslides\npractical\ndata\nworksheet\n\n\n\n\n1: Meet the toolkit\n\n\n\n.qmd\n\n\n2: Data visualization and ggplot\n\n\n\n.qmd\n\n\n3: Visualizing numerical and categorical data\n\n\n.zip\n.qmd\n\n\n4: Effective Visualization\n\n\n.zip\n.qmd\n\n\n5: Grammar of data wrangling I\n\n \n.zip\n.qmd\n\n\n6: Grammar of data wrangling II\n\n\n.zip\n.qmd\n\n\n7: Tidying Data\n\n\n.zip\n\n\n\n     More Practicals\n\n \n.zip\n.qmd\n\n\n8: Data Types and Data Classes\n\n  \n.zip\n.qmd\n\n\n9: Importing and Recoding Data\n\n  \n.zip\n.qmd\n\n\n10: Functions and Iteration"
  },
  {
    "objectID": "teaching/math-ds/index.html",
    "href": "teaching/math-ds/index.html",
    "title": "Mathematics for Social Scientists",
    "section": "",
    "text": "Schedule\n\n\n\n\nslides\n\n\n\n\n1: Preliminaries\n\n\n\n2: Algebra Review, Modular Arithmetic, Boolean Algebra\n\n\n\n3: Functions & Relations, Sequences & Series, Limits & Continuity\n\n\n\n4: Calculus Fundamentals: Differentiation\n\n\n\n5: Calculus Fundamentals: The Integral\n\n\n\n6: Extrema in One Dimension\n\n\n\n7: Introduction to Probability\n\n\n\n8: Discrete Distributions\n\n\n\n9: Continuous Distributions\n\n\n\n10: Introduction Linear Algebra: Fundamental Vector Operations\n\n\n\n11: Matrix Arithmetics & Matrix Properties\n\n\n\n12: Eigenvalues and Eigenvectors\n\n\n\n13: Multivariate Calculus & Optimization\n\n\n\n14: Multivariate Calculus & Constrained Optimization"
  },
  {
    "objectID": "teaching/stat-learn/material/01/01-R-you-ready.html",
    "href": "teaching/stat-learn/material/01/01-R-you-ready.html",
    "title": "R you ready?",
    "section": "",
    "text": "In this lab, we will introduce some simple R commands. The best way to learn a new language is to try out the commands. R can be downloaded from\nhttp://cran.r-project.org/\nWe recommend that you run R within an integrated development environment (IDE) such as RStudio, which can be freely downloaded from\nhttp://rstudio.com\nThe RStudio website also provides a cloud-based version of R, which does not require installing any software.\n\n\nR uses functions to perform operations. To run a function called funcname, we type funcname(input1, input2), where the inputs (or arguments) input1 and input2 tell R how to run the function. A function can have any number of inputs. For example, to create a vector of numbers, we use the function c() (for concatenate). Any numbers inside the parentheses are joined together. The following command instructs R to join together the numbers 1, 3, 2, and 5, and to save them as a vector named x. When we type x, it gives us back the vector.\n\nx &lt;- c(1, 3, 2, 5)\nx\n\n[1] 1 3 2 5\n\n\nNote that the &gt; is not part of the command; rather, it is printed by R to indicate that it is ready for another command to be entered. We can also save things using = rather than &lt;-:\n\nx = c(1, 6, 2)\nx\n\n[1] 1 6 2\n\ny = c(1, 4, 3)\n\nHitting the up arrow multiple times will display the previous commands, which can then be edited. This is useful since one often wishes to repeat a similar command. In addition, typing ?funcname will always cause R to open a new help file window with additional information about the function funcname().\nWe can tell R to add two sets of numbers together. It will then add the first number from x to the first number from y, and so on. However, x and y should be the same length. We can check their length using the length() function.\n\nlength(x)\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\nx + y\n\n[1]  2 10  5\n\n\nThe ls() function allows us to look at a list of all of the objects, such as data and functions, that we have saved so far. The rm() function can be used to delete any that we don’t want.\n\nls()\n\n[1] \"x\" \"y\"\n\nrm(x, y)\nls()\n\ncharacter(0)\n\n\nIt’s also possible to remove all objects at once:\n\nrm(list = ls())\n\nThe matrix() function can be used to create a matrix of numbers. Before we use the matrix() function, we can learn more about it:\n\n?matrix\n\nThe help file reveals that the matrix() function takes a number of inputs, but for now we focus on the first three: the data (the entries in the matrix), the number of rows, and the number of columns. First, we create a simple matrix.\n\nx &lt;- matrix(data = c(1, 2, 3, 4), nrow = 2, ncol = 2)\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nNote that we could just as well omit typing data=, nrow=, and ncol= in the matrix() command above: that is, we could just type\n\nx &lt;- matrix(c(1, 2, 3, 4), 2, 2)\n\nand this would have the same effect. However, it can sometimes be useful to specify the names of the arguments passed in, since otherwise R will assume that the function arguments are passed into the function in the same order that is given in the function’s help file. As this example illustrates, by default R creates matrices by successively filling in columns. Alternatively, the byrow = TRUE option can be used to populate the matrix in order of the rows.\n\nmatrix(c(1, 2, 3, 4), 2, 2, byrow = TRUE)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n\nNotice that in the above command we did not assign the matrix to a value such as x. In this case the matrix is printed to the screen but is not saved for future calculations. The sqrt() function returns the square root of each element of a vector or matrix. The command x^2 raises each element of x to the power 2; any powers are possible, including fractional or negative powers.\n\nsqrt(x)\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\n\nThe rnorm() function generates a vector of random normal variables, with first argument n the sample size. Each time we call this function, we will get a different answer. Here we create two correlated sets of numbers, x and y, and use the cor() function to compute the correlation between them.\n\nx &lt;- rnorm(50)\ny &lt;- x + rnorm(50, mean = 50, sd = .1)\ncor(x, y)\n\n[1] 0.9959562\n\n\nBy default, rnorm() creates standard normal random variables with a mean of \\(0\\) and a standard deviation of \\(1\\). However, the mean and standard deviation can be altered using the mean and sd arguments, as illustrated above. Sometimes we want our code to reproduce the exact same set of random numbers; we can use the set.seed() function to do this. The set.seed() function takes an (arbitrary) integer argument.\n\nset.seed(1303)\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\n\nWe use set.seed() throughout the labs whenever we perform calculations involving random quantities. In general this should allow the user to reproduce our results. However, as new versions of R become available, small discrepancies may arise between this book and the output from R.\nThe mean() and var() functions can be used to compute the mean and variance of a vector of numbers. Applying sqrt() to the output of var() will give the standard deviation. Or we can simply use the sd() function.\n\nset.seed(3)\ny &lt;- rnorm(100)\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768\n\n\n\n\n\nThe plot() function is the primary way to plot data in R. For instance, plot(x, y) produces a scatterplot of the numbers in x versus the numbers in y. There are many additional options that can be passed in to the plot() function. For example, passing in the argument xlab will result in a label on the \\(x\\)-axis. To find out more information about the plot() function, type ?plot.\n\nx &lt;- rnorm(100)\ny &lt;- rnorm(100)\nplot(x, y)\n\n\n\n\n\n\n\nplot(x, y, xlab = \"this is the x-axis\",\n    ylab = \"this is the y-axis\",\n    main = \"Plot of X vs Y\")\n\n\n\n\n\n\n\n\nWe will often want to save the output of an R plot. The command that we use to do this will depend on the file type that we would like to create. For instance, to create a pdf, we use the pdf() function, and to create a jpeg, we use the jpeg() function.\n\npdf(\"Figure.pdf\")\nplot(x, y, col = \"gray\")\ndev.off()\n\nquartz_off_screen \n                2 \n\n\nThe function dev.off() indicates to R that we are done creating the plot. Alternatively, we can simply copy the plot window and paste it into an appropriate file type, such as a Word document.\nThe function seq() can be used to create a sequence of numbers. For instance, seq(a, b) makes a vector of integers between a and b. There are many other options: for instance, seq(0, 1, length = 10) makes a sequence of 10 numbers that are equally spaced between 0 and 1. Typing 3:11 is a shorthand for seq(3, 11) for integer arguments.\n\nx &lt;- seq(1, 10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx &lt;- 1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx &lt;- seq(-pi, pi, length = 50)\n\nWe will now create some more sophisticated plots. The contour() function produces a contour plot in order to represent three-dimensional data; it is like a topographical map. It takes three arguments:\n\nA vector of the x values (the first dimension),\nA vector of the y values (the second dimension), and\nA matrix whose elements correspond to the z value (the third dimension) for each pair of (x, y) coordinates.\n\nAs with the plot() function, there are many other inputs that can be used to fine-tune the output of the contour() function. To learn more about these, take a look at the help file by typing ?contour.\n\ny &lt;- x\nf &lt;- outer(x, y, function(x, y) cos(y) / (1 + x^2))\ncontour(x, y, f)\ncontour(x, y, f, nlevels = 45, add = T)\n\n\n\n\n\n\n\nfa &lt;- (f - t(f)) / 2\ncontour(x, y, fa, nlevels = 15)\n\n\n\n\n\n\n\n\nThe image() function works the same way as contour(), except that it produces a color-coded plot whose colors depend on the z value. This is known as a heatmap, and is sometimes used to plot temperature in weather forecasts. Alternatively, persp() can be used to produce a three-dimensional plot. The arguments theta and phi control the angles at which the plot is viewed.\n\nimage(x, y, fa)\n\n\n\n\n\n\n\npersp(x, y, fa)\n\n\n\n\n\n\n\npersp(x, y, fa, theta = 30)\n\n\n\n\n\n\n\npersp(x, y, fa, theta = 30, phi = 20)\n\n\n\n\n\n\n\npersp(x, y, fa, theta = 30, phi = 70)\n\n\n\n\n\n\n\npersp(x, y, fa, theta = 30, phi = 40)\n\n\n\n\n\n\n\n\n\n\n\nWe often wish to examine part of a set of data. Suppose that our data is stored in the matrix A.\n\nA &lt;- matrix(1:16, 4, 4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\n\nThen, typing\n\nA[2, 3]\n\n[1] 10\n\n\nwill select the element corresponding to the second row and the third column. The first number after the open-bracket symbol [ always refers to the row, and the second number always refers to the column. We can also select multiple rows and columns at a time, by providing vectors as the indices.\n\nA[c(1, 3), c(2, 4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3, 2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2, ]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[, 1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\n\nThe last two examples include either no index for the columns or no index for the rows. These indicate that R should include all columns or all rows, respectively. R treats a single row or column of a matrix as a vector.\n\nA[1, ]\n\n[1]  1  5  9 13\n\n\nThe use of a negative sign - in the index tells R to keep all rows or columns except those indicated in the index.\n\nA[-c(1, 3), ]\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1, 3), -c(1, 3, 4)]\n\n[1] 6 8\n\n\nThe dim() function outputs the number of rows followed by the number of columns of a given matrix.\n\ndim(A)\n\n[1] 4 4\n\n\n\n\n\nFor most analyses, the first step involves importing a data set into R. The read.table() function is one of the primary ways to do this. The help file contains details about how to use this function. We can use the function write.table() to export data.\nBefore attempting to load a data set, we must make sure that R knows to search for the data in the proper directory. For example, on a Windows system one could select the directory using the Change dir ... option under the File menu. However, the details of how to do this depend on the operating system (e.g. Windows, Mac, Unix) that is being used, and so we do not give further details here.\nWe begin by loading in the Auto data set. This data is part of the ISLR2 library, discussed in Chapter 3. To illustrate the read.table() function, we load it now from a text file, Auto.data, which you can find on the textbook website. The following command will load the Auto.data file into R and store it as an object called Auto, in a format referred to as a data frame. Once the data has been loaded, the View() function can be used to view it in a spreadsheet-like window. (This function can sometimes be a bit finicky. If you have trouble using it, then try the head() function instead.) The head() function can also be used to view the first few rows of the data.\n\nAuto &lt;- read.table(\"01-data/Auto.data\")\nknitr::kable(head(Auto)) # note: you only write head(Auto) here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\n\n\n18.0\n8\n307.0\n130.0\n3504.\n12.0\n70\n1\nchevrolet chevelle malibu\n\n\n15.0\n8\n350.0\n165.0\n3693.\n11.5\n70\n1\nbuick skylark 320\n\n\n18.0\n8\n318.0\n150.0\n3436.\n11.0\n70\n1\nplymouth satellite\n\n\n16.0\n8\n304.0\n150.0\n3433.\n12.0\n70\n1\namc rebel sst\n\n\n17.0\n8\n302.0\n140.0\n3449.\n10.5\n70\n1\nford torino\n\n\n\n\n\nNote that Auto.data is simply a text file, which you could alternatively open on your computer using a standard text editor (you can even change the format to .txt). It is often a good idea to view a data set using a text editor or other software such as Excel before loading it into R.\nThis particular data set has not been loaded correctly, because R has assumed that the variable names are part of the data and so has included them in the first row. The data set also includes a number of missing observations, indicated by a question mark ?. Missing values are a common occurrence in real data sets. Using the option header = T (or header = TRUE) in the read.table() function tells R that the first line of the file contains the variable names, and using the option na.strings tells R that any time it sees a particular character or set of characters (such as a question mark), it should be treated as a missing element of the data matrix.\n\nAuto &lt;- read.table(\"01-data/Auto.data\", header = T, na.strings = \"?\", stringsAsFactors = T)\n# View(Auto)\n\nThe stringsAsFactors = T argument tells R that any variable containing character strings should be interpreted as a qualitative variable, and that each distinct character string represents a distinct level for that qualitative variable. An easy way to load data from Excel into R is to save it as a csv (comma-separated values) file, and then use the read.csv() function.\n\nAuto &lt;- read.csv(\"01-data/Auto.csv\", na.strings = \"?\", stringsAsFactors = T)\ndim(Auto)\n\n[1] 397   9\n\nknitr::kable(Auto[1:4, ])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\n\n\n\n\n18\n8\n307\n130\n3504\n12.0\n70\n1\nchevrolet chevelle malibu\n\n\n15\n8\n350\n165\n3693\n11.5\n70\n1\nbuick skylark 320\n\n\n18\n8\n318\n150\n3436\n11.0\n70\n1\nplymouth satellite\n\n\n16\n8\n304\n150\n3433\n12.0\n70\n1\namc rebel sst\n\n\n\n\n\nThe dim() function tells us that the data has \\(397\\) observations, or rows, and nine variables, or columns. There are various ways to deal with the missing data. In this case, only five of the rows contain missing observations, and so we choose to use the na.omit() function to simply remove these rows.\n\nAuto &lt;- na.omit(Auto)\ndim(Auto)\n\n[1] 392   9\n\n\nOnce the data are loaded correctly, we can use names() to check the variable names.\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\nWe can use the plot() function to produce scatterplots of the quantitative variables. However, simply typing the variable names will produce an error message, because R does not know to look in the Auto data set for those variables.\nTo refer to a variable, we must type the data set and the variable name joined with a $ symbol. Alternatively, we can use the attach() function in order to tell R to make the variables in this data frame available by name.\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\n\nThe cylinders variable is stored as a numeric vector, so R has treated it as quantitative. However, since there are only a small number of possible values for cylinders, one may prefer to treat it as a qualitative variable. The as.factor() function converts quantitative variables into qualitative variables.\n\ncylinders &lt;- as.factor(cylinders)\n\nIf the variable plotted on the \\(x\\)-axis is qualitative, then boxplots will automatically be produced by the plot() function. As usual, a number of options can be specified in order to customize the plots.\n\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col = \"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col = \"red\", varwidth = T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col = \"red\", varwidth = T,\n    horizontal = T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col = \"red\", varwidth = T,\n    xlab = \"cylinders\", ylab = \"MPG\")\n\n\n\n\n\n\n\n\nThe hist() function can be used to plot a histogram. Note that col = 2 has the same effect as col = \"red\".\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg, col = 2)\n\n\n\n\n\n\n\nhist(mpg, col = 2, breaks = 15)\n\n\n\n\n\n\n\n\nThe pairs() function creates a scatterplot matrix, i.e. a scatterplot for every pair of variables. We can also produce scatterplots for just a subset of the variables.\n\npairs(Auto)\n\n\n\n\n\n\n\npairs(\n    ~ mpg + displacement + horsepower + weight + acceleration,\n    data = Auto\n  )\n\n\n\n\n\n\n\n\nIn conjunction with the plot() function, identify() provides a useful interactive method for identifying the value of a particular variable for points on a plot. We pass in three arguments to identify(): the \\(x\\)-axis variable, the \\(y\\)-axis variable, and the variable whose values we would like to see printed for each point. Then clicking one or more points in the plot and hitting Escape will cause R to print the values of the variable of interest. The numbers printed under the identify() function correspond to the rows for the selected points.\n\nplot(horsepower, mpg)\nidentify(horsepower, mpg, name)\n\n\n\n\n\n\n\n\ninteger(0)\n\n\nThe summary() function produces a numerical summary of each variable in a particular data set.\n\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.00   1st Qu.:4.000   1st Qu.:105.0   1st Qu.: 75.0   1st Qu.:2225  \n Median :22.75   Median :4.000   Median :151.0   Median : 93.5   Median :2804  \n Mean   :23.45   Mean   :5.472   Mean   :194.4   Mean   :104.5   Mean   :2978  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:275.8   3rd Qu.:126.0   3rd Qu.:3615  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                                               \n  acceleration        year           origin                      name    \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   amc matador       :  5  \n 1st Qu.:13.78   1st Qu.:73.00   1st Qu.:1.000   ford pinto        :  5  \n Median :15.50   Median :76.00   Median :1.000   toyota corolla    :  5  \n Mean   :15.54   Mean   :75.98   Mean   :1.577   amc gremlin       :  4  \n 3rd Qu.:17.02   3rd Qu.:79.00   3rd Qu.:2.000   amc hornet        :  4  \n Max.   :24.80   Max.   :82.00   Max.   :3.000   chevrolet chevette:  4  \n                                                 (Other)           :365  \n\n\nFor qualitative variables such as name, R will list the number of observations that fall in each category. We can also produce a summary of just a single variable.\n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.00   22.75   23.45   29.00   46.60 \n\n\nOnce we have finished using R, we type q() in order to shut it down, or quit. When exiting R, we have the option to save the current workspace so that all objects (such as data sets) that we have created in this R session will be available next time. Before exiting R, we may want to save a record of all of the commands that we typed in the most recent session; this can be accomplished using the savehistory() function. Next time we enter R, we can load that history using the loadhistory() function, if we wish."
  },
  {
    "objectID": "teaching/stat-learn/material/01/01-R-you-ready.html#basic-commands",
    "href": "teaching/stat-learn/material/01/01-R-you-ready.html#basic-commands",
    "title": "R you ready?",
    "section": "",
    "text": "R uses functions to perform operations. To run a function called funcname, we type funcname(input1, input2), where the inputs (or arguments) input1 and input2 tell R how to run the function. A function can have any number of inputs. For example, to create a vector of numbers, we use the function c() (for concatenate). Any numbers inside the parentheses are joined together. The following command instructs R to join together the numbers 1, 3, 2, and 5, and to save them as a vector named x. When we type x, it gives us back the vector.\n\nx &lt;- c(1, 3, 2, 5)\nx\n\n[1] 1 3 2 5\n\n\nNote that the &gt; is not part of the command; rather, it is printed by R to indicate that it is ready for another command to be entered. We can also save things using = rather than &lt;-:\n\nx = c(1, 6, 2)\nx\n\n[1] 1 6 2\n\ny = c(1, 4, 3)\n\nHitting the up arrow multiple times will display the previous commands, which can then be edited. This is useful since one often wishes to repeat a similar command. In addition, typing ?funcname will always cause R to open a new help file window with additional information about the function funcname().\nWe can tell R to add two sets of numbers together. It will then add the first number from x to the first number from y, and so on. However, x and y should be the same length. We can check their length using the length() function.\n\nlength(x)\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\nx + y\n\n[1]  2 10  5\n\n\nThe ls() function allows us to look at a list of all of the objects, such as data and functions, that we have saved so far. The rm() function can be used to delete any that we don’t want.\n\nls()\n\n[1] \"x\" \"y\"\n\nrm(x, y)\nls()\n\ncharacter(0)\n\n\nIt’s also possible to remove all objects at once:\n\nrm(list = ls())\n\nThe matrix() function can be used to create a matrix of numbers. Before we use the matrix() function, we can learn more about it:\n\n?matrix\n\nThe help file reveals that the matrix() function takes a number of inputs, but for now we focus on the first three: the data (the entries in the matrix), the number of rows, and the number of columns. First, we create a simple matrix.\n\nx &lt;- matrix(data = c(1, 2, 3, 4), nrow = 2, ncol = 2)\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nNote that we could just as well omit typing data=, nrow=, and ncol= in the matrix() command above: that is, we could just type\n\nx &lt;- matrix(c(1, 2, 3, 4), 2, 2)\n\nand this would have the same effect. However, it can sometimes be useful to specify the names of the arguments passed in, since otherwise R will assume that the function arguments are passed into the function in the same order that is given in the function’s help file. As this example illustrates, by default R creates matrices by successively filling in columns. Alternatively, the byrow = TRUE option can be used to populate the matrix in order of the rows.\n\nmatrix(c(1, 2, 3, 4), 2, 2, byrow = TRUE)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n\nNotice that in the above command we did not assign the matrix to a value such as x. In this case the matrix is printed to the screen but is not saved for future calculations. The sqrt() function returns the square root of each element of a vector or matrix. The command x^2 raises each element of x to the power 2; any powers are possible, including fractional or negative powers.\n\nsqrt(x)\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\n\nThe rnorm() function generates a vector of random normal variables, with first argument n the sample size. Each time we call this function, we will get a different answer. Here we create two correlated sets of numbers, x and y, and use the cor() function to compute the correlation between them.\n\nx &lt;- rnorm(50)\ny &lt;- x + rnorm(50, mean = 50, sd = .1)\ncor(x, y)\n\n[1] 0.9959562\n\n\nBy default, rnorm() creates standard normal random variables with a mean of \\(0\\) and a standard deviation of \\(1\\). However, the mean and standard deviation can be altered using the mean and sd arguments, as illustrated above. Sometimes we want our code to reproduce the exact same set of random numbers; we can use the set.seed() function to do this. The set.seed() function takes an (arbitrary) integer argument.\n\nset.seed(1303)\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\n\nWe use set.seed() throughout the labs whenever we perform calculations involving random quantities. In general this should allow the user to reproduce our results. However, as new versions of R become available, small discrepancies may arise between this book and the output from R.\nThe mean() and var() functions can be used to compute the mean and variance of a vector of numbers. Applying sqrt() to the output of var() will give the standard deviation. Or we can simply use the sd() function.\n\nset.seed(3)\ny &lt;- rnorm(100)\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768"
  },
  {
    "objectID": "teaching/stat-learn/material/01/01-R-you-ready.html#graphics",
    "href": "teaching/stat-learn/material/01/01-R-you-ready.html#graphics",
    "title": "R you ready?",
    "section": "",
    "text": "The plot() function is the primary way to plot data in R. For instance, plot(x, y) produces a scatterplot of the numbers in x versus the numbers in y. There are many additional options that can be passed in to the plot() function. For example, passing in the argument xlab will result in a label on the \\(x\\)-axis. To find out more information about the plot() function, type ?plot.\n\nx &lt;- rnorm(100)\ny &lt;- rnorm(100)\nplot(x, y)\n\n\n\n\n\n\n\nplot(x, y, xlab = \"this is the x-axis\",\n    ylab = \"this is the y-axis\",\n    main = \"Plot of X vs Y\")\n\n\n\n\n\n\n\n\nWe will often want to save the output of an R plot. The command that we use to do this will depend on the file type that we would like to create. For instance, to create a pdf, we use the pdf() function, and to create a jpeg, we use the jpeg() function.\n\npdf(\"Figure.pdf\")\nplot(x, y, col = \"gray\")\ndev.off()\n\nquartz_off_screen \n                2 \n\n\nThe function dev.off() indicates to R that we are done creating the plot. Alternatively, we can simply copy the plot window and paste it into an appropriate file type, such as a Word document.\nThe function seq() can be used to create a sequence of numbers. For instance, seq(a, b) makes a vector of integers between a and b. There are many other options: for instance, seq(0, 1, length = 10) makes a sequence of 10 numbers that are equally spaced between 0 and 1. Typing 3:11 is a shorthand for seq(3, 11) for integer arguments.\n\nx &lt;- seq(1, 10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx &lt;- 1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx &lt;- seq(-pi, pi, length = 50)\n\nWe will now create some more sophisticated plots. The contour() function produces a contour plot in order to represent three-dimensional data; it is like a topographical map. It takes three arguments:\n\nA vector of the x values (the first dimension),\nA vector of the y values (the second dimension), and\nA matrix whose elements correspond to the z value (the third dimension) for each pair of (x, y) coordinates.\n\nAs with the plot() function, there are many other inputs that can be used to fine-tune the output of the contour() function. To learn more about these, take a look at the help file by typing ?contour.\n\ny &lt;- x\nf &lt;- outer(x, y, function(x, y) cos(y) / (1 + x^2))\ncontour(x, y, f)\ncontour(x, y, f, nlevels = 45, add = T)\n\n\n\n\n\n\n\nfa &lt;- (f - t(f)) / 2\ncontour(x, y, fa, nlevels = 15)\n\n\n\n\n\n\n\n\nThe image() function works the same way as contour(), except that it produces a color-coded plot whose colors depend on the z value. This is known as a heatmap, and is sometimes used to plot temperature in weather forecasts. Alternatively, persp() can be used to produce a three-dimensional plot. The arguments theta and phi control the angles at which the plot is viewed.\n\nimage(x, y, fa)\n\n\n\n\n\n\n\npersp(x, y, fa)\n\n\n\n\n\n\n\npersp(x, y, fa, theta = 30)\n\n\n\n\n\n\n\npersp(x, y, fa, theta = 30, phi = 20)\n\n\n\n\n\n\n\npersp(x, y, fa, theta = 30, phi = 70)\n\n\n\n\n\n\n\npersp(x, y, fa, theta = 30, phi = 40)"
  },
  {
    "objectID": "teaching/stat-learn/material/01/01-R-you-ready.html#indexing-data",
    "href": "teaching/stat-learn/material/01/01-R-you-ready.html#indexing-data",
    "title": "R you ready?",
    "section": "",
    "text": "We often wish to examine part of a set of data. Suppose that our data is stored in the matrix A.\n\nA &lt;- matrix(1:16, 4, 4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\n\nThen, typing\n\nA[2, 3]\n\n[1] 10\n\n\nwill select the element corresponding to the second row and the third column. The first number after the open-bracket symbol [ always refers to the row, and the second number always refers to the column. We can also select multiple rows and columns at a time, by providing vectors as the indices.\n\nA[c(1, 3), c(2, 4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3, 2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2, ]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[, 1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\n\nThe last two examples include either no index for the columns or no index for the rows. These indicate that R should include all columns or all rows, respectively. R treats a single row or column of a matrix as a vector.\n\nA[1, ]\n\n[1]  1  5  9 13\n\n\nThe use of a negative sign - in the index tells R to keep all rows or columns except those indicated in the index.\n\nA[-c(1, 3), ]\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1, 3), -c(1, 3, 4)]\n\n[1] 6 8\n\n\nThe dim() function outputs the number of rows followed by the number of columns of a given matrix.\n\ndim(A)\n\n[1] 4 4"
  },
  {
    "objectID": "teaching/stat-learn/material/01/01-R-you-ready.html#loading-data",
    "href": "teaching/stat-learn/material/01/01-R-you-ready.html#loading-data",
    "title": "R you ready?",
    "section": "",
    "text": "For most analyses, the first step involves importing a data set into R. The read.table() function is one of the primary ways to do this. The help file contains details about how to use this function. We can use the function write.table() to export data.\nBefore attempting to load a data set, we must make sure that R knows to search for the data in the proper directory. For example, on a Windows system one could select the directory using the Change dir ... option under the File menu. However, the details of how to do this depend on the operating system (e.g. Windows, Mac, Unix) that is being used, and so we do not give further details here.\nWe begin by loading in the Auto data set. This data is part of the ISLR2 library, discussed in Chapter 3. To illustrate the read.table() function, we load it now from a text file, Auto.data, which you can find on the textbook website. The following command will load the Auto.data file into R and store it as an object called Auto, in a format referred to as a data frame. Once the data has been loaded, the View() function can be used to view it in a spreadsheet-like window. (This function can sometimes be a bit finicky. If you have trouble using it, then try the head() function instead.) The head() function can also be used to view the first few rows of the data.\n\nAuto &lt;- read.table(\"01-data/Auto.data\")\nknitr::kable(head(Auto)) # note: you only write head(Auto) here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\n\n\n18.0\n8\n307.0\n130.0\n3504.\n12.0\n70\n1\nchevrolet chevelle malibu\n\n\n15.0\n8\n350.0\n165.0\n3693.\n11.5\n70\n1\nbuick skylark 320\n\n\n18.0\n8\n318.0\n150.0\n3436.\n11.0\n70\n1\nplymouth satellite\n\n\n16.0\n8\n304.0\n150.0\n3433.\n12.0\n70\n1\namc rebel sst\n\n\n17.0\n8\n302.0\n140.0\n3449.\n10.5\n70\n1\nford torino\n\n\n\n\n\nNote that Auto.data is simply a text file, which you could alternatively open on your computer using a standard text editor (you can even change the format to .txt). It is often a good idea to view a data set using a text editor or other software such as Excel before loading it into R.\nThis particular data set has not been loaded correctly, because R has assumed that the variable names are part of the data and so has included them in the first row. The data set also includes a number of missing observations, indicated by a question mark ?. Missing values are a common occurrence in real data sets. Using the option header = T (or header = TRUE) in the read.table() function tells R that the first line of the file contains the variable names, and using the option na.strings tells R that any time it sees a particular character or set of characters (such as a question mark), it should be treated as a missing element of the data matrix.\n\nAuto &lt;- read.table(\"01-data/Auto.data\", header = T, na.strings = \"?\", stringsAsFactors = T)\n# View(Auto)\n\nThe stringsAsFactors = T argument tells R that any variable containing character strings should be interpreted as a qualitative variable, and that each distinct character string represents a distinct level for that qualitative variable. An easy way to load data from Excel into R is to save it as a csv (comma-separated values) file, and then use the read.csv() function.\n\nAuto &lt;- read.csv(\"01-data/Auto.csv\", na.strings = \"?\", stringsAsFactors = T)\ndim(Auto)\n\n[1] 397   9\n\nknitr::kable(Auto[1:4, ])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\n\n\n\n\n18\n8\n307\n130\n3504\n12.0\n70\n1\nchevrolet chevelle malibu\n\n\n15\n8\n350\n165\n3693\n11.5\n70\n1\nbuick skylark 320\n\n\n18\n8\n318\n150\n3436\n11.0\n70\n1\nplymouth satellite\n\n\n16\n8\n304\n150\n3433\n12.0\n70\n1\namc rebel sst\n\n\n\n\n\nThe dim() function tells us that the data has \\(397\\) observations, or rows, and nine variables, or columns. There are various ways to deal with the missing data. In this case, only five of the rows contain missing observations, and so we choose to use the na.omit() function to simply remove these rows.\n\nAuto &lt;- na.omit(Auto)\ndim(Auto)\n\n[1] 392   9\n\n\nOnce the data are loaded correctly, we can use names() to check the variable names.\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\""
  },
  {
    "objectID": "teaching/stat-learn/material/01/01-R-you-ready.html#additional-graphical-and-numerical-summaries",
    "href": "teaching/stat-learn/material/01/01-R-you-ready.html#additional-graphical-and-numerical-summaries",
    "title": "R you ready?",
    "section": "",
    "text": "We can use the plot() function to produce scatterplots of the quantitative variables. However, simply typing the variable names will produce an error message, because R does not know to look in the Auto data set for those variables.\nTo refer to a variable, we must type the data set and the variable name joined with a $ symbol. Alternatively, we can use the attach() function in order to tell R to make the variables in this data frame available by name.\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\n\nThe cylinders variable is stored as a numeric vector, so R has treated it as quantitative. However, since there are only a small number of possible values for cylinders, one may prefer to treat it as a qualitative variable. The as.factor() function converts quantitative variables into qualitative variables.\n\ncylinders &lt;- as.factor(cylinders)\n\nIf the variable plotted on the \\(x\\)-axis is qualitative, then boxplots will automatically be produced by the plot() function. As usual, a number of options can be specified in order to customize the plots.\n\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col = \"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col = \"red\", varwidth = T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col = \"red\", varwidth = T,\n    horizontal = T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col = \"red\", varwidth = T,\n    xlab = \"cylinders\", ylab = \"MPG\")\n\n\n\n\n\n\n\n\nThe hist() function can be used to plot a histogram. Note that col = 2 has the same effect as col = \"red\".\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg, col = 2)\n\n\n\n\n\n\n\nhist(mpg, col = 2, breaks = 15)\n\n\n\n\n\n\n\n\nThe pairs() function creates a scatterplot matrix, i.e. a scatterplot for every pair of variables. We can also produce scatterplots for just a subset of the variables.\n\npairs(Auto)\n\n\n\n\n\n\n\npairs(\n    ~ mpg + displacement + horsepower + weight + acceleration,\n    data = Auto\n  )\n\n\n\n\n\n\n\n\nIn conjunction with the plot() function, identify() provides a useful interactive method for identifying the value of a particular variable for points on a plot. We pass in three arguments to identify(): the \\(x\\)-axis variable, the \\(y\\)-axis variable, and the variable whose values we would like to see printed for each point. Then clicking one or more points in the plot and hitting Escape will cause R to print the values of the variable of interest. The numbers printed under the identify() function correspond to the rows for the selected points.\n\nplot(horsepower, mpg)\nidentify(horsepower, mpg, name)\n\n\n\n\n\n\n\n\ninteger(0)\n\n\nThe summary() function produces a numerical summary of each variable in a particular data set.\n\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.00   1st Qu.:4.000   1st Qu.:105.0   1st Qu.: 75.0   1st Qu.:2225  \n Median :22.75   Median :4.000   Median :151.0   Median : 93.5   Median :2804  \n Mean   :23.45   Mean   :5.472   Mean   :194.4   Mean   :104.5   Mean   :2978  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:275.8   3rd Qu.:126.0   3rd Qu.:3615  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                                               \n  acceleration        year           origin                      name    \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   amc matador       :  5  \n 1st Qu.:13.78   1st Qu.:73.00   1st Qu.:1.000   ford pinto        :  5  \n Median :15.50   Median :76.00   Median :1.000   toyota corolla    :  5  \n Mean   :15.54   Mean   :75.98   Mean   :1.577   amc gremlin       :  4  \n 3rd Qu.:17.02   3rd Qu.:79.00   3rd Qu.:2.000   amc hornet        :  4  \n Max.   :24.80   Max.   :82.00   Max.   :3.000   chevrolet chevette:  4  \n                                                 (Other)           :365  \n\n\nFor qualitative variables such as name, R will list the number of observations that fall in each category. We can also produce a summary of just a single variable.\n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.00   22.75   23.45   29.00   46.60 \n\n\nOnce we have finished using R, we type q() in order to shut it down, or quit. When exiting R, we have the option to save the current workspace so that all objects (such as data sets) that we have created in this R session will be available next time. Before exiting R, we may want to save a record of all of the commands that we typed in the most recent session; this can be accomplished using the savehistory() function. Next time we enter R, we can load that history using the loadhistory() function, if we wish."
  },
  {
    "objectID": "teaching/stat-learn/material/01/01-R-you-ready.html#installation",
    "href": "teaching/stat-learn/material/01/01-R-you-ready.html#installation",
    "title": "R you ready?",
    "section": "Installation",
    "text": "Installation\nBefore getting started, make sure you have both tidyverse and tidymodels installed on your system. You can do this by running the following code in R:\n\n# Install the tidyverse and tidymodels collections of packages\ninstall.packages(\"tidyverse\")\ninstall.packages(\"tidymodels\")\n\n# To load them into your R session\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "teaching/stat-learn/material/01/01-R-you-ready.html#references",
    "href": "teaching/stat-learn/material/01/01-R-you-ready.html#references",
    "title": "R you ready?",
    "section": "References",
    "text": "References\n\nWickham, H., Averick, M., Bryan, J., Chang, W., D’Agostino McGowan, L., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., Takahashi, K., Vaughan, D., Wilke, C. O., Woo, K., & Yutani, H. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686. https://doi.org/10.21105/joss.01686\nKuhn, M., & Wickham, H. (2020). Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles. Retrieved from https://www.tidymodels.org\nWickham, H., & Grolemund, G. (2017). R for Data Science. O’Reilly Media. Retrieved from https://r4ds.had.co.nz\nThe tidyverse official documentation: https://www.tidyverse.org"
  },
  {
    "objectID": "teaching/stat-learn/material/09/09-trees.html",
    "href": "teaching/stat-learn/material/09/09-trees.html",
    "title": "Tree Based Methods",
    "section": "",
    "text": "Decision Tree\nRandom Forest\nGradient Boosting Tree\n\n\n\n\nnum trees\none\nmany\nmany\n\n\nmake predictions\nmode or mean of leaf node\neach tree votes\nsum of tree outputs\n\n\ntree independence\nNOT applicable\nindependent\ndependent\n\n\nData Used\nall\nbagging + random feature selection\nall\n\n\n\n\n\n\nLet’s load the penguin data set, and plot the bill length and bill depth for our three species:\n\n# Load libraries\nlibrary(readr)\nlibrary(ggplot2)\n\n# Read the data\npengwing &lt;- read_csv(\"09-data/penguins.csv\")\n\n# View first few rows (equivalent to .head())\nhead(pengwing)\n\n# A tibble: 6 × 9\n   ...1 species island    bill_length_mm bill_depth_mm flipper_length_mm\n  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n1     0 Adelie  Torgersen           39.1          18.7               181\n2     1 Adelie  Torgersen           39.5          17.4               186\n3     2 Adelie  Torgersen           40.3          18                 195\n4     3 Adelie  Torgersen           NA            NA                  NA\n5     4 Adelie  Torgersen           36.7          19.3               193\n6     5 Adelie  Torgersen           39.3          20.6               190\n# ℹ 3 more variables: body_mass_g &lt;dbl&gt;, sex &lt;chr&gt;, year &lt;dbl&gt;\n\n# Create the plot\nggplot(pengwing, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point() +\n  theme_minimal() +\n  labs(\n    x = \"Bill Length (in mm)\",\n    y = \"Bill Depth (in mm)\",\n    title = \"Bill Length vs. Bill Depth by Species\"\n  )\n\n\n\n\n\n\n\n\nWe could use a decision tree based on bill length and bill depth to classify penguins as different species. First, we could split on Bill Depth and decide that any penguin with a depth less than 16.5 mm, should be classified as a Gentoo penguin.\n\n# Choose a split value\nsplit1 &lt;- 16.5\n\nggplot(pengwing, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point() +\n  theme_minimal() +\n  labs(\n    x = \"Bill Length (in mm)\",\n    y = \"Bill Depth (in mm)\",\n    title = \"Bill Length vs. Bill Depth by Species\"\n  ) +\n  geom_hline(yintercept = split1, linewidth = 1, linetype = \"dashed\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThat bottom group looks GREAT. Now let’s look at the top group. Most of the Chinstrap penguins have longer bill lengths. Let’s say that if a penguin has a bill depth &gt; 16.5mm, then we will split on bill length at 44 to separate the Adelie and Chinstrap penguins.\n\n# Choose a split value \nsplit2 &lt;- 44\n\nggplot(pengwing, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point() +\n  theme_minimal() +\n  labs(\n    x = \"Bill Length (in mm)\",\n    y = \"Bill Depth (in mm)\",\n    title = \"Bill Length vs. Bill Depth by Species\"\n  ) +\n  geom_hline(yintercept = split1, linewidth = 1, linetype = \"dashed\") +\n  geom_segment(\n    x = split2, xend = split2,\n    y = split1, yend = 22,\n    linewidth = 0.6, linetype = \"dashed\", color = \"black\"\n  )\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nVoila! We’ve built a (very short) decision tree! It would look like this:\n\n# Install if needed\n# install.packages(\"DiagrammeR\")\n\nlibrary(DiagrammeR)\n\ngrViz(\"\ndigraph penguin_tree {\n\n  graph [layout = dot, rankdir = TB]\n\n  node [\n    shape = box,\n    style = rounded,\n    fontname = Helvetica,\n    fontsize = 14\n  ]\n\n  # Decision nodes\n  node1 [label = 'bill_depth &lt; 16.5']\n  node2 [label = 'bill_length &lt; 44']\n\n  # Leaf nodes\n  gentoo    [label = 'Gentoo', fillcolor = '#dbe9f6', style = 'rounded,filled']\n  chinstrap[label = 'Chinstrap', fillcolor = '#e3f0dd', style = 'rounded,filled']\n  adelie   [label = 'Adelie', fillcolor = '#f4d6d6', style = 'rounded,filled']\n\n  # Edges\n  node1 -&gt; gentoo     [label = 'yes']\n  node1 -&gt; node2      [label = 'no']\n\n  node2 -&gt; adelie     [label = 'yes']\n  node2 -&gt; chinstrap  [label = 'no']\n}\n\")\n\n\n\n\n\n\n\n\nEntropy is a measure of disorder/chaos. We want ordered and organized data in the leaf nodes of our decision trees. So we want LOW entropy. Entropy is defined as:\n\\[ E = -\\sum_1^N p_i* log_2(p_i) \\]\nWhere \\(N\\) is the number of categories or labels in our outcome variable.\nThis is compared to gini impurity which is:\n\\[GI = 1 - \\sum_1^N p_i^2\\]\n\n\nWHY do we want the leaf nodes of our tree to be ordered (have low entropy or impurity?)?\n\n\n\n\nWhen you split a node, we now have two new nodes. In order to calculate the chaos (entropy or gini impurity) of the split, we have to calculate the chaos (entropy or gini impurity) for EACH of the new nodes and then calculate the weighted average chaos (entropy or gini impurity).\nThe reason we weight each node differently in this calculation, is because if a node has more data in it, than it has more impact, and therefore its measure of chaos (entropy or gini impurity) should count more.\nIn general, once you’ve calculated the chaos (entropy or gini impurity) for each of the new nodes, you’ll use this formula to calculate the weighted average:\n\\[ WC = (\\frac{N_L}{Total}* C_L) + (\\frac{N_R}{Total}* C_R)\\]\nWhere \\(N_L\\) is the number of data points in the Left Node, \\(N_R\\) is the number of data points in the Right Node, and \\(Total\\) is the total number of data points in that split. \\(C_R\\) and \\(C_L\\) are the chaos measure (entropy of gini impurity) for the right and left nodes, respectively.\n\n\n\nLet’s first build a Decision Tree to classify patients as diabetic or not diabetic.\nGini impurity is probability of misclassifying a random data point from that node.\n\n# Packages\nlibrary(readr)\nlibrary(caret)\n\nLoading required package: lattice\n\nlibrary(rpart)\n\n# Read data + peek\nd &lt;- read_csv(\"09-data/diabetes2.csv\")\n\nRows: 768 Columns: 9\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (9): Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, D...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(d)\n\n# A tibble: 6 × 9\n  Pregnancies Glucose BloodPressure SkinThickness Insulin   BMI\n        &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1           6     148            72            35       0  33.6\n2           1      85            66            29       0  26.6\n3           8     183            64             0       0  23.3\n4           1      89            66            23      94  28.1\n5           0     137            40            35     168  43.1\n6           5     116            74             0       0  25.6\n# ℹ 3 more variables: DiabetesPedigreeFunction &lt;dbl&gt;, Age &lt;dbl&gt;, Outcome &lt;dbl&gt;\n\n# Predictors / outcome\npredictors &lt;- c(\"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\n                \"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\")\n\nX &lt;- d[, predictors]\ny &lt;- d$Outcome   # should be 0/1\n\n# Train/test split (80/20), like random_state=1234\nset.seed(1234)\nidx_train &lt;- createDataPartition(y, p = 0.8, list = FALSE)\nX_train &lt;- X[idx_train, , drop = FALSE]\nX_test  &lt;- X[-idx_train, , drop = FALSE]\ny_train &lt;- y[idx_train]\ny_test  &lt;- y[-idx_train]\n\n# Standardize using TRAIN stats only, then apply to both\npp &lt;- preProcess(X_train, method = c(\"center\", \"scale\"))\nX_train_sc &lt;- predict(pp, X_train)\nX_test_sc  &lt;- predict(pp, X_test)\n\n# Fit decision tree (classification)\ntrain_df &lt;- data.frame(X_train_sc, Outcome = factor(y_train))\ntest_df  &lt;- data.frame(X_test_sc,  Outcome = factor(y_test))\n\nset.seed(1234)\ntree &lt;- rpart(Outcome ~ ., data = train_df, method = \"class\")\n\n# Predict classes\npred_test  &lt;- predict(tree, newdata = test_df, type = \"class\")\npred_train &lt;- predict(tree, newdata = train_df, type = \"class\")\n\n# Confusion matrices (test + train), like ConfusionMatrixDisplay\nconfusionMatrix(pred_test,  test_df$Outcome)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 76 26\n         1 15 36\n                                          \n               Accuracy : 0.732           \n                 95% CI : (0.6545, 0.8003)\n    No Information Rate : 0.5948          \n    P-Value [Acc &gt; NIR] : 0.0002754       \n                                          \n                  Kappa : 0.4279          \n                                          \n Mcnemar's Test P-Value : 0.1183498       \n                                          \n            Sensitivity : 0.8352          \n            Specificity : 0.5806          \n         Pos Pred Value : 0.7451          \n         Neg Pred Value : 0.7059          \n             Prevalence : 0.5948          \n         Detection Rate : 0.4967          \n   Detection Prevalence : 0.6667          \n      Balanced Accuracy : 0.7079          \n                                          \n       'Positive' Class : 0               \n                                          \n\nconfusionMatrix(pred_train, train_df$Outcome)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 368  60\n         1  41 146\n                                          \n               Accuracy : 0.8358          \n                 95% CI : (0.8041, 0.8642)\n    No Information Rate : 0.665           \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.6227          \n                                          \n Mcnemar's Test P-Value : 0.07328         \n                                          \n            Sensitivity : 0.8998          \n            Specificity : 0.7087          \n         Pos Pred Value : 0.8598          \n         Neg Pred Value : 0.7807          \n             Prevalence : 0.6650          \n         Detection Rate : 0.5984          \n   Detection Prevalence : 0.6959          \n      Balanced Accuracy : 0.8042          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\nWe talked about different ways to “prune” or limit the depth of a tree, both directly and indirectly via max_depth and min_samples_leaf. Run the following code, what does it tell you:\n\n# Function to run repeated train/test evaluation for a given maxdepth\neval_depth &lt;- function(data, depth = NULL, reps = 50, test_prop = 0.2, seed = 1234) {\n  set.seed(seed)\n  n &lt;- nrow(data)\n\n  out &lt;- data.frame(rep = integer(0), split = character(0), acc = numeric(0))\n\n  for (r in 1:reps) {\n    idx_test &lt;- sample.int(n, size = floor(test_prop * n))\n    train_df &lt;- data[-idx_test, c(predictors, \"Outcome\")]\n    test_df  &lt;- data[idx_test,  c(predictors, \"Outcome\")]\n\n    ctrl &lt;- rpart.control(cp = 0, xval = 0)\n    if (!is.null(depth)) ctrl$maxdepth &lt;- depth\n\n    fit &lt;- rpart(Outcome ~ ., data = train_df, method = \"class\", control = ctrl)\n\n    pred_train &lt;- predict(fit, newdata = train_df, type = \"class\")\n    pred_test  &lt;- predict(fit, newdata = test_df,  type = \"class\")\n\n    acc_train &lt;- mean(pred_train == train_df$Outcome)\n    acc_test  &lt;- mean(pred_test  == test_df$Outcome)\n\n    out &lt;- rbind(out,\n                 data.frame(rep = r, split = \"Test\",  acc = acc_test),\n                 data.frame(rep = r, split = \"Train\", acc = acc_train))\n  }\n  out\n}\n\n# Run across depths 2-9 and \"none\"\ndepths &lt;- 2:9\nall_res &lt;- data.frame()\n\nfor (dep in depths) {\n  tmp &lt;- eval_depth(d, depth = dep, reps = 60, test_prop = 0.2, seed = 1234)\n  tmp$depth &lt;- as.character(dep)\n  all_res &lt;- rbind(all_res, tmp)\n}\n\n# \"none\" = no maxdepth cap (use rpart default maxdepth)\ntmp_none &lt;- eval_depth(d, depth = NULL, reps = 60, test_prop = 0.2, seed = 1234)\ntmp_none$depth &lt;- \"none\"\nall_res &lt;- rbind(all_res, tmp_none)\n\nall_res$depth &lt;- factor(all_res$depth, levels = c(as.character(2:9), \"none\"))\nall_res$split &lt;- factor(all_res$split, levels = c(\"Test\", \"Train\"))\n\n# Plot \nggplot(all_res, aes(x = depth, y = acc, fill = split)) +\n  geom_boxplot(width = 0.7, position = position_dodge(width = 0.8)) +\n  scale_fill_manual(values = c(\"Test\" = \"#D76B63\", \"Train\" = \"#78D7E6\")) +\n  labs(\n    x = \"Restriction of Depth of Tree\",\n    y = \"Accuracy\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nNow let’s copy and paste the code from above and build a Random Forest to predict diabetes instead of a single tree, and then using a Gradient Boosting Tree.\n\nlibrary(randomForest)\n\n\n# Predictors and outcome\npredictors &lt;- c(\n  \"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\n  \"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\"\n)\n\nX &lt;- d[, predictors]\ny &lt;- d$Outcome\n\n# Train/test split (80/20)\nset.seed(1234)\ntrain_idx &lt;- createDataPartition(y, p = 0.8, list = FALSE)\n\nX_train &lt;- X[train_idx, , drop = FALSE]\nX_test  &lt;- X[-train_idx, , drop = FALSE]\ny_train &lt;- y[train_idx]\ny_test  &lt;- y[-train_idx]\n\n# Standardize predictors (z-score using TRAIN stats)\npp &lt;- preProcess(X_train, method = c(\"center\", \"scale\"))\nX_train_sc &lt;- predict(pp, X_train)\nX_test_sc  &lt;- predict(pp, X_test)\n# z scoring not important, because none of the variables are influencing/compared to each other directly scale doesn't matter here. But z scoring wont hurt.\n\n\n# Combine into data frames for modeling\ntrain_df &lt;- data.frame(X_train_sc, Outcome = factor(y_train))\ntest_df  &lt;- data.frame(X_test_sc,  Outcome = factor(y_test))\n\n# Fit Random Forest\nset.seed(1234)\nrf_model &lt;- randomForest(\n  Outcome ~ .,\n  data = train_df,\n  ntree = 500,        # number of trees\n  mtry = 3,           # variables tried at each split (default ~ sqrt(p))\n  importance = TRUE\n)\n\n# Predictions\npred_train &lt;- predict(rf_model, train_df)\npred_test  &lt;- predict(rf_model, test_df)\n\n# Confusion matrices\nconfusionMatrix(pred_train, train_df$Outcome)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 409   0\n         1   0 206\n                                    \n               Accuracy : 1         \n                 95% CI : (0.994, 1)\n    No Information Rate : 0.665     \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16 \n                                    \n                  Kappa : 1         \n                                    \n Mcnemar's Test P-Value : NA        \n                                    \n            Sensitivity : 1.000     \n            Specificity : 1.000     \n         Pos Pred Value : 1.000     \n         Neg Pred Value : 1.000     \n             Prevalence : 0.665     \n         Detection Rate : 0.665     \n   Detection Prevalence : 0.665     \n      Balanced Accuracy : 1.000     \n                                    \n       'Positive' Class : 0         \n                                    \n\nconfusionMatrix(pred_test,  test_df$Outcome)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 77 29\n         1 14 33\n                                          \n               Accuracy : 0.719           \n                 95% CI : (0.6407, 0.7886)\n    No Information Rate : 0.5948          \n    P-Value [Acc &gt; NIR] : 0.0009444       \n                                          \n                  Kappa : 0.3936          \n                                          \n Mcnemar's Test P-Value : 0.0327626       \n                                          \n            Sensitivity : 0.8462          \n            Specificity : 0.5323          \n         Pos Pred Value : 0.7264          \n         Neg Pred Value : 0.7021          \n             Prevalence : 0.5948          \n         Detection Rate : 0.5033          \n   Detection Prevalence : 0.6928          \n      Balanced Accuracy : 0.6892          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\nlibrary(gbm)\n\n# Predictors and outcome\npredictors &lt;- c(\n  \"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\n  \"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\"\n)\n\nX &lt;- d[, predictors]\ny &lt;- d$Outcome # must be numeric 0/1 and not factor as earlier\n\n\n# Train/test split (80/20)\nset.seed(1234)\ntrain_idx &lt;- createDataPartition(y, p = 0.8, list = FALSE)\n\nX_train &lt;- X[train_idx, , drop = FALSE]\nX_test  &lt;- X[-train_idx, , drop = FALSE]\ny_train &lt;- y[train_idx]\ny_test  &lt;- y[-train_idx]\n\n# Standardize predictors (z-score using TRAIN stats)\npp &lt;- preProcess(X_train, method = c(\"center\", \"scale\"))\nX_train_sc &lt;- predict(pp, X_train)\nX_test_sc  &lt;- predict(pp, X_test)\n# z scoring not important, because none of the variables are influencing/compared to each other directly scale doesn't matter here. But z scoring wont hurt.\n\n\n# Combine into data frames for modeling\ntrain_df &lt;- data.frame(X_train_sc, Outcome = y_train) \ntest_df  &lt;- data.frame(X_test_sc,  Outcome = y_test)\n\n# Fit Gradient Boosting model\nset.seed(1234)\ngb_model &lt;- gbm(\n  Outcome ~ .,\n  data = train_df,\n  distribution = \"bernoulli\",   # binary classification\n  n.trees = 300,\n  interaction.depth = 3,        # tree depth\n  shrinkage = 0.05,              # learning rate\n  n.minobsinnode = 10,\n  bag.fraction = 0.8,\n  verbose = FALSE\n)\n\n# Predict probabilities\nprob_train &lt;- predict(gb_model, train_df, n.trees = 300, type = \"response\")\nprob_test  &lt;- predict(gb_model, test_df,  n.trees = 300, type = \"response\")\n\n# Convert probabilities to class labels (0.5 threshold)\npred_train &lt;- factor(ifelse(prob_train &gt; 0.5, 1, 0))\npred_test  &lt;- factor(ifelse(prob_test  &gt; 0.5, 1, 0))\n\n# Convert truth to factor with matching levels\ntrain_truth &lt;- factor(train_df$Outcome, levels = c(0, 1))\ntest_truth  &lt;- factor(test_df$Outcome,  levels = c(0, 1))\n\npred_train &lt;- factor(pred_train, levels = c(0, 1))\npred_test  &lt;- factor(pred_test,  levels = c(0, 1))\n\n# Confusion matrices\nconfusionMatrix(pred_train, train_truth)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 392  43\n         1  17 163\n                                          \n               Accuracy : 0.9024          \n                 95% CI : (0.8762, 0.9247)\n    No Information Rate : 0.665           \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.7739          \n                                          \n Mcnemar's Test P-Value : 0.001249        \n                                          \n            Sensitivity : 0.9584          \n            Specificity : 0.7913          \n         Pos Pred Value : 0.9011          \n         Neg Pred Value : 0.9056          \n             Prevalence : 0.6650          \n         Detection Rate : 0.6374          \n   Detection Prevalence : 0.7073          \n      Balanced Accuracy : 0.8748          \n                                          \n       'Positive' Class : 0               \n                                          \n\nconfusionMatrix(pred_test,  test_truth)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 78 29\n         1 13 33\n                                          \n               Accuracy : 0.7255          \n                 95% CI : (0.6476, 0.7945)\n    No Information Rate : 0.5948          \n    P-Value [Acc &gt; NIR] : 0.0005179       \n                                          \n                  Kappa : 0.4061          \n                                          \n Mcnemar's Test P-Value : 0.0206376       \n                                          \n            Sensitivity : 0.8571          \n            Specificity : 0.5323          \n         Pos Pred Value : 0.7290          \n         Neg Pred Value : 0.7174          \n             Prevalence : 0.5948          \n         Detection Rate : 0.5098          \n   Detection Prevalence : 0.6993          \n      Balanced Accuracy : 0.6947          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\n\nLastly, let’s take a quick look at what we’d need to change if we wanted to predict a continuous value instead of a categorical one. Let’s look at this data set that measures risk propensity. We’re going to predict BART Scores (a score where higher values mean you’re riskier), based on a bunch of different measures.\nThese are the variables in the data set:\nBART: Balloon Analogue Risk Task - Measures risk-taking behavior - Higher scores means more willingness to take risks - This is the outcome (target variable)\nBIS/BAS: Behavioral Inhibition / Behavioral Activation Scales - BISmeans sensitivity to punishment / avoidance - BAS means sensitivity to reward / approach behavior\nFemale - Binary variable (0/1)\nGoal of the model is to use psychological traits (BIS/BAS + gender) to predict risk-taking behavior (BART score) using linear regression, evaluated with 5-fold cross-validation.\n\n# read and clean data----\nbart &lt;- read_csv(\n  \"09-data/bart.csv\"\n)\n\nRows: 1000 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (7): BIS.Score, BAS.Drive.Score, BAS.Fun.Seeking.Score, BAS.Reward.Respo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Drop missing values\nbart &lt;- na.omit(bart)\n\n# Reset row names \nrownames(bart) &lt;- NULL\n\n# Define predictors and outcome----\n# Outcome\ny &lt;- bart$BART\n\n# Predictors\npredictors &lt;- setdiff(colnames(bart), \"BART\")\n\n# Continuous predictors (everything except Female)\ncontin &lt;- setdiff(predictors, \"Female\")\n\nX &lt;- bart[, predictors]\n\n# set up CV with 5 folds-----\nset.seed(1234)\nkf &lt;- createFolds(y, k = 5, returnTrain = TRUE)\n\n# Storage for metrics ----\nmse  &lt;- list(train = c(), test = c())\nmae  &lt;- list(train = c(), test = c())\nmape &lt;- list(train = c(), test = c())\nr2   &lt;- list(train = c(), test = c())\n\n\n# Cross-validation loop\nfor (i in seq_along(kf)) {\n\n  train_idx &lt;- kf[[i]]\n  test_idx  &lt;- setdiff(seq_len(nrow(X)), train_idx)\n\n  X_train &lt;- X[train_idx, ]\n  X_test  &lt;- X[test_idx, ]\n  y_train &lt;- y[train_idx]\n  y_test  &lt;- y[test_idx]\n\n  # Z-score continuous predictors only\n  pp &lt;- preProcess(X_train[, contin], method = c(\"center\", \"scale\"))\n\n  X_train_sc &lt;- X_train\n  X_test_sc  &lt;- X_test\n\n  X_train_sc[, contin] &lt;- predict(pp, X_train[, contin])\n  X_test_sc[, contin]  &lt;- predict(pp, X_test[, contin])\n\n  # Fit linear regression\n  train_df &lt;- data.frame(X_train_sc, BART = y_train)\n  test_df  &lt;- data.frame(X_test_sc,  BART = y_test)\n\n  lm_fit &lt;- lm(BART ~ ., data = train_df)\n\n  # Predictions\n  pred_train &lt;- predict(lm_fit, train_df)\n  pred_test  &lt;- predict(lm_fit, test_df)\n\n  # Metrics\n  mse$train  &lt;- c(mse$train, mean((y_train - pred_train)^2))\n  mse$test   &lt;- c(mse$test,  mean((y_test  - pred_test)^2))\n\n  mae$train  &lt;- c(mae$train, mean(abs(y_train - pred_train)))\n  mae$test   &lt;- c(mae$test,  mean(abs(y_test  - pred_test)))\n\n  mape$train &lt;- c(mape$train, mean(abs((y_train - pred_train) / y_train)))\n  mape$test  &lt;- c(mape$test,  mean(abs((y_test  - pred_test)  / y_test)))\n\n  r2$train   &lt;- c(r2$train, cor(y_train, pred_train)^2)\n  r2$test    &lt;- c(r2$test,  cor(y_test,  pred_test)^2)\n}\n\n\n# Create summary table-----\nresults_table &lt;- data.frame(\n  Metric = c(\"MSE\", \"MAE\", \"MAPE\", \"R²\"),\n  Train = c(\n    mean(mse$train),\n    mean(mae$train),\n    mean(mape$train),\n    mean(r2$train)\n  ),\n  Test = c(\n    mean(mse$test),\n    mean(mae$test),\n    mean(mape$test),\n    mean(r2$test)\n  )\n)\n\n# Print table\nresults_table\n\n  Metric        Train         Test\n1    MSE 143.01960222 146.17530363\n2    MAE  10.49211192  10.59655360\n3   MAPE   0.87809521   0.88646195\n4     R²   0.06309417   0.05407431\n\n\nWe can interpret the results as follows: The linear regression model shows poor predictive performance, explaining only 5–6% of the variance in BART scores. Train and test errors were nearly identical, indicating no overfitting but substantial underfitting. These results suggest that BIS/BAS traits and gender alone are insufficient predictors of risk-taking behavior as measured by the BART.\nNext step would be to try non-linear models (Random Forest, Gradient Boosting).\n\nset.seed(1234)\nfolds &lt;- createFolds(y, k = 5, returnTrain = TRUE)\n\nmse  &lt;- list(train = c(), test = c())\nmae  &lt;- list(train = c(), test = c())\nmape &lt;- list(train = c(), test = c())\nr2   &lt;- list(train = c(), test = c())\n\nfor (i in seq_along(folds)) {\n  tr &lt;- folds[[i]]\n  te &lt;- setdiff(seq_len(nrow(X)), tr)\n\n  X_train &lt;- X[tr, , drop = FALSE]\n  X_test  &lt;- X[te, , drop = FALSE]\n  y_train &lt;- y[tr]\n  y_test  &lt;- y[te]\n\n  # z-score continuous predictors using TRAIN stats\n  pp &lt;- preProcess(X_train[, contin, drop = FALSE], method = c(\"center\", \"scale\"))\n  X_train_sc &lt;- X_train; X_test_sc &lt;- X_test\n  X_train_sc[, contin] &lt;- predict(pp, X_train[, contin, drop = FALSE])\n  X_test_sc[, contin]  &lt;- predict(pp, X_test[, contin, drop = FALSE])\n\n  train_df &lt;- data.frame(X_train_sc, BART = y_train)\n  test_df  &lt;- data.frame(X_test_sc,  BART = y_test)\n\n  set.seed(1234)\n  rf_fit &lt;- randomForest(\n    BART ~ .,\n    data = train_df,\n    ntree = 500,\n    mtry = max(1, floor(sqrt(length(predictors))))\n  )\n\n  pred_train &lt;- predict(rf_fit, train_df)\n  pred_test  &lt;- predict(rf_fit, test_df)\n\n  mse$train  &lt;- c(mse$train, mean((y_train - pred_train)^2))\n  mse$test   &lt;- c(mse$test,  mean((y_test  - pred_test)^2))\n\n  mae$train  &lt;- c(mae$train, mean(abs(y_train - pred_train)))\n  mae$test   &lt;- c(mae$test,  mean(abs(y_test  - pred_test)))\n\n  mape$train &lt;- c(mape$train, mean(abs((y_train - pred_train) / y_train)))\n  mape$test  &lt;- c(mape$test,  mean(abs((y_test  - pred_test)  / y_test)))\n\n  r2$train   &lt;- c(r2$train, cor(y_train, pred_train)^2)\n  r2$test    &lt;- c(r2$test,  cor(y_test,  pred_test)^2)\n}\n\nrf_results &lt;- data.frame(\n  Model = \"Random Forest\",\n  Metric = c(\"MSE\", \"MAE\", \"MAPE\", \"R^2\"),\n  Train = c(mean(mse$train), mean(mae$train), mean(mape$train), mean(r2$train)),\n  Test  = c(mean(mse$test),  mean(mae$test),  mean(mape$test),  mean(r2$test))\n)\n\nrf_results\n\n          Model Metric      Train        Test\n1 Random Forest    MSE 49.4503557 133.4441967\n2 Random Forest    MAE  5.7125156   9.5465513\n3 Random Forest   MAPE  0.4700485   0.7912291\n4 Random Forest    R^2  0.7497229   0.1468698\n\n\n\nset.seed(1234)\nfolds &lt;- createFolds(y, k = 5, returnTrain = TRUE)\n\nmse_g  &lt;- list(train = c(), test = c())\nmae_g  &lt;- list(train = c(), test = c())\nmape_g &lt;- list(train = c(), test = c())\nr2_g   &lt;- list(train = c(), test = c())\n\nfor (i in seq_along(folds)) {\n  tr &lt;- folds[[i]]\n  te &lt;- setdiff(seq_len(nrow(X)), tr)\n\n  X_train &lt;- X[tr, , drop = FALSE]\n  X_test  &lt;- X[te, , drop = FALSE]\n  y_train &lt;- y[tr]\n  y_test  &lt;- y[te]\n\n  # z-score continuous predictors using TRAIN stats\n  pp &lt;- preProcess(X_train[, contin, drop = FALSE], method = c(\"center\", \"scale\"))\n  X_train_sc &lt;- X_train; X_test_sc &lt;- X_test\n  X_train_sc[, contin] &lt;- predict(pp, X_train[, contin, drop = FALSE])\n  X_test_sc[, contin]  &lt;- predict(pp, X_test[, contin, drop = FALSE])\n\n  train_df &lt;- data.frame(X_train_sc, BART = y_train)\n  test_df  &lt;- data.frame(X_test_sc,  BART = y_test)\n\n  set.seed(1234)\n  gb_fit &lt;- gbm(\n    BART ~ .,\n    data = train_df,\n    distribution = \"gaussian\",     # regression\n    n.trees = 1500,\n    interaction.depth = 3,\n    shrinkage = 0.01,\n    n.minobsinnode = 10,\n    bag.fraction = 0.8,\n    verbose = FALSE\n  )\n\n  pred_train &lt;- predict(gb_fit, train_df, n.trees = 1500)\n  pred_test  &lt;- predict(gb_fit, test_df,  n.trees = 1500)\n\n  mse_g$train  &lt;- c(mse_g$train, mean((y_train - pred_train)^2))\n  mse_g$test   &lt;- c(mse_g$test,  mean((y_test  - pred_test)^2))\n\n  mae_g$train  &lt;- c(mae_g$train, mean(abs(y_train - pred_train)))\n  mae_g$test   &lt;- c(mae_g$test,  mean(abs(y_test  - pred_test)))\n\n  mape_g$train &lt;- c(mape_g$train, mean(abs((y_train - pred_train) / y_train)))\n  mape_g$test  &lt;- c(mape_g$test,  mean(abs((y_test  - pred_test)  / y_test)))\n\n  r2_g$train   &lt;- c(r2_g$train, cor(y_train, pred_train)^2)\n  r2_g$test    &lt;- c(r2_g$test,  cor(y_test,  pred_test)^2)\n}\n\ngb_results &lt;- data.frame(\n  Model = \"Gradient Boosting\",\n  Metric = c(\"MSE\", \"MAE\", \"MAPE\", \"R^2\"),\n  Train = c(mean(mse_g$train), mean(mae_g$train), mean(mape_g$train), mean(r2_g$train)),\n  Test  = c(mean(mse_g$test),  mean(mae_g$test),  mean(mape_g$test),  mean(r2_g$test))\n)\n\ngb_results\n\n              Model Metric       Train        Test\n1 Gradient Boosting    MSE 100.5563924 131.6130762\n2 Gradient Boosting    MAE   8.2969512   9.5293227\n3 Gradient Boosting   MAPE   0.6858956   0.7808945\n4 Gradient Boosting    R^2   0.3510419   0.1564954\n\n\nLet’s combine the results to compare:\n\nall_results &lt;- rbind(rf_results, gb_results)\nall_results\n\n              Model Metric       Train        Test\n1     Random Forest    MSE  49.4503557 133.4441967\n2     Random Forest    MAE   5.7125156   9.5465513\n3     Random Forest   MAPE   0.4700485   0.7912291\n4     Random Forest    R^2   0.7497229   0.1468698\n5 Gradient Boosting    MSE 100.5563924 131.6130762\n6 Gradient Boosting    MAE   8.2969512   9.5293227\n7 Gradient Boosting   MAPE   0.6858956   0.7808945\n8 Gradient Boosting    R^2   0.3510419   0.1564954\n\n\nWhat do you conclude?\n\n\n\n\n\n\nInterpretation\n\n\n\n\n\nWhile Random Forest achieved excellent training performance, it substantially overfit and failed to generalize. Gradient Boosting produced slightly better test performance and more stable behavior, though overall predictive power remained modest. These results suggest that nonlinear relationships exist but that BIS/BAS measures and gender alone account for only a small portion of variance in BART risk-taking behavior.\n\n\n\n\n\n\n\n# Assuming rf_fit was trained on train_df\nrf_imp &lt;- importance(rf_fit)\n\n# Convert to data frame\nrf_imp_df &lt;- data.frame(\n  Feature = rownames(rf_imp),\n  Importance = rf_imp[, \"IncNodePurity\"]\n)\n\n# Sort by importance\nrf_imp_df &lt;- rf_imp_df[order(rf_imp_df$Importance, decreasing = TRUE), ]\n\nrf_imp_df\n\n                                                        Feature Importance\nBAS.Drive.Score                                 BAS.Drive.Score  17676.021\nBAS.Fun.Seeking.Score                     BAS.Fun.Seeking.Score  16728.337\nAge                                                         Age  15398.407\nBIS.Score                                             BIS.Score  14628.120\nBAS.Reward.Responsiveness.Score BAS.Reward.Responsiveness.Score  14473.869\nFemale                                                   Female   3253.537\n\n\n\n# Relative influence from gbm\ngb_imp &lt;- summary(gb_fit, plotit = FALSE)\n\ngb_imp\n\n                                                            var   rel.inf\nBAS.Fun.Seeking.Score                     BAS.Fun.Seeking.Score 25.462993\nBAS.Drive.Score                                 BAS.Drive.Score 25.375267\nBAS.Reward.Responsiveness.Score BAS.Reward.Responsiveness.Score 21.312970\nBIS.Score                                             BIS.Score 13.361662\nAge                                                         Age 12.722097\nFemale                                                   Female  1.765011\n\n\n\nlibrary(patchwork)\n\n# Random Forest plot\np_rf &lt;- ggplot(rf_imp_df, aes(x = reorder(Feature, Importance), y = Importance)) +\n  geom_col(fill = \"#2F6F73\") +\n  coord_flip() +\n  labs(\n    title = \"Random Forest\",\n    x = \"Feature\",\n    y = \"Importance (IncNodePurity)\"\n  ) +\n  theme_minimal()\n\n# Gradient Boosting plot\np_gb &lt;- ggplot(gb_imp, aes(x = reorder(var, rel.inf), y = rel.inf)) +\n  geom_col(fill = \"#F28E2B\") +\n  coord_flip() +\n  labs(\n    title = \"Gradient Boosting\",\n    x = \"Feature\",\n    y = \"Relative Influence\"\n  ) +\n  theme_minimal()\n\np_rf / p_gb\n\n\n\n\n\n\n\n\nFeature importance analyses from both Random Forest and Gradient Boosting consistently identified BAS-related traits, particularly Drive and Fun Seeking, as the most influential predictors of BART risk-taking behavior. In contrast, BIS, age, and gender showed relatively weak influence. Although these findings suggest that reward sensitivity plays a central role in risk-taking, the overall predictive power of the models remained modest, indicating substantial unexplained variability."
  },
  {
    "objectID": "teaching/stat-learn/material/09/09-trees.html#models",
    "href": "teaching/stat-learn/material/09/09-trees.html#models",
    "title": "Tree Based Methods",
    "section": "",
    "text": "Decision Tree\nRandom Forest\nGradient Boosting Tree\n\n\n\n\nnum trees\none\nmany\nmany\n\n\nmake predictions\nmode or mean of leaf node\neach tree votes\nsum of tree outputs\n\n\ntree independence\nNOT applicable\nindependent\ndependent\n\n\nData Used\nall\nbagging + random feature selection\nall"
  },
  {
    "objectID": "teaching/stat-learn/material/09/09-trees.html#decision-trees-graphically",
    "href": "teaching/stat-learn/material/09/09-trees.html#decision-trees-graphically",
    "title": "Tree Based Methods",
    "section": "",
    "text": "Let’s load the penguin data set, and plot the bill length and bill depth for our three species:\n\n# Load libraries\nlibrary(readr)\nlibrary(ggplot2)\n\n# Read the data\npengwing &lt;- read_csv(\"09-data/penguins.csv\")\n\n# View first few rows (equivalent to .head())\nhead(pengwing)\n\n# A tibble: 6 × 9\n   ...1 species island    bill_length_mm bill_depth_mm flipper_length_mm\n  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;\n1     0 Adelie  Torgersen           39.1          18.7               181\n2     1 Adelie  Torgersen           39.5          17.4               186\n3     2 Adelie  Torgersen           40.3          18                 195\n4     3 Adelie  Torgersen           NA            NA                  NA\n5     4 Adelie  Torgersen           36.7          19.3               193\n6     5 Adelie  Torgersen           39.3          20.6               190\n# ℹ 3 more variables: body_mass_g &lt;dbl&gt;, sex &lt;chr&gt;, year &lt;dbl&gt;\n\n# Create the plot\nggplot(pengwing, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point() +\n  theme_minimal() +\n  labs(\n    x = \"Bill Length (in mm)\",\n    y = \"Bill Depth (in mm)\",\n    title = \"Bill Length vs. Bill Depth by Species\"\n  )\n\n\n\n\n\n\n\n\nWe could use a decision tree based on bill length and bill depth to classify penguins as different species. First, we could split on Bill Depth and decide that any penguin with a depth less than 16.5 mm, should be classified as a Gentoo penguin.\n\n# Choose a split value\nsplit1 &lt;- 16.5\n\nggplot(pengwing, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point() +\n  theme_minimal() +\n  labs(\n    x = \"Bill Length (in mm)\",\n    y = \"Bill Depth (in mm)\",\n    title = \"Bill Length vs. Bill Depth by Species\"\n  ) +\n  geom_hline(yintercept = split1, linewidth = 1, linetype = \"dashed\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThat bottom group looks GREAT. Now let’s look at the top group. Most of the Chinstrap penguins have longer bill lengths. Let’s say that if a penguin has a bill depth &gt; 16.5mm, then we will split on bill length at 44 to separate the Adelie and Chinstrap penguins.\n\n# Choose a split value \nsplit2 &lt;- 44\n\nggplot(pengwing, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point() +\n  theme_minimal() +\n  labs(\n    x = \"Bill Length (in mm)\",\n    y = \"Bill Depth (in mm)\",\n    title = \"Bill Length vs. Bill Depth by Species\"\n  ) +\n  geom_hline(yintercept = split1, linewidth = 1, linetype = \"dashed\") +\n  geom_segment(\n    x = split2, xend = split2,\n    y = split1, yend = 22,\n    linewidth = 0.6, linetype = \"dashed\", color = \"black\"\n  )\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nVoila! We’ve built a (very short) decision tree! It would look like this:\n\n# Install if needed\n# install.packages(\"DiagrammeR\")\n\nlibrary(DiagrammeR)\n\ngrViz(\"\ndigraph penguin_tree {\n\n  graph [layout = dot, rankdir = TB]\n\n  node [\n    shape = box,\n    style = rounded,\n    fontname = Helvetica,\n    fontsize = 14\n  ]\n\n  # Decision nodes\n  node1 [label = 'bill_depth &lt; 16.5']\n  node2 [label = 'bill_length &lt; 44']\n\n  # Leaf nodes\n  gentoo    [label = 'Gentoo', fillcolor = '#dbe9f6', style = 'rounded,filled']\n  chinstrap[label = 'Chinstrap', fillcolor = '#e3f0dd', style = 'rounded,filled']\n  adelie   [label = 'Adelie', fillcolor = '#f4d6d6', style = 'rounded,filled']\n\n  # Edges\n  node1 -&gt; gentoo     [label = 'yes']\n  node1 -&gt; node2      [label = 'no']\n\n  node2 -&gt; adelie     [label = 'yes']\n  node2 -&gt; chinstrap  [label = 'no']\n}\n\")"
  },
  {
    "objectID": "teaching/stat-learn/material/09/09-trees.html#entropy",
    "href": "teaching/stat-learn/material/09/09-trees.html#entropy",
    "title": "Tree Based Methods",
    "section": "",
    "text": "Entropy is a measure of disorder/chaos. We want ordered and organized data in the leaf nodes of our decision trees. So we want LOW entropy. Entropy is defined as:\n\\[ E = -\\sum_1^N p_i* log_2(p_i) \\]\nWhere \\(N\\) is the number of categories or labels in our outcome variable.\nThis is compared to gini impurity which is:\n\\[GI = 1 - \\sum_1^N p_i^2\\]\n\n\nWHY do we want the leaf nodes of our tree to be ordered (have low entropy or impurity?)?"
  },
  {
    "objectID": "teaching/stat-learn/material/09/09-trees.html#measures-of-chaos-for-a-split",
    "href": "teaching/stat-learn/material/09/09-trees.html#measures-of-chaos-for-a-split",
    "title": "Tree Based Methods",
    "section": "",
    "text": "When you split a node, we now have two new nodes. In order to calculate the chaos (entropy or gini impurity) of the split, we have to calculate the chaos (entropy or gini impurity) for EACH of the new nodes and then calculate the weighted average chaos (entropy or gini impurity).\nThe reason we weight each node differently in this calculation, is because if a node has more data in it, than it has more impact, and therefore its measure of chaos (entropy or gini impurity) should count more.\nIn general, once you’ve calculated the chaos (entropy or gini impurity) for each of the new nodes, you’ll use this formula to calculate the weighted average:\n\\[ WC = (\\frac{N_L}{Total}* C_L) + (\\frac{N_R}{Total}* C_R)\\]\nWhere \\(N_L\\) is the number of data points in the Left Node, \\(N_R\\) is the number of data points in the Right Node, and \\(Total\\) is the total number of data points in that split. \\(C_R\\) and \\(C_L\\) are the chaos measure (entropy of gini impurity) for the right and left nodes, respectively."
  },
  {
    "objectID": "teaching/stat-learn/material/09/09-trees.html#decision-trees",
    "href": "teaching/stat-learn/material/09/09-trees.html#decision-trees",
    "title": "Tree Based Methods",
    "section": "",
    "text": "Let’s first build a Decision Tree to classify patients as diabetic or not diabetic.\nGini impurity is probability of misclassifying a random data point from that node.\n\n# Packages\nlibrary(readr)\nlibrary(caret)\n\nLoading required package: lattice\n\nlibrary(rpart)\n\n# Read data + peek\nd &lt;- read_csv(\"09-data/diabetes2.csv\")\n\nRows: 768 Columns: 9\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (9): Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, D...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(d)\n\n# A tibble: 6 × 9\n  Pregnancies Glucose BloodPressure SkinThickness Insulin   BMI\n        &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1           6     148            72            35       0  33.6\n2           1      85            66            29       0  26.6\n3           8     183            64             0       0  23.3\n4           1      89            66            23      94  28.1\n5           0     137            40            35     168  43.1\n6           5     116            74             0       0  25.6\n# ℹ 3 more variables: DiabetesPedigreeFunction &lt;dbl&gt;, Age &lt;dbl&gt;, Outcome &lt;dbl&gt;\n\n# Predictors / outcome\npredictors &lt;- c(\"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\n                \"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\")\n\nX &lt;- d[, predictors]\ny &lt;- d$Outcome   # should be 0/1\n\n# Train/test split (80/20), like random_state=1234\nset.seed(1234)\nidx_train &lt;- createDataPartition(y, p = 0.8, list = FALSE)\nX_train &lt;- X[idx_train, , drop = FALSE]\nX_test  &lt;- X[-idx_train, , drop = FALSE]\ny_train &lt;- y[idx_train]\ny_test  &lt;- y[-idx_train]\n\n# Standardize using TRAIN stats only, then apply to both\npp &lt;- preProcess(X_train, method = c(\"center\", \"scale\"))\nX_train_sc &lt;- predict(pp, X_train)\nX_test_sc  &lt;- predict(pp, X_test)\n\n# Fit decision tree (classification)\ntrain_df &lt;- data.frame(X_train_sc, Outcome = factor(y_train))\ntest_df  &lt;- data.frame(X_test_sc,  Outcome = factor(y_test))\n\nset.seed(1234)\ntree &lt;- rpart(Outcome ~ ., data = train_df, method = \"class\")\n\n# Predict classes\npred_test  &lt;- predict(tree, newdata = test_df, type = \"class\")\npred_train &lt;- predict(tree, newdata = train_df, type = \"class\")\n\n# Confusion matrices (test + train), like ConfusionMatrixDisplay\nconfusionMatrix(pred_test,  test_df$Outcome)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 76 26\n         1 15 36\n                                          \n               Accuracy : 0.732           \n                 95% CI : (0.6545, 0.8003)\n    No Information Rate : 0.5948          \n    P-Value [Acc &gt; NIR] : 0.0002754       \n                                          \n                  Kappa : 0.4279          \n                                          \n Mcnemar's Test P-Value : 0.1183498       \n                                          \n            Sensitivity : 0.8352          \n            Specificity : 0.5806          \n         Pos Pred Value : 0.7451          \n         Neg Pred Value : 0.7059          \n             Prevalence : 0.5948          \n         Detection Rate : 0.4967          \n   Detection Prevalence : 0.6667          \n      Balanced Accuracy : 0.7079          \n                                          \n       'Positive' Class : 0               \n                                          \n\nconfusionMatrix(pred_train, train_df$Outcome)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 368  60\n         1  41 146\n                                          \n               Accuracy : 0.8358          \n                 95% CI : (0.8041, 0.8642)\n    No Information Rate : 0.665           \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.6227          \n                                          \n Mcnemar's Test P-Value : 0.07328         \n                                          \n            Sensitivity : 0.8998          \n            Specificity : 0.7087          \n         Pos Pred Value : 0.8598          \n         Neg Pred Value : 0.7807          \n             Prevalence : 0.6650          \n         Detection Rate : 0.5984          \n   Detection Prevalence : 0.6959          \n      Balanced Accuracy : 0.8042          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\nWe talked about different ways to “prune” or limit the depth of a tree, both directly and indirectly via max_depth and min_samples_leaf. Run the following code, what does it tell you:\n\n# Function to run repeated train/test evaluation for a given maxdepth\neval_depth &lt;- function(data, depth = NULL, reps = 50, test_prop = 0.2, seed = 1234) {\n  set.seed(seed)\n  n &lt;- nrow(data)\n\n  out &lt;- data.frame(rep = integer(0), split = character(0), acc = numeric(0))\n\n  for (r in 1:reps) {\n    idx_test &lt;- sample.int(n, size = floor(test_prop * n))\n    train_df &lt;- data[-idx_test, c(predictors, \"Outcome\")]\n    test_df  &lt;- data[idx_test,  c(predictors, \"Outcome\")]\n\n    ctrl &lt;- rpart.control(cp = 0, xval = 0)\n    if (!is.null(depth)) ctrl$maxdepth &lt;- depth\n\n    fit &lt;- rpart(Outcome ~ ., data = train_df, method = \"class\", control = ctrl)\n\n    pred_train &lt;- predict(fit, newdata = train_df, type = \"class\")\n    pred_test  &lt;- predict(fit, newdata = test_df,  type = \"class\")\n\n    acc_train &lt;- mean(pred_train == train_df$Outcome)\n    acc_test  &lt;- mean(pred_test  == test_df$Outcome)\n\n    out &lt;- rbind(out,\n                 data.frame(rep = r, split = \"Test\",  acc = acc_test),\n                 data.frame(rep = r, split = \"Train\", acc = acc_train))\n  }\n  out\n}\n\n# Run across depths 2-9 and \"none\"\ndepths &lt;- 2:9\nall_res &lt;- data.frame()\n\nfor (dep in depths) {\n  tmp &lt;- eval_depth(d, depth = dep, reps = 60, test_prop = 0.2, seed = 1234)\n  tmp$depth &lt;- as.character(dep)\n  all_res &lt;- rbind(all_res, tmp)\n}\n\n# \"none\" = no maxdepth cap (use rpart default maxdepth)\ntmp_none &lt;- eval_depth(d, depth = NULL, reps = 60, test_prop = 0.2, seed = 1234)\ntmp_none$depth &lt;- \"none\"\nall_res &lt;- rbind(all_res, tmp_none)\n\nall_res$depth &lt;- factor(all_res$depth, levels = c(as.character(2:9), \"none\"))\nall_res$split &lt;- factor(all_res$split, levels = c(\"Test\", \"Train\"))\n\n# Plot \nggplot(all_res, aes(x = depth, y = acc, fill = split)) +\n  geom_boxplot(width = 0.7, position = position_dodge(width = 0.8)) +\n  scale_fill_manual(values = c(\"Test\" = \"#D76B63\", \"Train\" = \"#78D7E6\")) +\n  labs(\n    x = \"Restriction of Depth of Tree\",\n    y = \"Accuracy\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nNow let’s copy and paste the code from above and build a Random Forest to predict diabetes instead of a single tree, and then using a Gradient Boosting Tree.\n\nlibrary(randomForest)\n\n\n# Predictors and outcome\npredictors &lt;- c(\n  \"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\n  \"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\"\n)\n\nX &lt;- d[, predictors]\ny &lt;- d$Outcome\n\n# Train/test split (80/20)\nset.seed(1234)\ntrain_idx &lt;- createDataPartition(y, p = 0.8, list = FALSE)\n\nX_train &lt;- X[train_idx, , drop = FALSE]\nX_test  &lt;- X[-train_idx, , drop = FALSE]\ny_train &lt;- y[train_idx]\ny_test  &lt;- y[-train_idx]\n\n# Standardize predictors (z-score using TRAIN stats)\npp &lt;- preProcess(X_train, method = c(\"center\", \"scale\"))\nX_train_sc &lt;- predict(pp, X_train)\nX_test_sc  &lt;- predict(pp, X_test)\n# z scoring not important, because none of the variables are influencing/compared to each other directly scale doesn't matter here. But z scoring wont hurt.\n\n\n# Combine into data frames for modeling\ntrain_df &lt;- data.frame(X_train_sc, Outcome = factor(y_train))\ntest_df  &lt;- data.frame(X_test_sc,  Outcome = factor(y_test))\n\n# Fit Random Forest\nset.seed(1234)\nrf_model &lt;- randomForest(\n  Outcome ~ .,\n  data = train_df,\n  ntree = 500,        # number of trees\n  mtry = 3,           # variables tried at each split (default ~ sqrt(p))\n  importance = TRUE\n)\n\n# Predictions\npred_train &lt;- predict(rf_model, train_df)\npred_test  &lt;- predict(rf_model, test_df)\n\n# Confusion matrices\nconfusionMatrix(pred_train, train_df$Outcome)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 409   0\n         1   0 206\n                                    \n               Accuracy : 1         \n                 95% CI : (0.994, 1)\n    No Information Rate : 0.665     \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16 \n                                    \n                  Kappa : 1         \n                                    \n Mcnemar's Test P-Value : NA        \n                                    \n            Sensitivity : 1.000     \n            Specificity : 1.000     \n         Pos Pred Value : 1.000     \n         Neg Pred Value : 1.000     \n             Prevalence : 0.665     \n         Detection Rate : 0.665     \n   Detection Prevalence : 0.665     \n      Balanced Accuracy : 1.000     \n                                    \n       'Positive' Class : 0         \n                                    \n\nconfusionMatrix(pred_test,  test_df$Outcome)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 77 29\n         1 14 33\n                                          \n               Accuracy : 0.719           \n                 95% CI : (0.6407, 0.7886)\n    No Information Rate : 0.5948          \n    P-Value [Acc &gt; NIR] : 0.0009444       \n                                          \n                  Kappa : 0.3936          \n                                          \n Mcnemar's Test P-Value : 0.0327626       \n                                          \n            Sensitivity : 0.8462          \n            Specificity : 0.5323          \n         Pos Pred Value : 0.7264          \n         Neg Pred Value : 0.7021          \n             Prevalence : 0.5948          \n         Detection Rate : 0.5033          \n   Detection Prevalence : 0.6928          \n      Balanced Accuracy : 0.6892          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\nlibrary(gbm)\n\n# Predictors and outcome\npredictors &lt;- c(\n  \"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\n  \"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\"\n)\n\nX &lt;- d[, predictors]\ny &lt;- d$Outcome # must be numeric 0/1 and not factor as earlier\n\n\n# Train/test split (80/20)\nset.seed(1234)\ntrain_idx &lt;- createDataPartition(y, p = 0.8, list = FALSE)\n\nX_train &lt;- X[train_idx, , drop = FALSE]\nX_test  &lt;- X[-train_idx, , drop = FALSE]\ny_train &lt;- y[train_idx]\ny_test  &lt;- y[-train_idx]\n\n# Standardize predictors (z-score using TRAIN stats)\npp &lt;- preProcess(X_train, method = c(\"center\", \"scale\"))\nX_train_sc &lt;- predict(pp, X_train)\nX_test_sc  &lt;- predict(pp, X_test)\n# z scoring not important, because none of the variables are influencing/compared to each other directly scale doesn't matter here. But z scoring wont hurt.\n\n\n# Combine into data frames for modeling\ntrain_df &lt;- data.frame(X_train_sc, Outcome = y_train) \ntest_df  &lt;- data.frame(X_test_sc,  Outcome = y_test)\n\n# Fit Gradient Boosting model\nset.seed(1234)\ngb_model &lt;- gbm(\n  Outcome ~ .,\n  data = train_df,\n  distribution = \"bernoulli\",   # binary classification\n  n.trees = 300,\n  interaction.depth = 3,        # tree depth\n  shrinkage = 0.05,              # learning rate\n  n.minobsinnode = 10,\n  bag.fraction = 0.8,\n  verbose = FALSE\n)\n\n# Predict probabilities\nprob_train &lt;- predict(gb_model, train_df, n.trees = 300, type = \"response\")\nprob_test  &lt;- predict(gb_model, test_df,  n.trees = 300, type = \"response\")\n\n# Convert probabilities to class labels (0.5 threshold)\npred_train &lt;- factor(ifelse(prob_train &gt; 0.5, 1, 0))\npred_test  &lt;- factor(ifelse(prob_test  &gt; 0.5, 1, 0))\n\n# Convert truth to factor with matching levels\ntrain_truth &lt;- factor(train_df$Outcome, levels = c(0, 1))\ntest_truth  &lt;- factor(test_df$Outcome,  levels = c(0, 1))\n\npred_train &lt;- factor(pred_train, levels = c(0, 1))\npred_test  &lt;- factor(pred_test,  levels = c(0, 1))\n\n# Confusion matrices\nconfusionMatrix(pred_train, train_truth)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 392  43\n         1  17 163\n                                          \n               Accuracy : 0.9024          \n                 95% CI : (0.8762, 0.9247)\n    No Information Rate : 0.665           \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.7739          \n                                          \n Mcnemar's Test P-Value : 0.001249        \n                                          \n            Sensitivity : 0.9584          \n            Specificity : 0.7913          \n         Pos Pred Value : 0.9011          \n         Neg Pred Value : 0.9056          \n             Prevalence : 0.6650          \n         Detection Rate : 0.6374          \n   Detection Prevalence : 0.7073          \n      Balanced Accuracy : 0.8748          \n                                          \n       'Positive' Class : 0               \n                                          \n\nconfusionMatrix(pred_test,  test_truth)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 78 29\n         1 13 33\n                                          \n               Accuracy : 0.7255          \n                 95% CI : (0.6476, 0.7945)\n    No Information Rate : 0.5948          \n    P-Value [Acc &gt; NIR] : 0.0005179       \n                                          \n                  Kappa : 0.4061          \n                                          \n Mcnemar's Test P-Value : 0.0206376       \n                                          \n            Sensitivity : 0.8571          \n            Specificity : 0.5323          \n         Pos Pred Value : 0.7290          \n         Neg Pred Value : 0.7174          \n             Prevalence : 0.5948          \n         Detection Rate : 0.5098          \n   Detection Prevalence : 0.6993          \n      Balanced Accuracy : 0.6947          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\n\n\n\nLastly, let’s take a quick look at what we’d need to change if we wanted to predict a continuous value instead of a categorical one. Let’s look at this data set that measures risk propensity. We’re going to predict BART Scores (a score where higher values mean you’re riskier), based on a bunch of different measures.\nThese are the variables in the data set:\nBART: Balloon Analogue Risk Task - Measures risk-taking behavior - Higher scores means more willingness to take risks - This is the outcome (target variable)\nBIS/BAS: Behavioral Inhibition / Behavioral Activation Scales - BISmeans sensitivity to punishment / avoidance - BAS means sensitivity to reward / approach behavior\nFemale - Binary variable (0/1)\nGoal of the model is to use psychological traits (BIS/BAS + gender) to predict risk-taking behavior (BART score) using linear regression, evaluated with 5-fold cross-validation.\n\n# read and clean data----\nbart &lt;- read_csv(\n  \"09-data/bart.csv\"\n)\n\nRows: 1000 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (7): BIS.Score, BAS.Drive.Score, BAS.Fun.Seeking.Score, BAS.Reward.Respo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Drop missing values\nbart &lt;- na.omit(bart)\n\n# Reset row names \nrownames(bart) &lt;- NULL\n\n# Define predictors and outcome----\n# Outcome\ny &lt;- bart$BART\n\n# Predictors\npredictors &lt;- setdiff(colnames(bart), \"BART\")\n\n# Continuous predictors (everything except Female)\ncontin &lt;- setdiff(predictors, \"Female\")\n\nX &lt;- bart[, predictors]\n\n# set up CV with 5 folds-----\nset.seed(1234)\nkf &lt;- createFolds(y, k = 5, returnTrain = TRUE)\n\n# Storage for metrics ----\nmse  &lt;- list(train = c(), test = c())\nmae  &lt;- list(train = c(), test = c())\nmape &lt;- list(train = c(), test = c())\nr2   &lt;- list(train = c(), test = c())\n\n\n# Cross-validation loop\nfor (i in seq_along(kf)) {\n\n  train_idx &lt;- kf[[i]]\n  test_idx  &lt;- setdiff(seq_len(nrow(X)), train_idx)\n\n  X_train &lt;- X[train_idx, ]\n  X_test  &lt;- X[test_idx, ]\n  y_train &lt;- y[train_idx]\n  y_test  &lt;- y[test_idx]\n\n  # Z-score continuous predictors only\n  pp &lt;- preProcess(X_train[, contin], method = c(\"center\", \"scale\"))\n\n  X_train_sc &lt;- X_train\n  X_test_sc  &lt;- X_test\n\n  X_train_sc[, contin] &lt;- predict(pp, X_train[, contin])\n  X_test_sc[, contin]  &lt;- predict(pp, X_test[, contin])\n\n  # Fit linear regression\n  train_df &lt;- data.frame(X_train_sc, BART = y_train)\n  test_df  &lt;- data.frame(X_test_sc,  BART = y_test)\n\n  lm_fit &lt;- lm(BART ~ ., data = train_df)\n\n  # Predictions\n  pred_train &lt;- predict(lm_fit, train_df)\n  pred_test  &lt;- predict(lm_fit, test_df)\n\n  # Metrics\n  mse$train  &lt;- c(mse$train, mean((y_train - pred_train)^2))\n  mse$test   &lt;- c(mse$test,  mean((y_test  - pred_test)^2))\n\n  mae$train  &lt;- c(mae$train, mean(abs(y_train - pred_train)))\n  mae$test   &lt;- c(mae$test,  mean(abs(y_test  - pred_test)))\n\n  mape$train &lt;- c(mape$train, mean(abs((y_train - pred_train) / y_train)))\n  mape$test  &lt;- c(mape$test,  mean(abs((y_test  - pred_test)  / y_test)))\n\n  r2$train   &lt;- c(r2$train, cor(y_train, pred_train)^2)\n  r2$test    &lt;- c(r2$test,  cor(y_test,  pred_test)^2)\n}\n\n\n# Create summary table-----\nresults_table &lt;- data.frame(\n  Metric = c(\"MSE\", \"MAE\", \"MAPE\", \"R²\"),\n  Train = c(\n    mean(mse$train),\n    mean(mae$train),\n    mean(mape$train),\n    mean(r2$train)\n  ),\n  Test = c(\n    mean(mse$test),\n    mean(mae$test),\n    mean(mape$test),\n    mean(r2$test)\n  )\n)\n\n# Print table\nresults_table\n\n  Metric        Train         Test\n1    MSE 143.01960222 146.17530363\n2    MAE  10.49211192  10.59655360\n3   MAPE   0.87809521   0.88646195\n4     R²   0.06309417   0.05407431\n\n\nWe can interpret the results as follows: The linear regression model shows poor predictive performance, explaining only 5–6% of the variance in BART scores. Train and test errors were nearly identical, indicating no overfitting but substantial underfitting. These results suggest that BIS/BAS traits and gender alone are insufficient predictors of risk-taking behavior as measured by the BART.\nNext step would be to try non-linear models (Random Forest, Gradient Boosting).\n\nset.seed(1234)\nfolds &lt;- createFolds(y, k = 5, returnTrain = TRUE)\n\nmse  &lt;- list(train = c(), test = c())\nmae  &lt;- list(train = c(), test = c())\nmape &lt;- list(train = c(), test = c())\nr2   &lt;- list(train = c(), test = c())\n\nfor (i in seq_along(folds)) {\n  tr &lt;- folds[[i]]\n  te &lt;- setdiff(seq_len(nrow(X)), tr)\n\n  X_train &lt;- X[tr, , drop = FALSE]\n  X_test  &lt;- X[te, , drop = FALSE]\n  y_train &lt;- y[tr]\n  y_test  &lt;- y[te]\n\n  # z-score continuous predictors using TRAIN stats\n  pp &lt;- preProcess(X_train[, contin, drop = FALSE], method = c(\"center\", \"scale\"))\n  X_train_sc &lt;- X_train; X_test_sc &lt;- X_test\n  X_train_sc[, contin] &lt;- predict(pp, X_train[, contin, drop = FALSE])\n  X_test_sc[, contin]  &lt;- predict(pp, X_test[, contin, drop = FALSE])\n\n  train_df &lt;- data.frame(X_train_sc, BART = y_train)\n  test_df  &lt;- data.frame(X_test_sc,  BART = y_test)\n\n  set.seed(1234)\n  rf_fit &lt;- randomForest(\n    BART ~ .,\n    data = train_df,\n    ntree = 500,\n    mtry = max(1, floor(sqrt(length(predictors))))\n  )\n\n  pred_train &lt;- predict(rf_fit, train_df)\n  pred_test  &lt;- predict(rf_fit, test_df)\n\n  mse$train  &lt;- c(mse$train, mean((y_train - pred_train)^2))\n  mse$test   &lt;- c(mse$test,  mean((y_test  - pred_test)^2))\n\n  mae$train  &lt;- c(mae$train, mean(abs(y_train - pred_train)))\n  mae$test   &lt;- c(mae$test,  mean(abs(y_test  - pred_test)))\n\n  mape$train &lt;- c(mape$train, mean(abs((y_train - pred_train) / y_train)))\n  mape$test  &lt;- c(mape$test,  mean(abs((y_test  - pred_test)  / y_test)))\n\n  r2$train   &lt;- c(r2$train, cor(y_train, pred_train)^2)\n  r2$test    &lt;- c(r2$test,  cor(y_test,  pred_test)^2)\n}\n\nrf_results &lt;- data.frame(\n  Model = \"Random Forest\",\n  Metric = c(\"MSE\", \"MAE\", \"MAPE\", \"R^2\"),\n  Train = c(mean(mse$train), mean(mae$train), mean(mape$train), mean(r2$train)),\n  Test  = c(mean(mse$test),  mean(mae$test),  mean(mape$test),  mean(r2$test))\n)\n\nrf_results\n\n          Model Metric      Train        Test\n1 Random Forest    MSE 49.4503557 133.4441967\n2 Random Forest    MAE  5.7125156   9.5465513\n3 Random Forest   MAPE  0.4700485   0.7912291\n4 Random Forest    R^2  0.7497229   0.1468698\n\n\n\nset.seed(1234)\nfolds &lt;- createFolds(y, k = 5, returnTrain = TRUE)\n\nmse_g  &lt;- list(train = c(), test = c())\nmae_g  &lt;- list(train = c(), test = c())\nmape_g &lt;- list(train = c(), test = c())\nr2_g   &lt;- list(train = c(), test = c())\n\nfor (i in seq_along(folds)) {\n  tr &lt;- folds[[i]]\n  te &lt;- setdiff(seq_len(nrow(X)), tr)\n\n  X_train &lt;- X[tr, , drop = FALSE]\n  X_test  &lt;- X[te, , drop = FALSE]\n  y_train &lt;- y[tr]\n  y_test  &lt;- y[te]\n\n  # z-score continuous predictors using TRAIN stats\n  pp &lt;- preProcess(X_train[, contin, drop = FALSE], method = c(\"center\", \"scale\"))\n  X_train_sc &lt;- X_train; X_test_sc &lt;- X_test\n  X_train_sc[, contin] &lt;- predict(pp, X_train[, contin, drop = FALSE])\n  X_test_sc[, contin]  &lt;- predict(pp, X_test[, contin, drop = FALSE])\n\n  train_df &lt;- data.frame(X_train_sc, BART = y_train)\n  test_df  &lt;- data.frame(X_test_sc,  BART = y_test)\n\n  set.seed(1234)\n  gb_fit &lt;- gbm(\n    BART ~ .,\n    data = train_df,\n    distribution = \"gaussian\",     # regression\n    n.trees = 1500,\n    interaction.depth = 3,\n    shrinkage = 0.01,\n    n.minobsinnode = 10,\n    bag.fraction = 0.8,\n    verbose = FALSE\n  )\n\n  pred_train &lt;- predict(gb_fit, train_df, n.trees = 1500)\n  pred_test  &lt;- predict(gb_fit, test_df,  n.trees = 1500)\n\n  mse_g$train  &lt;- c(mse_g$train, mean((y_train - pred_train)^2))\n  mse_g$test   &lt;- c(mse_g$test,  mean((y_test  - pred_test)^2))\n\n  mae_g$train  &lt;- c(mae_g$train, mean(abs(y_train - pred_train)))\n  mae_g$test   &lt;- c(mae_g$test,  mean(abs(y_test  - pred_test)))\n\n  mape_g$train &lt;- c(mape_g$train, mean(abs((y_train - pred_train) / y_train)))\n  mape_g$test  &lt;- c(mape_g$test,  mean(abs((y_test  - pred_test)  / y_test)))\n\n  r2_g$train   &lt;- c(r2_g$train, cor(y_train, pred_train)^2)\n  r2_g$test    &lt;- c(r2_g$test,  cor(y_test,  pred_test)^2)\n}\n\ngb_results &lt;- data.frame(\n  Model = \"Gradient Boosting\",\n  Metric = c(\"MSE\", \"MAE\", \"MAPE\", \"R^2\"),\n  Train = c(mean(mse_g$train), mean(mae_g$train), mean(mape_g$train), mean(r2_g$train)),\n  Test  = c(mean(mse_g$test),  mean(mae_g$test),  mean(mape_g$test),  mean(r2_g$test))\n)\n\ngb_results\n\n              Model Metric       Train        Test\n1 Gradient Boosting    MSE 100.5563924 131.6130762\n2 Gradient Boosting    MAE   8.2969512   9.5293227\n3 Gradient Boosting   MAPE   0.6858956   0.7808945\n4 Gradient Boosting    R^2   0.3510419   0.1564954\n\n\nLet’s combine the results to compare:\n\nall_results &lt;- rbind(rf_results, gb_results)\nall_results\n\n              Model Metric       Train        Test\n1     Random Forest    MSE  49.4503557 133.4441967\n2     Random Forest    MAE   5.7125156   9.5465513\n3     Random Forest   MAPE   0.4700485   0.7912291\n4     Random Forest    R^2   0.7497229   0.1468698\n5 Gradient Boosting    MSE 100.5563924 131.6130762\n6 Gradient Boosting    MAE   8.2969512   9.5293227\n7 Gradient Boosting   MAPE   0.6858956   0.7808945\n8 Gradient Boosting    R^2   0.3510419   0.1564954\n\n\nWhat do you conclude?\n\n\n\n\n\n\nInterpretation\n\n\n\n\n\nWhile Random Forest achieved excellent training performance, it substantially overfit and failed to generalize. Gradient Boosting produced slightly better test performance and more stable behavior, though overall predictive power remained modest. These results suggest that nonlinear relationships exist but that BIS/BAS measures and gender alone account for only a small portion of variance in BART risk-taking behavior.\n\n\n\n\n\n\n\n# Assuming rf_fit was trained on train_df\nrf_imp &lt;- importance(rf_fit)\n\n# Convert to data frame\nrf_imp_df &lt;- data.frame(\n  Feature = rownames(rf_imp),\n  Importance = rf_imp[, \"IncNodePurity\"]\n)\n\n# Sort by importance\nrf_imp_df &lt;- rf_imp_df[order(rf_imp_df$Importance, decreasing = TRUE), ]\n\nrf_imp_df\n\n                                                        Feature Importance\nBAS.Drive.Score                                 BAS.Drive.Score  17676.021\nBAS.Fun.Seeking.Score                     BAS.Fun.Seeking.Score  16728.337\nAge                                                         Age  15398.407\nBIS.Score                                             BIS.Score  14628.120\nBAS.Reward.Responsiveness.Score BAS.Reward.Responsiveness.Score  14473.869\nFemale                                                   Female   3253.537\n\n\n\n# Relative influence from gbm\ngb_imp &lt;- summary(gb_fit, plotit = FALSE)\n\ngb_imp\n\n                                                            var   rel.inf\nBAS.Fun.Seeking.Score                     BAS.Fun.Seeking.Score 25.462993\nBAS.Drive.Score                                 BAS.Drive.Score 25.375267\nBAS.Reward.Responsiveness.Score BAS.Reward.Responsiveness.Score 21.312970\nBIS.Score                                             BIS.Score 13.361662\nAge                                                         Age 12.722097\nFemale                                                   Female  1.765011\n\n\n\nlibrary(patchwork)\n\n# Random Forest plot\np_rf &lt;- ggplot(rf_imp_df, aes(x = reorder(Feature, Importance), y = Importance)) +\n  geom_col(fill = \"#2F6F73\") +\n  coord_flip() +\n  labs(\n    title = \"Random Forest\",\n    x = \"Feature\",\n    y = \"Importance (IncNodePurity)\"\n  ) +\n  theme_minimal()\n\n# Gradient Boosting plot\np_gb &lt;- ggplot(gb_imp, aes(x = reorder(var, rel.inf), y = rel.inf)) +\n  geom_col(fill = \"#F28E2B\") +\n  coord_flip() +\n  labs(\n    title = \"Gradient Boosting\",\n    x = \"Feature\",\n    y = \"Relative Influence\"\n  ) +\n  theme_minimal()\n\np_rf / p_gb\n\n\n\n\n\n\n\n\nFeature importance analyses from both Random Forest and Gradient Boosting consistently identified BAS-related traits, particularly Drive and Fun Seeking, as the most influential predictors of BART risk-taking behavior. In contrast, BIS, age, and gender showed relatively weak influence. Although these findings suggest that reward sensitivity plays a central role in risk-taking, the overall predictive power of the models remained modest, indicating substantial unexplained variability."
  },
  {
    "objectID": "teaching/stat-learn/material/10/10-svm.html",
    "href": "teaching/stat-learn/material/10/10-svm.html",
    "title": "Support Vector Machines",
    "section": "",
    "text": "Support vector machines (SVMs) offer a direct approach to binary classification: try to find a hyperplane in some feature space that “best” separates the two classes. In practice, however, it is difficult (if not impossible) to find a hyperplane to perfectly separate the classes using just the original features. SVMs overcome this by extending the idea of finding a separating hyperplane in two ways:\n\nloosen what we mean by “perfectly separates”,\nuse the so-called kernel trick to enlarge the feature space to the point that perfect separation of classes is (more) likely.\n\nSVMs are quite versatile and have been applied to a wide variety of domains ranging from chemistry to pattern recognition. They are best used in binary classification scenarios. This brings up a question as to where SVMs are to be preferred to other binary classification techniques such as logistic regression. The honest response is, “it depends”, but here are some points to keep in mind when choosing between the two. A general point to keep in mind is that SVM algorithms tend to be expensive both in terms of memory and computation, issues that can start to hurt as the size of the dataset increases.\nWe use the e1071 library in R to demonstrate the support vector classifier and the SVM. Another option is the LiblineaR library, which is useful for very large linear problems.\n\n\n\n\n\n#load required library\nlibrary(e1071)\n\n#load built-in iris dataset\ndata(iris)\n\n#set seed to ensure reproducible results\nset.seed(42)\n\n#split into training and test sets\niris[, \"train\"] &lt;- ifelse(runif(nrow(iris)) &lt; 0.8, 1, 0)\n\n#separate training and test sets\ntrainset &lt;- iris[iris$train == 1,]\ntestset &lt;- iris[iris$train == 0,]\n\n#get column index of train flag\ntrainColNum &lt;- grep(\"train\", names(trainset))\n\n#remove train flag column from train and test sets\ntrainset &lt;- trainset[,-trainColNum]\ntestset &lt;- testset[,-trainColNum]\n\ndim(trainset)\n\n[1] 115   5\n\ndim(testset)\n\n[1] 35  5\n\n\n\n\n\n\n#get column index of predicted variable in dataset\ntypeColNum &lt;- grep(\"Species\", names(iris))\n\n#build model – linear kernel and C-classification (soft margin) with default cost (C=1)\nsvm_model &lt;- svm(Species~ ., data = trainset, \n                 method = \"C-classification\", \n                 kernel = \"linear\")\nsvm_model\n\n\nCall:\nsvm(formula = Species ~ ., data = trainset, method = \"C-classification\", \n    kernel = \"linear\")\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  1 \n\nNumber of Support Vectors:  24\n\n\n\n\n\nThe output from the SVM model show that there are 24 support vectors. If desired, these can be examined using the SV variable in the model, i.e via svm_model$SV.\n\n# support vectors\nsvm_model$SV\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width\n19   -0.25639203  1.76683447   -1.3228618  -1.3054201\n42   -1.70055936 -1.70445193   -1.5591789  -1.3054201\n45   -0.97847569  1.76683447   -1.2047033  -1.1709010\n53    1.18777530  0.14690082    0.5676747   0.3088091\n55    0.70638619 -0.54735646    0.3904369   0.3088091\n57    0.46569164  0.60973900    0.4495161   0.4433282\n58   -1.21917025 -1.47303284   -0.3775936  -0.3637864\n69    0.34534436 -1.93587102    0.3313576   0.3088091\n71   -0.01569747  0.37831991    0.5085954   0.7123664\n73    0.46569164 -1.24161374    0.5676747   0.3088091\n78    0.94708075 -0.08451828    0.6267539   0.5778473\n84    0.10464981 -0.77877556    0.6858332   0.4433282\n85   -0.61743386 -0.08451828    0.3313576   0.3088091\n86    0.10464981  0.84115809    0.3313576   0.4433282\n99   -0.97847569 -1.24161374   -0.5548314  -0.2292673\n107  -1.21917025 -1.24161374    0.3313576   0.5778473\n111   0.70638619  0.37831991    0.6858332   0.9814046\n117   0.70638619 -0.08451828    0.9221503   0.7123664\n124   0.46569164 -0.77877556    0.5676747   0.7123664\n130   1.54881714 -0.08451828    1.0993881   0.4433282\n138   0.58603892  0.14690082    0.9221503   0.7123664\n139   0.10464981 -0.08451828    0.5085954   0.7123664\n147   0.46569164 -1.24161374    0.6267539   0.8468855\n150  -0.01569747 -0.08451828    0.6858332   0.7123664\n\n\n\n\n\n\n# training set predictions\npred_train &lt;- predict(svm_model, trainset)\nmean(pred_train == trainset$Species)\n\n[1] 0.9826087\n\n# test set predictions\npred_test &lt;-predict(svm_model, testset)\nmean(pred_test == testset$Species)\n\n[1] 0.9142857\n\n\nThe test prediction accuracy indicates that the linear performs quite well on this dataset, confirming that it is indeed near linearly separable. To check performance by class, one can create a confusion matri. Another point is that we have used a soft-margin classification scheme with a cost \\(C=1\\). You can experiment with this by explicitly changing the value of \\(C\\). Again, I’ll leave this for you an exercise.\n\n# confusion matrix\ncm &lt;- table(pred_test, testset$Species)\ncm\n\n            \npred_test    setosa versicolor virginica\n  setosa         18          0         0\n  versicolor      0          5         3\n  virginica       0          0         9\n\n# accuracy\nsum(diag(cm)) / sum(cm)\n\n[1] 0.9142857\n\n\n\n\n\n\n\nlibrary(datasets) # penguins data come from here\nlibrary(tidyverse) \n# plot data\nggplot(penguins, aes(x = bill_len, y = bill_dep, color = species)) +\n  geom_point() +\n  theme_minimal() +\n  labs(\n    x = \"Bill Length (in mm)\",\n    y = \"Bill Depth (in mm)\",\n    title = \"Bill Length vs. Bill Depth by Species\"\n  )\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nThe penguins data set contains morphological measurements for individual penguins collected in the Palmer Archipelago, Antarctica. It includes observations from three species: Adélie, Chinstrap, and Gentoo—and records key physical characteristics such as bill length, bill depth, flipper length, body mass, sex, and island of origin. The data are commonly used for demonstrating exploratory data analysis, visualization, and classification methods, as the species show both overlap and separation in their physical traits.\nHere, we merge the Adélie and Chinstrap groups and restrict the analysis to two dimensions, using bill depth and flipper width as the discriminating variables. A helper function is used to plot an approximate version of the margin. note that we use ggplot2 here it needs to be loaded.\n\n# Helper to reproduce the plot (points, SV squares, margin lines)\nsepLinePlot &lt;- function(svm_fit, data,\n                        xvar = \"flipper_len\",\n                        yvar = \"bill_dep\") {\n\n  # response + class colors\n  yname &lt;- all.vars(formula(svm_fit))[1]\n  cls &lt;- factor(data[[yname]])\n  cols &lt;- c(\"red\", \"green\")[as.integer(cls)]\n\n  plot(data[[xvar]], data[[yvar]],\n       col = cols, pch = 1,\n       xlab = xvar, ylab = yvar)\n\n  # compute w and b from e1071 internals\n  w &lt;- as.numeric(t(svm_fit$coefs) %*% svm_fit$SV)\n  names(w) &lt;- colnames(svm_fit$SV)\n  b &lt;- -svm_fit$rho\n\n  # pull the correct coefficients for the plotted axes\n  wx &lt;- w[[xvar]]\n  wy &lt;- w[[yvar]]\n\n  xseq &lt;- seq(min(data[[xvar]], na.rm = TRUE),\n              max(data[[xvar]], na.rm = TRUE),\n              length.out = 200)\n\n  # if wy ~ 0 =&gt; vertical boundary in this coordinate system\n  if (abs(wy) &lt; 1e-12) {\n    x0 &lt;- -b / wx\n    abline(v = x0, col = \"cyan\", lwd = 2)\n    abline(v = (-b + 1) / wx, col = \"cyan\", lty = 2)\n    abline(v = (-b - 1) / wx, col = \"cyan\", lty = 2)\n  } else {\n    y0  &lt;- -(wx * xseq + b) / wy\n    yP1 &lt;- -(wx * xseq + b - 1) / wy\n    yM1 &lt;- -(wx * xseq + b + 1) / wy\n\n    lines(xseq, y0,  col = \"cyan\", lwd = 2)\n    lines(xseq, yP1, col = \"cyan\", lty = 2)\n    lines(xseq, yM1, col = \"cyan\", lty = 2)\n  }\n\n  # support vectors as grey squares\n  points(svm_fit$SV[, xvar],\n         svm_fit$SV[, yvar],\n         pch = 0, cex = 1.6, lwd = 1.2, col = \"grey60\")\n}\n\n\nlibrary(ggplot2)\n\n#merge Adelie & Chinstrap species to make the problem easier\npenguins_12_3 &lt;- penguins %&gt;%\n  mutate(species=dplyr::recode(species,`Adelie`=\"Adel/Chin\",`Chinstrap`=\"Adel/Chin\"))\n\n\n# Fit model, no scaling (important for plotting lines in original units)\nsv.fit.lin0 &lt;- e1071::svm(\n  species ~ bill_dep + flipper_len,\n  data   = penguins_12_3,\n  kernel = \"linear\",\n  scale  = FALSE\n)\n\n# Plot\nsepLinePlot(sv.fit.lin0, penguins_12_3)\n\n\n\n\n\n\n\n\nThe solid line represents the separating (hyper)plane, while the margins are shown as dashed lines. The margin is constructed by widening the separating line until it just touches points on either side of the boundary, known as support vectors, with the margin edges remaining parallel to the separating plane.\n\n\n\nAs an example, consider the penguins data while ignoring the Gentoo species, which results in a non-separable case with imperfectly separated groups. The analysis is restricted to two dimensions, using bill length and bill depth as the discriminating variables. In this setting there is no clean separating line between the groups; instead, the algorithm accommodates this by allowing slack, permitting a limited amount of crossover within the margin, as will be discussed in more detail later.\n\n# keep only Adelie and Chinstrap\npenguins_1_2 &lt;- penguins %&gt;% \n  dplyr::filter(species != \"Gentoo\")\n\n# fit linear SVM \nsv.fit.lin &lt;- e1071::svm(\n  species ~ bill_len + bill_dep,\n  data   = penguins_1_2,\n  kernel = \"linear\",\n  scale  = FALSE\n)\n\n# plot using the helper (bill_len on x, bill_dep on y)\nsepLinePlot(\n  sv.fit.lin,\n  penguins_1_2,\n  xvar = \"bill_dep\",\n  yvar = \"bill_len\"\n)\n\n\n\n\n\n\n\n\nThis plot shows the result of fitting a linear soft-margin support vector machine to the Adélie and Chinstrap penguins using bill depth (horizontal axis) and bill length (vertical axis).\nEach point represents an individual penguin, with colour indicating species. The solid cyan line is the estimated separating hyperplane, and the dashed cyan lines indicate the margins on either side. Because the two species overlap substantially in this two-dimensional feature space, there is no clean separating line. As a result, the fitted margin passes through dense regions of both classes.\nThe square-boxed points are the support vectors. In this non-separable setting, these include not only points lying exactly on the margin, but also points inside the margin and some that are on the wrong side of the separating line. These observations are the ones that determine the position and orientation of the hyperplane.\nMany observations fall within the margin and several lie on the “wrong” side of the separating line. This illustrates the role of slack variables in a soft-margin SVM: the algorithm allows violations of the margin, and even misclassifications, in order to achieve the best overall trade-off between margin width and classification error. Given the restriction to two variables and a linear decision boundary, this plot represents about as good a separation as can be achieved for these two species.\nEach element of the slack vector\n\\[\n\\boldsymbol{\\varepsilon} = (\\varepsilon_1, \\ldots, \\varepsilon_n)^\\top\n\\]\nis known as the slack for a given observation. The slack variables quantify how far each point lies from the margin for its assigned group and can be viewed as allowances for observations that fall on the “wrong side” of the separating plane, playing a role analogous to residuals in regression.\nWhen \\(\\varepsilon_i = 0\\), observation \\(i\\) lies on the correct side of the margin. If \\(0 &lt; \\varepsilon_i \\leq 1\\), observation \\(i\\) is still on the correct side of the separating plane but lies within the margin; such points are classified correctly, although in the case of perfectly separable groups no observations fall within the margin. When \\(\\varepsilon_i &gt; 1\\), observation \\(i\\) lies on the wrong side of the separating plane and is therefore misclassified.\nThe margin width \\(M\\) and the cost parameter \\(C\\) work together to control the use of slack. In this formulation, decreasing the cost \\(C\\) allows more slack, which in turn results in a larger number of support vectors and greater tolerance for margin violations.\n\n\n\nWe have tried to align the above formulation with the implementation of the support vector machine in the svm function from the e1071 package. In particular, we use the cost parameter \\(C\\) to modify the separating plane and illustrate the consequences of this choice. Shown next is the final SVM classification for the second example using the default cost \\(C = 1\\), which results in several misclassified observations.\n\nplot(sv.fit.lin,penguins_1_2,formula=bill_dep ~ bill_len)\n\n\n\n\n\n\n\n\nIn this plot, group membership is indicated by the colour of the symbols, from which it can be seen that there are seven misclassified observations. Points are shown with an open circle when they are not support vectors and with a cross when they are support vectors. The plot reveals a relatively large number of support vectors, in contrast to the seven support vectors observed in the completely separable case. We now decrease the cost parameter \\(C\\), forcing the margin \\(M\\) to shrink in order to accommodate the slack associated with the misclassified observations, which in turn increases the number of support vectors used. The advantage of decreasing \\(C\\), and hence increasing the number of support vectors, is that the resulting separating plane becomes less sensitive to a small number of outlying observations. The results for \\(C = 0.01\\) are shown next.\n\nsv.fit.lin.lowCost &lt;- svm(species ~ bill_len+bill_dep, data = penguins_1_2,kernel='linear',cost=0.01)\nplot(sv.fit.lin.lowCost,penguins_1_2,formula=bill_dep ~ bill_len)\n\n\n\n\n\n\n\nsummary(sv.fit.lin.lowCost)\n\n\nCall:\nsvm(formula = species ~ bill_len + bill_dep, data = penguins_1_2, \n    kernel = \"linear\", cost = 0.01)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  0.01 \n\nNumber of Support Vectors:  129\n\n ( 65 64 )\n\n\nNumber of Classes:  2 \n\nLevels: \n Adelie Chinstrap Gentoo\n\n\nThe summary of the fitted model shows that 129 support vectors were used, and the separating line can be seen to have shifted in an interesting way. There are now more misclassified observations, almost all of which belong to the Chinstrap species. This suggests that some of the original support vectors may have been overly influential; however, the adjustment has also introduced additional classification error.\n\n\n\n\nThe kernel trick is a concept that underlies a wide range of machine learning techniques and is based on two key ideas. First, groups that are not linearly separable in \\(p\\) dimensions may become separable in a higher-dimensional space of dimension \\(q &gt; p\\) through an appropriate transformation. Second, rather than explicitly computing this transformation, training the model in the higher-dimensional space, and then making predictions on new data, we can instead use a kernel function with a suitable inner product, which leads to an equivalent optimization problem and solution without ever working directly in the higher-dimensional space.\nA rather extreme example using two circles helps illustrate the idea. Let one group be defined by the relationship\n\\(X_1^2 + X_2^2 = 2^2\\),\nand the other by\n\\(X_1^2 + X_2^2 = 1^2\\).\nIf we transform the two-dimensional relationships into three dimensions using the mapping\n\\((X_1, X_2) \\mapsto (X_1, X_2, X_1^2 + X_2^2)\\),\nthe two groups become clearly separable.\nNote that we need an extra package for 3D plots. We also plot the 2D original set of relationships for comparison.\n\nlibrary(plot3D) # install if needed\n\nset.seed(1001)\nx1 &lt;- runif(100, -2, 2); y1 &lt;- sqrt(4 - x1^2)\nx2 &lt;- runif(100, -1, 1); y2 &lt;- sqrt(1 - x2^2)\ndf &lt;- data.frame(\n  x1 = c(x1, x1, x2, x2),\n  x2 = c(y1, -y1, y2, -y2)\n)\ndf$z &lt;- df$x1^2 + df$x2^2\nplot3D::scatter3D(df$x1, df$x2, df$z)\n\n\n\n\n\n\n\n# 2d correspondence\nplot(x2~x1,data=df,col=c(4,2,2,3)[round(z)]) \n\n\n\n\n\n\n\n\nBefore going into details, we run svm on the circle data using several different kernels. First the linear:\n\nsv.circ.lin &lt;- svm(factor(z)~ x1+x2,data = df, kernel='linear')\nplot(sv.circ.lin,df,formula=x2~x1,main=\"Linear Kernel\")\n\n\n\n\n\n\n\n\nUsing the Radial Kernel we get:\n\nsv.circ.rad &lt;- svm(factor(z)~ x1+x2,data = df, kernel='radial')\nplot(sv.circ.rad,df,formula=x2~x1)\n\n\n\n\n\n\n\n\n\n\nHere a support vector machine with a radial kernel is fitted to the penguin data using bill length and bill depth as predictors. The radial kernel implicitly maps the data into a higher-dimensional feature space, allowing the classifier to learn a non-linear decision boundary between the species. The subsequent plot displays this non-linear separating boundary in the original two-dimensional feature space, illustrating how the radial kernel can capture complex class structure that a linear SVM cannot.\n\nsv.fit.rad &lt;- svm(species ~ bill_len+bill_dep, data = penguins_1_2, kernel='radial')\nplot(sv.fit.rad,penguins_1_2,formula=bill_dep ~ bill_len)\n\n\n\n\n\n\n\n\nWe can adjust the model parameters to improve classification performance, but care must be taken to avoid overfitting; in practice, methods such as cross-validation should be used to select these parameters.\nBelow, the parameter \\(\\gamma\\) is increased to 2, which accentuates large differences between points.\n\nsv.fit.rad.gam2 &lt;- svm(species ~ bill_len+bill_dep, data = penguins_1_2,kernel='radial', gamma=2)\nplot(sv.fit.rad.gam2, penguins_1_2, formula=bill_dep ~ bill_len)\n\n\n\n\n\n\n\n\n\n\n\nThe basic idea is to apply the support vector machine recursively, comparing one group against all remaining groups, then comparing the second group against groups three and higher, and so on. As a final example, all species in the penguins data are used, with both linear and radial kernel fits under default settings.\n\nsv.fit.lin_all3 &lt;- svm(species ~ bill_len + bill_dep, data = penguins,kernel='linear')\nplot(sv.fit.lin_all3, penguins, formula=bill_dep ~ bill_len)\n\n\n\n\n\n\n\n\nAs before, Gentoo is easy to separate from the other species, but this multiclass extension does not substantially improve predictive performance. Distinguishing Adélie from Chinstrap remains difficult near the decision boundary, and the radial kernel provides only a modest improvement.\nThe radial variant is as follows:\n\nsv.fit.rad_all3 &lt;- svm(species ~ bill_len + bill_dep, data = penguins, kernel='radial')\nplot(sv.fit.rad_all3,penguins,formula=bill_dep ~ bill_len)\n\n\n\n\n\n\n\n\n\n\n\n\nAs done previously in this course, the basic idea is to withhold a portion of the training data and use it as a test set. First, one loops over the open tuning parameters, such as \\(C\\) and \\(\\gamma\\), fitting the model on the training set while holding these parameters fixed. The fitted model is then evaluated on the test set, and this process is repeated for all parameter combinations. The parameter values that yield the best performance are then selected. We applied this approach to the penguins data, still attempting classification using only two features.\nA grid search is conducted over \\(C = 0.01, 0.1, 1, 10, 100\\) and \\(\\gamma = 0.5, 1, 2, 3, 4\\). Somewhat surprisingly, the default parameter values performed best in this case.\n\nset.seed(1001)\ntune.out &lt;- tune(svm, species ~., data=penguins, kernel='radial', ranges = list(cost=c(0.01,0.1,1,10,100),gamma=c(0.5,1,2,3,4)))\nsummary(tune.out)\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost gamma\n    1   0.5\n\n- best performance: 0.01212678 \n\n- Detailed performance results:\n    cost gamma      error dispersion\n1  1e-02   0.5 0.56054495 0.07977601\n2  1e-01   0.5 0.03004679 0.02427130\n3  1e+00   0.5 0.01212678 0.02119780\n4  1e+01   0.5 0.02407531 0.02361808\n5  1e+02   0.5 0.02407531 0.02361808\n6  1e-02   1.0 0.56054495 0.07977601\n7  1e-01   1.0 0.21904650 0.06312732\n8  1e+00   1.0 0.01800914 0.02523584\n9  1e+01   1.0 0.02113414 0.02468684\n10 1e+02   1.0 0.02113414 0.02468684\n11 1e-02   2.0 0.56054495 0.07977601\n12 1e-01   2.0 0.53040393 0.08062374\n13 1e+00   2.0 0.04503629 0.02956429\n14 1e+01   2.0 0.04797746 0.03525477\n15 1e+02   2.0 0.04797746 0.03525477\n16 1e-02   3.0 0.56054495 0.07977601\n17 1e-01   3.0 0.56054495 0.07977601\n18 1e+00   3.0 0.09274589 0.07027869\n19 1e+01   3.0 0.08980472 0.06407604\n20 1e+02   3.0 0.08980472 0.06407604\n21 1e-02   4.0 0.56054495 0.07977601\n22 1e-01   4.0 0.56054495 0.07977601\n23 1e+00   4.0 0.13162131 0.07053983\n24 1e+01   4.0 0.12270865 0.06670692\n25 1e+02   4.0 0.12270865 0.06670692\n\n\n\n\n\n\n\n\nNote\n\n\n\nDispersion, as shown above, is analogous to a root mean squared error (RMSE), \\[\n\\mathrm{rmse} = \\sqrt{\\frac{1}{n}\\sum_i \\left(\\hat{y}_i - y_i\\right)^2},\n\\] and is a quantity we seek to minimize. However, it is possible to choose parameter values that yield improved within-sample performance but do not translate into improved out-of-sample performance.\n\n\nOur final model is thus:\n\nsv.fit.rad_all3 &lt;- svm(species ~ bill_len+bill_dep, data= penguins, kernel='radial', cost=1, gamma=.5)\nplot(sv.fit.rad_all3, penguins, formula = bill_dep ~ bill_len)\n\n\n\n\n\n\n\n\n\n\n\nThis section explores the use of radial kernels to allow decision boundaries to be both non-linear and non-contiguous. As a reminder, the linear and radial (sometimes called Gaussian) kernels are defined as follows.\nLinear kernel:\n\\[\nK(\\mathbf{X}_i, \\mathbf{X}_j) = \\langle \\mathbf{X}_i, \\mathbf{X}_j \\rangle,\n\\]\nwhere \\(\\langle \\cdot \\rangle\\) denotes the standard vector inner product, as defined in the notes.\nRadial kernel:\n\\[\nK(\\mathbf{X}_i, \\mathbf{X}_j) = \\exp\\!\\left(-\\gamma \\lVert \\mathbf{X}_i - \\mathbf{X}_j \\rVert^2\\right).\n\\]\nThis kernel is based on a distance measure and includes a single tuning parameter \\(\\gamma\\), which controls the influence of large distances between observations. Larger values of \\(\\gamma\\) emphasize local structure, while smaller values lead to smoother decision boundaries. We briefly explore the effect of this parameter here, with a more detailed discussion deferred to the next class.\nFor the following exercises, we use the Australian Crabs data (crabs), a benchmark data set that is notoriously difficult to cluster and relatively challenging to classify. The five primary predictors (measurements) are abbreviated as FL, RW, CL, CW, and BD. The data needs to be preprocessed so that sex and species are combined into a single categorical variable, group, with four levels:\n\nBlue Males\n\nOrange Males\n\nBlue Females\n\nOrange Females\n\nIn most analyses, we do not explicitly split the data into training and test sets. Instead, we rely on various forms of cross-validation to properly assess out-of-sample performance.\nYou are asked to compare and contrast SVM fits using the raw measures.\n\nRun svm with the following formula using all five measures: group ~ FL + RW + CL + CW + BD, first with the linear kernel and then with the radial kernel.\nCompare the accuracy (and optionally the confusion matrices) for these two runs.\nIf the radial-kernel fit performs worse, adjust the tuning parameter by setting gamma to 2, 4, or even 10, and then compare the accuracy of this updated fit.\nIf you would like to visualize the fits, try plotting RW vs. CW, but you must set slice to reasonable values for the remaining features.\n\nDetermine if the following statements are true/false based on your output:\n\nThe crabs data, examined using raw features seems to be divisible into groups using linear ‘cuts’\nIn order to perform well, use of the radial kernel requires higher penalties for large distances between observations\nVisualization of the svm solution is hindered by our choice for the slice?"
  },
  {
    "objectID": "teaching/stat-learn/material/10/10-svm.html#warm-up-svm-on-iris-dataset",
    "href": "teaching/stat-learn/material/10/10-svm.html#warm-up-svm-on-iris-dataset",
    "title": "Support Vector Machines",
    "section": "",
    "text": "#load required library\nlibrary(e1071)\n\n#load built-in iris dataset\ndata(iris)\n\n#set seed to ensure reproducible results\nset.seed(42)\n\n#split into training and test sets\niris[, \"train\"] &lt;- ifelse(runif(nrow(iris)) &lt; 0.8, 1, 0)\n\n#separate training and test sets\ntrainset &lt;- iris[iris$train == 1,]\ntestset &lt;- iris[iris$train == 0,]\n\n#get column index of train flag\ntrainColNum &lt;- grep(\"train\", names(trainset))\n\n#remove train flag column from train and test sets\ntrainset &lt;- trainset[,-trainColNum]\ntestset &lt;- testset[,-trainColNum]\n\ndim(trainset)\n\n[1] 115   5\n\ndim(testset)\n\n[1] 35  5\n\n\n\n\n\n\n#get column index of predicted variable in dataset\ntypeColNum &lt;- grep(\"Species\", names(iris))\n\n#build model – linear kernel and C-classification (soft margin) with default cost (C=1)\nsvm_model &lt;- svm(Species~ ., data = trainset, \n                 method = \"C-classification\", \n                 kernel = \"linear\")\nsvm_model\n\n\nCall:\nsvm(formula = Species ~ ., data = trainset, method = \"C-classification\", \n    kernel = \"linear\")\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  1 \n\nNumber of Support Vectors:  24\n\n\n\n\n\nThe output from the SVM model show that there are 24 support vectors. If desired, these can be examined using the SV variable in the model, i.e via svm_model$SV.\n\n# support vectors\nsvm_model$SV\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width\n19   -0.25639203  1.76683447   -1.3228618  -1.3054201\n42   -1.70055936 -1.70445193   -1.5591789  -1.3054201\n45   -0.97847569  1.76683447   -1.2047033  -1.1709010\n53    1.18777530  0.14690082    0.5676747   0.3088091\n55    0.70638619 -0.54735646    0.3904369   0.3088091\n57    0.46569164  0.60973900    0.4495161   0.4433282\n58   -1.21917025 -1.47303284   -0.3775936  -0.3637864\n69    0.34534436 -1.93587102    0.3313576   0.3088091\n71   -0.01569747  0.37831991    0.5085954   0.7123664\n73    0.46569164 -1.24161374    0.5676747   0.3088091\n78    0.94708075 -0.08451828    0.6267539   0.5778473\n84    0.10464981 -0.77877556    0.6858332   0.4433282\n85   -0.61743386 -0.08451828    0.3313576   0.3088091\n86    0.10464981  0.84115809    0.3313576   0.4433282\n99   -0.97847569 -1.24161374   -0.5548314  -0.2292673\n107  -1.21917025 -1.24161374    0.3313576   0.5778473\n111   0.70638619  0.37831991    0.6858332   0.9814046\n117   0.70638619 -0.08451828    0.9221503   0.7123664\n124   0.46569164 -0.77877556    0.5676747   0.7123664\n130   1.54881714 -0.08451828    1.0993881   0.4433282\n138   0.58603892  0.14690082    0.9221503   0.7123664\n139   0.10464981 -0.08451828    0.5085954   0.7123664\n147   0.46569164 -1.24161374    0.6267539   0.8468855\n150  -0.01569747 -0.08451828    0.6858332   0.7123664\n\n\n\n\n\n\n# training set predictions\npred_train &lt;- predict(svm_model, trainset)\nmean(pred_train == trainset$Species)\n\n[1] 0.9826087\n\n# test set predictions\npred_test &lt;-predict(svm_model, testset)\nmean(pred_test == testset$Species)\n\n[1] 0.9142857\n\n\nThe test prediction accuracy indicates that the linear performs quite well on this dataset, confirming that it is indeed near linearly separable. To check performance by class, one can create a confusion matri. Another point is that we have used a soft-margin classification scheme with a cost \\(C=1\\). You can experiment with this by explicitly changing the value of \\(C\\). Again, I’ll leave this for you an exercise.\n\n# confusion matrix\ncm &lt;- table(pred_test, testset$Species)\ncm\n\n            \npred_test    setosa versicolor virginica\n  setosa         18          0         0\n  versicolor      0          5         3\n  virginica       0          0         9\n\n# accuracy\nsum(diag(cm)) / sum(cm)\n\n[1] 0.9142857"
  },
  {
    "objectID": "teaching/stat-learn/material/10/10-svm.html#svm-on-penguins-data",
    "href": "teaching/stat-learn/material/10/10-svm.html#svm-on-penguins-data",
    "title": "Support Vector Machines",
    "section": "",
    "text": "library(datasets) # penguins data come from here\nlibrary(tidyverse) \n# plot data\nggplot(penguins, aes(x = bill_len, y = bill_dep, color = species)) +\n  geom_point() +\n  theme_minimal() +\n  labs(\n    x = \"Bill Length (in mm)\",\n    y = \"Bill Depth (in mm)\",\n    title = \"Bill Length vs. Bill Depth by Species\"\n  )\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nThe penguins data set contains morphological measurements for individual penguins collected in the Palmer Archipelago, Antarctica. It includes observations from three species: Adélie, Chinstrap, and Gentoo—and records key physical characteristics such as bill length, bill depth, flipper length, body mass, sex, and island of origin. The data are commonly used for demonstrating exploratory data analysis, visualization, and classification methods, as the species show both overlap and separation in their physical traits.\nHere, we merge the Adélie and Chinstrap groups and restrict the analysis to two dimensions, using bill depth and flipper width as the discriminating variables. A helper function is used to plot an approximate version of the margin. note that we use ggplot2 here it needs to be loaded.\n\n# Helper to reproduce the plot (points, SV squares, margin lines)\nsepLinePlot &lt;- function(svm_fit, data,\n                        xvar = \"flipper_len\",\n                        yvar = \"bill_dep\") {\n\n  # response + class colors\n  yname &lt;- all.vars(formula(svm_fit))[1]\n  cls &lt;- factor(data[[yname]])\n  cols &lt;- c(\"red\", \"green\")[as.integer(cls)]\n\n  plot(data[[xvar]], data[[yvar]],\n       col = cols, pch = 1,\n       xlab = xvar, ylab = yvar)\n\n  # compute w and b from e1071 internals\n  w &lt;- as.numeric(t(svm_fit$coefs) %*% svm_fit$SV)\n  names(w) &lt;- colnames(svm_fit$SV)\n  b &lt;- -svm_fit$rho\n\n  # pull the correct coefficients for the plotted axes\n  wx &lt;- w[[xvar]]\n  wy &lt;- w[[yvar]]\n\n  xseq &lt;- seq(min(data[[xvar]], na.rm = TRUE),\n              max(data[[xvar]], na.rm = TRUE),\n              length.out = 200)\n\n  # if wy ~ 0 =&gt; vertical boundary in this coordinate system\n  if (abs(wy) &lt; 1e-12) {\n    x0 &lt;- -b / wx\n    abline(v = x0, col = \"cyan\", lwd = 2)\n    abline(v = (-b + 1) / wx, col = \"cyan\", lty = 2)\n    abline(v = (-b - 1) / wx, col = \"cyan\", lty = 2)\n  } else {\n    y0  &lt;- -(wx * xseq + b) / wy\n    yP1 &lt;- -(wx * xseq + b - 1) / wy\n    yM1 &lt;- -(wx * xseq + b + 1) / wy\n\n    lines(xseq, y0,  col = \"cyan\", lwd = 2)\n    lines(xseq, yP1, col = \"cyan\", lty = 2)\n    lines(xseq, yM1, col = \"cyan\", lty = 2)\n  }\n\n  # support vectors as grey squares\n  points(svm_fit$SV[, xvar],\n         svm_fit$SV[, yvar],\n         pch = 0, cex = 1.6, lwd = 1.2, col = \"grey60\")\n}\n\n\nlibrary(ggplot2)\n\n#merge Adelie & Chinstrap species to make the problem easier\npenguins_12_3 &lt;- penguins %&gt;%\n  mutate(species=dplyr::recode(species,`Adelie`=\"Adel/Chin\",`Chinstrap`=\"Adel/Chin\"))\n\n\n# Fit model, no scaling (important for plotting lines in original units)\nsv.fit.lin0 &lt;- e1071::svm(\n  species ~ bill_dep + flipper_len,\n  data   = penguins_12_3,\n  kernel = \"linear\",\n  scale  = FALSE\n)\n\n# Plot\nsepLinePlot(sv.fit.lin0, penguins_12_3)\n\n\n\n\n\n\n\n\nThe solid line represents the separating (hyper)plane, while the margins are shown as dashed lines. The margin is constructed by widening the separating line until it just touches points on either side of the boundary, known as support vectors, with the margin edges remaining parallel to the separating plane.\n\n\n\nAs an example, consider the penguins data while ignoring the Gentoo species, which results in a non-separable case with imperfectly separated groups. The analysis is restricted to two dimensions, using bill length and bill depth as the discriminating variables. In this setting there is no clean separating line between the groups; instead, the algorithm accommodates this by allowing slack, permitting a limited amount of crossover within the margin, as will be discussed in more detail later.\n\n# keep only Adelie and Chinstrap\npenguins_1_2 &lt;- penguins %&gt;% \n  dplyr::filter(species != \"Gentoo\")\n\n# fit linear SVM \nsv.fit.lin &lt;- e1071::svm(\n  species ~ bill_len + bill_dep,\n  data   = penguins_1_2,\n  kernel = \"linear\",\n  scale  = FALSE\n)\n\n# plot using the helper (bill_len on x, bill_dep on y)\nsepLinePlot(\n  sv.fit.lin,\n  penguins_1_2,\n  xvar = \"bill_dep\",\n  yvar = \"bill_len\"\n)\n\n\n\n\n\n\n\n\nThis plot shows the result of fitting a linear soft-margin support vector machine to the Adélie and Chinstrap penguins using bill depth (horizontal axis) and bill length (vertical axis).\nEach point represents an individual penguin, with colour indicating species. The solid cyan line is the estimated separating hyperplane, and the dashed cyan lines indicate the margins on either side. Because the two species overlap substantially in this two-dimensional feature space, there is no clean separating line. As a result, the fitted margin passes through dense regions of both classes.\nThe square-boxed points are the support vectors. In this non-separable setting, these include not only points lying exactly on the margin, but also points inside the margin and some that are on the wrong side of the separating line. These observations are the ones that determine the position and orientation of the hyperplane.\nMany observations fall within the margin and several lie on the “wrong” side of the separating line. This illustrates the role of slack variables in a soft-margin SVM: the algorithm allows violations of the margin, and even misclassifications, in order to achieve the best overall trade-off between margin width and classification error. Given the restriction to two variables and a linear decision boundary, this plot represents about as good a separation as can be achieved for these two species.\nEach element of the slack vector\n\\[\n\\boldsymbol{\\varepsilon} = (\\varepsilon_1, \\ldots, \\varepsilon_n)^\\top\n\\]\nis known as the slack for a given observation. The slack variables quantify how far each point lies from the margin for its assigned group and can be viewed as allowances for observations that fall on the “wrong side” of the separating plane, playing a role analogous to residuals in regression.\nWhen \\(\\varepsilon_i = 0\\), observation \\(i\\) lies on the correct side of the margin. If \\(0 &lt; \\varepsilon_i \\leq 1\\), observation \\(i\\) is still on the correct side of the separating plane but lies within the margin; such points are classified correctly, although in the case of perfectly separable groups no observations fall within the margin. When \\(\\varepsilon_i &gt; 1\\), observation \\(i\\) lies on the wrong side of the separating plane and is therefore misclassified.\nThe margin width \\(M\\) and the cost parameter \\(C\\) work together to control the use of slack. In this formulation, decreasing the cost \\(C\\) allows more slack, which in turn results in a larger number of support vectors and greater tolerance for margin violations.\n\n\n\nWe have tried to align the above formulation with the implementation of the support vector machine in the svm function from the e1071 package. In particular, we use the cost parameter \\(C\\) to modify the separating plane and illustrate the consequences of this choice. Shown next is the final SVM classification for the second example using the default cost \\(C = 1\\), which results in several misclassified observations.\n\nplot(sv.fit.lin,penguins_1_2,formula=bill_dep ~ bill_len)\n\n\n\n\n\n\n\n\nIn this plot, group membership is indicated by the colour of the symbols, from which it can be seen that there are seven misclassified observations. Points are shown with an open circle when they are not support vectors and with a cross when they are support vectors. The plot reveals a relatively large number of support vectors, in contrast to the seven support vectors observed in the completely separable case. We now decrease the cost parameter \\(C\\), forcing the margin \\(M\\) to shrink in order to accommodate the slack associated with the misclassified observations, which in turn increases the number of support vectors used. The advantage of decreasing \\(C\\), and hence increasing the number of support vectors, is that the resulting separating plane becomes less sensitive to a small number of outlying observations. The results for \\(C = 0.01\\) are shown next.\n\nsv.fit.lin.lowCost &lt;- svm(species ~ bill_len+bill_dep, data = penguins_1_2,kernel='linear',cost=0.01)\nplot(sv.fit.lin.lowCost,penguins_1_2,formula=bill_dep ~ bill_len)\n\n\n\n\n\n\n\nsummary(sv.fit.lin.lowCost)\n\n\nCall:\nsvm(formula = species ~ bill_len + bill_dep, data = penguins_1_2, \n    kernel = \"linear\", cost = 0.01)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  0.01 \n\nNumber of Support Vectors:  129\n\n ( 65 64 )\n\n\nNumber of Classes:  2 \n\nLevels: \n Adelie Chinstrap Gentoo\n\n\nThe summary of the fitted model shows that 129 support vectors were used, and the separating line can be seen to have shifted in an interesting way. There are now more misclassified observations, almost all of which belong to the Chinstrap species. This suggests that some of the original support vectors may have been overly influential; however, the adjustment has also introduced additional classification error."
  },
  {
    "objectID": "teaching/stat-learn/material/10/10-svm.html#the-kernel-trick",
    "href": "teaching/stat-learn/material/10/10-svm.html#the-kernel-trick",
    "title": "Support Vector Machines",
    "section": "",
    "text": "The kernel trick is a concept that underlies a wide range of machine learning techniques and is based on two key ideas. First, groups that are not linearly separable in \\(p\\) dimensions may become separable in a higher-dimensional space of dimension \\(q &gt; p\\) through an appropriate transformation. Second, rather than explicitly computing this transformation, training the model in the higher-dimensional space, and then making predictions on new data, we can instead use a kernel function with a suitable inner product, which leads to an equivalent optimization problem and solution without ever working directly in the higher-dimensional space.\nA rather extreme example using two circles helps illustrate the idea. Let one group be defined by the relationship\n\\(X_1^2 + X_2^2 = 2^2\\),\nand the other by\n\\(X_1^2 + X_2^2 = 1^2\\).\nIf we transform the two-dimensional relationships into three dimensions using the mapping\n\\((X_1, X_2) \\mapsto (X_1, X_2, X_1^2 + X_2^2)\\),\nthe two groups become clearly separable.\nNote that we need an extra package for 3D plots. We also plot the 2D original set of relationships for comparison.\n\nlibrary(plot3D) # install if needed\n\nset.seed(1001)\nx1 &lt;- runif(100, -2, 2); y1 &lt;- sqrt(4 - x1^2)\nx2 &lt;- runif(100, -1, 1); y2 &lt;- sqrt(1 - x2^2)\ndf &lt;- data.frame(\n  x1 = c(x1, x1, x2, x2),\n  x2 = c(y1, -y1, y2, -y2)\n)\ndf$z &lt;- df$x1^2 + df$x2^2\nplot3D::scatter3D(df$x1, df$x2, df$z)\n\n\n\n\n\n\n\n# 2d correspondence\nplot(x2~x1,data=df,col=c(4,2,2,3)[round(z)]) \n\n\n\n\n\n\n\n\nBefore going into details, we run svm on the circle data using several different kernels. First the linear:\n\nsv.circ.lin &lt;- svm(factor(z)~ x1+x2,data = df, kernel='linear')\nplot(sv.circ.lin,df,formula=x2~x1,main=\"Linear Kernel\")\n\n\n\n\n\n\n\n\nUsing the Radial Kernel we get:\n\nsv.circ.rad &lt;- svm(factor(z)~ x1+x2,data = df, kernel='radial')\nplot(sv.circ.rad,df,formula=x2~x1)\n\n\n\n\n\n\n\n\n\n\nHere a support vector machine with a radial kernel is fitted to the penguin data using bill length and bill depth as predictors. The radial kernel implicitly maps the data into a higher-dimensional feature space, allowing the classifier to learn a non-linear decision boundary between the species. The subsequent plot displays this non-linear separating boundary in the original two-dimensional feature space, illustrating how the radial kernel can capture complex class structure that a linear SVM cannot.\n\nsv.fit.rad &lt;- svm(species ~ bill_len+bill_dep, data = penguins_1_2, kernel='radial')\nplot(sv.fit.rad,penguins_1_2,formula=bill_dep ~ bill_len)\n\n\n\n\n\n\n\n\nWe can adjust the model parameters to improve classification performance, but care must be taken to avoid overfitting; in practice, methods such as cross-validation should be used to select these parameters.\nBelow, the parameter \\(\\gamma\\) is increased to 2, which accentuates large differences between points.\n\nsv.fit.rad.gam2 &lt;- svm(species ~ bill_len+bill_dep, data = penguins_1_2,kernel='radial', gamma=2)\nplot(sv.fit.rad.gam2, penguins_1_2, formula=bill_dep ~ bill_len)\n\n\n\n\n\n\n\n\n\n\n\nThe basic idea is to apply the support vector machine recursively, comparing one group against all remaining groups, then comparing the second group against groups three and higher, and so on. As a final example, all species in the penguins data are used, with both linear and radial kernel fits under default settings.\n\nsv.fit.lin_all3 &lt;- svm(species ~ bill_len + bill_dep, data = penguins,kernel='linear')\nplot(sv.fit.lin_all3, penguins, formula=bill_dep ~ bill_len)\n\n\n\n\n\n\n\n\nAs before, Gentoo is easy to separate from the other species, but this multiclass extension does not substantially improve predictive performance. Distinguishing Adélie from Chinstrap remains difficult near the decision boundary, and the radial kernel provides only a modest improvement.\nThe radial variant is as follows:\n\nsv.fit.rad_all3 &lt;- svm(species ~ bill_len + bill_dep, data = penguins, kernel='radial')\nplot(sv.fit.rad_all3,penguins,formula=bill_dep ~ bill_len)"
  },
  {
    "objectID": "teaching/stat-learn/material/10/10-svm.html#cross-validation",
    "href": "teaching/stat-learn/material/10/10-svm.html#cross-validation",
    "title": "Support Vector Machines",
    "section": "",
    "text": "As done previously in this course, the basic idea is to withhold a portion of the training data and use it as a test set. First, one loops over the open tuning parameters, such as \\(C\\) and \\(\\gamma\\), fitting the model on the training set while holding these parameters fixed. The fitted model is then evaluated on the test set, and this process is repeated for all parameter combinations. The parameter values that yield the best performance are then selected. We applied this approach to the penguins data, still attempting classification using only two features.\nA grid search is conducted over \\(C = 0.01, 0.1, 1, 10, 100\\) and \\(\\gamma = 0.5, 1, 2, 3, 4\\). Somewhat surprisingly, the default parameter values performed best in this case.\n\nset.seed(1001)\ntune.out &lt;- tune(svm, species ~., data=penguins, kernel='radial', ranges = list(cost=c(0.01,0.1,1,10,100),gamma=c(0.5,1,2,3,4)))\nsummary(tune.out)\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost gamma\n    1   0.5\n\n- best performance: 0.01212678 \n\n- Detailed performance results:\n    cost gamma      error dispersion\n1  1e-02   0.5 0.56054495 0.07977601\n2  1e-01   0.5 0.03004679 0.02427130\n3  1e+00   0.5 0.01212678 0.02119780\n4  1e+01   0.5 0.02407531 0.02361808\n5  1e+02   0.5 0.02407531 0.02361808\n6  1e-02   1.0 0.56054495 0.07977601\n7  1e-01   1.0 0.21904650 0.06312732\n8  1e+00   1.0 0.01800914 0.02523584\n9  1e+01   1.0 0.02113414 0.02468684\n10 1e+02   1.0 0.02113414 0.02468684\n11 1e-02   2.0 0.56054495 0.07977601\n12 1e-01   2.0 0.53040393 0.08062374\n13 1e+00   2.0 0.04503629 0.02956429\n14 1e+01   2.0 0.04797746 0.03525477\n15 1e+02   2.0 0.04797746 0.03525477\n16 1e-02   3.0 0.56054495 0.07977601\n17 1e-01   3.0 0.56054495 0.07977601\n18 1e+00   3.0 0.09274589 0.07027869\n19 1e+01   3.0 0.08980472 0.06407604\n20 1e+02   3.0 0.08980472 0.06407604\n21 1e-02   4.0 0.56054495 0.07977601\n22 1e-01   4.0 0.56054495 0.07977601\n23 1e+00   4.0 0.13162131 0.07053983\n24 1e+01   4.0 0.12270865 0.06670692\n25 1e+02   4.0 0.12270865 0.06670692\n\n\n\n\n\n\n\n\nNote\n\n\n\nDispersion, as shown above, is analogous to a root mean squared error (RMSE), \\[\n\\mathrm{rmse} = \\sqrt{\\frac{1}{n}\\sum_i \\left(\\hat{y}_i - y_i\\right)^2},\n\\] and is a quantity we seek to minimize. However, it is possible to choose parameter values that yield improved within-sample performance but do not translate into improved out-of-sample performance.\n\n\nOur final model is thus:\n\nsv.fit.rad_all3 &lt;- svm(species ~ bill_len+bill_dep, data= penguins, kernel='radial', cost=1, gamma=.5)\nplot(sv.fit.rad_all3, penguins, formula = bill_dep ~ bill_len)"
  },
  {
    "objectID": "teaching/stat-learn/material/10/10-svm.html#exercise-radial-kernels-and-nonlinear-decision-boundaries",
    "href": "teaching/stat-learn/material/10/10-svm.html#exercise-radial-kernels-and-nonlinear-decision-boundaries",
    "title": "Support Vector Machines",
    "section": "",
    "text": "This section explores the use of radial kernels to allow decision boundaries to be both non-linear and non-contiguous. As a reminder, the linear and radial (sometimes called Gaussian) kernels are defined as follows.\nLinear kernel:\n\\[\nK(\\mathbf{X}_i, \\mathbf{X}_j) = \\langle \\mathbf{X}_i, \\mathbf{X}_j \\rangle,\n\\]\nwhere \\(\\langle \\cdot \\rangle\\) denotes the standard vector inner product, as defined in the notes.\nRadial kernel:\n\\[\nK(\\mathbf{X}_i, \\mathbf{X}_j) = \\exp\\!\\left(-\\gamma \\lVert \\mathbf{X}_i - \\mathbf{X}_j \\rVert^2\\right).\n\\]\nThis kernel is based on a distance measure and includes a single tuning parameter \\(\\gamma\\), which controls the influence of large distances between observations. Larger values of \\(\\gamma\\) emphasize local structure, while smaller values lead to smoother decision boundaries. We briefly explore the effect of this parameter here, with a more detailed discussion deferred to the next class.\nFor the following exercises, we use the Australian Crabs data (crabs), a benchmark data set that is notoriously difficult to cluster and relatively challenging to classify. The five primary predictors (measurements) are abbreviated as FL, RW, CL, CW, and BD. The data needs to be preprocessed so that sex and species are combined into a single categorical variable, group, with four levels:\n\nBlue Males\n\nOrange Males\n\nBlue Females\n\nOrange Females\n\nIn most analyses, we do not explicitly split the data into training and test sets. Instead, we rely on various forms of cross-validation to properly assess out-of-sample performance.\nYou are asked to compare and contrast SVM fits using the raw measures.\n\nRun svm with the following formula using all five measures: group ~ FL + RW + CL + CW + BD, first with the linear kernel and then with the radial kernel.\nCompare the accuracy (and optionally the confusion matrices) for these two runs.\nIf the radial-kernel fit performs worse, adjust the tuning parameter by setting gamma to 2, 4, or even 10, and then compare the accuracy of this updated fit.\nIf you would like to visualize the fits, try plotting RW vs. CW, but you must set slice to reasonable values for the remaining features.\n\nDetermine if the following statements are true/false based on your output:\n\nThe crabs data, examined using raw features seems to be divisible into groups using linear ‘cuts’\nIn order to perform well, use of the radial kernel requires higher penalties for large distances between observations\nVisualization of the svm solution is hindered by our choice for the slice?"
  },
  {
    "objectID": "teaching/stat-learn/material/02/02-linear-regression-I.html",
    "href": "teaching/stat-learn/material/02/02-linear-regression-I.html",
    "title": "Linear Regression I",
    "section": "",
    "text": "Linear regression models relationships using straight lines (or flat planes/hyperplanes when there are multiple predictors), following the equation \\(Y = mx + b\\).\nIf the true relationship between the predictor and the outcome isn’t actually linear, the model’s performance can suffer. More importantly, this poor performance often isn’t uniform and it may be especially bad for specific ranges of predictor values.\n\\[\\underbrace{Y_i}_\\text{Observed Value for Person i} = \\underbrace{\\beta_0}_\\text{intercept} + \\underbrace{\\beta_1}_\\text{Coefficient for X1} * \\underbrace{X_{i1}}_\\text{Value for person i on Variable X1} + ... + \\beta_p * X_{ip}\\]\n\n\nCan you think of a situation where it would be particularly problematic if our model consistently under- or over-predicted within certain ranges of the predictor?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nImagine we’re predicting house prices (\\(Y\\)) using square footage (\\(X\\)).\nIf we fit a linear model, it might do a decent job for mid-sized homes but fail at the extremes.\n\nFor very small homes, the model might over-predict price (since tiny houses often sell for disproportionately less).\n\nFor very large homes, it might under-predict (since luxury properties tend to have higher prices per square foot).\n\nThis would be especially problematic if those extreme cases are important to decision-making, for example, a real estate investor estimating renovation costs or an appraiser evaluating high-end homes.\nIn such cases, a nonlinear model (like polynomial regression or a tree-based model) might capture those curved patterns better.\n\n\n\n\n\n\nLinear regression models include two types of parameters: coefficients which describe how changes in the predictors affect the predicted outcome, and an intercept, which represents the predicted value when all predictors are zero.\nTo build an effective model, we need to estimate these parameters in a way that best fits the data. We covered two main approaches for doing this: Least Squares and Maximum Likelihood Estimation (MLE).\n\n\nAs the name implies, Least Squares aims to choose parameter values that minimize the sum of squared errors.\n\\[ \\text{SSE} = \\sum_{i = 1}^n(y_i - \\beta_0 - \\beta_1*x_i)^2 \\]\nthe \\(\\hat{y_i}\\) represents our model’s predicted value of \\(y_i\\). For a simple linear regression with only 1 predictor, we get our prediction using the formula:\n\\[ \\hat{y} = \\beta_0 + \\beta_1*x_i \\]\nSo let’s plug that in for \\(\\hat{y_i}\\):\n\\[ \\text{SSE} = \\sum_{i = 1}^n(y_i - \\beta_0 - \\beta_1*x_i)^2 \\]\nNow all we need to do is set the partial derivatives of the \\(\\text{SSE}\\) to 0 and solve. The formula above has two parameters that we’re interested in: \\(\\beta_0\\) and \\(\\beta_1\\), so we’ll take the partial derivatives of \\(\\text{SSE}\\) with respect to each of them:\n\\[\\frac{\\partial SSE}{\\partial \\beta_0} = \\sum_{i = 1}^n 2(y_i - \\beta_0 - \\beta_1*x_i)(-1)\\] \\[\\frac{\\partial SSE}{\\partial \\beta_1} = \\sum_{i = 1}^n 2(y_i - \\beta_0 - \\beta_1*x_i)(-x_i)\\]\nand set them equal to 0.\n\\[\\frac{\\partial SSE}{\\partial \\beta_0} = \\sum_{i = 1}^n 2(y_i - \\beta_0 - \\beta_1*x_i)(-1) = 0\\] \\[\\frac{\\partial SSE}{\\partial \\beta_1} = \\sum_{i = 1}^n 2(y_i - \\beta_0 - \\beta_1*x_i)(-x_i) = 0\\]\nthen we solve for \\(\\beta_0\\) and \\(\\beta_1\\) and we get:\n\\[\\beta_0 = \\bar{y} - \\hat{\\beta_1}* \\bar{x}\\]\nand\n\\[ \\beta_1 = \\frac{Cov(x,y)}{Var(x)} = Corr(x,y) * \\frac{sd(x)}{sd(y)} \\]\nThese values for \\(\\beta_0\\) and \\(\\beta_1\\) are the ones that minimize our Sum of Squared Errors (\\(\\text{SSE}\\)) and therefore give us a model that performs very well.\n\n\n\nAnother way to estimate the parameters (coefficients and intercept) of a linear regression model is through Maximum Likelihood Estimation (MLE), a method we’ll revisit several times in this course. MLE selects parameter values that make the observed training data as likely as possible under the model.\nRemember, a model is our mathematical description of the world. If a model assigns very low probability to data that looks like what we actually observed, it’s probably not a good description of reality.\nThe likelihood of an individual data point in our model is:\n\\[ p(y_i | x_i; \\beta_0, \\beta_1, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(y_i - (\\beta_0 + \\beta_1 * x_i))^2}{2\\sigma^2}}\\]\n\\(^\\text{(notice that the numerator in the exponent is just the squared error for that data point)}\\)\nIf we have multiple data points in our training data, we’ll multiply their likelihoods.\n\\[ \\prod_{i = 1}^{n} p(y_i | x_i; \\beta_0, \\beta_1, \\sigma^2) = \\prod_{i = 1}^{n}\\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(y_i - (\\beta_0 + \\beta_1 * x_i))^2}{2\\sigma^2}}\\]\nthis gives us the overall likelihood of our training data given the values of \\(\\beta_0\\), \\(\\beta_1\\). We want to choose values of \\(\\beta_0\\) and \\(\\beta_1\\) that maximize the likelihood from the equation above. To do so, we typically take the log of the likelihood (remember logs turn multiplications into sums, which makes the math easier) and maximize that by setting it’s partial derivatives (w.r.t \\(\\beta_0\\) and \\(\\beta_1\\)) equal to 0.\nWhen we do that, it turns out we get the exact same estimates as from least squares!\n\\[\\beta_0 = \\bar{y} - \\hat{\\beta_1}* \\bar{x}\\]\nand\n\\[ \\beta_1 = \\frac{Cov(x,y)}{Var(x)} = Corr(x,y) * \\frac{sd(x)}{sd(y)} \\]\nThese values for \\(\\beta_0\\) and \\(\\beta_1\\) are the ones that mazimize the likelihood of our data, and therefore give us a model that performs very well."
  },
  {
    "objectID": "teaching/stat-learn/material/02/02-linear-regression-I.html#question",
    "href": "teaching/stat-learn/material/02/02-linear-regression-I.html#question",
    "title": "Linear Regression I",
    "section": "",
    "text": "Can you think of a situation where it would be particularly problematic if our model consistently under- or over-predicted within certain ranges of the predictor?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nImagine we’re predicting house prices (\\(Y\\)) using square footage (\\(X\\)).\nIf we fit a linear model, it might do a decent job for mid-sized homes but fail at the extremes.\n\nFor very small homes, the model might over-predict price (since tiny houses often sell for disproportionately less).\n\nFor very large homes, it might under-predict (since luxury properties tend to have higher prices per square foot).\n\nThis would be especially problematic if those extreme cases are important to decision-making, for example, a real estate investor estimating renovation costs or an appraiser evaluating high-end homes.\nIn such cases, a nonlinear model (like polynomial regression or a tree-based model) might capture those curved patterns better."
  },
  {
    "objectID": "teaching/stat-learn/material/02/02-linear-regression-I.html#how-to-choose-model-parameters",
    "href": "teaching/stat-learn/material/02/02-linear-regression-I.html#how-to-choose-model-parameters",
    "title": "Linear Regression I",
    "section": "",
    "text": "Linear regression models include two types of parameters: coefficients which describe how changes in the predictors affect the predicted outcome, and an intercept, which represents the predicted value when all predictors are zero.\nTo build an effective model, we need to estimate these parameters in a way that best fits the data. We covered two main approaches for doing this: Least Squares and Maximum Likelihood Estimation (MLE).\n\n\nAs the name implies, Least Squares aims to choose parameter values that minimize the sum of squared errors.\n\\[ \\text{SSE} = \\sum_{i = 1}^n(y_i - \\beta_0 - \\beta_1*x_i)^2 \\]\nthe \\(\\hat{y_i}\\) represents our model’s predicted value of \\(y_i\\). For a simple linear regression with only 1 predictor, we get our prediction using the formula:\n\\[ \\hat{y} = \\beta_0 + \\beta_1*x_i \\]\nSo let’s plug that in for \\(\\hat{y_i}\\):\n\\[ \\text{SSE} = \\sum_{i = 1}^n(y_i - \\beta_0 - \\beta_1*x_i)^2 \\]\nNow all we need to do is set the partial derivatives of the \\(\\text{SSE}\\) to 0 and solve. The formula above has two parameters that we’re interested in: \\(\\beta_0\\) and \\(\\beta_1\\), so we’ll take the partial derivatives of \\(\\text{SSE}\\) with respect to each of them:\n\\[\\frac{\\partial SSE}{\\partial \\beta_0} = \\sum_{i = 1}^n 2(y_i - \\beta_0 - \\beta_1*x_i)(-1)\\] \\[\\frac{\\partial SSE}{\\partial \\beta_1} = \\sum_{i = 1}^n 2(y_i - \\beta_0 - \\beta_1*x_i)(-x_i)\\]\nand set them equal to 0.\n\\[\\frac{\\partial SSE}{\\partial \\beta_0} = \\sum_{i = 1}^n 2(y_i - \\beta_0 - \\beta_1*x_i)(-1) = 0\\] \\[\\frac{\\partial SSE}{\\partial \\beta_1} = \\sum_{i = 1}^n 2(y_i - \\beta_0 - \\beta_1*x_i)(-x_i) = 0\\]\nthen we solve for \\(\\beta_0\\) and \\(\\beta_1\\) and we get:\n\\[\\beta_0 = \\bar{y} - \\hat{\\beta_1}* \\bar{x}\\]\nand\n\\[ \\beta_1 = \\frac{Cov(x,y)}{Var(x)} = Corr(x,y) * \\frac{sd(x)}{sd(y)} \\]\nThese values for \\(\\beta_0\\) and \\(\\beta_1\\) are the ones that minimize our Sum of Squared Errors (\\(\\text{SSE}\\)) and therefore give us a model that performs very well.\n\n\n\nAnother way to estimate the parameters (coefficients and intercept) of a linear regression model is through Maximum Likelihood Estimation (MLE), a method we’ll revisit several times in this course. MLE selects parameter values that make the observed training data as likely as possible under the model.\nRemember, a model is our mathematical description of the world. If a model assigns very low probability to data that looks like what we actually observed, it’s probably not a good description of reality.\nThe likelihood of an individual data point in our model is:\n\\[ p(y_i | x_i; \\beta_0, \\beta_1, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(y_i - (\\beta_0 + \\beta_1 * x_i))^2}{2\\sigma^2}}\\]\n\\(^\\text{(notice that the numerator in the exponent is just the squared error for that data point)}\\)\nIf we have multiple data points in our training data, we’ll multiply their likelihoods.\n\\[ \\prod_{i = 1}^{n} p(y_i | x_i; \\beta_0, \\beta_1, \\sigma^2) = \\prod_{i = 1}^{n}\\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(y_i - (\\beta_0 + \\beta_1 * x_i))^2}{2\\sigma^2}}\\]\nthis gives us the overall likelihood of our training data given the values of \\(\\beta_0\\), \\(\\beta_1\\). We want to choose values of \\(\\beta_0\\) and \\(\\beta_1\\) that maximize the likelihood from the equation above. To do so, we typically take the log of the likelihood (remember logs turn multiplications into sums, which makes the math easier) and maximize that by setting it’s partial derivatives (w.r.t \\(\\beta_0\\) and \\(\\beta_1\\)) equal to 0.\nWhen we do that, it turns out we get the exact same estimates as from least squares!\n\\[\\beta_0 = \\bar{y} - \\hat{\\beta_1}* \\bar{x}\\]\nand\n\\[ \\beta_1 = \\frac{Cov(x,y)}{Var(x)} = Corr(x,y) * \\frac{sd(x)}{sd(y)} \\]\nThese values for \\(\\beta_0\\) and \\(\\beta_1\\) are the ones that mazimize the likelihood of our data, and therefore give us a model that performs very well."
  },
  {
    "objectID": "teaching/stat-learn/material/02/02-linear-regression-I.html#packages-needed",
    "href": "teaching/stat-learn/material/02/02-linear-regression-I.html#packages-needed",
    "title": "Linear Regression I",
    "section": "2.1 Packages needed",
    "text": "2.1 Packages needed\n\n# We use base R mostly but sometimes explore the tidyverse option, so load tidyverse\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "teaching/stat-learn/material/02/02-linear-regression-I.html#read-amazon-book-data",
    "href": "teaching/stat-learn/material/02/02-linear-regression-I.html#read-amazon-book-data",
    "title": "Linear Regression I",
    "section": "2.2 Read Amazon Book Data",
    "text": "2.2 Read Amazon Book Data\nThe dataset contains information about a collection of books sold on Amazon, including their list price, physical dimensions (such as height, width, and thickness), weight, and number of pages. It also includes the Amazon price, which serves as the outcome variable we aim to predict.\nWe’ll use this dataset to explore how different characteristics of a book relate to its selling price, applying linear regression to model and interpret these relationships.\n\n# Load the readr package (comes with tidyverse)\n# library(readr)\n\n# Read the Amazon data (tab-separated)\nama &lt;- read_tsv(\"02-data/amazon-books.txt\")\n\nRows: 325 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (5): Title, Author, Hard/ Paper, Publisher, ISBN-10\ndbl (8): List Price, Amazon Price, NumPages, Pub year, Height, Width, Thick,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# if you don't want to use tidyverse you can use:\n# ama &lt;- read.delim(\n#  \"amazon-books.txt\",\n#  header = TRUE,\n#  stringsAsFactors = FALSE)\n\n# look at the 10 first rows of the data\nknitr::kable(head(ama,10))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTitle\nAuthor\nList Price\nAmazon Price\nHard/ Paper\nNumPages\nPublisher\nPub year\nISBN-10\nHeight\nWidth\nThick\nWeight (oz)\n\n\n\n\n1,001 Facts that Will Scare the S#*t Out of You: The Ultimate Bathroom Reader\nCary McNeal\n12.95\n5.18\nP\n304\nAdams Media\n2010\n1605506249\n7.8\n5.5\n0.8\n11.2\n\n\n21: Bringing Down the House - Movie Tie-In: The Inside Story of Six M.I.T. Students Who Took Vegas for Millions\nBen Mezrich\n15.00\n10.20\nP\n273\nFree Press\n2008\n1416564195\n8.4\n5.5\n0.7\n7.2\n\n\n100 Best-Loved Poems (Dover Thrift Editions)\nSmith\n1.50\n1.50\nP\n96\nDover Publications\n1995\n486285537\n8.3\n5.2\n0.3\n4.0\n\n\n1421: The Year China Discovered America\nGavin Menzies\n15.99\n10.87\nP\n672\nHarper Perennial\n2008\n0061564893\n8.8\n6.0\n1.6\n28.8\n\n\n1493: Uncovering the New World Columbus Created\nCharles C. Mann\n30.50\n16.77\nP\n720\nKnopf\n2011\n0307265722\n8.0\n5.2\n1.4\n22.4\n\n\n1861: The Civil War Awakening\nAdam Goodheart\n28.95\n16.44\nH\n460\nKnopf\n2011\n1400040159\n8.9\n6.3\n1.7\n32.0\n\n\nA Christmas Carol and Other Christmas Writings (Penguin Classics)\nDickens\n20.00\n13.46\nH\n336\nPenguin Classics Hardcover\n2010\n141195851\n7.8\n5.3\n1.2\n15.5\n\n\nA Confederacy of Dunces\nJohn Kennedy Toole\n15.00\n8.44\nP\n405\nGrove Weidenfeld\n1987\n802130208\n8.2\n5.3\n0.8\n11.2\n\n\nA Dance With Dragons\nMartin RR, George\n35.00\n18.81\nH\nNA\nBantam\n2011\n553801473\n9.6\n6.5\n2.1\nNA\n\n\nA Farewell To Arms\nErnest Hemingway\n30.00\n19.80\nH\n304\nScribner\n1997\n0684837889\n9.6\n6.4\n1.1\n19.2"
  },
  {
    "objectID": "teaching/stat-learn/material/02/02-linear-regression-I.html#clean-the-data",
    "href": "teaching/stat-learn/material/02/02-linear-regression-I.html#clean-the-data",
    "title": "Linear Regression I",
    "section": "2.3 Clean the data",
    "text": "2.3 Clean the data\nLet’s remove missing values…\n\n# Check for missing (NA) values in each column\ncolSums(is.na(ama))\n\n       Title       Author   List Price Amazon Price  Hard/ Paper     NumPages \n           0            1            1            0            0            2 \n   Publisher     Pub year      ISBN-10       Height        Width        Thick \n           1            1            0            4            5            1 \n Weight (oz) \n           9 \n\ndim(ama)\n\n[1] 325  13\n\n# Drop rows with any missing values\nama &lt;- na.omit(ama)\n\n# Reset the row index (Rrownames)\nrownames(ama) &lt;- NULL\n\n# final dimensions of data\ndim(ama)\n\n[1] 310  13"
  },
  {
    "objectID": "teaching/stat-learn/material/02/02-linear-regression-I.html#separate-data-into-x-predictors-and-y-outcome",
    "href": "teaching/stat-learn/material/02/02-linear-regression-I.html#separate-data-into-x-predictors-and-y-outcome",
    "title": "Linear Regression I",
    "section": "2.4 Separate data into X (predictors) and y (outcome)",
    "text": "2.4 Separate data into X (predictors) and y (outcome)\nWe want to prepare our data for modeling by separating the predictors (\\(X\\)) from the outcome variable (\\(y\\)). The predictors are the input features used to explain or predict the outcome — in this case, various physical attributes of books. The outcome (Amazon Price) is the numeric value we’re trying to predict.\n\n# Define predictor column names\npredictors &lt;- c(\"List Price\", \"NumPages\", \"Weight (oz)\", \"Thick\", \"Height\", \"Width\")\n\n# Subset predictors (X)\nX &lt;- ama[, predictors]\n\n# Subset outcome (y)\ny &lt;- ama[[\"Amazon Price\"]]  # or ama$`Amazon Price`"
  },
  {
    "objectID": "teaching/stat-learn/material/02/02-linear-regression-I.html#fitting-the-model",
    "href": "teaching/stat-learn/material/02/02-linear-regression-I.html#fitting-the-model",
    "title": "Linear Regression I",
    "section": "2.5 Fitting the model",
    "text": "2.5 Fitting the model\nBefore fitting a regression model, it’s often useful to standardize the predictor variables so they’re on the same scale. Here, we transform each predictor into a z-score, meaning we subtract its mean and divide by its standard deviation.\nThis ensures that all predictors contribute comparably to the model, especially when their original units differ (for example, weight in ounces vs. height in inches).\nIn R, this can be done easily using the scale() function.\n\n# Standardize (z-score) the predictor variables\nX_z &lt;- as.data.frame(scale(X))\n\nNow we fit a linear model with lm(). This ensures that all steps are applied consistently and that the model is trained on properly scaled data.\n\n#Fit a linear regression model using the standardized data\nmodel &lt;- lm(y ~ ., data = X_z)\n\n# View model summary\nsummary(model)\n\n\nCall:\nlm(formula = y ~ ., data = X_z)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.6291  -1.6496  -0.3563   1.2966  22.9981 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   12.58765    0.18715  67.261  &lt; 2e-16 ***\n`List Price`  11.42032    0.23158  49.314  &lt; 2e-16 ***\nNumPages       0.23189    0.34195   0.678  0.49820    \n`Weight (oz)` -0.42037    0.33361  -1.260  0.20862    \nThick         -1.16151    0.35601  -3.263  0.00123 ** \nHeight        -0.09905    0.24145  -0.410  0.68194    \nWidth         -0.19750    0.24447  -0.808  0.41981    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.295 on 303 degrees of freedom\nMultiple R-squared:  0.9206,    Adjusted R-squared:  0.919 \nF-statistic: 585.4 on 6 and 303 DF,  p-value: &lt; 2.2e-16\n\n## Fitted (predicted) values\ny_pred &lt;- predict(model)\n\n# view some of the observed and predicted values\nhead(cbind(y,y_pred))\n\n      y     y_pred\n1  5.18  8.6869171\n2 10.20 10.9158750\n3  1.50  0.6459632\n4 10.87  7.8749115\n5 16.77 21.7538857\n6 16.44 18.0935043"
  },
  {
    "objectID": "teaching/stat-learn/material/02/02-linear-regression-I.html#evaluate-model-perfromance",
    "href": "teaching/stat-learn/material/02/02-linear-regression-I.html#evaluate-model-perfromance",
    "title": "Linear Regression I",
    "section": "2.6 Evaluate model perfromance",
    "text": "2.6 Evaluate model perfromance\nAfter fitting and predicting, we assess how well our model performs by comparing the predicted values (y_pred) to the actual outcomes (y).\nWe compute several common metrics:\n\nMSE (Mean Squared Error): the average of squared prediction errors and penalizes large errors more heavily.\n\nMAE (Mean Absolute Error): the average of absolute prediction errors and gives an intuitive sense of the typical deviation.\n\nMAPE (Mean Absolute Percentage Error): expresses prediction errors as a percentage of actual values.\n\nR² (R-squared): the proportion of variance in the outcome explained by the model (higher is better).\n\n\n# Evaluate model performance\nMSE  &lt;- mean((y - y_pred)^2)\nMAE  &lt;- mean(abs(y - y_pred))\nMAPE &lt;- mean(abs((y - y_pred) / y)) * 100\nR2   &lt;- 1 - sum((y - y_pred)^2) / sum((y - mean(y))^2)\n\n# alternatively for R2: summary(model)$r.squared\n\ncbind(MSE,MAE,MAPE,R2)\n\n          MSE     MAE     MAPE        R2\n[1,] 10.61234 2.16044 19.62124 0.9205886"
  },
  {
    "objectID": "teaching/stat-learn/material/02/02-linear-regression-I.html#question-1",
    "href": "teaching/stat-learn/material/02/02-linear-regression-I.html#question-1",
    "title": "Linear Regression I",
    "section": "2.7 Question",
    "text": "2.7 Question\nWhat do these metrics tell us about the model’s performance?\nDiscuss whether this model seems to fit the data well, and what potential issues (if any) might still exist.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThese results suggest that the model performs quite well:\n\nMSE (10.61) and MAE (2.16) indicate that, on average, predictions are only a few units away from the actual values.\nMSE penalizes larger errors more heavily, while MAE provides the average absolute deviation.\nMAPE (19.6%) means predictions are, on average, about 20% off from the true values, thus a reasonably good level of accuracy depending on context.\nR² = 0.92 indicates that 92% of the variation in the outcome variable is explained by the predictors, thus a strong fit.\n\nOverall, this model explains most of the variability in the data and makes relatively accurate predictions.\nHowever, we’d still want to check residual plots for patterns or bias (e.g., under- or over-prediction at certain ranges) to confirm the model’s assumptions hold true."
  },
  {
    "objectID": "teaching/stat-learn/material/02/02-linear-regression-I.html#checking-assumptions",
    "href": "teaching/stat-learn/material/02/02-linear-regression-I.html#checking-assumptions",
    "title": "Linear Regression I",
    "section": "2.8 Checking Assumptions",
    "text": "2.8 Checking Assumptions\nRemember there are 3 main assumptions\\(^*\\) of Linear Regression:\n\nLinearity\nHomoskedasticity\nNormality of Errors\n\n\\(^*\\) There’s also an assumption of Independence, aka that the value of one data point does not affect the value of another data point.\n\n2.8.1 Linearity\nWe assess linearity by either:\n\nplotting one predictor at a time against the outcome, and see if there are any clear non-linear patterns.\n\nWe will use ggplot here and use a faceted plot:\n\n# The base R version:\n\n# Set up a 2x3 layout for the plots\npar(mfrow = c(2, 3))\n# Loop through each predictor and plot Amazon Price vs that predictor\nfor (c in predictors) {\n  plot(\n    ama[[c]], ama[[\"Amazon Price\"]],\n    main = paste(\"Amazon Price vs.\", c),\n    xlab = c,\n    ylab = \"Amazon Price\",\n    pch = 19, col = \"steelblue\"\n  )\n}\n\n\n\n\n\n\n\n# predictors vector from earlier:\n# predictors &lt;- c(\"List Price\", \"NumPages\", \"Weight (oz)\", \"Thick\", \"Height\", \"Width\")\n\n# Reshape to long format: one column for predictor name, one for its values\nama_long &lt;- ama |&gt;\n  pivot_longer(\n    cols = all_of(predictors),\n    names_to = \"predictor\",\n    values_to = \"value\"\n  )\n\n# Faceted scatterplot: one panel per predictor\nggplot(ama_long, aes(x = value, y = `Amazon Price`)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~ predictor, scales = \"free_x\") +\n  labs(\n    title = \"Amazon Price vs. Predictors\",\n    x = NULL,\n    y = \"Amazon Price\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNext we plot the predicted values (x-axis) by the residuals (y-axis) and look for clear non-linear patterns:\n\n# The base R version:\n\n# Create a data frame of residuals and predictions\nfitdata &lt;- data.frame(\n  errors = y - y_pred,\n  pred = y_pred\n)\n\n# Base R residual plot\nplot(\n  fitdata$pred, fitdata$errors,\n  main = \"Residuals vs. Predicted Values\",\n  xlab = \"Predicted Values\",\n  ylab = \"Residuals\",\n  pch = 19, col = \"steelblue\"\n)\n\n# Add horizontal reference line at 0\nabline(h = 0, col = \"red\", lwd = 2, lty = 2)\n\n\n\n\n\n\n\n# Residual plot using ggplot\nggplot(fitdata, aes(x = pred, y = errors)) +\n  geom_point(color = \"black\", alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  theme_minimal() +\n  labs(\n    title = \"Residuals vs. Predicted Values\",\n    x = \"Predicted Values\",\n    y = \"Residuals\"\n  )\n\n\n\n\n\n\n\n\n\nNote: from now on I will only use ggplot.\n\n\n\n2.8.2 Homoskedasticity\nWe assess homoskedasticity (constant variance of residuals) using the same residual plot as above. Recall that “homo” means same and “hetero” means different, while “skedasticity” refers to the spread or variance of the errors.\nWhen examining the plot, look for whether the residuals appear to have a consistent spread across the range of predicted values.\nDo some areas along the x-axis show much larger or smaller errors than others? The variance doesn’t need to be perfectly uniform, but noticeable patterns or changes in spread could suggest heteroskedasticity, meaning the model’s error variance isn’t constant.\n\n\n2.8.3 Normality\nNormality doesn’t really impact prediction, in fact many argue it doesnt even impact inference (\\(p\\)-values/confidence intervals). So we won’t dwell on it here. Here’s code to check it using something called a QQ (Quantile-Quantile plot). A Q–Q plot compares the distribution of the model’s residuals to a theoretical normal distribution.\nIf the residuals are approximately normal, the points should fall along the dashed red line.\n\n# Q–Q plot of residuals (normality check)\nggplot(fitdata, aes(sample = errors)) +\n  stat_qq(color = \"black\", alpha = 0.5) +\n  stat_qq_line(color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  theme_minimal() +\n  labs(\n    title = \"Q–Q Plot of Residuals\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  )"
  },
  {
    "objectID": "teaching/stat-learn/material/02/02-linear-regression-I.html#summary",
    "href": "teaching/stat-learn/material/02/02-linear-regression-I.html#summary",
    "title": "Linear Regression I",
    "section": "2.9 Summary",
    "text": "2.9 Summary\nOverall, we can observe that:\n\nLinearity:\n\nFrom the individual predictor plots, List Price shows a clear linear relationship with Amazon Price, suggesting it fits well within a linear framework.\nThe other predictors (e.g., Height, Width, Thick, Weight (oz), NumPages) show weak or nonlinear patterns, often clustering at common physical book sizes. This indicates that the linear model captures the trend in List Price effectively but may not fully represent the relationships of the other variables.\nList Price displays a strong and roughly linear relationship with Amazon Price. As the list price increases, the Amazon price tends to increase in a nearly proportional way.\nThe remaining predictors: (Height, Width, Thick, Weight (oz), and NumPages) show weak or nonlinear relationships. Many of these variables form visible clusters or plateaus, likely reflecting standard book formats or physical constraints (e.g., most books share similar dimensions).\nThese patterns suggest that while List Price is likely a strong predictor, the other features may require nonlinear transformations or may contribute less explanatory power in a linear model.\n\nHomoskedasticity (Constant Variance):\n\nIn several cases, there appears to be heteroskedasticity, where the variability in Amazon Price increases with the predictor (especially noticeable for List Price and Weight (oz)).\nThe residuals vs. predicted values plot shows that residuals are mostly centered around zero, but the spread is not perfectly uniform. There appears to be slightly greater variability at lower predicted values, which suggests mild heteroskedasticity.\nHowever, no severe funneling pattern is observed, so the assumption is only moderately violated.\n\n\nIn summary, the data indicate that a simple linear regression may fit well for List Price, but more complex relationships could exist for the other predictors."
  },
  {
    "objectID": "teaching/stat-learn/material/02/02-linear-regression-I.html#model-coefficients",
    "href": "teaching/stat-learn/material/02/02-linear-regression-I.html#model-coefficients",
    "title": "Linear Regression I",
    "section": "2.10 Model Coefficients",
    "text": "2.10 Model Coefficients\nAfter fitting a linear regression model, we can inspect the estimated coefficients to understand how each predictor contributes to the outcome. Each coefficient represents the expected change in the response variable (Amazon Price) for a one-unit increase in that predictor, holding all other variables constant.\nIn R, the coef() function returns both the intercept and the slope coefficients for each predictor.\nBy converting these values into a data frame, we can easily view and interpret them in a tabular format.\nThe intercept represents the model’s predicted value when all predictors are zero, while each coefficient shows the direction and strength of the predictor’s relationship with the outcome.\n\n# Extract model coefficients and intercept\n# (assuming you already have a fitted model called 'model' from lm())\n\n# Create a data frame of coefficients\ncoefficients_df &lt;- data.frame(\n  Name = names(coef(model)),\n  Coef = as.numeric(coef(model))\n)\n\n# Display the table of coefficients\ncoefficients_df\n\n           Name        Coef\n1   (Intercept) 12.58764516\n2  `List Price` 11.42032143\n3      NumPages  0.23189316\n4 `Weight (oz)` -0.42036500\n5         Thick -1.16150737\n6        Height -0.09904558\n7         Width -0.19749780\n\n\nThe model’s coefficients show how each predictor relates to the Amazon Price, holding all other variables constant.\n\nThe Intercept (12.59) represents the predicted Amazon Price when all predictors are zero though not meaningful in practice, it serves as a baseline for the model.\n\nList Price (11.42) has by far the largest positive effect: for each additional unit increase in list price, the Amazon Price is predicted to increase by roughly $11.42, all else equal.\n\nNumPages (0.23) also shows a small positive relationship, suggesting that books with more pages tend to cost slightly more.\n\nThe remaining variables, Weight (oz), Thick, Height, and Width, have negative coefficients, indicating that when holding the other factors constant, increases in these dimensions are associated with slightly lower predicted prices. These effects are relatively small and may reflect overlapping information among the physical features of books.\n\nOverall, the model suggests that List Price is the dominant predictor of Amazon Price, while the other physical attributes contribute modestly or redundantly to the price prediction."
  },
  {
    "objectID": "teaching/stat-learn/material/02/02-linear-regression-I.html#question-2",
    "href": "teaching/stat-learn/material/02/02-linear-regression-I.html#question-2",
    "title": "Linear Regression I",
    "section": "2.11 Question",
    "text": "2.11 Question\nLooking at the coefficients, most physical attributes (Weight (oz), Thick, Height, Width) have negative values.\nWhat might explain why these coefficients are negative, even though we might expect larger or heavier books to cost more?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe negative coefficients likely arise from multicollinearity; the physical attributes of a book (height, width, thickness, and weight) are strongly correlated with one another and with NumPages and List Price.\nWhen these correlated predictors are included together in the same linear model, their individual coefficients can shift direction or appear negative, even if the overall relationship with price is positive.\nIn other words, once List Price and NumPages are accounted for, the remaining variation explained by size and weight might actually correspond to lower prices (e.g., large but inexpensive books such as textbooks or coffee-table books).\nThis doesn’t mean heavier or thicker books are truly cheaper, it means their unique contribution to predicting price, after controlling for other variables, happens to be negative."
  },
  {
    "objectID": "teaching/stat-learn/material/02/02-linear-regression-I.html#modeling-beyoncé-song-popularity",
    "href": "teaching/stat-learn/material/02/02-linear-regression-I.html#modeling-beyoncé-song-popularity",
    "title": "Linear Regression I",
    "section": "3.1 Modeling Beyoncé Song Popularity",
    "text": "3.1 Modeling Beyoncé Song Popularity\nThis dataset contains song-level data scraped from Spotify’s API, and includes a variety of numerical features that describe Beyoncé’s songs, such as danceability, energy, loudness, speechiness, Chhoose your own continuous response variable to model and predict.\nThink about what interests you most:\n\nDo you want to predict how danceable a song is based on its acoustic and rhythmic qualities?\nOr are you more curious about what makes a song sound energetic, acoustic, or happy (valence)?\n\nChoose any continuous numeric variable as your response, then use the other features as predictors to build a linear regression model.\nYour goal is to see whether the musical characteristics of Beyoncé’s songs can meaningfully explain or predict that chosen outcome.\n\n# Load data\nb &lt;- read.csv(\"02-data/Beyonce_data.csv\")\n\n# Preview the data\n\n# Define predictor variables\n\n# Fit a linear regression model\n\n# Generate predictions\n\n# Assess model performance\n\n# Check residuals (assumptions)"
  },
  {
    "objectID": "teaching/stat-learn/material/02/02-linear-regression-I.html#simulation",
    "href": "teaching/stat-learn/material/02/02-linear-regression-I.html#simulation",
    "title": "Linear Regression I",
    "section": "3.2 Simulation",
    "text": "3.2 Simulation\nSometimes, we’ll run a simulation to see how our models perform in different scenarios. This helps us learn more about how the model works. Below is a custom function linear_simulation() that\n\nGenerates fake data about cats’ weights and lengths\nFits a Linear Regression Model\nGrabs the coefficients\nReturns the data and Coefficients for us to look at\n\n\nlinear_simulation &lt;- function(n = 100, trueCoef = 0.04, intercept = 0.2, error_sd = 1) {\n  \n  # 1. Generate fake data for cat length and weight -----------------------\n  # Simulate \"cat length\" from a standard normal distribution\n  length &lt;- rnorm(n, mean = 0, sd = 1)\n  \n  # Simulate \"weight\" using a linear model with some random error\n  weight &lt;- intercept + length * trueCoef + rnorm(n, mean = 0, sd = error_sd)\n  \n  cats &lt;- data.frame(length = length, weight = weight)\n  \n  \n  # 2. Fit a Linear Regression Model -------------------------------------\n  model &lt;- lm(weight ~ length, data = cats)\n  \n  \n  # 3. Extract Coefficients ----------------------------------------------\n  coef_table &lt;- data.frame(\n    Names = names(coef(model)),\n    Coef = as.numeric(coef(model))\n  )\n  \n  \n  # 4. Return data and coefficients\n  return(list(coef = coef_table, data = cats))\n}\n\nThe data are created using a known linear equation:\n\\[\n\\text{weight} = \\text{intercept} + (\\text{trueCoef} \\times \\text{length}) + \\text{random error}\n\\]\nWe can run this simulation 100’s of times.\n\nn is the number of samples in each fake dataset\ntrueCoef is the true coefficient for cat length\nintercept is the true intercept for the model\nerror_sd tells us how spread out the data is around the regression line\n\nAfter generating the data, the function fits a linear regression model using lm() and returns both the estimated coefficients and the simulated data. This allows us to explore how well the model recovers the true coefficient values (trueCoef and intercept) as we vary the number of observations or the noise level.\nTo explore how sampling variability affects our regression estimates, we run the linear_simulation() function 500 times.\nEach run generates a new random dataset of cat lengths and weights, fits a linear regression, and stores the estimated coefficients.\nWe then combine the results into two data frames:\n\ncoef_df contains the estimated coefficients (intercept and length) from each simulation.\ndata_df contains all the simulated data points, labeled by which simulation they came from.\n\nBy analyzing or visualizing coef_df, we can see how much the estimated slope and intercept vary across repeated samples, an empirical demonstration of sampling variability and the distribution of regression estimates.\n\n# Play around with these numbers \nn &lt;- 100\ntrueCoef &lt;- 0.45   # don't change\nintercept &lt;- 6     # don't change\nerror_sd &lt;- 1\n\n# Run regression simulation 500 times \nset.seed(123)  # for reproducibility\n\n# Run regression simulation 500 times \nsimulations &lt;- replicate(\n  500,\n  linear_simulation(n = n, trueCoef = trueCoef, intercept = intercept, error_sd = error_sd),\n  simplify = FALSE\n)\n\n# Extract coefficients from 500 simulations\ncoef_df &lt;- do.call(rbind, lapply(simulations, function(x) x$coef))\ncoef_df$simulation_no &lt;- rep(1:500, each = nrow(simulations[[1]]$coef))\n\n# Extract data from 500 simulations\ndata_df &lt;- do.call(rbind, lapply(simulations, function(x) x$data))\ndata_df$simulation_no &lt;- rep(1:500, each = n)\n\nNow that we’ve run a bunch of simulations with the SAME true coefficient and intercept (but different random samples), let’s look at the results of our 500 regression models.\nFirst, let’s just make some scatter plots to see some of the simulations. Notice how similar or different the simulations are from each other.\n\n# Choose number of simulations to visualize\nn_plot &lt;- 9\n\n# Subset data for the first 9 simulations\nchosen_datasets &lt;- subset(data_df, simulation_no &lt; n_plot)\n\n# Faceted scatterplots for each simulation\nggplot(chosen_datasets, aes(x = length, y = weight, color = factor(simulation_no))) +\n  geom_point() +\n  facet_wrap(~ simulation_no) +\n  theme_minimal() +\n  labs(\n    title = \"Simulated Datasets of Cat Length vs Weight\",\n    x = \"Length (standardized units)\",\n    y = \"Weight\",\n    color = \"Simulation Number\"\n  )\n\n\n\n\n\n\n\n\nLet’s look at the coefficient values from all the linear regressions we ran. This histogram shows the estimated length coefficient from 500 repeated regression simulations. Although each dataset was generated from the same true underlying relationship, random variation in the data causes the estimated slope to differ slightly across runs.\nThe red dashed line marks the mean estimated coefficient, which should be close to the true value (trueCoef = 0.45).\nThis plot illustrates the sampling distribution of the slope estimate, that is how regression coefficients vary across repeated samples due to random noise, even when the underlying relationship remains constant.\n\n# Filter to include only the 'length' coefficients\ncoef_only &lt;- subset(coef_df, Names == \"length\")\n\n# Calculate mean coefficient value\nmean_coef &lt;- mean(coef_only$Coef)\n\n# Plot histogram of coefficient estimates\nggplot(coef_only, aes(x = Coef)) +\n  geom_histogram(color = \"black\", fill = \"steelblue\", bins = 30) +\n  geom_vline(xintercept = mean_coef, color = \"red\", linetype = \"dashed\", linewidth = 1.5) +\n  theme_minimal() +\n  labs(\n    title = \"Distribution of Length Coefficients Across 500 Simulations\",\n    x = \"Estimated Coefficient for Length\",\n    y = \"Frequency\"\n  )"
  },
  {
    "objectID": "teaching/stat-learn/material/02/02-linear-regression-I.html#question-3",
    "href": "teaching/stat-learn/material/02/02-linear-regression-I.html#question-3",
    "title": "Linear Regression I",
    "section": "3.3 Question",
    "text": "3.3 Question\nLook at the different values you got for the coefficient of length. We set the TRUE coefficient value to be 0.45, think about and describe how spread apart the estimates from our 500 regression models are. Does seeing how different our coefficient estimates can be change how you think about the coefficient estimates you get in regression models on real data?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nAcross the 500 simulated regressions, the estimated length coefficients cluster around the true value of 0.45, but they vary from sample to sample.\nThis spread reflects sampling variability; even though each dataset comes from the same true relationship, random noise in the data produces slightly different estimates each time.\nMost of the slopes are fairly close to 0.45, but some are noticeably higher or lower.\nThis shows that our coefficient estimates aren’t fixed truths, they’re estimates with uncertainty.\nIf we repeatedly sampled new data from the same population, our slope estimate would fluctuate around the true value, forming a distribution of possible outcomes.\nSeeing this variation highlights why we report confidence intervals and \\(p\\)-values in regression: they describe the uncertainty around our estimated coefficients.\nIn real-world analyses, a single regression result is just one possible estimate from a range of plausible values."
  },
  {
    "objectID": "teaching/stat-learn/material/02/02-linear-regression-I.html#play-with-n-and-error_sd",
    "href": "teaching/stat-learn/material/02/02-linear-regression-I.html#play-with-n-and-error_sd",
    "title": "Linear Regression I",
    "section": "3.4 Play with n and error_sd",
    "text": "3.4 Play with n and error_sd\nHere are some suggestions:\n\nChange n, the number of data points in each sample, to be very small (say 10), how does this change the results you saw?\nChange n, the number of data points in each sample, to be very large (say 1,000), how does this change the results you saw?\nChange the error_sd term, this is a measure of how much error is in the model. Less error means that data is scattered tightly around the regression line, more error means that the data is scatters very loosely around the regression line. How does changing error_sd change the results you originally saw?"
  },
  {
    "objectID": "teaching/stat-learn/material/02/02-linear-regression-I.html#violations-of-linearity",
    "href": "teaching/stat-learn/material/02/02-linear-regression-I.html#violations-of-linearity",
    "title": "Linear Regression I",
    "section": "3.5 Violations of Linearity",
    "text": "3.5 Violations of Linearity\nHere, you’ll compare how linear regression performs on data that does and does not meet the assumption of linearity.\n\nIn the first code chunk (#nonLinReg), fit a linear regression model on data that is nonlinear, that is, data where the relationship between x and y bends or curves.\nIn the second code chunk (#LinReg), fit a model on linear data, that is data that satisfies the assumption of linearity.\nFor both models, create a residual plot showing the predicted values on the x-axis and the residuals (errors) on the y-axis.\n\n\n# Load nonlinear data\ndf_nonlin &lt;- read.csv(\"02-data/xy-nonlin.csv\")\n\n# Separate X and y\nX &lt;- data.frame(x = df_nonlin$x)\ny &lt;- df_nonlin$y\n\n# Z-score the predictor (standardize x)\nx_z &lt;- as.numeric(scale(X$x))\n\n# Fit linear regression on standardized x\n\n# Predict on training data\n\n# Prediction + error (residuals) data frame\n\n# Residual plot \n\n\n# Load nonlinear data\ndf_lin &lt;- read.csv(\"02-data/xy-lin.csv\")\n\n# Separate X and y\nX &lt;- data.frame(x = df_lin$x)\ny &lt;- df_lin$y\n\n# Z-score the predictor (standardize x)\nx_z &lt;- as.numeric(scale(X$x))\n\n# Fit linear regression on standardized x\n\n# Predict on training data\n\n# Prediction + error (residuals) data frame\n\n# Residual plot"
  },
  {
    "objectID": "teaching/stat-learn/material/02/02-linear-regression-I.html#questions",
    "href": "teaching/stat-learn/material/02/02-linear-regression-I.html#questions",
    "title": "Linear Regression I",
    "section": "3.6 Questions",
    "text": "3.6 Questions\n\nHow do the residual patterns differ between the linear and nonlinear datasets? What visual cues suggest that the linearity assumption is being violated?\nWhat are some possible consequences of violating the assumption of linearity?\nIf we used a linear regression model on the nonlinear data, are there specific ranges of x where the model would consistently over-predict or under-predict the outcome? How can you tell from the residual plot?"
  },
  {
    "objectID": "teaching/stat-learn/material/04/04_Classification_1.html",
    "href": "teaching/stat-learn/material/04/04_Classification_1.html",
    "title": "Classification I",
    "section": "",
    "text": "In our Linear Regression lectures, we talked about adding non-linearity through Feature Engineering, but that’s not the only way! We can also use link functions to add non-linearity.\nLink functions are just algebra we do to the linear prediction (\\(\\mathbf{X}\\beta\\)) in order to get the predicted value we actually want (e.g a probability).\n\\[\\underbrace{y = \\mathbf{X}\\beta}_\\text{Linear Model}\\] \\[\\underbrace{y = g^{-1}(\\mathbf{X}\\beta)}_\\text{Generalized Linear Model}\\]\nOddly, we often specify our link function using it’s inverse, hence the \\(g^{-1}()\\) instead of \\(g()\\). \\(g^{-1}()\\) takes the linear prediction and transforms it into our desired predicted value. \\(g()\\) takes our desired predicted value and transforms it back into our linear prediction.\nIn logistic regression, our goal is to predict a probability that a data point is in group 1. We talked about using:\n\nLinear Probability Models \\(g^{-1}: y = x\\)\nOdds Models \\(g^{-1}: y = e^x\\)\nLogistic Regression: \\(g^{-1}: y = \\frac{e^x}{1 + e^x}\\)\n\nLogistic Regression using the link function \\(g(x) = log{\\frac{x}{1-x}}\\) and inverse link \\(g^{-1}: y = \\frac{e^x}{1 + e^x}\\) gave us a great sigmoid shape that takes linear predictions (\\(y = \\mathbf{X}\\beta\\)) and turns them into predicted probabilities (\\(p = \\frac{e^{\\mathbf{X}\\beta}}{1 + e^{\\mathbf{X}\\beta}}\\)).\n\n\nJust like with Linear Regression, we can use Maximum Likelihood Estimation to choose the parameters (intercept and coefficients) of the model. But we have a different likelihood.\nIn a linear regression, we assumed that our errors are normally distributed around the regression line. For logistic regression, we assume that our errors are Bernoulli distributed. The Bernoulli distribution is a discrete distribution (since our outcome is discrete, a.k.a categorical) that tells you the proability of being 0 or 1.\n\n\n\nThe formula for a Bernoulli distribution for a single data point \\(x\\) is:\n\\[ f(y;p(x)) = p(x)^{y} * (1-p(x))^{1-y}\\]\nwhere \\(y\\) is the group the data point belongs to (either 0 or 1), and \\(p(x)\\) is the predicted probability of that data point being a 1.\nFor example, let’s say we’re looking at the probability that it’s sunny tomorrow. The predicted probability, according to the weather channel is \\(p(x) = 0.8\\). The likelihood of it being sunny (\\(k = 1\\)) is:\n\\[ f(1;0.8) = 0.8^1 * (1-0.8)^{1-1} = 0.8\\]\nThe likelihood of it not being sunny (\\(k = 0\\)) is: \\[ f(0;0.8) = 0.8^0 * (1-0.8)^{1-0} = 0.2\\]\n\n\n\nBut we don’t just have a SINGLE data point when fitting a logistic regression, we have MANY. So, we multiply the likelihood of each data point together to get the likelihood of the dataset:\n\\[\\prod_{i = 1}^n p(x_i)^{y_i} * (1-p(x_i))^{1-y_i}\\]\nWe want to choose parameters (e.g. \\(\\beta_0\\), or \\(\\beta_1\\)) that maximize this likelihood function. And how to we maximize it? We take it’s (partial) derivatives and set them equal to zero!\nHowever, it turns out that its much easier to work with the log of this likelihood function, so we’re often working with the log likelihood and taking it’s derivatives (this will still find the optimal parameters for the model as the values that maximize the log likelihood will also maximize the likelihood):\n\\[\\sum_{i = 1}^n y_i * log(p(x_i)) + (1-y_i) * log(1-p(x_i))\\]\n\n\n\nNow it turns out, if we multiply the log loss by \\(-\\frac{1}{N}\\), this log-loss is a really great loss function for logistic regression. Loss functions are metrics that\n\nmeasure the performance of your model, and\nhave lower scores indicate better performing models\n\n\\[-\\frac{1}{N} \\sum_{i = 1}^n y_i * log(p(x_i)) + (1-y_i) * log(1-p(x_i))\\]\nLog-Loss (also called Binary Cross Entropy) does just that! Thus we often use it as a loss function for Logistic Regression.\n\n\n\nLet’s build a Logistic Regression model in R. We’ll follow a similar workflow to what we used for linear models:\n\nSeparate your data into predictors (X) and outcome (y), and optionally set up a train/test split.\nCreate a model formula and initialize the logistic regression model using glm() with family = binomial.\nFit the model to the training data.\nUse predict() on new data to obtain predicted probabilities or class predictions.\nAssess the model’s performance (e.g., accuracy, confusion matrix, ROC curve).\n\n\n# Split data (example dataset)\nset.seed(123)\nidx &lt;- sample(seq_len(nrow(df)), size = 0.8*nrow(df))\ntrain &lt;- df[idx, ]\ntest  &lt;- df[-idx, ]\n\n# 1. Define model formula\nformula &lt;- outcome ~ predictor1 + predictor2\n\n# 2 & 3. Fit logistic regression model\nlog_model &lt;- glm(formula, data = train, family = binomial)\n\n# 4. Predict probabilities\npred_probs &lt;- predict(log_model, test, type = \"response\")\n\n# Convert to class prediction (optional threshold = 0.5)\npred_class &lt;- ifelse(pred_probs &gt; 0.5, 1, 0)\n\n# 5. Assess model\ntable(Predicted = pred_class, Actual = test$outcome)\n\n\n\n\nLet’s do an example with logistic regression to classify cancer diagnosis. We will: 1. Load and lightly clean the dataset.\n2. Select predictors whose names end with \"mean\".\n3. Split the data into training and testing sets (80/20).\n4. Fit a logistic regression (glm, binomial family).\n5. Predict class probabilities on the test set.\n6. Evaluate performance using binary cross-entropy (log loss).\nWe import the Breast Cancer dataset and drop any rows with missing values to ensure the model can be fit without errors.\n\nbc &lt;- read.csv(\n  \"04-Data/BreastCancer.csv\",\n  stringsAsFactors = FALSE\n)\n\nbc &lt;- na.omit(bc)\nnrow(bc)\n\n[1] 569\n\n\nThe outcome is diagnosis (Benign B vs Malignant M). As predictors we only use columns whose names end in “mean”.\n\n# columns ending with \"mean\"\npredictors &lt;- grep(\"mean$\", names(bc), value = TRUE)\n\n# modeling frame: outcome + predictors\ndf &lt;- data.frame(\n  diagnosis = factor(bc$diagnosis, levels = c(\"B\",\"M\")), # B=0, M=1\n  bc[, predictors]\n)\n\n\nstr(df[, c(\"diagnosis\", predictors[1:5])])\n\n'data.frame':   569 obs. of  6 variables:\n $ diagnosis      : Factor w/ 2 levels \"B\",\"M\": 2 2 2 2 2 2 2 2 2 2 ...\n $ radius_mean    : num  18 20.6 19.7 11.4 20.3 ...\n $ texture_mean   : num  10.4 17.8 21.2 20.4 14.3 ...\n $ perimeter_mean : num  122.8 132.9 130 77.6 135.1 ...\n $ area_mean      : num  1001 1326 1203 386 1297 ...\n $ smoothness_mean: num  0.1184 0.0847 0.1096 0.1425 0.1003 ...\n\n\nWe split once and keep a fixed seed for reproducibility. The model will train on train and be evaluated on test.\n\nset.seed(123)\nn &lt;- nrow(df)\nidx_train &lt;- sample.int(n, size = floor(0.8 * n))\ntrain &lt;- df[idx_train, ]\ntest  &lt;- df[-idx_train, ]\nc(n_train = nrow(train), n_test = nrow(test))\n\nn_train  n_test \n    455     114 \n\n\nWe specify a formula that uses all “mean” predictors and fit a logistic regression using the binomial family.\n\nformula &lt;- as.formula(paste(\"diagnosis ~\", paste(predictors, collapse = \" + \")))\nlog_model &lt;- glm(formula, data = train, family = binomial)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nsummary(log_model)$coefficients[1:6, , drop = FALSE]  # peek at a few coefficients\n\n                   Estimate  Std. Error    z value     Pr(&gt;|z|)\n(Intercept)     -8.33989439 14.53963337 -0.5735973 5.662403e-01\nradius_mean     -0.74101892  3.99281153 -0.1855883 8.527677e-01\ntexture_mean     0.38855965  0.07352802  5.2845112 1.260408e-07\nperimeter_mean  -0.27927112  0.54267867 -0.5146160 6.068214e-01\narea_mean        0.04017311  0.01952026  2.0580216 3.958806e-02\nsmoothness_mean 72.05651889 34.28104247  2.1019349 3.555898e-02\n\n\nWe obtain predicted probabilities of malignancy for each test observation.\n\np_test &lt;- predict(log_model, newdata = test, type = \"response\")\nhead(p_test)\n\n        1         9        15        17        18        28 \n0.9999327 0.9878218 0.9184778 0.7153792 0.9987491 0.9998869 \n\n\nLower log loss indicates better calibrated probability predictions. We map B -&gt; 0 and M -&gt; 1 and compute the average cross-entropy.\n\n# Binary cross-entropy / log loss helper\nlog_loss &lt;- function(y, p, eps = 1e-15) {\n  p &lt;- pmin(pmax(p, eps), 1 - eps)    # avoid log(0)\n  -mean(y * log(p) + (1 - y) * log(1 - p))\n}\n\ny_test &lt;- ifelse(test$diagnosis == \"M\", 1, 0)\n\nloss &lt;- log_loss(y_test, p_test)\nloss\n\n[1] 0.1193837\n\n\nA log loss of 0.119 indicates strong model performance. Log loss measures how well the model’s predicted probabilities match the true outcomes where lower is better. A value close to 0 means the model is making accurate and well-calibrated predictions, showing high confidence when it is correct and low confidence when uncertain.\n\n\n\n\n\n\nNote\n\n\n\n\nDiagnosis is encoded with levels c(“B”,“M”) so that M corresponds to the positive class (1) for log-loss computation.\nNo feature scaling is required for logistic regression to work, but standardization can sometimes help convergence or interpretability.\nLog loss evaluates the quality of probabilities, not just the final class labels.\n\n\n\nLogistic Regression coefficients are by default in terms of log odds meaning that they tell you how much the predicted log odds of being in group 1 will change when the predictor increases by 1-unit. We grab the coefficients for the model above:\n\n# ---- Extract Coefficients ----\ncoefs &lt;- summary(log_model)$coefficients   # from glm()\n\n# Convert to a data frame with names\ncoef_df &lt;- data.frame(\n  Name = rownames(coefs),\n  Coef = coefs[, \"Estimate\"],\n  row.names = NULL\n)\n\n# ---- Add Odds Ratios ----\ncoef_df$Odds &lt;- exp(coef_df$Coef)\n\ncoef_df\n\n                     Name         Coef         Odds\n1             (Intercept)  -8.33989439 2.387976e-04\n2             radius_mean  -0.74101892 4.766280e-01\n3            texture_mean   0.38855965 1.474855e+00\n4          perimeter_mean  -0.27927112 7.563348e-01\n5               area_mean   0.04017311 1.040991e+00\n6         smoothness_mean  72.05651889 1.966747e+31\n7        compactness_mean  -1.35444224 2.580912e-01\n8          concavity_mean   6.66078793 7.811662e+02\n9     concave.points_mean  74.10069490 1.518878e+32\n10          symmetry_mean  14.00627785 1.210178e+06\n11 fractal_dimension_mean -39.60760668 6.289773e-18\n\n\n\n\n\nHow do you interpret the results?\n\n\n\n\n\n\nInterpreting the Coefficients\n\n\n\n\n\n\nPositive coefficients (Odds &gt; 1) increase the likelihood of a malignant diagnosis.\nVariables like texture_mean, area_mean, concavity_mean, concave.points_mean, and symmetry_mean strongly raise the probability of cancer, with very large odds ratios indicating powerful predictors.\nNegative coefficients (Odds &lt; 1) decrease the likelihood of malignancy.\nHigher radius_mean, perimeter_mean, compactness_mean, and fractal_dimension_mean values point more toward benign tumors.\nThe intercept represents the baseline log-odds when predictors are zero (not directly interpretable on its own).\n\nOverall, features related to concavity, smoothness, and symmetry strongly increase cancer risk, while higher compactness, radius, and fractal dimension values are associated with benign masses.\n\n\n\n\n\n\nWhen you’re presenting your Logistic Regression Models to non-data people, you might want to be able to tell them which variables have the biggest impact on the predicted value. Typically, we might use coefficients for this because they give us a single number that summarizes the relationship between our predictors and our predicted value.\nHowever, log odds are difficult to understand intuitively, especially if you’re not a data person. Thus, we might want a different way to present our results. Luckily, if we exponentiate our log odds coefficients, we get odds coefficients. These are easier to understand, as most people understand intuitively what odds are.\nRemember, for odds the important threshold value is \\(1\\). So any odds coefficient \\(&gt;1\\) has a direct/positive relationship with the outcome and anything with an odds coefficient \\(&lt; 1\\) has an inverse/negative relationship with the outcome.\nYou can also use the odds coefs to give people an intuitive understanding of the relationship. If the odds coef is \\(2\\) then increasing the predictor by 1-unit causes your predicted odds to double. Similarly, if the odds coef is \\(0.5\\) then increasing the predictor by 1-unit causes your predicted odds to halve. If the odds coef is \\(1.25\\) then increasing the predictor by 1-unit causes your predicted odds to increase by \\(25\\%\\)."
  },
  {
    "objectID": "teaching/stat-learn/material/04/04_Classification_1.html#maximum-likelihood-estimation",
    "href": "teaching/stat-learn/material/04/04_Classification_1.html#maximum-likelihood-estimation",
    "title": "Classification I",
    "section": "",
    "text": "Just like with Linear Regression, we can use Maximum Likelihood Estimation to choose the parameters (intercept and coefficients) of the model. But we have a different likelihood.\nIn a linear regression, we assumed that our errors are normally distributed around the regression line. For logistic regression, we assume that our errors are Bernoulli distributed. The Bernoulli distribution is a discrete distribution (since our outcome is discrete, a.k.a categorical) that tells you the proability of being 0 or 1."
  },
  {
    "objectID": "teaching/stat-learn/material/04/04_Classification_1.html#bernoulli-likelihood",
    "href": "teaching/stat-learn/material/04/04_Classification_1.html#bernoulli-likelihood",
    "title": "Classification I",
    "section": "",
    "text": "The formula for a Bernoulli distribution for a single data point \\(x\\) is:\n\\[ f(y;p(x)) = p(x)^{y} * (1-p(x))^{1-y}\\]\nwhere \\(y\\) is the group the data point belongs to (either 0 or 1), and \\(p(x)\\) is the predicted probability of that data point being a 1.\nFor example, let’s say we’re looking at the probability that it’s sunny tomorrow. The predicted probability, according to the weather channel is \\(p(x) = 0.8\\). The likelihood of it being sunny (\\(k = 1\\)) is:\n\\[ f(1;0.8) = 0.8^1 * (1-0.8)^{1-1} = 0.8\\]\nThe likelihood of it not being sunny (\\(k = 0\\)) is: \\[ f(0;0.8) = 0.8^0 * (1-0.8)^{1-0} = 0.2\\]"
  },
  {
    "objectID": "teaching/stat-learn/material/04/04_Classification_1.html#likelihood-function",
    "href": "teaching/stat-learn/material/04/04_Classification_1.html#likelihood-function",
    "title": "Classification I",
    "section": "",
    "text": "But we don’t just have a SINGLE data point when fitting a logistic regression, we have MANY. So, we multiply the likelihood of each data point together to get the likelihood of the dataset:\n\\[\\prod_{i = 1}^n p(x_i)^{y_i} * (1-p(x_i))^{1-y_i}\\]\nWe want to choose parameters (e.g. \\(\\beta_0\\), or \\(\\beta_1\\)) that maximize this likelihood function. And how to we maximize it? We take it’s (partial) derivatives and set them equal to zero!\nHowever, it turns out that its much easier to work with the log of this likelihood function, so we’re often working with the log likelihood and taking it’s derivatives (this will still find the optimal parameters for the model as the values that maximize the log likelihood will also maximize the likelihood):\n\\[\\sum_{i = 1}^n y_i * log(p(x_i)) + (1-y_i) * log(1-p(x_i))\\]"
  },
  {
    "objectID": "teaching/stat-learn/material/04/04_Classification_1.html#loss-function",
    "href": "teaching/stat-learn/material/04/04_Classification_1.html#loss-function",
    "title": "Classification I",
    "section": "",
    "text": "Now it turns out, if we multiply the log loss by \\(-\\frac{1}{N}\\), this log-loss is a really great loss function for logistic regression. Loss functions are metrics that\n\nmeasure the performance of your model, and\nhave lower scores indicate better performing models\n\n\\[-\\frac{1}{N} \\sum_{i = 1}^n y_i * log(p(x_i)) + (1-y_i) * log(1-p(x_i))\\]\nLog-Loss (also called Binary Cross Entropy) does just that! Thus we often use it as a loss function for Logistic Regression."
  },
  {
    "objectID": "teaching/stat-learn/material/04/04_Classification_1.html#logistic-regression-in-r",
    "href": "teaching/stat-learn/material/04/04_Classification_1.html#logistic-regression-in-r",
    "title": "Classification I",
    "section": "",
    "text": "Let’s build a Logistic Regression model in R. We’ll follow a similar workflow to what we used for linear models:\n\nSeparate your data into predictors (X) and outcome (y), and optionally set up a train/test split.\nCreate a model formula and initialize the logistic regression model using glm() with family = binomial.\nFit the model to the training data.\nUse predict() on new data to obtain predicted probabilities or class predictions.\nAssess the model’s performance (e.g., accuracy, confusion matrix, ROC curve).\n\n\n# Split data (example dataset)\nset.seed(123)\nidx &lt;- sample(seq_len(nrow(df)), size = 0.8*nrow(df))\ntrain &lt;- df[idx, ]\ntest  &lt;- df[-idx, ]\n\n# 1. Define model formula\nformula &lt;- outcome ~ predictor1 + predictor2\n\n# 2 & 3. Fit logistic regression model\nlog_model &lt;- glm(formula, data = train, family = binomial)\n\n# 4. Predict probabilities\npred_probs &lt;- predict(log_model, test, type = \"response\")\n\n# Convert to class prediction (optional threshold = 0.5)\npred_class &lt;- ifelse(pred_probs &gt; 0.5, 1, 0)\n\n# 5. Assess model\ntable(Predicted = pred_class, Actual = test$outcome)"
  },
  {
    "objectID": "teaching/stat-learn/material/04/04_Classification_1.html#breast-cancer-data",
    "href": "teaching/stat-learn/material/04/04_Classification_1.html#breast-cancer-data",
    "title": "Classification I",
    "section": "",
    "text": "Let’s do an example with logistic regression to classify cancer diagnosis. We will: 1. Load and lightly clean the dataset.\n2. Select predictors whose names end with \"mean\".\n3. Split the data into training and testing sets (80/20).\n4. Fit a logistic regression (glm, binomial family).\n5. Predict class probabilities on the test set.\n6. Evaluate performance using binary cross-entropy (log loss).\nWe import the Breast Cancer dataset and drop any rows with missing values to ensure the model can be fit without errors.\n\nbc &lt;- read.csv(\n  \"04-Data/BreastCancer.csv\",\n  stringsAsFactors = FALSE\n)\n\nbc &lt;- na.omit(bc)\nnrow(bc)\n\n[1] 569\n\n\nThe outcome is diagnosis (Benign B vs Malignant M). As predictors we only use columns whose names end in “mean”.\n\n# columns ending with \"mean\"\npredictors &lt;- grep(\"mean$\", names(bc), value = TRUE)\n\n# modeling frame: outcome + predictors\ndf &lt;- data.frame(\n  diagnosis = factor(bc$diagnosis, levels = c(\"B\",\"M\")), # B=0, M=1\n  bc[, predictors]\n)\n\n\nstr(df[, c(\"diagnosis\", predictors[1:5])])\n\n'data.frame':   569 obs. of  6 variables:\n $ diagnosis      : Factor w/ 2 levels \"B\",\"M\": 2 2 2 2 2 2 2 2 2 2 ...\n $ radius_mean    : num  18 20.6 19.7 11.4 20.3 ...\n $ texture_mean   : num  10.4 17.8 21.2 20.4 14.3 ...\n $ perimeter_mean : num  122.8 132.9 130 77.6 135.1 ...\n $ area_mean      : num  1001 1326 1203 386 1297 ...\n $ smoothness_mean: num  0.1184 0.0847 0.1096 0.1425 0.1003 ...\n\n\nWe split once and keep a fixed seed for reproducibility. The model will train on train and be evaluated on test.\n\nset.seed(123)\nn &lt;- nrow(df)\nidx_train &lt;- sample.int(n, size = floor(0.8 * n))\ntrain &lt;- df[idx_train, ]\ntest  &lt;- df[-idx_train, ]\nc(n_train = nrow(train), n_test = nrow(test))\n\nn_train  n_test \n    455     114 \n\n\nWe specify a formula that uses all “mean” predictors and fit a logistic regression using the binomial family.\n\nformula &lt;- as.formula(paste(\"diagnosis ~\", paste(predictors, collapse = \" + \")))\nlog_model &lt;- glm(formula, data = train, family = binomial)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nsummary(log_model)$coefficients[1:6, , drop = FALSE]  # peek at a few coefficients\n\n                   Estimate  Std. Error    z value     Pr(&gt;|z|)\n(Intercept)     -8.33989439 14.53963337 -0.5735973 5.662403e-01\nradius_mean     -0.74101892  3.99281153 -0.1855883 8.527677e-01\ntexture_mean     0.38855965  0.07352802  5.2845112 1.260408e-07\nperimeter_mean  -0.27927112  0.54267867 -0.5146160 6.068214e-01\narea_mean        0.04017311  0.01952026  2.0580216 3.958806e-02\nsmoothness_mean 72.05651889 34.28104247  2.1019349 3.555898e-02\n\n\nWe obtain predicted probabilities of malignancy for each test observation.\n\np_test &lt;- predict(log_model, newdata = test, type = \"response\")\nhead(p_test)\n\n        1         9        15        17        18        28 \n0.9999327 0.9878218 0.9184778 0.7153792 0.9987491 0.9998869 \n\n\nLower log loss indicates better calibrated probability predictions. We map B -&gt; 0 and M -&gt; 1 and compute the average cross-entropy.\n\n# Binary cross-entropy / log loss helper\nlog_loss &lt;- function(y, p, eps = 1e-15) {\n  p &lt;- pmin(pmax(p, eps), 1 - eps)    # avoid log(0)\n  -mean(y * log(p) + (1 - y) * log(1 - p))\n}\n\ny_test &lt;- ifelse(test$diagnosis == \"M\", 1, 0)\n\nloss &lt;- log_loss(y_test, p_test)\nloss\n\n[1] 0.1193837\n\n\nA log loss of 0.119 indicates strong model performance. Log loss measures how well the model’s predicted probabilities match the true outcomes where lower is better. A value close to 0 means the model is making accurate and well-calibrated predictions, showing high confidence when it is correct and low confidence when uncertain.\n\n\n\n\n\n\nNote\n\n\n\n\nDiagnosis is encoded with levels c(“B”,“M”) so that M corresponds to the positive class (1) for log-loss computation.\nNo feature scaling is required for logistic regression to work, but standardization can sometimes help convergence or interpretability.\nLog loss evaluates the quality of probabilities, not just the final class labels.\n\n\n\nLogistic Regression coefficients are by default in terms of log odds meaning that they tell you how much the predicted log odds of being in group 1 will change when the predictor increases by 1-unit. We grab the coefficients for the model above:\n\n# ---- Extract Coefficients ----\ncoefs &lt;- summary(log_model)$coefficients   # from glm()\n\n# Convert to a data frame with names\ncoef_df &lt;- data.frame(\n  Name = rownames(coefs),\n  Coef = coefs[, \"Estimate\"],\n  row.names = NULL\n)\n\n# ---- Add Odds Ratios ----\ncoef_df$Odds &lt;- exp(coef_df$Coef)\n\ncoef_df\n\n                     Name         Coef         Odds\n1             (Intercept)  -8.33989439 2.387976e-04\n2             radius_mean  -0.74101892 4.766280e-01\n3            texture_mean   0.38855965 1.474855e+00\n4          perimeter_mean  -0.27927112 7.563348e-01\n5               area_mean   0.04017311 1.040991e+00\n6         smoothness_mean  72.05651889 1.966747e+31\n7        compactness_mean  -1.35444224 2.580912e-01\n8          concavity_mean   6.66078793 7.811662e+02\n9     concave.points_mean  74.10069490 1.518878e+32\n10          symmetry_mean  14.00627785 1.210178e+06\n11 fractal_dimension_mean -39.60760668 6.289773e-18"
  },
  {
    "objectID": "teaching/stat-learn/material/04/04_Classification_1.html#question",
    "href": "teaching/stat-learn/material/04/04_Classification_1.html#question",
    "title": "Classification I",
    "section": "",
    "text": "How do you interpret the results?\n\n\n\n\n\n\nInterpreting the Coefficients\n\n\n\n\n\n\nPositive coefficients (Odds &gt; 1) increase the likelihood of a malignant diagnosis.\nVariables like texture_mean, area_mean, concavity_mean, concave.points_mean, and symmetry_mean strongly raise the probability of cancer, with very large odds ratios indicating powerful predictors.\nNegative coefficients (Odds &lt; 1) decrease the likelihood of malignancy.\nHigher radius_mean, perimeter_mean, compactness_mean, and fractal_dimension_mean values point more toward benign tumors.\nThe intercept represents the baseline log-odds when predictors are zero (not directly interpretable on its own).\n\nOverall, features related to concavity, smoothness, and symmetry strongly increase cancer risk, while higher compactness, radius, and fractal dimension values are associated with benign masses."
  },
  {
    "objectID": "teaching/stat-learn/material/04/04_Classification_1.html#the-problem-with-logistic-regression-coefficients",
    "href": "teaching/stat-learn/material/04/04_Classification_1.html#the-problem-with-logistic-regression-coefficients",
    "title": "Classification I",
    "section": "",
    "text": "When you’re presenting your Logistic Regression Models to non-data people, you might want to be able to tell them which variables have the biggest impact on the predicted value. Typically, we might use coefficients for this because they give us a single number that summarizes the relationship between our predictors and our predicted value.\nHowever, log odds are difficult to understand intuitively, especially if you’re not a data person. Thus, we might want a different way to present our results. Luckily, if we exponentiate our log odds coefficients, we get odds coefficients. These are easier to understand, as most people understand intuitively what odds are.\nRemember, for odds the important threshold value is \\(1\\). So any odds coefficient \\(&gt;1\\) has a direct/positive relationship with the outcome and anything with an odds coefficient \\(&lt; 1\\) has an inverse/negative relationship with the outcome.\nYou can also use the odds coefs to give people an intuitive understanding of the relationship. If the odds coef is \\(2\\) then increasing the predictor by 1-unit causes your predicted odds to double. Similarly, if the odds coef is \\(0.5\\) then increasing the predictor by 1-unit causes your predicted odds to halve. If the odds coef is \\(1.25\\) then increasing the predictor by 1-unit causes your predicted odds to increase by \\(25\\%\\)."
  },
  {
    "objectID": "teaching/stat-learn/material/04/04_Classification_1.html#hyperparameters",
    "href": "teaching/stat-learn/material/04/04_Classification_1.html#hyperparameters",
    "title": "Classification I",
    "section": "2.1 Hyperparameters",
    "text": "2.1 Hyperparameters\nHyperparameters are parameters in our model that are NOT chosen by the algorithm (we must supply them). We can either choose them:\n\nbased on domain expertise (knowledge about the data)\nbased on the data (hyperparameter tuning)\n\nWhy do we have to use a validation set when hyperparameter tuning?\nIn this classwork we’ll use ggplot to plot the boundaries of knn, and see how the size, shape, and overlap of clusters affect these boundries.\n\nNote: this will only work with specific 2D data, if you wanted to use it for your own data you’d need to change the code to do so\n\n\nplotKNN2D &lt;- function(Xdf, y, k = 5) {\n  # Xdf: data frame with exactly 2 numeric features\n  # y: factor labels\n  \n  if (ncol(Xdf) != 2) stop(\"Xdf must have exactly 2 columns (2D only)\")\n  if (!is.factor(y)) y &lt;- factor(y)\n  \n  library(class)\n  library(ggplot2)\n  \n  # Feature names\n  f1 &lt;- colnames(Xdf)[1]\n  f2 &lt;- colnames(Xdf)[2]\n  \n  # Create grid range\n  x0_range &lt;- seq(min(Xdf[[f1]]) - sd(Xdf[[f1]]),\n                  max(Xdf[[f1]]) + sd(Xdf[[f1]]),\n                  length.out = 100)\n  \n  x1_range &lt;- seq(min(Xdf[[f2]]) - sd(Xdf[[f2]]),\n                  max(Xdf[[f2]]) + sd(Xdf[[f2]]),\n                  length.out = 100)\n  \n  grid &lt;- expand.grid(\n    f1 = x0_range,\n    f2 = x1_range\n  )\n  colnames(grid) &lt;- c(f1, f2)\n  \n  # Predict using KNN\n  pred &lt;- knn(train = Xdf, test = grid, cl = y, k = k)\n  grid$pred &lt;- pred\n  \n  # Plot using tidy eval with .data\n  p &lt;- ggplot() +\n    geom_point(\n      data = grid,\n      aes(x = .data[[f1]], y = .data[[f2]], color = pred),\n      alpha = 0.25, size = 0.6\n    ) +\n    geom_point(\n      data = Xdf,\n      aes(x = .data[[f1]], y = .data[[f2]], color = y),\n      size = 2\n    ) +\n    theme_minimal() +\n    labs(color = \"Class\",\n         title = paste(\"KNN Decision Boundary (k =\", k, \")\")) +\n    scale_color_manual(values = c(\"#E69F00\", \"#0072B2\"))\n  \n  p\n}"
  },
  {
    "objectID": "teaching/stat-learn/material/04/04_Classification_1.html#lets-explore",
    "href": "teaching/stat-learn/material/04/04_Classification_1.html#lets-explore",
    "title": "Classification I",
    "section": "2.2 Let’s Explore",
    "text": "2.2 Let’s Explore\nLet’s test this function with some fake data:\n\n# --- Generate Fake Data (two blobs) ---\nset.seed(1)\nn &lt;- 200\nn_per &lt;- n / 2\n\n# centers: (-5, -5) and (5, 5); cluster_std = 1\nX1 &lt;- cbind(rnorm(n_per, mean = -5, sd = 1),\n            rnorm(n_per, mean = -5, sd = 1))\nX2 &lt;- cbind(rnorm(n_per, mean =  5, sd = 1),\n            rnorm(n_per, mean =  5, sd = 1))\n\nX &lt;- rbind(X1, X2)\ncolnames(X) &lt;- c(\"X1\", \"X2\")\nX &lt;- as.data.frame(X)\n\n# labels 0/1 as a factor\ny &lt;- factor(c(rep(0, n_per), rep(1, n_per)))\n\n# --- Plot KNN decision boundary (k = 1) ---\n# assumes plotKNN2D(Xdf, y, k) is already defined\nplotKNN2D(X, y, k = 1)\n\n\n\n\n\n\n\n\nUsing the dataset knnclasswork.csv and using the plotKNN2d() function, build KNN models with K = 1, 3, 5, 20, 50, 100.\nHow does the decision boundary change as K changes?\n\ndd &lt;- read.csv(\n  \"04-Data/KNNclasswork.csv\"\n)\n# k = 1\nplotKNN2D(dd[, c(\"X1\", \"X2\")], dd$y, 1)\n\n\n\n\n\n\n\n# k = 3\n\n# k = 5\n\n# k = 20\n\n# k = 50\n\n# k = 100"
  },
  {
    "objectID": "teaching/stat-learn/material/04/04_Classification_1.html#how-does-changing-k-affect-the-decision-boundary-imbalanced-classes",
    "href": "teaching/stat-learn/material/04/04_Classification_1.html#how-does-changing-k-affect-the-decision-boundary-imbalanced-classes",
    "title": "Classification I",
    "section": "2.3 How does changing k affect the decision boundary (imbalanced classes)?",
    "text": "2.3 How does changing k affect the decision boundary (imbalanced classes)?\nNow let’s see how changing k affects the boundary when the groups have different numbers of samples. Using the plotKNN2d() function, and the data loaded below (dd2), examine what happens to the decision boundaries as you try different k’s (try 1,3,5,10, 25, 50, and 100).\nHow does changing k affect the decision boundary when the groups are imbalanced?\n\ndd2 &lt;- read.csv(\n  \"04-Data/KNNclasswork2.csv\"\n)\n# k = 1\nplotKNN2D(dd2[, c(\"X1\", \"X2\")], dd2$y, 1)\n\n\n\n\n\n\n\n# k = 3\n\n# k = 5\n\n# k = 20\n\n# k = 50\n\n# k = 100\n\n\n\n\n\n\n\nAnswers\n\n\n\n\n\nIn balanced data, increasing k shifts the boundary from very wiggly and noise-sensitive to smooth and generalized.\n\nToo small k = overfitting\ntoo large k = underfitting.\n\nFor imbalanced case, as k increases, the decision boundary becomes smoother and shifts toward the majority class, eventually overwhelming the minority class and reducing its predicted region, especially at very high k.\n\nSmall k = preserves minority class but noisy\nLarge k = smooth but biased toward majority"
  },
  {
    "objectID": "teaching/stat-learn/material/04/04_Classification_1.html#logistic-regression-1",
    "href": "teaching/stat-learn/material/04/04_Classification_1.html#logistic-regression-1",
    "title": "Classification I",
    "section": "3.1 Logistic Regression",
    "text": "3.1 Logistic Regression\nPractice building Logistic Regression models. Using the purchases dataset, build a Logistic Regression model that predicts whether or not customers signed up for a rewards program based on their age, income, and whether they had made a previous purchase. Use an 80/20 Train-Test-Split. Note that logistic regression is not scale-based (it’s a linear model, not a distance-based one), so it doesn’t need standardization to function correctly. However, standardizing can improve training stability and interpretation consistency, especially when variables differ wildly in scale. Interpret the coefficients in terms of log-odds, odds and probability."
  },
  {
    "objectID": "teaching/stat-learn/material/04/04_Classification_1.html#recommendation-systems-knn",
    "href": "teaching/stat-learn/material/04/04_Classification_1.html#recommendation-systems-knn",
    "title": "Classification I",
    "section": "3.2 Recommendation Systems (KNN)",
    "text": "3.2 Recommendation Systems (KNN)\n“If you like _________________ you should listen to ___________________ by Taylor Swift.”\nWe’re going to build a Recommendation System to recommend Taylor Swift songs for people by letting users select a song, and then recommending the most similar songs (according to danceability, energy, instrumentalness, valence, loudness, liveness, speechiness, acousticness).\nTo do this, we’re going to load in our training data called TaylorSwiftSpotify.csv, fit a NearestNeighbors() model, and then for each song in our new data called KNNCompareSpotify.csv we’ll find the 10 most similar songs and recommend them!\nBelow you have some code to get you started, note that you will nedd to install the package FNN. Fill in the missing parts!\n\n# --- Read data ---\ntraining_data &lt;- read.csv(\n  \"04-Data/TaylorSwiftSpotify.csv\",\n  stringsAsFactors = FALSE\n)\nnew_data &lt;- read.csv(\n  \"04-Data//KNNCompareSpotify.csv\",\n  stringsAsFactors = FALSE\n)\n\n# --- Features ---\nfeat &lt;- c(\"danceability\", \"energy\", \"instrumentalness\", \"valence\",\n          \"loudness\", \"liveness\", \"speechiness\", \"acousticness\")\n\n# --- Z-score using TRAINING stats only (avoid division by zero) ---\n\n# --- Nearest Neighbors (k = 10) ---\n\n# --- Attach neighbors to new_data ---\n# Store neighbor indices as a semicolon-separated string for each row (CSV-friendly)\n\n# --- Write as CSV file"
  },
  {
    "objectID": "teaching/stat-learn/index.html",
    "href": "teaching/stat-learn/index.html",
    "title": "Statistical Learning",
    "section": "",
    "text": "Schedule\n\n\n\n\nslides\npractical\ndata\nworksheet\n\n\n\n\n1: Introduction: What is Statistical Learning?\n\n\n.zip\n.qmd\n\n\n2: Linear Regression I\n\n\n.zip\n.qmd\n\n\n3: Linear Regression II\n\n\n.zip\n.qmd\n\n\n4: Classification I\n\n\n.zip\n.qmd\n\n\n5: Classification II\n\n\n.zip\n.qmd\n\n\n6: Model Validation\n\n\n.zip\n.qmd\n\n\n7: Model Selection & Regularization\n\n\n.zip\n.qmd\n\n\n8: “Non-linear” Linear Regression\n\n\n\n.qmd\n\n\n9: Tree Based Methods\n\n\n.zip\n.qmd\n\n\n10: Support Vector Machines\n\n\n\n.qmd\n\n\n11: Neural Networks\n\n\n.zip\n.qmd\n\n\n12: Clustering\n\n\n\n\n\n\n13: Principal Component Analysis\n\n\n\n\n\n\n14: Review"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/k-means.html",
    "href": "teaching/tidyverse-II/material/case-studies/k-means.html",
    "title": "K-means clustering with tidy data principles",
    "section": "",
    "text": "We only require the tidymodels package here.\nK-means clustering serves as a useful example of applying tidy data principles to statistical analysis, and especially the distinction between the three tidying functions:\n\ntidy()\naugment()\nglance()\n\nLet’s start by generating some random two-dimensional data with three clusters. Data in each cluster will come from a multivariate gaussian distribution, with different means for each cluster:\n\nlibrary(tidymodels)\n\nset.seed(77)\n\ncenters &lt;- tibble(\n  cluster = factor(1:3), \n  num_points = c(100, 150, 50),  # number points in each cluster\n  x1 = c(5, 0, -3),              # x1 coordinate of cluster center\n  x2 = c(-1, 1, -2)              # x2 coordinate of cluster center\n)\n\nlabelled_points &lt;- \n  centers %&gt;%\n  mutate(\n    x1 = map2(num_points, x1, rnorm),\n    x2 = map2(num_points, x2, rnorm)\n  ) %&gt;% \n  select(-num_points) %&gt;% \n  unnest(cols = c(x1, x2))\n\nggplot(labelled_points, aes(x1, x2, color = cluster)) +\n  geom_point(alpha = 0.3)\n\n\n\n\n\n\n\n\nThis is an ideal case for k-means clustering."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/k-means.html#introduction",
    "href": "teaching/tidyverse-II/material/case-studies/k-means.html#introduction",
    "title": "K-means clustering with tidy data principles",
    "section": "",
    "text": "We only require the tidymodels package here.\nK-means clustering serves as a useful example of applying tidy data principles to statistical analysis, and especially the distinction between the three tidying functions:\n\ntidy()\naugment()\nglance()\n\nLet’s start by generating some random two-dimensional data with three clusters. Data in each cluster will come from a multivariate gaussian distribution, with different means for each cluster:\n\nlibrary(tidymodels)\n\nset.seed(77)\n\ncenters &lt;- tibble(\n  cluster = factor(1:3), \n  num_points = c(100, 150, 50),  # number points in each cluster\n  x1 = c(5, 0, -3),              # x1 coordinate of cluster center\n  x2 = c(-1, 1, -2)              # x2 coordinate of cluster center\n)\n\nlabelled_points &lt;- \n  centers %&gt;%\n  mutate(\n    x1 = map2(num_points, x1, rnorm),\n    x2 = map2(num_points, x2, rnorm)\n  ) %&gt;% \n  select(-num_points) %&gt;% \n  unnest(cols = c(x1, x2))\n\nggplot(labelled_points, aes(x1, x2, color = cluster)) +\n  geom_point(alpha = 0.3)\n\n\n\n\n\n\n\n\nThis is an ideal case for k-means clustering."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/k-means.html#how-does-k-means-work",
    "href": "teaching/tidyverse-II/material/case-studies/k-means.html#how-does-k-means-work",
    "title": "K-means clustering with tidy data principles",
    "section": "How does K-means work?",
    "text": "How does K-means work?\nRather than using equations, this short animation using the artwork of Allison Horst explains the clustering process:"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/k-means.html#clustering-in-r",
    "href": "teaching/tidyverse-II/material/case-studies/k-means.html#clustering-in-r",
    "title": "K-means clustering with tidy data principles",
    "section": "Clustering in R",
    "text": "Clustering in R\nWe’ll use the built-in kmeans() function, which accepts a data frame with all numeric columns as it’s primary argument.\n\npoints &lt;- \n  labelled_points %&gt;% \n  select(-cluster)\n\nkclust &lt;- kmeans(points, centers = 3)\nkclust\n#&gt; K-means clustering with 3 clusters of sizes 49, 153, 98\n#&gt; \n#&gt; Cluster means:\n#&gt;           x1         x2\n#&gt; 1 -2.6692834 -1.8040871\n#&gt; 2 -0.1223998  1.1122543\n#&gt; 3  5.0697846 -0.9176465\n#&gt; \n#&gt; Clustering vector:\n#&gt;   [1] 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [38] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [75] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [112] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [149] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [186] 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [223] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1\n#&gt; [260] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [297] 1 1 1 1\n#&gt; \n#&gt; Within cluster sum of squares by cluster:\n#&gt; [1]  92.23037 256.70331 209.30745\n#&gt;  (between_SS / total_SS =  83.9 %)\n#&gt; \n#&gt; Available components:\n#&gt; \n#&gt; [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n#&gt; [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\nsummary(kclust)\n#&gt;              Length Class  Mode   \n#&gt; cluster      300    -none- numeric\n#&gt; centers        6    -none- numeric\n#&gt; totss          1    -none- numeric\n#&gt; withinss       3    -none- numeric\n#&gt; tot.withinss   1    -none- numeric\n#&gt; betweenss      1    -none- numeric\n#&gt; size           3    -none- numeric\n#&gt; iter           1    -none- numeric\n#&gt; ifault         1    -none- numeric\n\nThe output is a list of vectors, where each component has a different length. There’s one of length 300, the same as our original data set. There are two elements of length 3 (withinss and tot.withinss) and centers is a matrix with 3 rows. And then there are the elements of length 1: totss, tot.withinss, betweenss, and iter. (The value ifault indicates possible algorithm problems.)\nThese differing lengths have important meaning when we want to tidy our data set; they signify that each type of component communicates a different kind of information.\n\ncluster (300 values) contains information about each point\ncenters, withinss, and size (3 values) contain information about each cluster\ntotss, tot.withinss, betweenss, and iter (1 value) contain information about the full clustering\n\nWhich of these do we want to extract? There is no right answer; each of them may be interesting to an analyst. Because they communicate entirely different information (not to mention there’s no straightforward way to combine them), they are extracted by separate functions. augment adds the point classifications to the original data set:\n\naugment(kclust, points)\n#&gt; # A tibble: 300 × 3\n#&gt;       x1      x2 .cluster\n#&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;   \n#&gt;  1  4.45 -1.69   3       \n#&gt;  2  6.09  0.764  3       \n#&gt;  3  5.64 -0.340  3       \n#&gt;  4  6.04  0.0481 3       \n#&gt;  5  5.17 -0.929  3       \n#&gt;  6  6.14 -2.09   3       \n#&gt;  7  4.03 -0.849  3       \n#&gt;  8  4.87 -0.0558 3       \n#&gt;  9  5.15 -1.38   3       \n#&gt; 10  6.44 -1.23   3       \n#&gt; # ℹ 290 more rows\n\nThe tidy() function summarizes on a per-cluster level:\n\ntidy(kclust)\n#&gt; # A tibble: 3 × 5\n#&gt;       x1     x2  size withinss cluster\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;fct&gt;  \n#&gt; 1 -2.67  -1.80     49     92.2 1      \n#&gt; 2 -0.122  1.11    153    257.  2      \n#&gt; 3  5.07  -0.918    98    209.  3\n\nAnd as it always does, the glance() function extracts a single-row summary:\n\nglance(kclust)\n#&gt; # A tibble: 1 × 4\n#&gt;   totss tot.withinss betweenss  iter\n#&gt;   &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 3457.         558.     2899.     2"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/k-means.html#exploratory-clustering",
    "href": "teaching/tidyverse-II/material/case-studies/k-means.html#exploratory-clustering",
    "title": "K-means clustering with tidy data principles",
    "section": "Exploratory clustering",
    "text": "Exploratory clustering\nWhile these summaries are useful, they would not have been too difficult to extract out from the data set yourself. The real power comes from combining these analyses with other tools like dplyr.\nLet’s say we want to explore the effect of different choices of k, from 1 to 9, on this clustering. First cluster the data 9 times, each using a different value of k, then create columns containing the tidied, glanced and augmented data:\n\nkclusts &lt;- \n  tibble(k = 1:9) %&gt;%\n  mutate(\n    kclust = map(k, ~kmeans(points, .x)),\n    tidied = map(kclust, tidy),\n    glanced = map(kclust, glance),\n    augmented = map(kclust, augment, points)\n  )\n\nkclusts\n#&gt; # A tibble: 9 × 5\n#&gt;       k kclust   tidied           glanced          augmented         \n#&gt;   &lt;int&gt; &lt;list&gt;   &lt;list&gt;           &lt;list&gt;           &lt;list&gt;            \n#&gt; 1     1 &lt;kmeans&gt; &lt;tibble [1 × 5]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [300 × 3]&gt;\n#&gt; 2     2 &lt;kmeans&gt; &lt;tibble [2 × 5]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [300 × 3]&gt;\n#&gt; 3     3 &lt;kmeans&gt; &lt;tibble [3 × 5]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [300 × 3]&gt;\n#&gt; 4     4 &lt;kmeans&gt; &lt;tibble [4 × 5]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [300 × 3]&gt;\n#&gt; 5     5 &lt;kmeans&gt; &lt;tibble [5 × 5]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [300 × 3]&gt;\n#&gt; 6     6 &lt;kmeans&gt; &lt;tibble [6 × 5]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [300 × 3]&gt;\n#&gt; 7     7 &lt;kmeans&gt; &lt;tibble [7 × 5]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [300 × 3]&gt;\n#&gt; 8     8 &lt;kmeans&gt; &lt;tibble [8 × 5]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [300 × 3]&gt;\n#&gt; 9     9 &lt;kmeans&gt; &lt;tibble [9 × 5]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [300 × 3]&gt;\n\nWe can turn these into three separate data sets each representing a different type of data: using tidy(), using augment(), and using glance(). Each of these goes into a separate data set as they represent different types of data.\n\nclusters &lt;- \n  kclusts %&gt;%\n  unnest(cols = c(tidied))\n\nassignments &lt;- \n  kclusts %&gt;% \n  unnest(cols = c(augmented))\n\nclusterings &lt;- \n  kclusts %&gt;%\n  unnest(cols = c(glanced))\n\nNow we can plot the original points using the data from augment(), with each point colored according to the predicted cluster.\n\np1 &lt;- \n  ggplot(assignments, aes(x = x1, y = x2)) +\n  geom_point(aes(color = .cluster), alpha = 0.8) + \n  facet_wrap(~ k)\np1\n\n\n\n\n\n\n\n\nAlready we get a good sense of the proper number of clusters (3), and how the k-means algorithm functions when k is too high or too low. We can then add the centers of the cluster using the data from tidy():\n\np2 &lt;- p1 + geom_point(data = clusters, size = 10, shape = \"x\")\np2\n\n\n\n\n\n\n\n\nThe data from glance() fills a different but equally important purpose; it lets us view trends of some summary statistics across values of k. Of particular interest is the total within sum of squares, saved in the tot.withinss column.\n\nggplot(clusterings, aes(k, tot.withinss)) +\n  geom_line() +\n  geom_point()\n\n\n\n\n\n\n\n\nThis represents the variance within the clusters. It decreases as k increases, but notice a bend (or “elbow”) around k = 3. This bend indicates that additional clusters beyond the third have little value. (See here for a more mathematically rigorous interpretation and implementation of this method). Thus, all three methods of tidying data provided by broom are useful for summarizing clustering output."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/k-means.html#session-info",
    "href": "teaching/tidyverse-II/material/case-studies/k-means.html#session-info",
    "title": "K-means clustering with tidy data principles",
    "section": "Session information",
    "text": "Session information\n\n#&gt; ─ Session info ─────────────────────────────────────────────────────\n#&gt;  version  R version 4.4.1 (2024-06-14)\n#&gt;  language (EN)\n#&gt;  date     2025-05-05\n#&gt;  pandoc   3.2\n#&gt;  quarto   1.7.29\n#&gt; \n#&gt; ─ Packages ─────────────────────────────────────────────────────────\n#&gt;  package      version date (UTC) source\n#&gt;  broom        1.0.7   2024-09-26 CRAN (R 4.4.1)\n#&gt;  dials        1.4.0   2025-02-13 CRAN (R 4.4.1)\n#&gt;  dplyr        1.1.4   2023-11-17 CRAN (R 4.4.0)\n#&gt;  ggplot2      3.5.1   2024-04-23 CRAN (R 4.4.0)\n#&gt;  infer        1.0.7   2024-03-25 CRAN (R 4.4.0)\n#&gt;  parsnip      1.3.1   2025-03-12 CRAN (R 4.4.1)\n#&gt;  purrr        1.0.4   2025-02-05 CRAN (R 4.4.1)\n#&gt;  recipes      1.2.0   2025-03-17 CRAN (R 4.4.1)\n#&gt;  rlang        1.1.5   2025-01-17 CRAN (R 4.4.1)\n#&gt;  rsample      1.2.1   2024-03-25 CRAN (R 4.4.0)\n#&gt;  tibble       3.2.1   2023-03-20 CRAN (R 4.4.0)\n#&gt;  tidymodels   1.3.0   2025-02-21 CRAN (R 4.4.1)\n#&gt;  tune         1.3.0   2025-02-21 CRAN (R 4.4.1)\n#&gt;  workflows    1.2.0   2025-02-19 CRAN (R 4.4.1)\n#&gt;  yardstick    1.3.2   2025-01-22 CRAN (R 4.4.1)\n#&gt; \n#&gt; ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/infer.html",
    "href": "teaching/tidyverse-II/material/case-studies/infer.html",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "",
    "text": "This article only requires the tidymodels package.\nThe tidymodels package infer implements an expressive grammar to perform statistical inference that coheres with the tidyverse design framework. Rather than providing methods for specific statistical tests, this package consolidates the principles that are shared among common hypothesis tests into a set of 4 main verbs (functions), supplemented with many utilities to visualize and extract information from their outputs.\nRegardless of which hypothesis test we’re using, we’re still asking the same kind of question:\n\nIs the effect or difference in our observed data real, or due to chance?\n\nTo answer this question, we start by assuming that the observed data came from some world where “nothing is going on” (i.e. the observed effect was simply due to random chance), and call this assumption our null hypothesis. (In reality, we might not believe in the null hypothesis at all; the null hypothesis is in opposition to the alternate hypothesis, which supposes that the effect present in the observed data is actually due to the fact that “something is going on.”) We then calculate a test statistic from our data that describes the observed effect. We can use this test statistic to calculate a p-value, giving the probability that our observed data could come about if the null hypothesis was true. If this probability is below some pre-defined significance level \\(\\alpha\\), then we can reject our null hypothesis.\nIf you are new to hypothesis testing, take a look at\n\nSection 9.2 of Statistical Inference via Data Science\nThe American Statistical Association’s recent statement on p-values\n\nThe workflow of this package is designed around these ideas. Starting from some data set,\n\nspecify() allows you to specify the variable, or relationship between variables, that you’re interested in,\nhypothesize() allows you to declare the null hypothesis,\ngenerate() allows you to generate data reflecting the null hypothesis, and\ncalculate() allows you to calculate a distribution of statistics from the generated data to form the null distribution.\n\nThroughout this vignette, we make use of gss, a data set available in infer containing a sample of 500 observations of 11 variables from the General Social Survey.\n\nlibrary(tidymodels) # Includes the infer package\n\n# Set seed\nset.seed(1234)\n\n# load in the data set\ndata(gss)\n\n# take a look at its structure\ndplyr::glimpse(gss)\n#&gt; Rows: 500\n#&gt; Columns: 11\n#&gt; $ year    &lt;dbl&gt; 2014, 1994, 1998, 1996, 1994, 1996, 1990, 2016, 2000, 1998, 20…\n#&gt; $ age     &lt;dbl&gt; 36, 34, 24, 42, 31, 32, 48, 36, 30, 33, 21, 30, 38, 49, 25, 56…\n#&gt; $ sex     &lt;fct&gt; male, female, male, male, male, female, female, female, female…\n#&gt; $ college &lt;fct&gt; degree, no degree, degree, no degree, degree, no degree, no de…\n#&gt; $ partyid &lt;fct&gt; ind, rep, ind, ind, rep, rep, dem, ind, rep, dem, dem, ind, de…\n#&gt; $ hompop  &lt;dbl&gt; 3, 4, 1, 4, 2, 4, 2, 1, 5, 2, 4, 3, 4, 4, 2, 2, 3, 2, 1, 2, 5,…\n#&gt; $ hours   &lt;dbl&gt; 50, 31, 40, 40, 40, 53, 32, 20, 40, 40, 23, 52, 38, 72, 48, 40…\n#&gt; $ income  &lt;ord&gt; $25000 or more, $20000 - 24999, $25000 or more, $25000 or more…\n#&gt; $ class   &lt;fct&gt; middle class, working class, working class, working class, mid…\n#&gt; $ finrela &lt;fct&gt; below average, below average, below average, above average, ab…\n#&gt; $ weight  &lt;dbl&gt; 0.8960034, 1.0825000, 0.5501000, 1.0864000, 1.0825000, 1.08640…\n\nEach row is an individual survey response, containing some basic demographic information on the respondent as well as some additional variables. See ?gss for more information on the variables included and their source. Note that this data (and our examples on it) are for demonstration purposes only, and will not necessarily provide accurate estimates unless weighted properly. For these examples, let’s suppose that this data set is a representative sample of a population we want to learn about: American adults."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/infer.html#introduction",
    "href": "teaching/tidyverse-II/material/case-studies/infer.html#introduction",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "",
    "text": "This article only requires the tidymodels package.\nThe tidymodels package infer implements an expressive grammar to perform statistical inference that coheres with the tidyverse design framework. Rather than providing methods for specific statistical tests, this package consolidates the principles that are shared among common hypothesis tests into a set of 4 main verbs (functions), supplemented with many utilities to visualize and extract information from their outputs.\nRegardless of which hypothesis test we’re using, we’re still asking the same kind of question:\n\nIs the effect or difference in our observed data real, or due to chance?\n\nTo answer this question, we start by assuming that the observed data came from some world where “nothing is going on” (i.e. the observed effect was simply due to random chance), and call this assumption our null hypothesis. (In reality, we might not believe in the null hypothesis at all; the null hypothesis is in opposition to the alternate hypothesis, which supposes that the effect present in the observed data is actually due to the fact that “something is going on.”) We then calculate a test statistic from our data that describes the observed effect. We can use this test statistic to calculate a p-value, giving the probability that our observed data could come about if the null hypothesis was true. If this probability is below some pre-defined significance level \\(\\alpha\\), then we can reject our null hypothesis.\nIf you are new to hypothesis testing, take a look at\n\nSection 9.2 of Statistical Inference via Data Science\nThe American Statistical Association’s recent statement on p-values\n\nThe workflow of this package is designed around these ideas. Starting from some data set,\n\nspecify() allows you to specify the variable, or relationship between variables, that you’re interested in,\nhypothesize() allows you to declare the null hypothesis,\ngenerate() allows you to generate data reflecting the null hypothesis, and\ncalculate() allows you to calculate a distribution of statistics from the generated data to form the null distribution.\n\nThroughout this vignette, we make use of gss, a data set available in infer containing a sample of 500 observations of 11 variables from the General Social Survey.\n\nlibrary(tidymodels) # Includes the infer package\n\n# Set seed\nset.seed(1234)\n\n# load in the data set\ndata(gss)\n\n# take a look at its structure\ndplyr::glimpse(gss)\n#&gt; Rows: 500\n#&gt; Columns: 11\n#&gt; $ year    &lt;dbl&gt; 2014, 1994, 1998, 1996, 1994, 1996, 1990, 2016, 2000, 1998, 20…\n#&gt; $ age     &lt;dbl&gt; 36, 34, 24, 42, 31, 32, 48, 36, 30, 33, 21, 30, 38, 49, 25, 56…\n#&gt; $ sex     &lt;fct&gt; male, female, male, male, male, female, female, female, female…\n#&gt; $ college &lt;fct&gt; degree, no degree, degree, no degree, degree, no degree, no de…\n#&gt; $ partyid &lt;fct&gt; ind, rep, ind, ind, rep, rep, dem, ind, rep, dem, dem, ind, de…\n#&gt; $ hompop  &lt;dbl&gt; 3, 4, 1, 4, 2, 4, 2, 1, 5, 2, 4, 3, 4, 4, 2, 2, 3, 2, 1, 2, 5,…\n#&gt; $ hours   &lt;dbl&gt; 50, 31, 40, 40, 40, 53, 32, 20, 40, 40, 23, 52, 38, 72, 48, 40…\n#&gt; $ income  &lt;ord&gt; $25000 or more, $20000 - 24999, $25000 or more, $25000 or more…\n#&gt; $ class   &lt;fct&gt; middle class, working class, working class, working class, mid…\n#&gt; $ finrela &lt;fct&gt; below average, below average, below average, above average, ab…\n#&gt; $ weight  &lt;dbl&gt; 0.8960034, 1.0825000, 0.5501000, 1.0864000, 1.0825000, 1.08640…\n\nEach row is an individual survey response, containing some basic demographic information on the respondent as well as some additional variables. See ?gss for more information on the variables included and their source. Note that this data (and our examples on it) are for demonstration purposes only, and will not necessarily provide accurate estimates unless weighted properly. For these examples, let’s suppose that this data set is a representative sample of a population we want to learn about: American adults."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/infer.html#specify-variables",
    "href": "teaching/tidyverse-II/material/case-studies/infer.html#specify-variables",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "Specify variables",
    "text": "Specify variables\nThe specify() function can be used to specify which of the variables in the data set you’re interested in. If you’re only interested in, say, the age of the respondents, you might write:\n\ngss %&gt;%\n  specify(response = age)\n#&gt; Response: age (numeric)\n#&gt; # A tibble: 500 × 1\n#&gt;      age\n#&gt;    &lt;dbl&gt;\n#&gt;  1    36\n#&gt;  2    34\n#&gt;  3    24\n#&gt;  4    42\n#&gt;  5    31\n#&gt;  6    32\n#&gt;  7    48\n#&gt;  8    36\n#&gt;  9    30\n#&gt; 10    33\n#&gt; # ℹ 490 more rows\n\nOn the front end, the output of specify() just looks like it selects off the columns in the dataframe that you’ve specified. What do we see if we check the class of this object, though?\n\ngss %&gt;%\n  specify(response = age) %&gt;%\n  class()\n#&gt; [1] \"infer\"      \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nWe can see that the infer class has been appended on top of the dataframe classes; this new class stores some extra metadata.\nIf you’re interested in two variables (age and partyid, for example) you can specify() their relationship in one of two (equivalent) ways:\n\n# as a formula\ngss %&gt;%\n  specify(age ~ partyid)\n#&gt; Response: age (numeric)\n#&gt; Explanatory: partyid (factor)\n#&gt; # A tibble: 500 × 2\n#&gt;      age partyid\n#&gt;    &lt;dbl&gt; &lt;fct&gt;  \n#&gt;  1    36 ind    \n#&gt;  2    34 rep    \n#&gt;  3    24 ind    \n#&gt;  4    42 ind    \n#&gt;  5    31 rep    \n#&gt;  6    32 rep    \n#&gt;  7    48 dem    \n#&gt;  8    36 ind    \n#&gt;  9    30 rep    \n#&gt; 10    33 dem    \n#&gt; # ℹ 490 more rows\n\n# with the named arguments\ngss %&gt;%\n  specify(response = age, explanatory = partyid)\n#&gt; Response: age (numeric)\n#&gt; Explanatory: partyid (factor)\n#&gt; # A tibble: 500 × 2\n#&gt;      age partyid\n#&gt;    &lt;dbl&gt; &lt;fct&gt;  \n#&gt;  1    36 ind    \n#&gt;  2    34 rep    \n#&gt;  3    24 ind    \n#&gt;  4    42 ind    \n#&gt;  5    31 rep    \n#&gt;  6    32 rep    \n#&gt;  7    48 dem    \n#&gt;  8    36 ind    \n#&gt;  9    30 rep    \n#&gt; 10    33 dem    \n#&gt; # ℹ 490 more rows\n\nIf you’re doing inference on one proportion or a difference in proportions, you will need to use the success argument to specify which level of your response variable is a success. For instance, if you’re interested in the proportion of the population with a college degree, you might use the following code:\n\n# specifying for inference on proportions\ngss %&gt;%\n  specify(response = college, success = \"degree\")\n#&gt; Response: college (factor)\n#&gt; # A tibble: 500 × 1\n#&gt;    college  \n#&gt;    &lt;fct&gt;    \n#&gt;  1 degree   \n#&gt;  2 no degree\n#&gt;  3 degree   \n#&gt;  4 no degree\n#&gt;  5 degree   \n#&gt;  6 no degree\n#&gt;  7 no degree\n#&gt;  8 degree   \n#&gt;  9 degree   \n#&gt; 10 no degree\n#&gt; # ℹ 490 more rows"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/infer.html#declare-the-hypothesis",
    "href": "teaching/tidyverse-II/material/case-studies/infer.html#declare-the-hypothesis",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "Declare the hypothesis",
    "text": "Declare the hypothesis\nThe next step in the infer pipeline is often to declare a null hypothesis using hypothesize(). The first step is to supply one of “independence” or “point” to the null argument. If your null hypothesis assumes independence between two variables, then this is all you need to supply to hypothesize():\n\ngss %&gt;%\n  specify(college ~ partyid, success = \"degree\") %&gt;%\n  hypothesize(null = \"independence\")\n#&gt; Response: college (factor)\n#&gt; Explanatory: partyid (factor)\n#&gt; Null Hypot...\n#&gt; # A tibble: 500 × 2\n#&gt;    college   partyid\n#&gt;    &lt;fct&gt;     &lt;fct&gt;  \n#&gt;  1 degree    ind    \n#&gt;  2 no degree rep    \n#&gt;  3 degree    ind    \n#&gt;  4 no degree ind    \n#&gt;  5 degree    rep    \n#&gt;  6 no degree rep    \n#&gt;  7 no degree dem    \n#&gt;  8 degree    ind    \n#&gt;  9 degree    rep    \n#&gt; 10 no degree dem    \n#&gt; # ℹ 490 more rows\n\nIf you’re doing inference on a point estimate, you will also need to provide one of p (the true proportion of successes, between 0 and 1), mu (the true mean), med (the true median), or sigma (the true standard deviation). For instance, if the null hypothesis is that the mean number of hours worked per week in our population is 40, we would write:\n\ngss %&gt;%\n  specify(response = hours) %&gt;%\n  hypothesize(null = \"point\", mu = 40)\n#&gt; Response: hours (numeric)\n#&gt; Null Hypothesis: point\n#&gt; # A tibble: 500 × 1\n#&gt;    hours\n#&gt;    &lt;dbl&gt;\n#&gt;  1    50\n#&gt;  2    31\n#&gt;  3    40\n#&gt;  4    40\n#&gt;  5    40\n#&gt;  6    53\n#&gt;  7    32\n#&gt;  8    20\n#&gt;  9    40\n#&gt; 10    40\n#&gt; # ℹ 490 more rows\n\nAgain, from the front-end, the dataframe outputted from hypothesize() looks almost exactly the same as it did when it came out of specify(), but infer now “knows” your null hypothesis."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/infer.html#generate-the-distribution",
    "href": "teaching/tidyverse-II/material/case-studies/infer.html#generate-the-distribution",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "Generate the distribution",
    "text": "Generate the distribution\nOnce we’ve asserted our null hypothesis using hypothesize(), we can construct a null distribution based on this hypothesis. We can do this using one of several methods, supplied in the type argument:\n\nbootstrap: A bootstrap sample will be drawn for each replicate, where a sample of size equal to the input sample size is drawn (with replacement) from the input sample data.\n\npermute: For each replicate, each input value will be randomly reassigned (without replacement) to a new output value in the sample.\n\nsimulate: A value will be sampled from a theoretical distribution with parameters specified in hypothesize() for each replicate. (This option is currently only applicable for testing point estimates.)\n\nContinuing on with our example above, about the average number of hours worked a week, we might write:\n\ngss %&gt;%\n  specify(response = hours) %&gt;%\n  hypothesize(null = \"point\", mu = 40) %&gt;%\n  generate(reps = 5000, type = \"bootstrap\")\n#&gt; Response: hours (numeric)\n#&gt; Null Hypothesis: point\n#&gt; # A tibble: 2,500,000 × 2\n#&gt; # Groups:   replicate [5,000]\n#&gt;    replicate hours\n#&gt;        &lt;int&gt; &lt;dbl&gt;\n#&gt;  1         1  58.6\n#&gt;  2         1  35.6\n#&gt;  3         1  28.6\n#&gt;  4         1  38.6\n#&gt;  5         1  28.6\n#&gt;  6         1  38.6\n#&gt;  7         1  38.6\n#&gt;  8         1  57.6\n#&gt;  9         1  58.6\n#&gt; 10         1  38.6\n#&gt; # ℹ 2,499,990 more rows\n\nIn the above example, we take 5000 bootstrap samples to form our null distribution.\nTo generate a null distribution for the independence of two variables, we could also randomly reshuffle the pairings of explanatory and response variables to break any existing association. For instance, to generate 5000 replicates that can be used to create a null distribution under the assumption that political party affiliation is not affected by age:\n\ngss %&gt;%\n  specify(partyid ~ age) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 5000, type = \"permute\")\n#&gt; Response: partyid (factor)\n#&gt; Explanatory: age (numeric)\n#&gt; Null Hypothes...\n#&gt; # A tibble: 2,500,000 × 3\n#&gt; # Groups:   replicate [5,000]\n#&gt;    partyid   age replicate\n#&gt;    &lt;fct&gt;   &lt;dbl&gt;     &lt;int&gt;\n#&gt;  1 ind        36         1\n#&gt;  2 ind        34         1\n#&gt;  3 ind        24         1\n#&gt;  4 rep        42         1\n#&gt;  5 dem        31         1\n#&gt;  6 dem        32         1\n#&gt;  7 dem        48         1\n#&gt;  8 rep        36         1\n#&gt;  9 ind        30         1\n#&gt; 10 dem        33         1\n#&gt; # ℹ 2,499,990 more rows"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/infer.html#calculate-statistics",
    "href": "teaching/tidyverse-II/material/case-studies/infer.html#calculate-statistics",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "Calculate statistics",
    "text": "Calculate statistics\nDepending on whether you’re carrying out computation-based inference or theory-based inference, you will either supply calculate() with the output of generate() or hypothesize(), respectively. The function, for one, takes in a stat argument, which is currently one of \"mean\", \"median\", \"sum\", \"sd\", \"prop\", \"count\", \"diff in means\", \"diff in medians\", \"diff in props\", \"Chisq\", \"F\", \"t\", \"z\", \"slope\", or \"correlation\". For example, continuing our example above to calculate the null distribution of mean hours worked per week:\n\ngss %&gt;%\n  specify(response = hours) %&gt;%\n  hypothesize(null = \"point\", mu = 40) %&gt;%\n  generate(reps = 5000, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"mean\")\n#&gt; Response: hours (numeric)\n#&gt; Null Hypothesis: point\n#&gt; # A tibble: 5,000 × 2\n#&gt;    replicate  stat\n#&gt;        &lt;int&gt; &lt;dbl&gt;\n#&gt;  1         1  39.8\n#&gt;  2         2  39.6\n#&gt;  3         3  39.8\n#&gt;  4         4  39.2\n#&gt;  5         5  39.0\n#&gt;  6         6  39.8\n#&gt;  7         7  40.6\n#&gt;  8         8  40.6\n#&gt;  9         9  40.4\n#&gt; 10        10  39.0\n#&gt; # ℹ 4,990 more rows\n\nThe output of calculate() here shows us the sample statistic (in this case, the mean) for each of our 1000 replicates. If you’re carrying out inference on differences in means, medians, or proportions, or \\(t\\) and \\(z\\) statistics, you will need to supply an order argument, giving the order in which the explanatory variables should be subtracted. For instance, to find the difference in mean age of those that have a college degree and those that don’t, we might write:\n\ngss %&gt;%\n  specify(age ~ college) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 5000, type = \"permute\") %&gt;%\n  calculate(\"diff in means\", order = c(\"degree\", \"no degree\"))\n#&gt; Response: age (numeric)\n#&gt; Explanatory: college (factor)\n#&gt; Null Hypothes...\n#&gt; # A tibble: 5,000 × 2\n#&gt;    replicate    stat\n#&gt;        &lt;int&gt;   &lt;dbl&gt;\n#&gt;  1         1 -0.0378\n#&gt;  2         2  1.55  \n#&gt;  3         3  0.465 \n#&gt;  4         4  1.39  \n#&gt;  5         5 -0.161 \n#&gt;  6         6 -0.179 \n#&gt;  7         7  0.0151\n#&gt;  8         8  0.914 \n#&gt;  9         9 -1.32  \n#&gt; 10        10 -0.426 \n#&gt; # ℹ 4,990 more rows"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/infer.html#other-utilities",
    "href": "teaching/tidyverse-II/material/case-studies/infer.html#other-utilities",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "Other utilities",
    "text": "Other utilities\nThe infer package also offers several utilities to extract meaning out of summary statistics and null distributions; the package provides functions to visualize where a statistic is relative to a distribution (with visualize()), calculate p-values (with get_p_value()), and calculate confidence intervals (with get_confidence_interval()).\nTo illustrate, we’ll go back to the example of determining whether the mean number of hours worked per week is 40 hours.\n\n# find the point estimate\npoint_estimate &lt;- gss %&gt;%\n  specify(response = hours) %&gt;%\n  calculate(stat = \"mean\")\n\n# generate a null distribution\nnull_dist &lt;- gss %&gt;%\n  specify(response = hours) %&gt;%\n  hypothesize(null = \"point\", mu = 40) %&gt;%\n  generate(reps = 5000, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"mean\")\n\nOur point estimate 41.382 seems pretty close to 40, but a little bit different. We might wonder if this difference is just due to random chance, or if the mean number of hours worked per week in the population really isn’t 40.\nWe could initially just visualize the null distribution.\n\nnull_dist %&gt;%\n  visualize()\n\n\n\n\n\n\n\n\nWhere does our sample’s observed statistic lie on this distribution? We can use the obs_stat argument to specify this.\n\nnull_dist %&gt;%\n  visualize() +\n  shade_p_value(obs_stat = point_estimate, direction = \"two_sided\")\n\n\n\n\n\n\n\n\nNotice that infer has also shaded the regions of the null distribution that are as (or more) extreme than our observed statistic. (Also, note that we now use the + operator to apply the shade_p_value() function. This is because visualize() outputs a plot object from ggplot2 instead of a dataframe, and the + operator is needed to add the p-value layer to the plot object.) The red bar looks like it’s slightly far out on the right tail of the null distribution, so observing a sample mean of 41.382 hours would be somewhat unlikely if the mean was actually 40 hours. How unlikely, though?\n\n# get a two-tailed p-value\np_value &lt;- null_dist %&gt;%\n  get_p_value(obs_stat = point_estimate, direction = \"two_sided\")\n\np_value\n#&gt; # A tibble: 1 × 1\n#&gt;   p_value\n#&gt;     &lt;dbl&gt;\n#&gt; 1   0.046\n\nIt looks like the p-value is 0.046, which is pretty small—if the true mean number of hours worked per week was actually 40, the probability of our sample mean being this far (1.382 hours) from 40 would be 0.046. This may or may not be statistically significantly different, depending on the significance level \\(\\alpha\\) you decided on before you ran this analysis. If you had set \\(\\alpha = .05\\), then this difference would be statistically significant, but if you had set \\(\\alpha = .01\\), then it would not be.\nTo get a confidence interval around our estimate, we can write:\n\n# start with the null distribution\nnull_dist %&gt;%\n  # calculate the confidence interval around the point estimate\n  get_confidence_interval(point_estimate = point_estimate,\n                          # at the 95% confidence level\n                          level = .95,\n                          # using the standard error\n                          type = \"se\")\n#&gt; # A tibble: 1 × 2\n#&gt;   lower_ci upper_ci\n#&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1     40.1     42.7\n\nAs you can see, 40 hours per week is not contained in this interval, which aligns with our previous conclusion that this finding is significant at the confidence level \\(\\alpha = .05\\)."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/infer.html#theoretical-methods",
    "href": "teaching/tidyverse-II/material/case-studies/infer.html#theoretical-methods",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "Theoretical methods",
    "text": "Theoretical methods\nThe infer package also provides functionality to use theoretical methods for \"Chisq\", \"F\" and \"t\" test statistics.\nGenerally, to find a null distribution using theory-based methods, use the same code that you would use to find the null distribution using randomization-based methods, but skip the generate() step. For example, if we wanted to find a null distribution for the relationship between age (age) and party identification (partyid) using randomization, we could write:\n\nnull_f_distn &lt;- gss %&gt;%\n   specify(age ~ partyid) %&gt;%\n   hypothesize(null = \"independence\") %&gt;%\n   generate(reps = 5000, type = \"permute\") %&gt;%\n   calculate(stat = \"F\")\n\nTo find the null distribution using theory-based methods, instead, skip the generate() step entirely:\n\nnull_f_distn_theoretical &lt;- gss %&gt;%\n   specify(age ~ partyid) %&gt;%\n   hypothesize(null = \"independence\") %&gt;%\n   calculate(stat = \"F\")\n\nWe’ll calculate the observed statistic to make use of in the following visualizations; this procedure is the same, regardless of the methods used to find the null distribution.\n\nF_hat &lt;- gss %&gt;% \n  specify(age ~ partyid) %&gt;%\n  calculate(stat = \"F\")\n\nNow, instead of just piping the null distribution into visualize(), as we would do if we wanted to visualize the randomization-based null distribution, we also need to provide method = \"theoretical\" to visualize().\n\nvisualize(null_f_distn_theoretical, method = \"theoretical\") +\n  shade_p_value(obs_stat = F_hat, direction = \"greater\")\n\n\n\n\n\n\n\n\nTo get a sense of how the theory-based and randomization-based null distributions relate, we can pipe the randomization-based null distribution into visualize() and also specify method = \"both\"\n\nvisualize(null_f_distn, method = \"both\") +\n  shade_p_value(obs_stat = F_hat, direction = \"greater\")\n\n\n\n\n\n\n\n\nThat’s it! See help(package = \"infer\") for a full list of functions and vignettes."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/infer.html#session-info",
    "href": "teaching/tidyverse-II/material/case-studies/infer.html#session-info",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "Session information",
    "text": "Session information\n\n#&gt; ─ Session info ─────────────────────────────────────────────────────\n#&gt;  version  R version 4.5.0 (2025-04-11)\n#&gt;  language (EN)\n#&gt;  date     2025-07-07\n#&gt;  pandoc   3.2\n#&gt;  quarto   1.7.29\n#&gt; \n#&gt; ─ Packages ─────────────────────────────────────────────────────────\n#&gt;  package      version date (UTC) source\n#&gt;  broom        1.0.8   2025-03-28 CRAN (R 4.5.0)\n#&gt;  dials        1.4.0   2025-02-13 CRAN (R 4.5.0)\n#&gt;  dplyr        1.1.4   2023-11-17 CRAN (R 4.5.0)\n#&gt;  ggplot2      3.5.2   2025-04-09 CRAN (R 4.5.0)\n#&gt;  infer        1.0.8   2025-04-14 CRAN (R 4.5.0)\n#&gt;  parsnip      1.3.1   2025-03-12 CRAN (R 4.5.0)\n#&gt;  purrr        1.0.4   2025-02-05 CRAN (R 4.5.0)\n#&gt;  recipes      1.3.0   2025-04-17 CRAN (R 4.5.0)\n#&gt;  rlang        1.1.6   2025-04-11 CRAN (R 4.5.0)\n#&gt;  rsample      1.3.0   2025-04-02 CRAN (R 4.5.0)\n#&gt;  tibble       3.2.1   2023-03-20 CRAN (R 4.5.0)\n#&gt;  tidymodels   1.3.0   2025-02-21 CRAN (R 4.5.0)\n#&gt;  tune         1.3.0   2025-02-21 CRAN (R 4.5.0)\n#&gt;  workflows    1.2.0   2025-02-19 CRAN (R 4.5.0)\n#&gt;  yardstick    1.3.2   2025-01-22 CRAN (R 4.5.0)\n#&gt; \n#&gt; ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/coefficients.html",
    "href": "teaching/tidyverse-II/material/case-studies/coefficients.html",
    "title": "Working with model coefficients",
    "section": "",
    "text": "There are many types of statistical models with diverse kinds of structure. Some models have coefficients (a.k.a. weights) for each term in the model. Familiar examples of such models are linear or logistic regression, but more complex models (e.g. neural networks, MARS) can also have model coefficients. When we work with models that use weights or coefficients, we often want to examine the estimated coefficients.\nThis article describes how to retrieve the estimated coefficients from models fit using tidymodels. To use code in this article, you will need to install the following packages: glmnet and tidymodels."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/coefficients.html#introduction",
    "href": "teaching/tidyverse-II/material/case-studies/coefficients.html#introduction",
    "title": "Working with model coefficients",
    "section": "",
    "text": "There are many types of statistical models with diverse kinds of structure. Some models have coefficients (a.k.a. weights) for each term in the model. Familiar examples of such models are linear or logistic regression, but more complex models (e.g. neural networks, MARS) can also have model coefficients. When we work with models that use weights or coefficients, we often want to examine the estimated coefficients.\nThis article describes how to retrieve the estimated coefficients from models fit using tidymodels. To use code in this article, you will need to install the following packages: glmnet and tidymodels."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/coefficients.html#linear-regression",
    "href": "teaching/tidyverse-II/material/case-studies/coefficients.html#linear-regression",
    "title": "Working with model coefficients",
    "section": "Linear regression",
    "text": "Linear regression\nLet’s start with a linear regression model:\n\\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\ldots + \\hat{\\beta}_px_p\\]\nThe \\(\\beta\\) values are the coefficients and the \\(x_j\\) are model predictors, or features.\nLet’s use the Chicago train data where we predict the ridership at the Clark and Lake station (column name: ridership) with the previous ridership data 14 days prior at three of the stations.\nThe data are in the modeldata package:\n\nlibrary(tidymodels)\ntidymodels_prefer()\ntheme_set(theme_bw())\n\ndata(Chicago)\n\nChicago &lt;- Chicago %&gt;% select(ridership, Clark_Lake, Austin, Harlem)\n\n\nA single model\nLet’s start by fitting only a single parsnip model object. We’ll create a model specification using linear_reg().\n\n\n\n\n\n\nNote\n\n\n\nThe default engine is \"lm\" so no call to set_engine() is required.\n\n\nThe fit() function estimates the model coefficients, given a formula and data set.\n\nlm_spec &lt;- linear_reg()\nlm_fit &lt;- fit(lm_spec, ridership ~ ., data = Chicago)\nlm_fit\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ridership ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)   Clark_Lake       Austin       Harlem  \n#&gt;      1.6778       0.9035       0.6123      -0.5550\n\nThe best way to retrieve the fitted parameters is to use the tidy() method. This function, in the broom package, returns the coefficients and their associated statistics in a data frame with standardized column names:\n\ntidy(lm_fit)\n#&gt; # A tibble: 4 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)    1.68     0.156      10.7  1.11e- 26\n#&gt; 2 Clark_Lake     0.904    0.0280     32.3  5.14e-210\n#&gt; 3 Austin         0.612    0.320       1.91 5.59e-  2\n#&gt; 4 Harlem        -0.555    0.165      -3.36 7.85e-  4\n\nWe’ll use this function in subsequent sections.\n\n\nResampled or tuned models\nThe tidymodels framework emphasizes the use of resampling methods to evaluate and characterize how well a model works. While time series resampling methods are appropriate for these data, we can also use the bootstrap to resample the data. This is a standard resampling approach when evaluating the uncertainty in statistical estimates.\nWe’ll use five bootstrap resamples of the data to simplify the plots and output (normally, we would use a larger number of resamples for more reliable estimates).\n\nset.seed(123)\nbt &lt;- bootstraps(Chicago, times = 5)\n\nWith resampling, we fit the same model to the different simulated versions of the data set produced by resampling. The tidymodels function fit_resamples() is the recommended approach for doing so.\n\n\n\n\n\n\nWarning\n\n\n\nThe fit_resamples() function does not automatically save the model objects for each resample since these can be quite large and its main purpose is estimating performance. However, we can pass a function to fit_resamples() that can save the model object or any other aspect of the fit.\n\n\nThis function takes a single argument that represents the fitted workflow object (even if you don’t give fit_resamples() a workflow).\nFrom this, we can extract the model fit. There are two “levels” of model objects that are available:\n\nThe parsnip model object, which wraps the underlying model object. We retrieve this using the extract_fit_parsnip() function.\nThe underlying model object (a.k.a. the engine fit) via the extract_fit_engine().\n\nWe’ll use the latter option and then tidy this model object as we did in the previous section. Let’s add this to the control function so that we can re-use it.\n\nget_lm_coefs &lt;- function(x) {\n  x %&gt;% \n    # get the lm model object\n    extract_fit_engine() %&gt;% \n    # transform its format\n    tidy()\n}\ntidy_ctrl &lt;- control_grid(extract = get_lm_coefs)\n\nThis argument is then passed to fit_resamples():\n\nlm_res &lt;- \n  lm_spec %&gt;% \n  fit_resamples(ridership ~ ., resamples = bt, control = tidy_ctrl)\nlm_res\n#&gt; # Resampling results\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 5 × 5\n#&gt;   splits              id         .metrics         .notes           .extracts\n#&gt;   &lt;list&gt;              &lt;chr&gt;      &lt;list&gt;           &lt;list&gt;           &lt;list&gt;   \n#&gt; 1 &lt;split [5698/2076]&gt; Bootstrap1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n#&gt; 2 &lt;split [5698/2098]&gt; Bootstrap2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n#&gt; 3 &lt;split [5698/2064]&gt; Bootstrap3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n#&gt; 4 &lt;split [5698/2082]&gt; Bootstrap4 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n#&gt; 5 &lt;split [5698/2088]&gt; Bootstrap5 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;\n\nNote that there is a .extracts column in our resampling results. This object contains the output of our get_lm_coefs() function for each resample. The structure of the elements of this column is a little complex. Let’s start by looking at the first element (which corresponds to the first resample):\n\nlm_res$.extracts[[1]]\n#&gt; # A tibble: 1 × 2\n#&gt;   .extracts        .config             \n#&gt;   &lt;list&gt;           &lt;chr&gt;               \n#&gt; 1 &lt;tibble [4 × 5]&gt; Preprocessor1_Model1\n\nThere is another column in this element called .extracts that has the results of the tidy() function call:\n\nlm_res$.extracts[[1]]$.extracts[[1]]\n#&gt; # A tibble: 4 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)    1.40     0.157       8.90 7.23e- 19\n#&gt; 2 Clark_Lake     0.842    0.0280     30.1  2.39e-184\n#&gt; 3 Austin         1.46     0.320       4.54 5.70e-  6\n#&gt; 4 Harlem        -0.637    0.163      -3.92 9.01e-  5\n\nThese nested columns can be flattened via the purrr unnest() function:\n\nlm_res %&gt;% \n  select(id, .extracts) %&gt;% \n  unnest(.extracts) \n#&gt; # A tibble: 5 × 3\n#&gt;   id         .extracts        .config             \n#&gt;   &lt;chr&gt;      &lt;list&gt;           &lt;chr&gt;               \n#&gt; 1 Bootstrap1 &lt;tibble [4 × 5]&gt; Preprocessor1_Model1\n#&gt; 2 Bootstrap2 &lt;tibble [4 × 5]&gt; Preprocessor1_Model1\n#&gt; 3 Bootstrap3 &lt;tibble [4 × 5]&gt; Preprocessor1_Model1\n#&gt; 4 Bootstrap4 &lt;tibble [4 × 5]&gt; Preprocessor1_Model1\n#&gt; 5 Bootstrap5 &lt;tibble [4 × 5]&gt; Preprocessor1_Model1\n\nWe still have a column of nested tibbles, so we can run the same command again to get the data into a more useful format:\n\nlm_coefs &lt;- \n  lm_res %&gt;% \n  select(id, .extracts) %&gt;% \n  unnest(.extracts) %&gt;% \n  unnest(.extracts)\n\nlm_coefs %&gt;% select(id, term, estimate, p.value)\n#&gt; # A tibble: 20 × 4\n#&gt;    id         term        estimate   p.value\n#&gt;    &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 Bootstrap1 (Intercept)    1.40  7.23e- 19\n#&gt;  2 Bootstrap1 Clark_Lake     0.842 2.39e-184\n#&gt;  3 Bootstrap1 Austin         1.46  5.70e-  6\n#&gt;  4 Bootstrap1 Harlem        -0.637 9.01e-  5\n#&gt;  5 Bootstrap2 (Intercept)    1.69  2.87e- 28\n#&gt;  6 Bootstrap2 Clark_Lake     0.911 1.06e-219\n#&gt;  7 Bootstrap2 Austin         0.595 5.93e-  2\n#&gt;  8 Bootstrap2 Harlem        -0.580 3.88e-  4\n#&gt;  9 Bootstrap3 (Intercept)    1.27  3.43e- 16\n#&gt; 10 Bootstrap3 Clark_Lake     0.859 5.03e-194\n#&gt; 11 Bootstrap3 Austin         1.09  6.77e-  4\n#&gt; 12 Bootstrap3 Harlem        -0.470 4.34e-  3\n#&gt; 13 Bootstrap4 (Intercept)    1.95  2.91e- 34\n#&gt; 14 Bootstrap4 Clark_Lake     0.974 1.47e-233\n#&gt; 15 Bootstrap4 Austin        -0.116 7.21e-  1\n#&gt; 16 Bootstrap4 Harlem        -0.620 2.11e-  4\n#&gt; 17 Bootstrap5 (Intercept)    1.87  1.98e- 33\n#&gt; 18 Bootstrap5 Clark_Lake     0.901 1.16e-210\n#&gt; 19 Bootstrap5 Austin         0.494 1.15e-  1\n#&gt; 20 Bootstrap5 Harlem        -0.512 1.73e-  3\n\nThat’s better! Now, let’s plot the model coefficients for each resample:\n\nlm_coefs %&gt;%\n  filter(term != \"(Intercept)\") %&gt;% \n  ggplot(aes(x = term, y = estimate, group = id, col = id)) +  \n  geom_hline(yintercept = 0, lty = 3) + \n  geom_line(alpha = 0.3, lwd = 1.2) + \n  labs(y = \"Coefficient\", x = NULL) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nThere seems to be a lot of uncertainty in the coefficient for the Austin station data, but less for the other two.\nLooking at the code for unnesting the results, you may find the double-nesting structure excessive or cumbersome. However, the extraction functionality is flexible, and a simpler structure would prevent many use cases."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/coefficients.html#more-complex-a-glmnet-model",
    "href": "teaching/tidyverse-II/material/case-studies/coefficients.html#more-complex-a-glmnet-model",
    "title": "Working with model coefficients",
    "section": "More complex: a glmnet model",
    "text": "More complex: a glmnet model\nThe glmnet model can fit the same linear regression model structure shown above. It uses regularization (a.k.a penalization) to estimate the model parameters. This has the benefit of shrinking the coefficients towards zero, important in situations where there are strong correlations between predictors or if some feature selection is required. Both of these cases are true for our Chicago train data set.\nThere are two types of penalization that this model uses:\n\nLasso (a.k.a. \\(L_1\\)) penalties can shrink the model terms so much that they are absolute zero (i.e. their effect is entirely removed from the model).\nWeight decay (a.k.a ridge regression or \\(L_2\\)) uses a different type of penalty that is most useful for highly correlated predictors.\n\nThe glmnet model has two primary tuning parameters, the total amount of penalization and the mixture of the two penalty types. For example, this specification:\n\nglmnet_spec &lt;- \n  linear_reg(penalty = 0.1, mixture = 0.95) %&gt;% \n  set_engine(\"glmnet\")\n\nhas a penalty that is 95% lasso and 5% weight decay. The total amount of these two penalties is 0.1 (which is fairly high).\n\n\n\n\n\n\nNote\n\n\n\nModels with regularization require that predictors are all on the same scale. The ridership at our three stations are very different, but glmnet automatically centers and scales the data. You can use recipes to center and scale your data yourself.\n\n\nLet’s combine the model specification with a formula in a model workflow() and then fit the model to the data:\n\nglmnet_wflow &lt;- \n  workflow() %&gt;% \n  add_model(glmnet_spec) %&gt;% \n  add_formula(ridership ~ .)\n\nglmnet_fit &lt;- fit(glmnet_wflow, Chicago)\nglmnet_fit\n#&gt; ══ Workflow [trained] ════════════════════════════════════════════════\n#&gt; Preprocessor: Formula\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ──────────────────────────────────────────────────────\n#&gt; ridership ~ .\n#&gt; \n#&gt; ── Model ─────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~0.95) \n#&gt; \n#&gt;    Df  %Dev Lambda\n#&gt; 1   0  0.00 6.1040\n#&gt; 2   1 12.75 5.5620\n#&gt; 3   1 23.45 5.0680\n#&gt; 4   1 32.43 4.6180\n#&gt; 5   1 39.95 4.2070\n#&gt; 6   1 46.25 3.8340\n#&gt; 7   1 51.53 3.4930\n#&gt; 8   1 55.94 3.1830\n#&gt; 9   1 59.62 2.9000\n#&gt; 10  1 62.70 2.6420\n#&gt; 11  2 65.28 2.4080\n#&gt; 12  2 67.44 2.1940\n#&gt; 13  2 69.23 1.9990\n#&gt; 14  2 70.72 1.8210\n#&gt; 15  2 71.96 1.6600\n#&gt; 16  2 73.00 1.5120\n#&gt; 17  2 73.86 1.3780\n#&gt; 18  2 74.57 1.2550\n#&gt; 19  2 75.17 1.1440\n#&gt; 20  2 75.66 1.0420\n#&gt; 21  2 76.07 0.9496\n#&gt; 22  2 76.42 0.8653\n#&gt; 23  2 76.70 0.7884\n#&gt; 24  2 76.94 0.7184\n#&gt; 25  2 77.13 0.6545\n#&gt; 26  2 77.30 0.5964\n#&gt; 27  2 77.43 0.5434\n#&gt; 28  2 77.55 0.4951\n#&gt; 29  2 77.64 0.4512\n#&gt; 30  2 77.72 0.4111\n#&gt; 31  2 77.78 0.3746\n#&gt; 32  2 77.84 0.3413\n#&gt; 33  2 77.88 0.3110\n#&gt; 34  2 77.92 0.2833\n#&gt; 35  2 77.95 0.2582\n#&gt; 36  2 77.98 0.2352\n#&gt; 37  2 78.00 0.2143\n#&gt; 38  2 78.01 0.1953\n#&gt; 39  2 78.03 0.1779\n#&gt; 40  2 78.04 0.1621\n#&gt; 41  2 78.05 0.1477\n#&gt; 42  2 78.06 0.1346\n#&gt; 43  2 78.07 0.1226\n#&gt; 44  2 78.07 0.1118\n#&gt; 45  2 78.08 0.1018\n#&gt; 46  2 78.08 0.0928\n#&gt; \n#&gt; ...\n#&gt; and 9 more lines.\n\nIn this output, the term lambda is used to represent the penalty.\nNote that the output shows many values of the penalty despite our specification of penalty = 0.1. It turns out that this model fits a “path” of penalty values. Even though we are interested in a value of 0.1, we can get the model coefficients for many associated values of the penalty from the same model object.\nLet’s look at two different approaches to obtaining the coefficients. Both will use the tidy() method. One will tidy a glmnet object and the other will tidy a tidymodels object.\n\nUsing glmnet penalty values\nThis glmnet fit contains multiple penalty values which depend on the data set; changing the data (or the mixture amount) often produces a different set of values. For this data set, there are 55 penalties available. To get the set of penalties produced for this data set, we can extract the engine fit and tidy:\n\nglmnet_fit %&gt;% \n  extract_fit_engine() %&gt;% \n  tidy() %&gt;% \n  rename(penalty = lambda) %&gt;%   # &lt;- for consistent naming\n  filter(term != \"(Intercept)\")\n#&gt; # A tibble: 99 × 5\n#&gt;    term        step estimate penalty dev.ratio\n#&gt;    &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 Clark_Lake     2   0.0753    5.56     0.127\n#&gt;  2 Clark_Lake     3   0.145     5.07     0.234\n#&gt;  3 Clark_Lake     4   0.208     4.62     0.324\n#&gt;  4 Clark_Lake     5   0.266     4.21     0.400\n#&gt;  5 Clark_Lake     6   0.319     3.83     0.463\n#&gt;  6 Clark_Lake     7   0.368     3.49     0.515\n#&gt;  7 Clark_Lake     8   0.413     3.18     0.559\n#&gt;  8 Clark_Lake     9   0.454     2.90     0.596\n#&gt;  9 Clark_Lake    10   0.491     2.64     0.627\n#&gt; 10 Clark_Lake    11   0.526     2.41     0.653\n#&gt; # ℹ 89 more rows\n\nThis works well but, it turns out that our penalty value (0.1) is not in the list produced by the model! The underlying package has functions that use interpolation to produce coefficients for this specific value, but the tidy() method for glmnet objects does not use it.\n\n\nUsing specific penalty values\nIf we run the tidy() method on the workflow or parsnip object, a different function is used that returns the coefficients for the penalty value that we specified:\n\ntidy(glmnet_fit)\n#&gt; # A tibble: 4 × 3\n#&gt;   term        estimate penalty\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 (Intercept)    1.69      0.1\n#&gt; 2 Clark_Lake     0.846     0.1\n#&gt; 3 Austin         0.271     0.1\n#&gt; 4 Harlem         0         0.1\n\nFor any another (single) penalty, we can use an additional argument:\n\ntidy(glmnet_fit, penalty = 5.5620)  # A value from above\n#&gt; # A tibble: 4 × 3\n#&gt;   term        estimate penalty\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 (Intercept)  12.6       5.56\n#&gt; 2 Clark_Lake    0.0753    5.56\n#&gt; 3 Austin        0         5.56\n#&gt; 4 Harlem        0         5.56\n\nThe reason for having two tidy() methods is that, with tidymodels, the focus is on using a specific penalty value.\n\n\nTuning a glmnet model\nIf we know a priori acceptable values for penalty and mixture, we can use the fit_resamples() function as we did before with linear regression. Otherwise, we can tune those parameters with the tidymodels tune_*() functions.\nLet’s tune our glmnet model over both parameters with this grid:\n\npen_vals &lt;- 10^seq(-3, 0, length.out = 10)\ngrid &lt;- crossing(penalty = pen_vals, mixture = c(0.1, 1.0))\ngrid\n#&gt; # A tibble: 20 × 2\n#&gt;    penalty mixture\n#&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 0.001       0.1\n#&gt;  2 0.001       1  \n#&gt;  3 0.00215     0.1\n#&gt;  4 0.00215     1  \n#&gt;  5 0.00464     0.1\n#&gt;  6 0.00464     1  \n#&gt;  7 0.01        0.1\n#&gt;  8 0.01        1  \n#&gt;  9 0.0215      0.1\n#&gt; 10 0.0215      1  \n#&gt; 11 0.0464      0.1\n#&gt; 12 0.0464      1  \n#&gt; 13 0.1         0.1\n#&gt; 14 0.1         1  \n#&gt; 15 0.215       0.1\n#&gt; 16 0.215       1  \n#&gt; 17 0.464       0.1\n#&gt; 18 0.464       1  \n#&gt; 19 1           0.1\n#&gt; 20 1           1\n\nHere is where more glmnet-related complexity comes in: we know that each resample and each value of mixture will probably produce a different set of penalty values contained in the model object. How can we look at the coefficients at the specific penalty values that we are using to tune?\nThe approach that we suggest is to use the special path_values option for glmnet. Details are described in the technical documentation about glmnet and tidymodels but in short, this parameter will assign the collection of penalty values used by each glmnet fit (regardless of the data or value of mixture).\nWe can pass these as an engine argument and then update our previous workflow object:\n\nglmnet_tune_spec &lt;- \n  linear_reg(penalty = tune(), mixture = tune()) %&gt;% \n  set_engine(\"glmnet\", path_values = pen_vals)\n\nglmnet_wflow &lt;- \n  glmnet_wflow %&gt;% \n  update_model(glmnet_tune_spec)\n\nNow we will use an extraction function similar to when we used ordinary least squares. We add an additional argument to retain coefficients that are shrunk to zero by the lasso penalty:\n\nget_glmnet_coefs &lt;- function(x) {\n  x %&gt;% \n    extract_fit_engine() %&gt;% \n    tidy(return_zeros = TRUE) %&gt;% \n    rename(penalty = lambda)\n}\nparsnip_ctrl &lt;- control_grid(extract = get_glmnet_coefs)\n\nglmnet_res &lt;- \n  glmnet_wflow %&gt;% \n  tune_grid(\n    resamples = bt,\n    grid = grid,\n    control = parsnip_ctrl\n  )\nglmnet_res\n#&gt; # Tuning results\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 5 × 5\n#&gt;   splits              id         .metrics          .notes           .extracts\n#&gt;   &lt;list&gt;              &lt;chr&gt;      &lt;list&gt;            &lt;list&gt;           &lt;list&gt;   \n#&gt; 1 &lt;split [5698/2076]&gt; Bootstrap1 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n#&gt; 2 &lt;split [5698/2098]&gt; Bootstrap2 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n#&gt; 3 &lt;split [5698/2064]&gt; Bootstrap3 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n#&gt; 4 &lt;split [5698/2082]&gt; Bootstrap4 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n#&gt; 5 &lt;split [5698/2088]&gt; Bootstrap5 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;\n\nAs noted before, the elements of the main .extracts column have an embedded list column with the results of get_glmnet_coefs():\n\nglmnet_res$.extracts[[1]] %&gt;% head()\n#&gt; # A tibble: 6 × 4\n#&gt;   penalty mixture .extracts         .config              \n#&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;list&gt;            &lt;chr&gt;                \n#&gt; 1       1     0.1 &lt;tibble [40 × 5]&gt; Preprocessor1_Model01\n#&gt; 2       1     0.1 &lt;tibble [40 × 5]&gt; Preprocessor1_Model02\n#&gt; 3       1     0.1 &lt;tibble [40 × 5]&gt; Preprocessor1_Model03\n#&gt; 4       1     0.1 &lt;tibble [40 × 5]&gt; Preprocessor1_Model04\n#&gt; 5       1     0.1 &lt;tibble [40 × 5]&gt; Preprocessor1_Model05\n#&gt; 6       1     0.1 &lt;tibble [40 × 5]&gt; Preprocessor1_Model06\n\nglmnet_res$.extracts[[1]]$.extracts[[1]] %&gt;% head()\n#&gt; # A tibble: 6 × 5\n#&gt;   term         step estimate penalty dev.ratio\n#&gt;   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)     1    0.568  1          0.769\n#&gt; 2 (Intercept)     2    0.432  0.464      0.775\n#&gt; 3 (Intercept)     3    0.607  0.215      0.779\n#&gt; 4 (Intercept)     4    0.846  0.1        0.781\n#&gt; 5 (Intercept)     5    1.06   0.0464     0.782\n#&gt; 6 (Intercept)     6    1.22   0.0215     0.783\n\nAs before, we’ll have to use a double unnest(). Since the penalty value is in both the top-level and lower-level .extracts, we’ll use select() to get rid of the first version (but keep mixture):\n\nglmnet_res %&gt;% \n  select(id, .extracts) %&gt;% \n  unnest(.extracts) %&gt;% \n  select(id, mixture, .extracts) %&gt;%  # &lt;- removes the first penalty column\n  unnest(.extracts)\n\nBut wait! We know that each glmnet fit contains all of the coefficients. This means, for a specific resample and value of mixture, the results are the same:\n\nall.equal(\n  # First bootstrap, first `mixture`, first `penalty`\n  glmnet_res$.extracts[[1]]$.extracts[[1]],\n  # First bootstrap, first `mixture`, second `penalty`\n  glmnet_res$.extracts[[1]]$.extracts[[2]]\n)\n#&gt; [1] TRUE\n\nFor this reason, we’ll add a slice(1) when grouping by id and mixture. This will get rid of the replicated results.\n\nglmnet_coefs &lt;- \n  glmnet_res %&gt;% \n  select(id, .extracts) %&gt;% \n  unnest(.extracts) %&gt;% \n  select(id, mixture, .extracts) %&gt;% \n  group_by(id, mixture) %&gt;%          # ┐\n  slice(1) %&gt;%                       # │ Remove the redundant results\n  ungroup() %&gt;%                      # ┘\n  unnest(.extracts)\n\nglmnet_coefs %&gt;% \n  select(id, penalty, mixture, term, estimate) %&gt;% \n  filter(term != \"(Intercept)\")\n#&gt; # A tibble: 300 × 5\n#&gt;    id         penalty mixture term       estimate\n#&gt;    &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n#&gt;  1 Bootstrap1 1           0.1 Clark_Lake    0.391\n#&gt;  2 Bootstrap1 0.464       0.1 Clark_Lake    0.485\n#&gt;  3 Bootstrap1 0.215       0.1 Clark_Lake    0.590\n#&gt;  4 Bootstrap1 0.1         0.1 Clark_Lake    0.680\n#&gt;  5 Bootstrap1 0.0464      0.1 Clark_Lake    0.746\n#&gt;  6 Bootstrap1 0.0215      0.1 Clark_Lake    0.793\n#&gt;  7 Bootstrap1 0.01        0.1 Clark_Lake    0.817\n#&gt;  8 Bootstrap1 0.00464     0.1 Clark_Lake    0.828\n#&gt;  9 Bootstrap1 0.00215     0.1 Clark_Lake    0.834\n#&gt; 10 Bootstrap1 0.001       0.1 Clark_Lake    0.837\n#&gt; # ℹ 290 more rows\n\nNow we have the coefficients. Let’s look at how they behave as more regularization is used:\n\nglmnet_coefs %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(mixture = format(mixture)) %&gt;% \n  ggplot(aes(x = penalty, y = estimate, col = mixture, groups = id)) + \n  geom_hline(yintercept = 0, lty = 3) +\n  geom_line(alpha = 0.5, lwd = 1.2) + \n  facet_wrap(~ term) + \n  scale_x_log10() +\n  scale_color_brewer(palette = \"Accent\") +\n  labs(y = \"coefficient\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nNotice a couple of things:\n\nWith a pure lasso model (i.e., mixture = 1), the Austin station predictor is selected out in each resample. With a mixture of both penalties, its influence increases. Also, as the penalty increases, the uncertainty in this coefficient decreases.\nThe Harlem predictor is either quickly selected out of the model or goes from negative to positive."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/coefficients.html#session-info",
    "href": "teaching/tidyverse-II/material/case-studies/coefficients.html#session-info",
    "title": "Working with model coefficients",
    "section": "Session information",
    "text": "Session information\n\n#&gt; ─ Session info ─────────────────────────────────────────────────────\n#&gt;  version  R version 4.5.0 (2025-04-11)\n#&gt;  language (EN)\n#&gt;  date     2025-06-30\n#&gt;  pandoc   3.2\n#&gt;  quarto   1.7.29\n#&gt; \n#&gt; ─ Packages ─────────────────────────────────────────────────────────\n#&gt;  package      version date (UTC) source\n#&gt;  broom        1.0.8   2025-03-28 CRAN (R 4.5.0)\n#&gt;  dials        1.4.0   2025-02-13 CRAN (R 4.5.0)\n#&gt;  dplyr        1.1.4   2023-11-17 CRAN (R 4.5.0)\n#&gt;  ggplot2      3.5.2   2025-04-09 CRAN (R 4.5.0)\n#&gt;  glmnet       4.1-8   2023-08-22 CRAN (R 4.5.0)\n#&gt;  infer        1.0.8   2025-04-14 CRAN (R 4.5.0)\n#&gt;  parsnip      1.3.1   2025-03-12 CRAN (R 4.5.0)\n#&gt;  purrr        1.0.4   2025-02-05 CRAN (R 4.5.0)\n#&gt;  recipes      1.3.0   2025-04-17 CRAN (R 4.5.0)\n#&gt;  rlang        1.1.6   2025-04-11 CRAN (R 4.5.0)\n#&gt;  rsample      1.3.0   2025-04-02 CRAN (R 4.5.0)\n#&gt;  tibble       3.2.1   2023-03-20 CRAN (R 4.5.0)\n#&gt;  tidymodels   1.3.0   2025-02-21 CRAN (R 4.5.0)\n#&gt;  tune         1.3.0   2025-02-21 CRAN (R 4.5.0)\n#&gt;  workflows    1.2.0   2025-02-19 CRAN (R 4.5.0)\n#&gt;  yardstick    1.3.2   2025-01-22 CRAN (R 4.5.0)\n#&gt; \n#&gt; ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/bootstrap.html",
    "href": "teaching/tidyverse-II/material/case-studies/bootstrap.html",
    "title": "Bootstrap resampling and tidy regression models",
    "section": "",
    "text": "Bootstrapping consists of randomly sampling a data set with replacement, then performing the analysis individually on each bootstrapped replicate. The variation in the resulting estimate is then a reasonable approximation of the variance in our estimate.\nLet’s say we want to fit a nonlinear model to the weight/mileage relationship in the mtcars data set.\n\nlibrary(tidymodels)\n\nggplot(mtcars, aes(mpg, wt)) + \n    geom_point()\n\n\n\n\n\n\n\n\nWe might use the method of nonlinear least squares (via the nls() function) to fit a model.\n\nnlsfit &lt;- nls(mpg ~ k / wt + b, mtcars, start = list(k = 1, b = 0))\nsummary(nlsfit)\n#&gt; \n#&gt; Formula: mpg ~ k/wt + b\n#&gt; \n#&gt; Parameters:\n#&gt;   Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; k   45.829      4.249  10.786 7.64e-12 ***\n#&gt; b    4.386      1.536   2.855  0.00774 ** \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.774 on 30 degrees of freedom\n#&gt; \n#&gt; Number of iterations to convergence: 1 \n#&gt; Achieved convergence tolerance: 6.813e-09\n\nggplot(mtcars, aes(wt, mpg)) +\n    geom_point() +\n    geom_line(aes(y = predict(nlsfit)))\n\n\n\n\n\n\n\n\nWhile this does provide a p-value and confidence intervals for the parameters, these are based on model assumptions that may not hold in real data. Bootstrapping is a popular method for providing confidence intervals and predictions that are more robust to the nature of the data."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/bootstrap.html#introduction",
    "href": "teaching/tidyverse-II/material/case-studies/bootstrap.html#introduction",
    "title": "Bootstrap resampling and tidy regression models",
    "section": "",
    "text": "Bootstrapping consists of randomly sampling a data set with replacement, then performing the analysis individually on each bootstrapped replicate. The variation in the resulting estimate is then a reasonable approximation of the variance in our estimate.\nLet’s say we want to fit a nonlinear model to the weight/mileage relationship in the mtcars data set.\n\nlibrary(tidymodels)\n\nggplot(mtcars, aes(mpg, wt)) + \n    geom_point()\n\n\n\n\n\n\n\n\nWe might use the method of nonlinear least squares (via the nls() function) to fit a model.\n\nnlsfit &lt;- nls(mpg ~ k / wt + b, mtcars, start = list(k = 1, b = 0))\nsummary(nlsfit)\n#&gt; \n#&gt; Formula: mpg ~ k/wt + b\n#&gt; \n#&gt; Parameters:\n#&gt;   Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; k   45.829      4.249  10.786 7.64e-12 ***\n#&gt; b    4.386      1.536   2.855  0.00774 ** \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.774 on 30 degrees of freedom\n#&gt; \n#&gt; Number of iterations to convergence: 1 \n#&gt; Achieved convergence tolerance: 6.813e-09\n\nggplot(mtcars, aes(wt, mpg)) +\n    geom_point() +\n    geom_line(aes(y = predict(nlsfit)))\n\n\n\n\n\n\n\n\nWhile this does provide a p-value and confidence intervals for the parameters, these are based on model assumptions that may not hold in real data. Bootstrapping is a popular method for providing confidence intervals and predictions that are more robust to the nature of the data."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/bootstrap.html#bootstrapping-models",
    "href": "teaching/tidyverse-II/material/case-studies/bootstrap.html#bootstrapping-models",
    "title": "Bootstrap resampling and tidy regression models",
    "section": "Bootstrapping models",
    "text": "Bootstrapping models\nWe can use the bootstraps() function in the rsample package to sample bootstrap replications. First, we construct 2000 bootstrap replicates of the data, each of which has been randomly sampled with replacement. The resulting object is an rset, which is a data frame with a column of rsplit objects.\nAn rsplit object has two main components: an analysis data set and an assessment data set, accessible via analysis(rsplit) and assessment(rsplit) respectively. For bootstrap samples, the analysis data set is the bootstrap sample itself, and the assessment data set consists of all the out-of-bag samples.\n\nset.seed(27)\nboots &lt;- bootstraps(mtcars, times = 2000, apparent = TRUE)\nboots\n#&gt; # Bootstrap sampling with apparent sample \n#&gt; # A tibble: 2,001 × 2\n#&gt;    splits          id           \n#&gt;    &lt;list&gt;          &lt;chr&gt;        \n#&gt;  1 &lt;split [32/13]&gt; Bootstrap0001\n#&gt;  2 &lt;split [32/10]&gt; Bootstrap0002\n#&gt;  3 &lt;split [32/13]&gt; Bootstrap0003\n#&gt;  4 &lt;split [32/11]&gt; Bootstrap0004\n#&gt;  5 &lt;split [32/9]&gt;  Bootstrap0005\n#&gt;  6 &lt;split [32/10]&gt; Bootstrap0006\n#&gt;  7 &lt;split [32/11]&gt; Bootstrap0007\n#&gt;  8 &lt;split [32/13]&gt; Bootstrap0008\n#&gt;  9 &lt;split [32/11]&gt; Bootstrap0009\n#&gt; 10 &lt;split [32/11]&gt; Bootstrap0010\n#&gt; # ℹ 1,991 more rows\n\nLet’s create a helper function to fit an nls() model on each bootstrap sample, and then use purrr::map() to apply this function to all the bootstrap samples at once. Similarly, we create a column of tidy coefficient information by unnesting.\n\nfit_nls_on_bootstrap &lt;- function(split) {\n    nls(mpg ~ k / wt + b, analysis(split), start = list(k = 1, b = 0))\n}\n\nboot_models &lt;-\n  boots %&gt;% \n  mutate(model = map(splits, fit_nls_on_bootstrap),\n         coef_info = map(model, tidy))\n\nboot_coefs &lt;- \n  boot_models %&gt;% \n  unnest(coef_info)\n\nThe unnested coefficient information contains a summary of each replication combined in a single data frame:\n\nboot_coefs\n#&gt; # A tibble: 4,002 × 8\n#&gt;    splits          id          model term  estimate std.error statistic  p.value\n#&gt;    &lt;list&gt;          &lt;chr&gt;       &lt;lis&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1 &lt;split [32/13]&gt; Bootstrap0… &lt;nls&gt; k        42.1       4.05     10.4  1.91e-11\n#&gt;  2 &lt;split [32/13]&gt; Bootstrap0… &lt;nls&gt; b         5.39      1.43      3.78 6.93e- 4\n#&gt;  3 &lt;split [32/10]&gt; Bootstrap0… &lt;nls&gt; k        49.9       5.66      8.82 7.82e-10\n#&gt;  4 &lt;split [32/10]&gt; Bootstrap0… &lt;nls&gt; b         3.73      1.92      1.94 6.13e- 2\n#&gt;  5 &lt;split [32/13]&gt; Bootstrap0… &lt;nls&gt; k        37.8       2.68     14.1  9.01e-15\n#&gt;  6 &lt;split [32/13]&gt; Bootstrap0… &lt;nls&gt; b         6.73      1.17      5.75 2.78e- 6\n#&gt;  7 &lt;split [32/11]&gt; Bootstrap0… &lt;nls&gt; k        45.6       4.45     10.2  2.70e-11\n#&gt;  8 &lt;split [32/11]&gt; Bootstrap0… &lt;nls&gt; b         4.75      1.62      2.93 6.38e- 3\n#&gt;  9 &lt;split [32/9]&gt;  Bootstrap0… &lt;nls&gt; k        43.6       4.63      9.41 1.85e-10\n#&gt; 10 &lt;split [32/9]&gt;  Bootstrap0… &lt;nls&gt; b         5.89      1.68      3.51 1.44e- 3\n#&gt; # ℹ 3,992 more rows"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/bootstrap.html#confidence-intervals",
    "href": "teaching/tidyverse-II/material/case-studies/bootstrap.html#confidence-intervals",
    "title": "Bootstrap resampling and tidy regression models",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nWe can then calculate confidence intervals (using what is called the percentile method):\n\npercentile_intervals &lt;- int_pctl(boot_models, coef_info)\npercentile_intervals\n#&gt; # A tibble: 2 × 6\n#&gt;   term   .lower .estimate .upper .alpha .method   \n#&gt;   &lt;chr&gt;   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n#&gt; 1 b      0.0475      4.12   7.31   0.05 percentile\n#&gt; 2 k     37.6        46.7   59.8    0.05 percentile\n\nOr we can use histograms to get a more detailed idea of the uncertainty in each estimate:\n\nggplot(boot_coefs, aes(estimate)) +\n  geom_histogram(bins = 30) +\n  facet_wrap( ~ term, scales = \"free\") +\n  geom_vline(aes(xintercept = .lower), data = percentile_intervals, col = \"blue\") +\n  geom_vline(aes(xintercept = .upper), data = percentile_intervals, col = \"blue\")\n\n\n\n\n\n\n\n\nThe rsample package also has functions for other types of confidence intervals."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/bootstrap.html#possible-model-fits",
    "href": "teaching/tidyverse-II/material/case-studies/bootstrap.html#possible-model-fits",
    "title": "Bootstrap resampling and tidy regression models",
    "section": "Possible model fits",
    "text": "Possible model fits\nWe can use augment() to visualize the uncertainty in the fitted curve. Since there are so many bootstrap samples, we’ll only show a sample of the model fits in our visualization:\n\nboot_aug &lt;- \n  boot_models %&gt;% \n  sample_n(200) %&gt;% \n  mutate(augmented = map(model, augment)) %&gt;% \n  unnest(augmented)\n\nboot_aug\n#&gt; # A tibble: 6,400 × 8\n#&gt;    splits          id            model  coef_info   mpg    wt .fitted .resid\n#&gt;    &lt;list&gt;          &lt;chr&gt;         &lt;list&gt; &lt;list&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1 &lt;split [32/11]&gt; Bootstrap1644 &lt;nls&gt;  &lt;tibble&gt;   16.4  4.07    15.6  0.829\n#&gt;  2 &lt;split [32/11]&gt; Bootstrap1644 &lt;nls&gt;  &lt;tibble&gt;   19.7  2.77    21.9 -2.21 \n#&gt;  3 &lt;split [32/11]&gt; Bootstrap1644 &lt;nls&gt;  &lt;tibble&gt;   19.2  3.84    16.4  2.84 \n#&gt;  4 &lt;split [32/11]&gt; Bootstrap1644 &lt;nls&gt;  &lt;tibble&gt;   21.4  2.78    21.8 -0.437\n#&gt;  5 &lt;split [32/11]&gt; Bootstrap1644 &lt;nls&gt;  &lt;tibble&gt;   26    2.14    27.8 -1.75 \n#&gt;  6 &lt;split [32/11]&gt; Bootstrap1644 &lt;nls&gt;  &lt;tibble&gt;   33.9  1.84    32.0  1.88 \n#&gt;  7 &lt;split [32/11]&gt; Bootstrap1644 &lt;nls&gt;  &lt;tibble&gt;   32.4  2.2     27.0  5.35 \n#&gt;  8 &lt;split [32/11]&gt; Bootstrap1644 &lt;nls&gt;  &lt;tibble&gt;   30.4  1.62    36.1 -5.70 \n#&gt;  9 &lt;split [32/11]&gt; Bootstrap1644 &lt;nls&gt;  &lt;tibble&gt;   21.5  2.46    24.4 -2.86 \n#&gt; 10 &lt;split [32/11]&gt; Bootstrap1644 &lt;nls&gt;  &lt;tibble&gt;   26    2.14    27.8 -1.75 \n#&gt; # ℹ 6,390 more rows\n\n\nggplot(boot_aug, aes(wt, mpg)) +\n  geom_line(aes(y = .fitted, group = id), alpha = .2, col = \"blue\") +\n  geom_point()\n\n\n\n\n\n\n\n\nWith only a few small changes, we could easily perform bootstrapping with other kinds of predictive or hypothesis testing models, since the tidy() and augment() functions works for many statistical outputs. As another example, we could use smooth.spline(), which fits a cubic smoothing spline to data:\n\nfit_spline_on_bootstrap &lt;- function(split) {\n    data &lt;- analysis(split)\n    smooth.spline(data$wt, data$mpg, df = 4)\n}\n\nboot_splines &lt;- \n  boots %&gt;% \n  sample_n(200) %&gt;% \n  mutate(spline = map(splits, fit_spline_on_bootstrap),\n         aug_train = map(spline, augment))\n\nsplines_aug &lt;- \n  boot_splines %&gt;% \n  unnest(aug_train)\n\nggplot(splines_aug, aes(x, y)) +\n  geom_line(aes(y = .fitted, group = id), alpha = 0.2, col = \"blue\") +\n  geom_point()"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/bootstrap.html#session-info",
    "href": "teaching/tidyverse-II/material/case-studies/bootstrap.html#session-info",
    "title": "Bootstrap resampling and tidy regression models",
    "section": "Session information",
    "text": "Session information\n\n#&gt; ─ Session info ─────────────────────────────────────────────────────\n#&gt;  version  R version 4.4.1 (2024-06-14)\n#&gt;  language (EN)\n#&gt;  date     2025-05-05\n#&gt;  pandoc   3.2\n#&gt;  quarto   1.7.29\n#&gt; \n#&gt; ─ Packages ─────────────────────────────────────────────────────────\n#&gt;  package      version date (UTC) source\n#&gt;  broom        1.0.7   2024-09-26 CRAN (R 4.4.1)\n#&gt;  dials        1.4.0   2025-02-13 CRAN (R 4.4.1)\n#&gt;  dplyr        1.1.4   2023-11-17 CRAN (R 4.4.0)\n#&gt;  ggplot2      3.5.1   2024-04-23 CRAN (R 4.4.0)\n#&gt;  infer        1.0.7   2024-03-25 CRAN (R 4.4.0)\n#&gt;  parsnip      1.3.1   2025-03-12 CRAN (R 4.4.1)\n#&gt;  purrr        1.0.4   2025-02-05 CRAN (R 4.4.1)\n#&gt;  recipes      1.2.0   2025-03-17 CRAN (R 4.4.1)\n#&gt;  rlang        1.1.5   2025-01-17 CRAN (R 4.4.1)\n#&gt;  rsample      1.2.1   2024-03-25 CRAN (R 4.4.0)\n#&gt;  tibble       3.2.1   2023-03-20 CRAN (R 4.4.0)\n#&gt;  tidymodels   1.3.0   2025-02-21 CRAN (R 4.4.1)\n#&gt;  tune         1.3.0   2025-02-21 CRAN (R 4.4.1)\n#&gt;  workflows    1.2.0   2025-02-19 CRAN (R 4.4.1)\n#&gt;  yardstick    1.3.2   2025-01-22 CRAN (R 4.4.1)\n#&gt; \n#&gt; ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "teaching/intro-stats/index.html",
    "href": "teaching/intro-stats/index.html",
    "title": "Introductory Statistics",
    "section": "",
    "text": "The lecture notes can be found in the following bookdown:\nFoundational Statistics (work in progress)"
  },
  {
    "objectID": "teaching/sna/material/01/01-intro-pkgs.html",
    "href": "teaching/sna/material/01/01-intro-pkgs.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "Throughout the course we will use a variety of different packages of doing network analysis, modeling and visualization. Make sure to install them all and have them ready to load when needed:\n\ninstall.packages(\"igraph\")   \ninstall.packages(\"statnet\")  #installs ergm, network, and sna\ninstall.packages(\"snahelper\")\ninstall.packages(\"netUtils\")\ninstall.packages(\"ggraph\")\ninstall.packages(\"backbone\")\ninstall.packages(\"netrankr\")\ninstall.packages(\"signnet\")\ninstall.packages(\"intergraph\")\ninstall.packages(\"graphlayouts\")\ninstall.packages(\"visNetwork\")\ninstall.packages(\"patchwork\")\ninstall.packages(\"edgebundle\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"gganimate\")\ninstall.packages(\"ggforce\")\ninstall.packages(\"rsiena\")\n\ninstall.packages(\"remotes\")\nremotes::install_github(\"schochastics/networkdata\")\n\n\n\nWe will strat with loading the following packages:\n\nlibrary(igraph)\nlibrary(networkdata)\nlibrary(netUtils)\n\n\n\n\n(Interactive Session)\n\n\n\nnever load sna and igraph at the same time\n\nlibrary(sna)\n\n\ndata(\"flo_marriage\")\ndegree(flo_marriage)\n\nError in FUN(X[[i]], ...): as.edgelist.sna input must be an adjacency matrix/array, edgelist matrix, network, or sparse matrix, or list thereof.\n\n\nIf for any reason you have to load both, you can circumvent the error by explicitly stating package first\n\nigraph::degree(flo_marriage)\n\n  Acciaiuoli      Albizzi    Barbadori     Bischeri   Castellani       Ginori \n           1            3            2            3            3            1 \n    Guadagni Lamberteschi       Medici        Pazzi      Peruzzi        Pucci \n           4            1            6            1            3            0 \n     Ridolfi     Salviati      Strozzi   Tornabuoni \n           3            2            4            3 \n\n\nThe package intergraph can be used to transform an igraph object into a network object and vice versa.\n\n#install.packages(\"intergraph\")\nlibrary(intergraph)\nasNetwork(flo_marriage)\n\n Network attributes:\n  vertices = 16 \n  directed = FALSE \n  hyper = FALSE \n  loops = FALSE \n  multiple = FALSE \n  bipartite = FALSE \n  total edges= 20 \n    missing edges= 0 \n    non-missing edges= 20 \n\n Vertex attribute names: \n    vertex.names wealth X.priors X.ties \n\nNo edge attributes\n\ndegree(asNetwork(flo_marriage))\n\n [1]  2  6  4  6  6  2  8  2 12  2  6  0  6  4  8  6\n\n\nYou can unload a package without restarting R/RStudio.\n\ndetach(\"package:sna\", unload = TRUE)"
  },
  {
    "objectID": "teaching/sna/material/01/01-intro-pkgs.html#r-packages-for-network-analysis",
    "href": "teaching/sna/material/01/01-intro-pkgs.html#r-packages-for-network-analysis",
    "title": "Social Network Analysis",
    "section": "",
    "text": "Throughout the course we will use a variety of different packages of doing network analysis, modeling and visualization. Make sure to install them all and have them ready to load when needed:\n\ninstall.packages(\"igraph\")   \ninstall.packages(\"statnet\")  #installs ergm, network, and sna\ninstall.packages(\"snahelper\")\ninstall.packages(\"netUtils\")\ninstall.packages(\"ggraph\")\ninstall.packages(\"backbone\")\ninstall.packages(\"netrankr\")\ninstall.packages(\"signnet\")\ninstall.packages(\"intergraph\")\ninstall.packages(\"graphlayouts\")\ninstall.packages(\"visNetwork\")\ninstall.packages(\"patchwork\")\ninstall.packages(\"edgebundle\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"gganimate\")\ninstall.packages(\"ggforce\")\ninstall.packages(\"rsiena\")\n\ninstall.packages(\"remotes\")\nremotes::install_github(\"schochastics/networkdata\")\n\n\n\nWe will strat with loading the following packages:\n\nlibrary(igraph)\nlibrary(networkdata)\nlibrary(netUtils)\n\n\n\n\n(Interactive Session)\n\n\n\nnever load sna and igraph at the same time\n\nlibrary(sna)\n\n\ndata(\"flo_marriage\")\ndegree(flo_marriage)\n\nError in FUN(X[[i]], ...): as.edgelist.sna input must be an adjacency matrix/array, edgelist matrix, network, or sparse matrix, or list thereof.\n\n\nIf for any reason you have to load both, you can circumvent the error by explicitly stating package first\n\nigraph::degree(flo_marriage)\n\n  Acciaiuoli      Albizzi    Barbadori     Bischeri   Castellani       Ginori \n           1            3            2            3            3            1 \n    Guadagni Lamberteschi       Medici        Pazzi      Peruzzi        Pucci \n           4            1            6            1            3            0 \n     Ridolfi     Salviati      Strozzi   Tornabuoni \n           3            2            4            3 \n\n\nThe package intergraph can be used to transform an igraph object into a network object and vice versa.\n\n#install.packages(\"intergraph\")\nlibrary(intergraph)\nasNetwork(flo_marriage)\n\n Network attributes:\n  vertices = 16 \n  directed = FALSE \n  hyper = FALSE \n  loops = FALSE \n  multiple = FALSE \n  bipartite = FALSE \n  total edges= 20 \n    missing edges= 0 \n    non-missing edges= 20 \n\n Vertex attribute names: \n    vertex.names wealth X.priors X.ties \n\nNo edge attributes\n\ndegree(asNetwork(flo_marriage))\n\n [1]  2  6  4  6  6  2  8  2 12  2  6  0  6  4  8  6\n\n\nYou can unload a package without restarting R/RStudio.\n\ndetach(\"package:sna\", unload = TRUE)"
  },
  {
    "objectID": "teaching/sna/material/01/01-intro-pkgs.html#handling-network-data",
    "href": "teaching/sna/material/01/01-intro-pkgs.html#handling-network-data",
    "title": "Social Network Analysis",
    "section": "Handling network data",
    "text": "Handling network data\n\nInbuilt network data\nThe networkdata package includes around 1000 datsets and more than 2000 networks. Throughout the course will use several examples using data from this package. Spend some time exploring datasets in this package (you will be asked to choose and work on one of them for you empirical study).\n\ndata(package = \"networkdata\")\n\n\nFreeman’s datasets from http://moreno.ss.uci.edu/data (not available anymore)\nMovie networks from https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/T4HBA3\nCovert networks from https://sites.google.com/site/ucinetsoftware/datasets/covert-networks\nAnimal networks from https://bansallab.github.io/asnr/\nShakespeare’s plays networks build with data from https://github.com/mallaham/Shakespeare-Plays\nSome networks from http://konect.uni-koblenz.de/\nTennis networks compiled from https://github.com/JeffSackmann (please give credit to him if you use this data)\nStar Wars Character Interactions (Episode 1-7) from https://github.com/evelinag/StarWars-social-network"
  },
  {
    "objectID": "teaching/sna/material/09/09-rgm.html",
    "href": "teaching/sna/material/09/09-rgm.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "In this session, we introduces a range of models used to represent and understand the structure of social and relational networks. We begin with the classic \\(G(n,p)\\) random graph model, in which each pair of nodes is connected independently with a fixed probability. While analytically tractable and conceptually simple, \\(G(n,p)\\) falls short in capturing the structural complexity of real-world networks; it fails to account for common features such as clustering, skewed degree distributions, or short average path lengths.\nTo address these shortcomings, we explore several alternative models that incorporate more realistic structural constraints. The small-world model captures the coexistence of high local clustering and global reachability observed in many social systems. The configuration model enables the generation of random graphs with a fixed degree sequence, offering more control over node-level connectivity patterns. For each model, we discuss its definition, how to simulate or estimate it, and its relevance for modeling real network data.\n\n\n\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(networkdata)\nlibrary(tidyverse)\n\n\n\n\nTo see why the \\(G(n, p)\\) model is often an inadequate representation of real-world networks, we can compare its properties to those of an actual empirical network. A typical social or informational network displays three features that are not captured well by \\(G(n, p)\\): a right-skewed degree distribution (with hubs), high clustering or triadic closure, and short average path lengths. While \\(G(n, p)\\) can match the density of a network, it assumes a binomial (or normal) degree distribution, minimal clustering, and does not account for structural heterogeneity.\nThe example below uses the igraph package in R and a real network dataset of moderate-to-large size to illustrate these differences. We load a real-world network; a network of co-appearances of characters in Victor Hugo’s novel “Les Miserables” which can be loaded from the networkdata package.\n\n\n\n\n\n\n\n\n\nWe compute this its key structural properties, then generate a random graph with the same number of nodes and expected density using sample_gnp(). We then compare the two in terms of degree distribution, transitivity (clustering), and average geodesic distance.\nThe code below summarizes key structural properties of the observed Les Misérables network and the corresponding \\(G(n, p)\\) random graph. These include the global clustering coefficient (measuring the tendency of nodes to form closed triads), the average geodesic distance (a measure of path efficiency), and the maximum degree (the highest number of connections any single node has).\nThe observed network shows substantially higher clustering, a slightly shorter average path length, and a much larger maximum degree. These results highlight that the empirical network is both more locally cohesive and more hierarchically structured than its random counterpart. The presence of hubs and local clusters—common in real-world networks—is not reproduced by the \\(G(n, p)\\) model, which assumes uniform and independent edge probabilities.\nTogether, these differences support the conclusion that random tie formation alone cannot explain the structure of this network.\n\nlibrary(knitr)\n# Load the Les Misérables network from networkdata\ndata(\"miserables\")\ng_obs &lt;- miserables\n\n# Basic stats of the observed network\nn &lt;- vcount(g_obs)\nm &lt;- ecount(g_obs)\ndensity_obs &lt;- edge_density(g_obs)\ndeg_obs &lt;- degree(g_obs)\nclustering_obs &lt;- transitivity(g_obs, type = \"global\")\ndist_obs &lt;- mean_distance(g_obs, directed = FALSE, unconnected = TRUE)\n\n# Generate a G(n, p) graph with the same density\nset.seed(123)\ng_gnp &lt;- sample_gnp(n = n, p = density_obs, directed = FALSE)\n\ndeg_gnp &lt;- degree(g_gnp)\nclustering_gnp &lt;- transitivity(g_gnp, type = \"global\")\ndist_gnp &lt;- mean_distance(g_gnp, directed = FALSE, unconnected = TRUE)\n\n# Combine comparison into a data frame\ncomparison &lt;- data.frame(\n  Model = c(\"Observed\", \"G(n, p)\"),\n  Clustering = c(clustering_obs, clustering_gnp),\n  AvgPathLength = c(dist_obs, dist_gnp),\n  MaxDegree = c(max(deg_obs), max(deg_gnp))\n)\n\n# Format with kable\nkable(comparison, caption = \"Comparison of structural features: Observed vs G(n, p)\",\n      digits = 3, align = \"c\")\n\n\nComparison of structural features: Observed vs G(n, p)\n\n\nModel\nClustering\nAvgPathLength\nMaxDegree\n\n\n\n\nObserved\n0.499\n4.861\n36\n\n\nG(n, p)\n0.086\n2.636\n12\n\n\n\n\n\nTo further illustrate the limitations of the \\(G(n, p)\\) model, we also examine the degree distributions of the observed network and the simulated random graph. Real-world networks often exhibit right-skewed degree distributions, with many nodes having few connections and a small number of hubs with very high degree. In contrast, the \\(G(n, p)\\) model produces a binomial (and approximately normal) degree distribution, where most nodes have degrees clustered around the mean. By comparing these two distributions side by side, we can observe how poorly the random model captures the heterogeneity present in the empirical network.\n\n# Plot the degree distributions\ndf_deg &lt;- data.frame(\n  Degree = c(deg_obs, deg_gnp),\n  Type = rep(c(\"Observed\", \"G(n, p)\"), times = c(length(deg_obs), length(deg_gnp)))\n)\n\nggplot(df_deg, aes(x = Degree, fill = Type)) +\n  geom_histogram(position = \"identity\", bins = 20, alpha = 0.6, color = \"white\") +\n  facet_wrap(~Type, scales = \"free_y\") +\n  labs(x = \"Node Degree\", y = \"Frequency\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"skyblue\", \"tomato\")) +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\nThis example underscores the need for more realistic network models that can capture multiple structural properties simultaneously. While the \\(G(n, p)\\) model offers a useful theoretical baseline, its assumptions of uniform edge probability and independent tie formation lead to networks with unrealistic degree distributions. In particular, it fails to capture the heterogeneity observed in many real-world systems, where some nodes act as hubs while others have very few connections. To address this limitation, we turn to the configuration model, which allows us to fix the degree sequence of the network and thereby preserve node-level connectivity patterns. This model represents a natural next step toward our second random graph model.\n\n\n\n\n\n\nNote: \\(G(n, p)\\) and CUG Given Density\n\n\n\nThe \\(G(n, p)\\) model is mathematically equivalent to a Conditional Uniform Graph (CUG) test given density. In both cases, edges are formed between node pairs independently with fixed probability \\(p\\), and the overall network density is preserved on average across simulations.\nHowever, there are key differences in interpretation and usage:\n\nThe \\(G(n, p)\\) model is a generative model used to define a probability distribution over the space of graphs with \\(n\\) nodes and tie probability \\(p\\). It is often used in theoretical network science as a baseline or null model.\nA CUG test given density is a hypothesis testing framework. It conditions on the observed number of nodes and the expected density, and tests whether an observed network statistic (e.g., mutual ties, clustering) deviates significantly from what would be expected by chance.\n\nIn practice, simulating random graphs under the \\(G(n, p)\\) model is functionally identical to conducting a CUG test with fixed density. The distinction lies in whether the model is used for generative modeling or for evaluating the statistical significance of observed network features.\n\n\n\n\n\nwe continue with the Les Misérables co-appearance network and compare it to a random network generated from the configuration model. The goal is to assess how well the configuration model replicates key structural features of the observed network when it exactly preserves the degree sequence but randomizes the specific tie configuration.\nWe use the igraph package to compute network properties and simulate the configuration model using sample_degseq(). The configuration model guarantees that each node retains its observed degree. We create a comparison table and visualize the degree distribution as before.\n\n# Get observed degree sequence\ndeg_seq &lt;- degree(g_obs)\n\n# Compute observed properties\nclustering_obs &lt;- transitivity(g_obs, type = \"global\")\ndist_obs &lt;- mean_distance(g_obs, directed = FALSE, unconnected = TRUE)\n\n# Simulate configuration model with the same degree sequence\nset.seed(123)\ng_conf &lt;- sample_degseq(deg_seq, method = \"fast.heur.simple\")\n\n# Compute simulated properties\nclustering_conf &lt;- transitivity(g_conf, type = \"global\")\ndist_conf &lt;- mean_distance(g_conf, directed = FALSE, unconnected = TRUE)\n\n# Degree distribution comparison\ndeg_conf &lt;- degree(g_conf)\ndeg_df &lt;- data.frame(\n  Degree = c(deg_seq, deg_conf),\n  Type = rep(c(\"Observed\", \"Configuration Model\"), times = c(length(deg_seq), length(deg_conf)))\n)\n\n# Summary table\nconf_comparison &lt;- data.frame(\n  Model = c(\"Observed\", \"Configuration Model\"),\n  Clustering = c(clustering_obs, clustering_conf),\n  AvgPathLength = c(dist_obs, dist_conf),\n  MaxDegree = c(max(deg_seq), max(deg_conf))\n)\n\n# Display comparison table with kable\nkable(conf_comparison, caption = \"Comparison of structural features: Observed vs Configuration Model\",\n      digits = 3, align = \"c\")\n\n\nComparison of structural features: Observed vs Configuration Model\n\n\nModel\nClustering\nAvgPathLength\nMaxDegree\n\n\n\n\nObserved\n0.499\n4.861\n36\n\n\nConfiguration Model\n0.239\n2.503\n36\n\n\n\n\n# degree distribution plot\nggplot(deg_df, aes(x = Degree, fill = Type)) +\n  geom_histogram(position = \"identity\", bins = 20, alpha = 0.6, color = \"white\") +\n  facet_wrap(~Type, scales = \"free_y\") +\n  labs(x = \"Node Degree\", y = \"Frequency\") +\n  scale_fill_manual(values = c(\"skyblue\", \"tomato\")) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\nAs expected, the degree distribution of the simulated network matches that of the original exactly. However, when we examine higher-order properties, such as the global clustering coefficient and average path length, we find notable differences. The observed network has significantly more clustering, suggesting the presence of structured triadic closure that is not reproduced by the configuration model’s randomized pairing process. The average path length may also differ, although it often remains in the same general range.\nThese results highlight an important distinction: while the configuration model controls for degree-based features, it does not account for clustering, community structure, or other forms of structural dependency. As such, it is useful as a baseline or null model for testing whether observed patterns can be explained by degree alone.\n\n\n\n\n\n\nNote: Configuration Model vs. CUG Given Degree\n\n\n\nThe configuration model and a Conditional Uniform Graph (CUG) test given degree both generate random networks that preserve the observed degree sequence. In this sense, they are conceptually aligned: both assume that node-level connectivity (i.e., degrees) is fixed and use this constraint to explore how other structural features might arise by chance.\nThe key distinction lies in how each is used. The configuration model is a generative model; it produces random graphs that exactly match a specified degree sequence, often for theoretical or simulation purposes. A CUG test given degree, on the other hand, is a hypothesis testing framework. It evaluates whether a particular network statistic (such as clustering or transitivity) in the observed network is unusually high or low compared to what would be expected under random tie arrangement, given the same degree sequence.\n\n\n\n\n\nTo evaluate how well the small-world model captures structural features of a real network, we simulate a small-world graph using the same number of nodes and approximate average degree as the Les Misérables co-appearance network. We then compare the simulated graph to the observed one in terms of degree distribution, clustering, and average path length.\nThe simulation uses igraph::sample_smallworld(), which generates a Watts–Strogatz small-world graph by starting from a regular ring lattice and randomly rewiring edges with a given probability \\(p\\). We set \\(p = 0.05\\) to introduce moderate randomness while maintaining local structure (we discuss the choice of \\(p\\) in more detail below).\n\navg_deg_obs &lt;- mean(deg_obs)\nk &lt;- round(avg_deg_obs / 2)  # average degree per side for ring lattice\n\n# Simulate small-world graph\nset.seed(123)\ng_sw &lt;- sample_smallworld(dim = 1, size = n, nei = k, p = 0.05)\n\n# Compute properties\nclustering_obs &lt;- transitivity(g_obs, type = \"global\")\nclustering_sw &lt;- transitivity(g_sw, type = \"global\")\n\ndist_obs &lt;- mean_distance(g_obs, directed = FALSE, unconnected = TRUE)\ndist_sw &lt;- mean_distance(g_sw, directed = FALSE, unconnected = TRUE)\n\ndeg_sw &lt;- degree(g_sw)\n\n# Comparison table\nsw_comparison &lt;- data.frame(\n  Model = c(\"Observed\", \"Small-World\"),\n  Clustering = c(clustering_obs, clustering_sw),\n  AvgPathLength = c(dist_obs, dist_sw),\n  MaxDegree = c(max(deg_obs), max(deg_sw))\n)\n\n# Print formatted table\nkable(sw_comparison, caption = \"Comparison of structural features: Observed vs Small-World Model\",\n      digits = 3, align = \"c\")\n\n\nComparison of structural features: Observed vs Small-World Model\n\n\nModel\nClustering\nAvgPathLength\nMaxDegree\n\n\n\n\nObserved\n0.499\n4.861\n36\n\n\nSmall-World\n0.498\n3.778\n7\n\n\n\n\n# Degree distribution\ndeg_df &lt;- data.frame(\n  Degree = c(deg_sw, deg_obs),\n  Type = rep(c(\"Small-World\",\"Observed\"), times = c(length(deg_obs), length(deg_sw)))\n)\n # Reverse the factor levels\ndeg_df$Type &lt;- factor(deg_df$Type, levels = c(\"Small-World\", \"Observed\"))\n\nggplot(deg_df, aes(x = Degree, fill = Type)) +\n  geom_histogram(position = \"identity\", bins = 20, alpha = 0.6, color = \"white\") +\n  facet_wrap(~Type, scales = \"free_y\") +\n  labs(x = \"Node Degree\", y = \"Frequency\") +\n  scale_fill_manual(values = c(\"skyblue\",\"tomato\")) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\nThe simulation demonstrates how the small-world model approximates certain properties of the observed network. As shown in the table, the simulated network achieves a relatively short average path length, similar to that of the Les Misérables network, due to the introduction of random long-range ties. The clustering coefficient remains substantial, reflecting the model’s ability to preserve local neighborhood structure.\nHowever, the degree distribution in the small-world model shown in remains relatively narrow, with most nodes having degrees close to the average. This limitation highlights that while the small-world model captures some global and local properties, it does not account for degree heterogeneity. The results show that the small-world model offers a useful structural middle ground between regular and fully random graphs but still lacks the full complexity observed in empirical networks.\nNote that the choice of the rewiring probability \\(p\\) in the small-world model is crucial, as it balances regularity and randomness. Small values of \\(p\\) (e.g., between 0.01 and 0.2) are typically chosen to introduce enough randomness to significantly reduce path lengths, while still preserving high clustering. If \\(p\\) is too low, the network remains overly regular; if \\(p\\) is too high, the network behaves like a random graph and loses its local structure. In practice, \\(p\\) is often selected empirically to achieve small-world characteristics (high clustering and short average path length) relative to the number of nodes and degree.\nTo illustrate how the small-world model transitions between regular and random structure, we simulate multiple networks with the same number of nodes as Les Misérables network with varying values of the rewiring probability \\(p\\) and track how two key properties (clustering and average path length) change. This helps identify a “sweet spot” for \\(p\\) where the network retains high clustering but achieves short global paths, capturing the essence of small-world structure. The results are shown in ?@fig-rewire.\n\n\n\n\n\n\n\n\n\nThe plot shows a sharp transition in network structure as \\(p\\) increases. At \\(p = 0\\), the network is a regular lattice: clustering is high, but average path length is long. As \\(p\\) increases slightly (e.g., \\(p \\approx 0.1\\)), the average path length drops rapidly due to the introduction of long-range shortcuts, while clustering remains relatively high. This intermediate range is where small-world characteristics emerge.\nAs \\(p\\) approaches 1, the network becomes increasingly random (think \\(G(n,p)\\)): clustering drops off, and path length stabilizes at a low level. This demonstrates the trade-off between local cohesion and global efficiency controlled by the rewiring parameter \\(p\\).\n\n\n\nChoose a network of yourself and analyze and compare results using the three introduced random graph models."
  },
  {
    "objectID": "teaching/sna/material/09/09-rgm.html#packages-needed",
    "href": "teaching/sna/material/09/09-rgm.html#packages-needed",
    "title": "Social Network Analysis",
    "section": "",
    "text": "library(igraph)\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(networkdata)\nlibrary(tidyverse)"
  },
  {
    "objectID": "teaching/sna/material/09/09-rgm.html#the-erdősrényi-model-gn-p",
    "href": "teaching/sna/material/09/09-rgm.html#the-erdősrényi-model-gn-p",
    "title": "Social Network Analysis",
    "section": "",
    "text": "To see why the \\(G(n, p)\\) model is often an inadequate representation of real-world networks, we can compare its properties to those of an actual empirical network. A typical social or informational network displays three features that are not captured well by \\(G(n, p)\\): a right-skewed degree distribution (with hubs), high clustering or triadic closure, and short average path lengths. While \\(G(n, p)\\) can match the density of a network, it assumes a binomial (or normal) degree distribution, minimal clustering, and does not account for structural heterogeneity.\nThe example below uses the igraph package in R and a real network dataset of moderate-to-large size to illustrate these differences. We load a real-world network; a network of co-appearances of characters in Victor Hugo’s novel “Les Miserables” which can be loaded from the networkdata package.\n\n\n\n\n\n\n\n\n\nWe compute this its key structural properties, then generate a random graph with the same number of nodes and expected density using sample_gnp(). We then compare the two in terms of degree distribution, transitivity (clustering), and average geodesic distance.\nThe code below summarizes key structural properties of the observed Les Misérables network and the corresponding \\(G(n, p)\\) random graph. These include the global clustering coefficient (measuring the tendency of nodes to form closed triads), the average geodesic distance (a measure of path efficiency), and the maximum degree (the highest number of connections any single node has).\nThe observed network shows substantially higher clustering, a slightly shorter average path length, and a much larger maximum degree. These results highlight that the empirical network is both more locally cohesive and more hierarchically structured than its random counterpart. The presence of hubs and local clusters—common in real-world networks—is not reproduced by the \\(G(n, p)\\) model, which assumes uniform and independent edge probabilities.\nTogether, these differences support the conclusion that random tie formation alone cannot explain the structure of this network.\n\nlibrary(knitr)\n# Load the Les Misérables network from networkdata\ndata(\"miserables\")\ng_obs &lt;- miserables\n\n# Basic stats of the observed network\nn &lt;- vcount(g_obs)\nm &lt;- ecount(g_obs)\ndensity_obs &lt;- edge_density(g_obs)\ndeg_obs &lt;- degree(g_obs)\nclustering_obs &lt;- transitivity(g_obs, type = \"global\")\ndist_obs &lt;- mean_distance(g_obs, directed = FALSE, unconnected = TRUE)\n\n# Generate a G(n, p) graph with the same density\nset.seed(123)\ng_gnp &lt;- sample_gnp(n = n, p = density_obs, directed = FALSE)\n\ndeg_gnp &lt;- degree(g_gnp)\nclustering_gnp &lt;- transitivity(g_gnp, type = \"global\")\ndist_gnp &lt;- mean_distance(g_gnp, directed = FALSE, unconnected = TRUE)\n\n# Combine comparison into a data frame\ncomparison &lt;- data.frame(\n  Model = c(\"Observed\", \"G(n, p)\"),\n  Clustering = c(clustering_obs, clustering_gnp),\n  AvgPathLength = c(dist_obs, dist_gnp),\n  MaxDegree = c(max(deg_obs), max(deg_gnp))\n)\n\n# Format with kable\nkable(comparison, caption = \"Comparison of structural features: Observed vs G(n, p)\",\n      digits = 3, align = \"c\")\n\n\nComparison of structural features: Observed vs G(n, p)\n\n\nModel\nClustering\nAvgPathLength\nMaxDegree\n\n\n\n\nObserved\n0.499\n4.861\n36\n\n\nG(n, p)\n0.086\n2.636\n12\n\n\n\n\n\nTo further illustrate the limitations of the \\(G(n, p)\\) model, we also examine the degree distributions of the observed network and the simulated random graph. Real-world networks often exhibit right-skewed degree distributions, with many nodes having few connections and a small number of hubs with very high degree. In contrast, the \\(G(n, p)\\) model produces a binomial (and approximately normal) degree distribution, where most nodes have degrees clustered around the mean. By comparing these two distributions side by side, we can observe how poorly the random model captures the heterogeneity present in the empirical network.\n\n# Plot the degree distributions\ndf_deg &lt;- data.frame(\n  Degree = c(deg_obs, deg_gnp),\n  Type = rep(c(\"Observed\", \"G(n, p)\"), times = c(length(deg_obs), length(deg_gnp)))\n)\n\nggplot(df_deg, aes(x = Degree, fill = Type)) +\n  geom_histogram(position = \"identity\", bins = 20, alpha = 0.6, color = \"white\") +\n  facet_wrap(~Type, scales = \"free_y\") +\n  labs(x = \"Node Degree\", y = \"Frequency\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"skyblue\", \"tomato\")) +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\nThis example underscores the need for more realistic network models that can capture multiple structural properties simultaneously. While the \\(G(n, p)\\) model offers a useful theoretical baseline, its assumptions of uniform edge probability and independent tie formation lead to networks with unrealistic degree distributions. In particular, it fails to capture the heterogeneity observed in many real-world systems, where some nodes act as hubs while others have very few connections. To address this limitation, we turn to the configuration model, which allows us to fix the degree sequence of the network and thereby preserve node-level connectivity patterns. This model represents a natural next step toward our second random graph model.\n\n\n\n\n\n\nNote: \\(G(n, p)\\) and CUG Given Density\n\n\n\nThe \\(G(n, p)\\) model is mathematically equivalent to a Conditional Uniform Graph (CUG) test given density. In both cases, edges are formed between node pairs independently with fixed probability \\(p\\), and the overall network density is preserved on average across simulations.\nHowever, there are key differences in interpretation and usage:\n\nThe \\(G(n, p)\\) model is a generative model used to define a probability distribution over the space of graphs with \\(n\\) nodes and tie probability \\(p\\). It is often used in theoretical network science as a baseline or null model.\nA CUG test given density is a hypothesis testing framework. It conditions on the observed number of nodes and the expected density, and tests whether an observed network statistic (e.g., mutual ties, clustering) deviates significantly from what would be expected by chance.\n\nIn practice, simulating random graphs under the \\(G(n, p)\\) model is functionally identical to conducting a CUG test with fixed density. The distinction lies in whether the model is used for generative modeling or for evaluating the statistical significance of observed network features."
  },
  {
    "objectID": "teaching/sna/material/09/09-rgm.html#the-configuration-model",
    "href": "teaching/sna/material/09/09-rgm.html#the-configuration-model",
    "title": "Social Network Analysis",
    "section": "",
    "text": "we continue with the Les Misérables co-appearance network and compare it to a random network generated from the configuration model. The goal is to assess how well the configuration model replicates key structural features of the observed network when it exactly preserves the degree sequence but randomizes the specific tie configuration.\nWe use the igraph package to compute network properties and simulate the configuration model using sample_degseq(). The configuration model guarantees that each node retains its observed degree. We create a comparison table and visualize the degree distribution as before.\n\n# Get observed degree sequence\ndeg_seq &lt;- degree(g_obs)\n\n# Compute observed properties\nclustering_obs &lt;- transitivity(g_obs, type = \"global\")\ndist_obs &lt;- mean_distance(g_obs, directed = FALSE, unconnected = TRUE)\n\n# Simulate configuration model with the same degree sequence\nset.seed(123)\ng_conf &lt;- sample_degseq(deg_seq, method = \"fast.heur.simple\")\n\n# Compute simulated properties\nclustering_conf &lt;- transitivity(g_conf, type = \"global\")\ndist_conf &lt;- mean_distance(g_conf, directed = FALSE, unconnected = TRUE)\n\n# Degree distribution comparison\ndeg_conf &lt;- degree(g_conf)\ndeg_df &lt;- data.frame(\n  Degree = c(deg_seq, deg_conf),\n  Type = rep(c(\"Observed\", \"Configuration Model\"), times = c(length(deg_seq), length(deg_conf)))\n)\n\n# Summary table\nconf_comparison &lt;- data.frame(\n  Model = c(\"Observed\", \"Configuration Model\"),\n  Clustering = c(clustering_obs, clustering_conf),\n  AvgPathLength = c(dist_obs, dist_conf),\n  MaxDegree = c(max(deg_seq), max(deg_conf))\n)\n\n# Display comparison table with kable\nkable(conf_comparison, caption = \"Comparison of structural features: Observed vs Configuration Model\",\n      digits = 3, align = \"c\")\n\n\nComparison of structural features: Observed vs Configuration Model\n\n\nModel\nClustering\nAvgPathLength\nMaxDegree\n\n\n\n\nObserved\n0.499\n4.861\n36\n\n\nConfiguration Model\n0.239\n2.503\n36\n\n\n\n\n# degree distribution plot\nggplot(deg_df, aes(x = Degree, fill = Type)) +\n  geom_histogram(position = \"identity\", bins = 20, alpha = 0.6, color = \"white\") +\n  facet_wrap(~Type, scales = \"free_y\") +\n  labs(x = \"Node Degree\", y = \"Frequency\") +\n  scale_fill_manual(values = c(\"skyblue\", \"tomato\")) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\nAs expected, the degree distribution of the simulated network matches that of the original exactly. However, when we examine higher-order properties, such as the global clustering coefficient and average path length, we find notable differences. The observed network has significantly more clustering, suggesting the presence of structured triadic closure that is not reproduced by the configuration model’s randomized pairing process. The average path length may also differ, although it often remains in the same general range.\nThese results highlight an important distinction: while the configuration model controls for degree-based features, it does not account for clustering, community structure, or other forms of structural dependency. As such, it is useful as a baseline or null model for testing whether observed patterns can be explained by degree alone.\n\n\n\n\n\n\nNote: Configuration Model vs. CUG Given Degree\n\n\n\nThe configuration model and a Conditional Uniform Graph (CUG) test given degree both generate random networks that preserve the observed degree sequence. In this sense, they are conceptually aligned: both assume that node-level connectivity (i.e., degrees) is fixed and use this constraint to explore how other structural features might arise by chance.\nThe key distinction lies in how each is used. The configuration model is a generative model; it produces random graphs that exactly match a specified degree sequence, often for theoretical or simulation purposes. A CUG test given degree, on the other hand, is a hypothesis testing framework. It evaluates whether a particular network statistic (such as clustering or transitivity) in the observed network is unusually high or low compared to what would be expected under random tie arrangement, given the same degree sequence."
  },
  {
    "objectID": "teaching/sna/material/09/09-rgm.html#the-small-world-model",
    "href": "teaching/sna/material/09/09-rgm.html#the-small-world-model",
    "title": "Social Network Analysis",
    "section": "",
    "text": "To evaluate how well the small-world model captures structural features of a real network, we simulate a small-world graph using the same number of nodes and approximate average degree as the Les Misérables co-appearance network. We then compare the simulated graph to the observed one in terms of degree distribution, clustering, and average path length.\nThe simulation uses igraph::sample_smallworld(), which generates a Watts–Strogatz small-world graph by starting from a regular ring lattice and randomly rewiring edges with a given probability \\(p\\). We set \\(p = 0.05\\) to introduce moderate randomness while maintaining local structure (we discuss the choice of \\(p\\) in more detail below).\n\navg_deg_obs &lt;- mean(deg_obs)\nk &lt;- round(avg_deg_obs / 2)  # average degree per side for ring lattice\n\n# Simulate small-world graph\nset.seed(123)\ng_sw &lt;- sample_smallworld(dim = 1, size = n, nei = k, p = 0.05)\n\n# Compute properties\nclustering_obs &lt;- transitivity(g_obs, type = \"global\")\nclustering_sw &lt;- transitivity(g_sw, type = \"global\")\n\ndist_obs &lt;- mean_distance(g_obs, directed = FALSE, unconnected = TRUE)\ndist_sw &lt;- mean_distance(g_sw, directed = FALSE, unconnected = TRUE)\n\ndeg_sw &lt;- degree(g_sw)\n\n# Comparison table\nsw_comparison &lt;- data.frame(\n  Model = c(\"Observed\", \"Small-World\"),\n  Clustering = c(clustering_obs, clustering_sw),\n  AvgPathLength = c(dist_obs, dist_sw),\n  MaxDegree = c(max(deg_obs), max(deg_sw))\n)\n\n# Print formatted table\nkable(sw_comparison, caption = \"Comparison of structural features: Observed vs Small-World Model\",\n      digits = 3, align = \"c\")\n\n\nComparison of structural features: Observed vs Small-World Model\n\n\nModel\nClustering\nAvgPathLength\nMaxDegree\n\n\n\n\nObserved\n0.499\n4.861\n36\n\n\nSmall-World\n0.498\n3.778\n7\n\n\n\n\n# Degree distribution\ndeg_df &lt;- data.frame(\n  Degree = c(deg_sw, deg_obs),\n  Type = rep(c(\"Small-World\",\"Observed\"), times = c(length(deg_obs), length(deg_sw)))\n)\n # Reverse the factor levels\ndeg_df$Type &lt;- factor(deg_df$Type, levels = c(\"Small-World\", \"Observed\"))\n\nggplot(deg_df, aes(x = Degree, fill = Type)) +\n  geom_histogram(position = \"identity\", bins = 20, alpha = 0.6, color = \"white\") +\n  facet_wrap(~Type, scales = \"free_y\") +\n  labs(x = \"Node Degree\", y = \"Frequency\") +\n  scale_fill_manual(values = c(\"skyblue\",\"tomato\")) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\nThe simulation demonstrates how the small-world model approximates certain properties of the observed network. As shown in the table, the simulated network achieves a relatively short average path length, similar to that of the Les Misérables network, due to the introduction of random long-range ties. The clustering coefficient remains substantial, reflecting the model’s ability to preserve local neighborhood structure.\nHowever, the degree distribution in the small-world model shown in remains relatively narrow, with most nodes having degrees close to the average. This limitation highlights that while the small-world model captures some global and local properties, it does not account for degree heterogeneity. The results show that the small-world model offers a useful structural middle ground between regular and fully random graphs but still lacks the full complexity observed in empirical networks.\nNote that the choice of the rewiring probability \\(p\\) in the small-world model is crucial, as it balances regularity and randomness. Small values of \\(p\\) (e.g., between 0.01 and 0.2) are typically chosen to introduce enough randomness to significantly reduce path lengths, while still preserving high clustering. If \\(p\\) is too low, the network remains overly regular; if \\(p\\) is too high, the network behaves like a random graph and loses its local structure. In practice, \\(p\\) is often selected empirically to achieve small-world characteristics (high clustering and short average path length) relative to the number of nodes and degree.\nTo illustrate how the small-world model transitions between regular and random structure, we simulate multiple networks with the same number of nodes as Les Misérables network with varying values of the rewiring probability \\(p\\) and track how two key properties (clustering and average path length) change. This helps identify a “sweet spot” for \\(p\\) where the network retains high clustering but achieves short global paths, capturing the essence of small-world structure. The results are shown in ?@fig-rewire.\n\n\n\n\n\n\n\n\n\nThe plot shows a sharp transition in network structure as \\(p\\) increases. At \\(p = 0\\), the network is a regular lattice: clustering is high, but average path length is long. As \\(p\\) increases slightly (e.g., \\(p \\approx 0.1\\)), the average path length drops rapidly due to the introduction of long-range shortcuts, while clustering remains relatively high. This intermediate range is where small-world characteristics emerge.\nAs \\(p\\) approaches 1, the network becomes increasingly random (think \\(G(n,p)\\)): clustering drops off, and path length stabilizes at a low level. This demonstrates the trade-off between local cohesion and global efficiency controlled by the rewiring parameter \\(p\\)."
  },
  {
    "objectID": "teaching/sna/material/09/09-rgm.html#exercise",
    "href": "teaching/sna/material/09/09-rgm.html#exercise",
    "title": "Social Network Analysis",
    "section": "",
    "text": "Choose a network of yourself and analyze and compare results using the three introduced random graph models."
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms1.html",
    "href": "teaching/sna/material/10/10-ergms1.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "We’re going to follow the ERGM modelling outline:\n\nspecify and estimate model parameters that should govern evolution of network\nsimulate other random networks based on specified models\ncompare the goodness of fit of observed to model networks.\n\nThe following resource is useful for looking up different model terms: ERGM terms.\nNote that we now are performing stochastic simulation – in some of the cases, your output will differ slightly from mine and between different runs (you can however use set.seed() to get exactly the same results).\n\n\n\nlibrary(statnet) # also loads the ERGM package\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(intergraph)\nlibrary(patchwork)\nlibrary(networkdata)\n\n\n\n\nWe will be primarily be working with matrix, network and graph objects. Note that ergm primarily requires network and adjacency matrices, but since we will be using ggraph to visualize networks we also need graph objects. We try to keep it clear here by using suffix g, net and mat to clarify object assignment."
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms1.html#packages-needed",
    "href": "teaching/sna/material/10/10-ergms1.html#packages-needed",
    "title": "Social Network Analysis",
    "section": "",
    "text": "library(statnet) # also loads the ERGM package\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(intergraph)\nlibrary(patchwork)\nlibrary(networkdata)"
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms1.html#object-types",
    "href": "teaching/sna/material/10/10-ergms1.html#object-types",
    "title": "Social Network Analysis",
    "section": "",
    "text": "We will be primarily be working with matrix, network and graph objects. Note that ergm primarily requires network and adjacency matrices, but since we will be using ggraph to visualize networks we also need graph objects. We try to keep it clear here by using suffix g, net and mat to clarify object assignment."
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms1.html#model-1-dyadic-independencebernoulli-graph",
    "href": "teaching/sna/material/10/10-ergms1.html#model-1-dyadic-independencebernoulli-graph",
    "title": "Social Network Analysis",
    "section": "Model 1: Dyadic independence/Bernoulli graph",
    "text": "Model 1: Dyadic independence/Bernoulli graph\n\nEstimation\nWe begin by specifying a Bernoulli model using the ergm function. This is done by only including number of edges as a term in the model (recall from lecture that this implies dyadic independence). Run the model and print out summary of model fit using below code:\n\nflom_mod1 &lt;- ergm(flom_net ~ edges) # fit the model\nsummary(flom_mod1) # get a summary of model\n\nCall:\nergm(formula = flom_net ~ edges)\n\nMaximum Likelihood Results:\n\n      Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges  -1.6094     0.2449      0  -6.571   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 166.4  on 120  degrees of freedom\n Residual Deviance: 108.1  on 119  degrees of freedom\n \nAIC: 110.1  BIC: 112.9  (Smaller is better. MC Std. Err. = 0)\n\n\nYou can also just print the estimated coefficient using only flom_mod1.\nQ1. How can you interpret the parameter estimate?\nThe log-odds of any tie occurring is: \\[ -1.609 \\times \\textrm{change in the number of ties} = -1.609 \\times 1 \\] for all ties, since the addition of any tie to the network changes the number of ties by 1. Corresponding probability is: \\[\\frac{\\exp{(-1.609)}}{1+\\exp{(-1.609)}}=0.1667\\] which is what you would expect, since there are 20/120 ties."
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms1.html#model-2-transitivity-effect-added",
    "href": "teaching/sna/material/10/10-ergms1.html#model-2-transitivity-effect-added",
    "title": "Social Network Analysis",
    "section": "Model 2: Transitivity effect added",
    "text": "Model 2: Transitivity effect added\n\nEstimation\nNext, we add a term the number of completed triangles/triads (which would indicate transitivity).\n\nset.seed(1) #include if you want the same results shown here\nflom_mod2 &lt;- ergm(flom_net ~ edges + triangle)\nsummary(flom_mod2) \n\nCall:\nergm(formula = flom_net ~ edges + triangle)\n\nMonte Carlo Maximum Likelihood Results:\n\n         Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges     -1.6913     0.3219      0  -5.254   &lt;1e-04 ***\ntriangle   0.1808     0.5567      0   0.325    0.745    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 166.4  on 120  degrees of freedom\n Residual Deviance: 108.1  on 118  degrees of freedom\n \nAIC: 112.1  BIC: 117.6  (Smaller is better. MC Std. Err. = 0.01061)\n\n\nQ2 How can you interpret the parameter estimates?\nQ3 What do the parameter estimates tell us about the configurations specified in the model?\nConditional log-odds of two actors forming a tie is:\n\n\\(-1.644\\times\\) change in the number of ties + \\(0.134 \\times\\) change in number of triangles\nif the tie will not add any triangles to the network, its log-odds is: -1.644\nif it will add one triangle to the network, its log-odds is: -1.644 + 0.134\nif it will add two triangles to the network, its log-odds is: -1.644 + 0.134 \\(\\times\\) 2\n\n\n\nMCMC diagnostics\nYou can use mcmc.diagnostics(flom_mod2) to observe the behavior of the MCMC estimation algorithm and check for degeneracy. What you want to see in the MCMC diagnostics: the MCMC sample statistics varying randomly around the observed values at each step in the trace plots (which means the chain is mixing well) and the difference between the observed and simulated values of the sample statistics should have a roughly bell-shaped distribution, centered at 0 (which means no difference):\n\nmcmc.diagnostics(flom_mod2, center = TRUE)\n\n\n\n\n\n\n\n\nSample statistics summary:\n\nIterations = 14336:262144\nThinning interval = 1024 \nNumber of chains = 1 \nSample size per chain = 243 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n           Mean    SD Naive SE Time-series SE\nedges    0.2058 4.957   0.3180         0.3180\ntriangle 0.2222 2.866   0.1839         0.1839\n\n2. Quantiles for each variable:\n\n         2.5% 25% 50% 75% 97.5%\nedges      -9  -3   0   4  9.95\ntriangle   -3  -2   0   1  7.95\n\n\nAre sample statistics significantly different from observed?\n               edges  triangle    (Omni)\ndiff.      0.2057613 0.2222222        NA\ntest stat. 0.6471018 1.2086209 1.6694087\nP-val.     0.5175661 0.2268085 0.4367585\n\nSample statistics cross-correlations:\n             edges  triangle\nedges    1.0000000 0.7771561\ntriangle 0.7771561 1.0000000\n\nSample statistics auto-correlation:\nChain 1 \n                edges    triangle\nLag 0     1.000000000  1.00000000\nLag 1024  0.056425983 -0.04043396\nLag 2048  0.006273791  0.01940035\nLag 3072 -0.051649675 -0.02455474\nLag 4096 -0.034487041  0.02913158\nLag 5120 -0.023329356  0.01178056\n\nSample statistics burn-in diagnostic (Geweke):\nChain 1 \n\nFraction in 1st window = 0.1\nFraction in 2nd window = 0.5 \n\n   edges triangle \n1.458524 1.483872 \n\nIndividual P-values (lower = worse):\n    edges  triangle \n0.1446962 0.1378428 \nJoint P-value (lower = worse):  0.1011719 \n\nNote: MCMC diagnostics shown here are from the last round of\n  simulation, prior to computation of final parameter estimates.\n  Because the final estimates are refinements of those used for this\n  simulation run, these diagnostics may understate model performance.\n  To directly assess the performance of the final model on in-model\n  statistics, please use the GOF command: gof(ergmFitObject,\n  GOF=~model).\n\n\nQ4 How would you interpret these results?\n\n\nSimulation\nWhen we have estimated the coefficients of an ERGM, we have defined a probability distribution across all networks of the same size. If the model is a good fit to the observed data, networks drawn from this distribution resemble the observed data. To draw networks from this distribution we use the simulate() function. We draw ten networks from the specified model and use the below command to get a summary of what the network statistics edges and triangles are for each of the ten sampled networks.\n\nflom_mod2.sim &lt;- simulate(flom_mod2, nsim = 10)\nsummary(flom_mod2.sim)\n\nNumber of Networks: 10 \nModel: flom_net ~ edges + triangle \nReference: ~Bernoulli \nConstraints: ~. ~. - observed \nStored network statistics:\n      edges triangle\n [1,]    16        3\n [2,]    26        7\n [3,]    18        1\n [4,]    17        1\n [5,]    22        1\n [6,]    18        1\n [7,]    11        1\n [8,]    22        4\n [9,]    20        3\n[10,]    26        6\nattr(,\"monitored\")\n[1] FALSE FALSE\n\n\nNumber of Networks: 10 \nModel: flom_net ~ edges + triangle \nReference: ~Bernoulli \nConstraints: ~. ~. - observed \n\n\nThis should give you a list over the ten networks and columns representing how many edges and triangles are apparent in each simulated case. Since you have listed all the simulated networks, you can simply call each one of them individually. For example, in the below, we call simulated networks 1 and 2:\n\nflom_mod2.sim[[1]]\nflom_mod2.sim[[2]]\n\nYou can also choose one of the networks to visualize, below is an example for the tenth, i.e. last on the list of, simulated network:\n\nflom.sim_g &lt;-asIgraph(flom_mod2.sim[[10]])\nflom.sim_p &lt;- ggraph(flom.sim_g, layout = \"stress\") + \n  geom_edge_link0(edge_colour = \"#666060\", \n                  edge_width = 0.8, edge_alpha = 1) +\n  geom_node_point(fill = \"#808080\", colour = \"#808080\",  \n                  size = 7, shape = 21, stroke = 0.9) +\n  theme_graph() + \n  theme(legend.position = \"none\") +\n  ggtitle(\"Simulated network\")\nflom.sim_p\n\n\n\n\n\n\n\n\nThese simulations are crucial for examining the goodness of fit which we will do next.\n\n\n3. Goodness of Fit\nThe MCMC algorithm draws a dyad at random at each step, and evaluates the probability of a tie from the perspective of these two nodes. That probability is governed by the ergm-terms specified in the model, and the current estimates of the coefficients on these terms. Once the estimates converge, simulations from the model will produce networks that are centered on the observed model statistics i.e. those we control for (otherwise it is a sign that something has gone wrong in the estimation process). The networks will also have other emergent global properties that are not represented by explicit terms in the model. Thus, goodness of fit can be done in two ways, where the first is to be preferred:\n\nevaluate the fit to the specified terms in the model (done by default)\nevaluate the fit of terms not specified in the model to emergent global network properties\n\nIf the first does not indicate something off in the estimation process, you can use the second where three terms that can be used to evaluate the fit to emergent global network properties:\n\nthe node level (degree)\nthe edge level (esp: edgewise share partners)\nthe dyad level (geodesic distances)\n\nWe check now whether the specified model above fits the observed data and how well it reproduces it. We do this by choosing a network statistic (that is not specified in the model), and comparing the value of this statistic to the distribution of values we get in simulated networks from our model. We use the gof() function.\n\nflom_mod2.gof &lt;- gof(flom_mod2) # this will produce 4 plots\npar(mfrow=c(2,2)) # figure orientation with 2 rows and 2 columns\nplot(flom_mod2.gof) # gof plots\n\n\n\n\n\n\n\n\nTo get an output containing the summary of the gof:\n\nflom_mod2.gof # summary output of gof\n\nQ5 How would you interpret the goodness of fit here?"
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms1.html#model-1-homophily-and-clustering",
    "href": "teaching/sna/material/10/10-ergms1.html#model-1-homophily-and-clustering",
    "title": "Social Network Analysis",
    "section": "Model 1: homophily and clustering",
    "text": "Model 1: homophily and clustering\n\nEstimation\nWe are interested in running an ERGM with the following statistics (as done during lecture)\n\nnodecov(“practice”)\nmatch(“practice”)\ngwesp(decay = 0.693)\n\nQ6 Can you recall what these statistics represent? To run the ERGM:\n\nlaw_mod1 &lt;- ergm(law_net ~ edges\n  + nodecov(\"practice\") + match(\"practice\")\n  + gwesp(0.693, fixed = TRUE)\n)\nsummary(law_mod1)\n\nCall:\nergm(formula = law_net ~ edges + nodecov(\"practice\") + match(\"practice\") + \n    gwesp(0.693, fixed = TRUE))\n\nMonte Carlo Maximum Likelihood Results:\n\n                   Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges              -4.38591    0.32189      0 -13.626  &lt; 1e-04 ***\nnodecov.practice    0.18221    0.07121      0   2.559 0.010498 *  \nnodematch.practice  0.60364    0.16760      0   3.602 0.000316 ***\ngwesp.fixed.0.693   1.14125    0.16112      0   7.083  &lt; 1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 873.4  on 630  degrees of freedom\n Residual Deviance: 503.4  on 626  degrees of freedom\n \nAIC: 511.4  BIC: 529.2  (Smaller is better. MC Std. Err. = 0.3047)\n\n\nSee lecture slides for the interpretation of these coefficients.\n\n\nMCMC diagnostics\nCheck the model by running MCMC diagnostics to observe what is happening with the simulation algorithm:\n\nmcmc.diagnostics(law_mod1, center = TRUE)\n\n\n\n\n\n\n\n\nSample statistics summary:\n\nIterations = 122880:2424832\nThinning interval = 2048 \nNumber of chains = 1 \nSample size per chain = 1125 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n                    Mean    SD Naive SE Time-series SE\nedges              3.588 30.18   0.8997          2.050\nnodecov.practice   1.178 29.10   0.8677          1.771\nnodematch.practice 2.573 18.51   0.5520          1.187\ngwesp.fixed.0.693  6.930 59.05   1.7605          4.047\n\n2. Quantiles for each variable:\n\n                     2.5%    25%  50%   75% 97.5%\nedges               -70.0 -14.00  8.0 25.00  49.0\nnodecov.practice    -75.0 -12.00  6.0 21.00  43.9\nnodematch.practice  -41.8  -8.00  4.0 15.00  33.0\ngwesp.fixed.0.693  -132.5 -27.95 12.5 49.26 101.6\n\n\nAre sample statistics significantly different from observed?\n                edges nodecov.practice nodematch.practice gwesp.fixed.0.693\ndiff.      3.58755556        1.1777778         2.57333333        6.93039339\ntest stat. 1.74979703        0.6652089         2.16772023        1.71264367\nP-val.     0.08015334        0.5059169         0.03017998        0.08677811\n                 (Omni)\ndiff.                NA\ntest stat. 1.890437e+01\nP-val.     9.736672e-04\n\nSample statistics cross-correlations:\n                       edges nodecov.practice nodematch.practice\nedges              1.0000000        0.8720346          0.9463973\nnodecov.practice   0.8720346        1.0000000          0.8292965\nnodematch.practice 0.9463973        0.8292965          1.0000000\ngwesp.fixed.0.693  0.9942961        0.8753042          0.9426539\n                   gwesp.fixed.0.693\nedges                      0.9942961\nnodecov.practice           0.8753042\nnodematch.practice         0.9426539\ngwesp.fixed.0.693          1.0000000\n\nSample statistics auto-correlation:\nChain 1 \n              edges nodecov.practice nodematch.practice gwesp.fixed.0.693\nLag 0     1.0000000       1.00000000          1.0000000         1.0000000\nLag 2048  0.7091548       0.61237659          0.6442048         0.6814578\nLag 4096  0.4717707       0.37544290          0.4192158         0.4478894\nLag 6144  0.3156917       0.22673439          0.2935639         0.2997408\nLag 8192  0.2111325       0.13342547          0.1785633         0.2016442\nLag 10240 0.1487237       0.07132966          0.1299507         0.1454400\n\nSample statistics burn-in diagnostic (Geweke):\nChain 1 \n\nFraction in 1st window = 0.1\nFraction in 2nd window = 0.5 \n\n             edges   nodecov.practice nodematch.practice  gwesp.fixed.0.693 \n          1.493081           0.917114           1.500024           1.640398 \n\nIndividual P-values (lower = worse):\n             edges   nodecov.practice nodematch.practice  gwesp.fixed.0.693 \n         0.1354159          0.3590829          0.1336083          0.1009225 \nJoint P-value (lower = worse):  0.005343654 \n\nNote: MCMC diagnostics shown here are from the last round of\n  simulation, prior to computation of final parameter estimates.\n  Because the final estimates are refinements of those used for this\n  simulation run, these diagnostics may understate model performance.\n  To directly assess the performance of the final model on in-model\n  statistics, please use the GOF command: gof(ergmFitObject,\n  GOF=~model).\n\n\nQ6 Do you see any problems with model degeneracy here? Is the estimation process working as it should?\n\n\nGoodness of Fit\nGoodness of fit can be checked as done earlier:\n\nlaw_mod1.gof &lt;- gof(law_mod1) # this will produce 4 plots\npar(mfrow = c(2, 2)) # figure orientation with 2 rows and 2 columns\nplot(law_mod1.gof)\n\n\n\n\n\n\n\n\nNote that you should not use esp to assess goodness of fit since it was explicitly modeled via the gwesp term in the specified model."
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms1.html#model-1-reciprocity-effect",
    "href": "teaching/sna/material/10/10-ergms1.html#model-1-reciprocity-effect",
    "title": "Social Network Analysis",
    "section": "Model 1: Reciprocity effect",
    "text": "Model 1: Reciprocity effect\n\nEstimation\n\nknecht4_mod1 &lt;- ergm(knecht4_net ~ edges + mutual)\nsummary(knecht4_mod1) \n\nCall:\nergm(formula = knecht4_net ~ edges + mutual)\n\nMonte Carlo Maximum Likelihood Results:\n\n       Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges   -2.1889     0.1450      0 -15.100   &lt;1e-04 ***\nmutual   2.4115     0.3212      0   7.507   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 901.1  on 650  degrees of freedom\n Residual Deviance: 562.9  on 648  degrees of freedom\n \nAIC: 566.9  BIC: 575.8  (Smaller is better. MC Std. Err. = 0.7652)\n\n\nQ7 How do you interpret these results?\n\n\nMCMC diagnostics\n\nmcmc.diagnostics(knecht4_mod1)\n\n\n\n\n\n\n\n\nSample statistics summary:\n\nIterations = 14336:262144\nThinning interval = 1024 \nNumber of chains = 1 \nSample size per chain = 243 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean     SD Naive SE Time-series SE\nedges  -1.0988 11.869   0.7614         0.9010\nmutual -0.3333  5.357   0.3436         0.4182\n\n2. Quantiles for each variable:\n\n         2.5%  25% 50% 75% 97.5%\nedges  -26.00 -9.5   0   7    19\nmutual -10.95 -4.0   0   3    11\n\n\nAre sample statistics significantly different from observed?\n                edges     mutual    (Omni)\ndiff.      -1.0987654 -0.3333333        NA\ntest stat. -1.2194889 -0.7970729 1.8580854\nP-val.      0.2226587  0.4254087 0.3982094\n\nSample statistics cross-correlations:\n           edges    mutual\nedges  1.0000000 0.8121613\nmutual 0.8121613 1.0000000\n\nSample statistics auto-correlation:\nChain 1 \n               edges      mutual\nLag 0     1.00000000  1.00000000\nLag 1024  0.16476519  0.19190028\nLag 2048 -0.03275616 -0.03862647\nLag 3072 -0.04234861 -0.10047043\nLag 4096  0.03109986  0.06411610\nLag 5120  0.05887512  0.04907514\n\nSample statistics burn-in diagnostic (Geweke):\nChain 1 \n\nFraction in 1st window = 0.1\nFraction in 2nd window = 0.5 \n\n      edges      mutual \n-0.09833024 -0.92194343 \n\nIndividual P-values (lower = worse):\n    edges    mutual \n0.9216701 0.3565581 \nJoint P-value (lower = worse):  0.3677926 \n\nNote: MCMC diagnostics shown here are from the last round of\n  simulation, prior to computation of final parameter estimates.\n  Because the final estimates are refinements of those used for this\n  simulation run, these diagnostics may understate model performance.\n  To directly assess the performance of the final model on in-model\n  statistics, please use the GOF command: gof(ergmFitObject,\n  GOF=~model).\n\n\nQ8 How do you interpret these results?\n\n\nGoodness of fit\nNote that since we now are considering a directed network, we need to separate in- and out-degree when assessing the goodness of fit:\n\nknecht4_mod1.gof &lt;- gof(knecht4_mod1) # this will produce 4 plots\npar(mfrow = c(3,2)) # figure orientation with 2 rows and 2 columns\nplot(knecht4_mod1.gof)\n\n\n\n\n\n\n\n\nQ9 How do you interpret these results?"
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms1.html#model-2-reciprocity-and-homophily-effect",
    "href": "teaching/sna/material/10/10-ergms1.html#model-2-reciprocity-and-homophily-effect",
    "title": "Social Network Analysis",
    "section": "Model 2: Reciprocity and homophily effect",
    "text": "Model 2: Reciprocity and homophily effect\nNow we also include a homophily effect, i.e. do students tend to befriend others of the same gender?\nQ10 Run the usual steps of fitting and ERGM, checking the estimation algorithm and assessing the goodness of fit. The ERGM syntax is shown below. What can you conclude?\n\nknecht4_mod2 &lt;- ergm(knecht4_net ~ edges +  nodecov(\"gender\") + \n                       nodematch(\"gender\") + mutual)\nsummary(knecht4_mod2) \n\nCall:\nergm(formula = knecht4_net ~ edges + nodecov(\"gender\") + nodematch(\"gender\") + \n    mutual)\n\nMonte Carlo Maximum Likelihood Results:\n\n                 Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges             -4.1981     0.3843      0 -10.925   &lt;1e-04 ***\nnodecov.gender     0.4966     0.1192      0   4.167   &lt;1e-04 ***\nnodematch.gender   1.2061     0.2257      0   5.344   &lt;1e-04 ***\nmutual             2.0843     0.3735      0   5.580   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 901.1  on 650  degrees of freedom\n Residual Deviance: 520.4  on 646  degrees of freedom\n \nAIC: 528.4  BIC: 546.3  (Smaller is better. MC Std. Err. = 0.8236)"
  },
  {
    "objectID": "teaching/sna/material/02/02-graph-theory.html",
    "href": "teaching/sna/material/02/02-graph-theory.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "We’ll use the igraph package to explore key graph-theoretical concepts.\n\nlibrary(igraph)"
  },
  {
    "objectID": "teaching/sna/material/02/02-graph-theory.html#graph-theory-in-r-with-igraph",
    "href": "teaching/sna/material/02/02-graph-theory.html#graph-theory-in-r-with-igraph",
    "title": "Social Network Analysis",
    "section": "",
    "text": "We’ll use the igraph package to explore key graph-theoretical concepts.\n\nlibrary(igraph)"
  },
  {
    "objectID": "teaching/sna/material/02/02-graph-theory.html#undirected-graph",
    "href": "teaching/sna/material/02/02-graph-theory.html#undirected-graph",
    "title": "Social Network Analysis",
    "section": "Undirected Graph",
    "text": "Undirected Graph\nWe’ll use an undirected graph with 8 nodes.\n\ng &lt;- graph_from_literal(\n  1 -- 2, \n  1 -- 3,\n  2 -- 4,\n  3 -- 5,\n  4 -- 5,\n  5 -- 6,\n  6 -- 7,\n  6 -- 8,\n  7 -- 8\n)\n\nplot(g, vertex.label.cex = 1.2, vertex.size = 20)\n\n\n\n\n\n\n\n\n\n\nDegree & Degree Distribution\n\ndeg &lt;- degree(g)\ndeg\n\n1 2 3 4 5 6 7 8 \n2 2 2 2 3 3 2 2 \n\n# Degree distribution\ndist &lt;- degree_distribution(g)\nplot(dist, type = \"h\", main = \"Degree Distribution\", xlab = \"Degree\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n\nIdentify nodes with the highest and lowest degree. How does the distribution look?\n\n\n\nGraph Diameter\n\ndiameter(g)\n\n[1] 4\n\n\nWhat is the longest shortest path in the graph?\n\n\n\nShortest Paths\nFind shortest paths from node 1 to all others:\n\nsp &lt;- distances(g, v = 1)\nsp\n\n  1 2 3 4 5 6 7 8\n1 0 1 1 2 2 3 4 4\n\n\nDo you understand the output? What is the shortest path from node 1 to node 6?\n\n\n\nAdjacency Matrix\n\nadj_matrix &lt;- as_adjacency_matrix(g, sparse = FALSE)\nadj_matrix\n\n  1 2 3 4 5 6 7 8\n1 0 1 1 0 0 0 0 0\n2 1 0 0 1 0 0 0 0\n3 1 0 0 0 1 0 0 0\n4 0 1 0 0 1 0 0 0\n5 0 0 1 1 0 1 0 0\n6 0 0 0 0 1 0 1 1\n7 0 0 0 0 0 1 0 1\n8 0 0 0 0 0 1 1 0\n\n\nUse the adjacency matrix to compute the degree of each node. Compare with degree(g).\n\n\n\nCutpoints (Articulation Points)\n\narticulation_points &lt;- articulation_points(g)\narticulation_points\n\n+ 2/8 vertices, named, from be810f5:\n[1] 6 5\n\n\n\n\nBridges (Critical Edges)\n\nbridge_edges &lt;- which(is.na(edge_connectivity(g)))\nE(g)[bridge_edges]\n\n+ 0/9 edges from be810f5 (vertex names):\n\n# Alternatively use:\nbridges(g)\n\n+ 1/9 edge from be810f5 (vertex names):\n[1] 5--6\n\n\n\n\nVisualize Cutpoints and Bridges\n\nV(g)$color &lt;- ifelse(V(g) %in% articulation_points, \"red\", \"skyblue\")\nE(g)$color &lt;- ifelse(E(g) %in% bridges(g), \"red\", \"black\")\n\nplot(g, vertex.size = 20, vertex.label.cex = 1.2)\n\n\n\n\n\n\n\n\nWhich nodes and edges are critical to keeping the graph connected?"
  },
  {
    "objectID": "teaching/sna/material/02/02-graph-theory.html#directed-graph",
    "href": "teaching/sna/material/02/02-graph-theory.html#directed-graph",
    "title": "Social Network Analysis",
    "section": "Directed Graph",
    "text": "Directed Graph\nNow let’s work with a directed graph.\n\ng_dir &lt;- graph_from_literal(\n  A -+ B, A -+ C,\n  B -+ D,\n  C -+ D,\n  D -+ E,\n  E -+ F,\n  F -+ C\n)\n\nplot(g_dir, vertex.label.cex = 1.2, vertex.color = \"lightcoral\", edge.arrow.size = 0.5)\n\n\n\n\n\n\n\n\n\nIn-Degree and Out-Degree\n\ndegree(g_dir, mode = \"in\")   # incoming links\n\nA B C D E F \n0 1 2 2 1 1 \n\ndegree(g_dir, mode = \"out\")  # outgoing links\n\nA B C D E F \n2 1 1 1 1 1 \n\n\n\n\nStrongly Connected Components\n\ncomponents(g_dir, mode = \"strong\")\n\n$membership\nA B C D E F \n1 2 3 3 3 3 \n\n$csize\n[1] 1 1 4\n\n$no\n[1] 3\n\n\n\n\nDirected Paths and Diameter\n\ndiameter(g_dir, directed = TRUE)\n\n[1] 4\n\n\nExplore how cycles and direction affect path lengths."
  },
  {
    "objectID": "teaching/sna/material/02/02-graph-theory.html#exercises",
    "href": "teaching/sna/material/02/02-graph-theory.html#exercises",
    "title": "Social Network Analysis",
    "section": "Exercises",
    "text": "Exercises\n\nCreate some other small unidrected and directed graphs and see how the above measure vary on them\nimport the Florentine marriage and business network from the networkdata package and compute the appropriate measures from above on it\n\n\nlibrary(networkdata)\ndata(\"flo_marriage\")\ndata(\"flo_business\")"
  },
  {
    "objectID": "teaching/sna/material/05/05-special-nets.html",
    "href": "teaching/sna/material/05/05-special-nets.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "Signed Network and Two-Mode Networks\n\nExercise 1: Correlates of War\nLoad the “Correlates of War” dataset from the signnet package. The “cowList” dataset contains a list of 52 signed networks of inter-state relations over time (1946-1999). Two countries are connected by a positive tie if they form an alliance or have a peace treaty. A negative tie exists between countries who are at war or in other kinds of conflicts.\n\nlibrary(signnet)\ndata(\"cowList\")\n\nThe dataset includes 51 networks of international relations between nations (aggregated on 3 year time windows). A positive tie indicates some form of alliance between countries and a negative tie conflict or war.\nExcept for the first and last task, you can either do the exercises for all networks or choose one specific time window\n\nDid the number of conflicts increase or decrease over time?\nBefore computing any balance scores: What would you expect in terms of structural balance?\nCalculate the triangle based balance score. Does it mach your intuition?\nVisualize a network using ggsigned() and decide whether it makes sense to estimate a regular blockmodel or if you need to specify a general one and compute the blockmodel\n\n\n\nExercise 2: Two Mode Projections\nThe file senate15.csv contains all bill cosponsorships of the US Senate from 2015 to 2017.\n\nread the file dat &lt;- read.csv(\"senate15.csv\")\nCreate a data frame of the senators\nsenators &lt;- unique(dat[,1:2])\nConstruct the two mode network of senators and bills with\ng &lt;- bipartite_from_data_frame(dat,\"bill\",\"name\")\nprint the igraph object and check how the two modes are distinguished\ncheck ?bipartite.projection and only create the weighted projection between senators. Call the network proj. What does the weight indicate?\nInspect the plot. Can you see any structure?\ndelete edges with a weight less than x, where you should try different values for x. What do you think are “good” values for x? Does a structure emerge for any x?\n\n\n\n\n\n\n\nTip\n\n\n\nFor the exploratory part of the exercise you can do the following:\n\nV(proj)$party &lt;- senators$party[match(V(proj)$name,senators$name)]\nV(proj)$color &lt;- ifelse(V(proj)$party==\"D\",\"blue\",\n                 ifelse(V(proj)$party==\"R\",\"red\",\"yellow\"))\n\nplot(proj,layout = layout_with_kk,vertex.label = NA,vertex.size = 3)\n\nto delete edges, use delete_edges()"
  },
  {
    "objectID": "teaching/sna/material/04/04-descriptives-2-bm.html",
    "href": "teaching/sna/material/04/04-descriptives-2-bm.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "library(igraph)\nlibrary(networkdata)\nlibrary(netUtils)\nlibrary(ggraph)\nlibrary(kableExtra)\nlibrary(corrplot)\nlibrary(tidygraph)"
  },
  {
    "objectID": "teaching/sna/material/04/04-descriptives-2-bm.html#blockmodels-using-concor",
    "href": "teaching/sna/material/04/04-descriptives-2-bm.html#blockmodels-using-concor",
    "title": "Social Network Analysis",
    "section": "Blockmodels using CONCOR",
    "text": "Blockmodels using CONCOR\nThe example we considered in lecture concerns the relatively small network. What happens when we apply the method of iterated correlations (CONCOR) to a bigger network, something like the one shown below?\n\n\n\n\n\n\n\n\nFigure 1: An undirected graph.\n\n\n\n\n\nFirst save the function below for computing the correlation distance between pairs of nodes:\n\n#function:\ncorr.dist &lt;- function(x) {\n         r &lt;- nrow(x)\n         c &lt;- ncol(x)\n         r.c &lt;- matrix(0, r, r)\n         c.c &lt;- matrix(0, c, c)\n         r.m &lt;- rowMeans(x)\n         c.m &lt;- colMeans(x)\n         \n         for (i in 1: r) {\n              for (j in 1:r) {\n                   r.x &lt;- x[i, ] - r.m[i]\n                   r.y &lt;- x[j, ] - r.m[j]\n                   r.xy &lt;- r.x * r.y\n                   r.xx &lt;- r.x^2\n                   r.yy &lt;- r.y^2\n                   r.num &lt;- sum(r.xy)\n                   r.den &lt;- sqrt(sum(r.xx)) * sqrt(sum(r.yy))\n                   r.c[i, j] &lt;- round(r.num / r.den, 2)\n              }\n         }\n         rownames(r.c) &lt;- rownames(x)\n         colnames(r.c) &lt;- rownames(x)\n         return(r.c)\n}\n\nLet’s start by definig the graph:\n\nfr &lt;- c(rep(1, 5), rep(2, 5), rep(3, 5), rep(4, 3), rep(20, 3))\n    to &lt;- c(5:9, 10:14, 15:19, 1:3, 5, 21, 22)\n    edge.dat &lt;- data.frame(fr, to)\n    node.dat &lt;- data.frame(name = toupper(letters[union(fr, to)]))\n    gr &lt;- tbl_graph(edges = edge.dat, nodes = node.dat, directed = FALSE)\n    gr &lt;- as_tbl_graph(simplify(gr)) \n\nWell, we can begin by computing the correlation distance across all the \\(22\\) nodes in that network.\nNote that even before we do any iterated correlations of correlation matrices we can see that the peripheral, single-connection nodes \\(E, F, G, H\\), \\(I, J, K, L, M\\) and \\(N, O, P, Q, R\\) are perfectly structurally equivalent. This makes sense, because all the nodes in each of these three groups have identical neighborhoods, since they happen to be connected to the same central node \\(A\\) for the first group, \\(B\\) for the second group and \\(C\\) for the third group. Note also that \\(U\\) and \\(V\\) are structurally equivalent, since their neighborhoods are the same: Their single connection is to node \\(S\\).\n\na &lt;- matrix(as_adjacency_matrix(gr), nrow = length(V(gr)))\n    rownames(a) &lt;- V(gr)$name\n    colnames(a) &lt;- V(gr)$name\n    \nb &lt;- corr.dist(a)\n\nWhat happens when we take the correlation distance of the correlation distance matrix shown in Table 1 (a), and the correlation distance of the resulting matrix, and keep going until we only have zeros and ones? The results is Table 1 (b). This matrix seems to reveal a much deeper pattern of commonalities in structural positions across the nodes in Figure 1.\n\nb &lt;- corr.dist(a)\nc &lt;- b\nk &lt;- 1\nwhile (mean(abs(c)) != 1) {\n      c &lt;- corr.dist(c)\n      k &lt;- k + 1\n  }\n\n\n\n\n\n\n\n\n\n\n(a) Original Correlation Distance Matrix.\n\n\n\n\n\n\n\nA\nB\nC\nD\nT\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nP\nQ\nR\nS\nU\nV\n\n\n\n\nA\n1.00\n-0.15\n-0.15\n-0.24\n-0.19\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n0.05\n-0.13\n-0.13\n\n\nB\n-0.15\n1.00\n-0.15\n-0.24\n-0.19\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.24\n-0.13\n-0.13\n\n\nC\n-0.15\n-0.15\n1.00\n-0.24\n-0.19\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.24\n-0.13\n-0.13\n\n\nD\n-0.24\n-0.24\n-0.24\n1.00\n0.34\n0.55\n0.55\n0.55\n0.55\n0.55\n0.55\n0.55\n0.55\n0.55\n0.55\n0.55\n0.55\n0.55\n0.55\n-0.16\n-0.09\n-0.09\n\n\nT\n-0.19\n-0.19\n-0.19\n0.34\n1.00\n0.69\n0.69\n0.69\n0.69\n-0.07\n-0.07\n-0.07\n-0.07\n-0.07\n-0.07\n-0.07\n-0.07\n-0.07\n-0.07\n-0.13\n0.69\n0.69\n\n\nE\n-0.13\n-0.13\n-0.13\n0.55\n0.69\n1.00\n1.00\n1.00\n1.00\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.09\n-0.05\n-0.05\n\n\nF\n-0.13\n-0.13\n-0.13\n0.55\n0.69\n1.00\n1.00\n1.00\n1.00\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.09\n-0.05\n-0.05\n\n\nG\n-0.13\n-0.13\n-0.13\n0.55\n0.69\n1.00\n1.00\n1.00\n1.00\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.09\n-0.05\n-0.05\n\n\nH\n-0.13\n-0.13\n-0.13\n0.55\n0.69\n1.00\n1.00\n1.00\n1.00\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.09\n-0.05\n-0.05\n\n\nI\n-0.13\n-0.13\n-0.13\n0.55\n-0.07\n-0.05\n-0.05\n-0.05\n-0.05\n1.00\n1.00\n1.00\n1.00\n1.00\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.09\n-0.05\n-0.05\n\n\nJ\n-0.13\n-0.13\n-0.13\n0.55\n-0.07\n-0.05\n-0.05\n-0.05\n-0.05\n1.00\n1.00\n1.00\n1.00\n1.00\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.09\n-0.05\n-0.05\n\n\nK\n-0.13\n-0.13\n-0.13\n0.55\n-0.07\n-0.05\n-0.05\n-0.05\n-0.05\n1.00\n1.00\n1.00\n1.00\n1.00\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.09\n-0.05\n-0.05\n\n\nL\n-0.13\n-0.13\n-0.13\n0.55\n-0.07\n-0.05\n-0.05\n-0.05\n-0.05\n1.00\n1.00\n1.00\n1.00\n1.00\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.09\n-0.05\n-0.05\n\n\nM\n-0.13\n-0.13\n-0.13\n0.55\n-0.07\n-0.05\n-0.05\n-0.05\n-0.05\n1.00\n1.00\n1.00\n1.00\n1.00\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.09\n-0.05\n-0.05\n\n\nN\n-0.13\n-0.13\n-0.13\n0.55\n-0.07\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n1.00\n1.00\n1.00\n1.00\n1.00\n-0.09\n-0.05\n-0.05\n\n\nO\n-0.13\n-0.13\n-0.13\n0.55\n-0.07\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n1.00\n1.00\n1.00\n1.00\n1.00\n-0.09\n-0.05\n-0.05\n\n\nP\n-0.13\n-0.13\n-0.13\n0.55\n-0.07\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n1.00\n1.00\n1.00\n1.00\n1.00\n-0.09\n-0.05\n-0.05\n\n\nQ\n-0.13\n-0.13\n-0.13\n0.55\n-0.07\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n1.00\n1.00\n1.00\n1.00\n1.00\n-0.09\n-0.05\n-0.05\n\n\nR\n-0.13\n-0.13\n-0.13\n0.55\n-0.07\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n1.00\n1.00\n1.00\n1.00\n1.00\n-0.09\n-0.05\n-0.05\n\n\nS\n0.05\n-0.24\n-0.24\n-0.16\n-0.13\n-0.09\n-0.09\n-0.09\n-0.09\n-0.09\n-0.09\n-0.09\n-0.09\n-0.09\n-0.09\n-0.09\n-0.09\n-0.09\n-0.09\n1.00\n-0.09\n-0.09\n\n\nU\n-0.13\n-0.13\n-0.13\n-0.09\n0.69\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.09\n1.00\n1.00\n\n\nV\n-0.13\n-0.13\n-0.13\n-0.09\n0.69\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.09\n1.00\n1.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Original Correlation Distance Matrix After Ten Iterations.\n\n\n\n\n\n\n\nA\nB\nC\nD\nT\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nP\nQ\nR\nS\nU\nV\n\n\n\n\nA\n1\n1\n1\n-1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n\n\nB\n1\n1\n1\n-1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n\n\nC\n1\n1\n1\n-1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n\n\nD\n-1\n-1\n-1\n1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n\n\nT\n1\n1\n1\n-1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n\n\nE\n1\n1\n1\n-1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n\n\nF\n1\n1\n1\n-1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n\n\nG\n1\n1\n1\n-1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n\n\nH\n1\n1\n1\n-1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n\n\nI\n-1\n-1\n-1\n1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n\n\nJ\n-1\n-1\n-1\n1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n\n\nK\n-1\n-1\n-1\n1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n\n\nL\n-1\n-1\n-1\n1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n\n\nM\n-1\n-1\n-1\n1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n\n\nN\n-1\n-1\n-1\n1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n\n\nO\n-1\n-1\n-1\n1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n\n\nP\n-1\n-1\n-1\n1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n\n\nQ\n-1\n-1\n-1\n1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n\n\nR\n-1\n-1\n-1\n1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n\n\nS\n1\n1\n1\n-1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n\n\nU\n1\n1\n1\n-1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n\n\nV\n1\n1\n1\n-1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Correlation Distance Matrix in (a) with Rows and Columns Reshuffled to Show Hidden Pattern.\n\n\n\n\n\n\n\nV\nU\nS\nH\nG\nF\nE\nT\nC\nA\nB\nR\nQ\nP\nO\nN\nM\nL\nK\nJ\nD\nI\n\n\n\n\nV\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\nU\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\nS\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\nH\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\nG\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\nF\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\nE\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\nT\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\nC\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\nA\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\nB\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\nR\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nQ\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nP\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nO\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nN\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nM\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nL\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nK\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nJ\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nD\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nI\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\n\n\n\n\nTable 1: Correlation Distance Matrices Corresponding to an Undirected Graph.\n\n\n\nRunning the below code to reshufffke nodes gives the result in Table 1 (c):\n\nrs &lt;- corrMatOrder(c, order = \"hclust\")\nd &lt;- c[rs, rs]\n\nSo it turns out that there is indeed a secret pattern! The reshuffling shows that the nodes in the network can be divided into two blocks such within blocks all nodes are structurally similar (and some structurally equivalent) and across blocks, all nodes are structurally dissimilar. Thus \\(V, U, S, H, G, F, E, T, C, A, B\\) are members of one structurally similar block (let’s called them “Block 1”), and nodes \\(R, Q, P, O, N, M, L, K, J, D, I\\) are members of another structurally similar block (let’s called them “Block 2”). Nodes in “Block 1” are structurally dissimilar from nodes in “Block 2,” but structurally similar to one another and vice versa. To illustrate, Figure 2 is the same as Figure 1, but this time nodes are colored by their memberships in two separate blocks.\n\n    node.color &lt;- c(rep(\"tan3\",3), \"steelblue\", \"tan3\", rep(\"tan3\", 4), rep(\"steelblue\", 10), \"tan3\", rep(\"tan3\", 2))\n    p &lt;- ggraph(gr, layout = 'kk') \n    p &lt;- p + geom_edge_link(color = \"black\", width = 1.15) \n    p &lt;- p + geom_node_point(aes(x = x, y = y), color = node.color, size = 22)\n    p &lt;- p + geom_node_text(aes(label = name), size = 10, color = \"white\")\n    p &lt;- p + theme_graph()\n    p\n\n\n\n\n\n\n\nFigure 2: An undirected graph with block membership indicated by node color.\n\n\n\n\n\nNote that we haven’t changed any of the information in Table 1 (b) to get Table 1 (c). If you check, the row and column entries for each node in both figures are identical. It’s just that we changed the way the rows ordered vertically and the way the columns are ordered horizontally. For instance, node \\(A\\)’s pattern of connections is negatively correlated with node \\(I\\)’s in Table 1 (b), and has the same negative correlation entry in Table 1 (c). The same goes for each one of node \\(A\\)’s other correlations, and the same for each node in the table. Table 1 (b) and Table 1 (c) contain the same information it’s just that Table 1 (c) makes it easier to see a hidden pattern.\nThis property of the method of iterated correlations is the basis of a strategy for uncovering blocks of structurally similar actors in a network developed by a team of sociologists, physicists, and mathematicians working at Harvard in the 1970s. The technique is called blockmodeling. Let’s see how it works.\n\nWe Need to go Deeper!\nOf course, as Leo says: “We need to go deeper.” And indeed we can. What happens if we do the same analysis as above, but this time in the two node-induced subgraphs defined by the set of structurally similar nodes in each of the two blocks we uncovered in the original graph?\nRunning the below codes gives you the results in Table 2 (a) and Table 2 (b):\n\n    b1 &lt;- b[rs[1:11], rs[1:11]]\n    b2 &lt;- b[rs[12:22], rs[12:22]]\n    \n    c1 &lt;- b1\n    while (mean(abs(c1)) != 1) {\n      c1 &lt;- corr.dist(c1)\n      }\n    \n    c2 &lt;- b2\n    while (mean(abs(c2[c2 != 0])) != 1) {\n      c2 &lt;- corr.dist(c2)\n      }\n    rs1 &lt;- corrMatOrder(c1, order = \"hclust\")\n    d1 &lt;- c1[rs1, rs1]\n    d1\n    rs2 &lt;- corrMatOrder(c2, order = \"hclust\")\n    d2 &lt;- c2[rs2, rs2]\n    d2\n\n\n\n\nTable 2: Subgraph Blockmodels\n\n\n\n\n\n\n\n(a) Blockmodel of a subgraph.\n\n\n\n\n\n\n\nB\nA\nC\nS\nV\nU\nT\nE\nF\nH\nG\n\n\n\n\nB\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n\n\nA\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n\n\nC\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n\n\nS\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n\n\nV\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n\n\nU\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n\n\nT\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n\n\nE\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n\n\nF\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n\n\nH\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n\n\nG\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Another Blockmodel of another subgraph\n\n\n\n\n\n\n\nI\nJ\nK\nM\nL\nD\nN\nO\nP\nR\nQ\n\n\n\n\nI\n1\n1\n1\n1\n1\n0\n-1\n-1\n-1\n-1\n-1\n\n\nJ\n1\n1\n1\n1\n1\n0\n-1\n-1\n-1\n-1\n-1\n\n\nK\n1\n1\n1\n1\n1\n0\n-1\n-1\n-1\n-1\n-1\n\n\nM\n1\n1\n1\n1\n1\n0\n-1\n-1\n-1\n-1\n-1\n\n\nL\n1\n1\n1\n1\n1\n0\n-1\n-1\n-1\n-1\n-1\n\n\nD\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nN\n-1\n-1\n-1\n-1\n-1\n0\n1\n1\n1\n1\n1\n\n\nO\n-1\n-1\n-1\n-1\n-1\n0\n1\n1\n1\n1\n1\n\n\nP\n-1\n-1\n-1\n-1\n-1\n0\n1\n1\n1\n1\n1\n\n\nR\n-1\n-1\n-1\n-1\n-1\n0\n1\n1\n1\n1\n1\n\n\nQ\n-1\n-1\n-1\n-1\n-1\n0\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that Table 1 (b) separates our original Block 2 into two further sub-blocks. Let’s call them “Block 2a” and “Block 2b.” Block 2a is composed of nodes \\(A, B, C, S, U, V\\) and Block 2b is composed of nodes \\(E, F, G, H, T\\).\nLet’s separates our original Block 2 into three further sub-blocks, as shown in Table 1 (b). There’s the block composed of nodes \\(I, J, K, L, M\\). Let’s call this “Block 2a”, the block composed of nodes \\(N, O, P, Q, R\\). Let’s call this “Block 2b.” Then, there’s node \\(D\\). Note that this node is only structurally similar to itself and is neither similar nor dissimilar to the other nodes in the subgraph \\(d^{corr} = 0\\), so it occupies a position all by itself! Let’s call it “Block 2c.”\n\n    n &lt;- c(\"B\", \"A\", \"C\", \"S\", \"V\", \"U\")\n    b3 &lt;- b[n, n]\n\n    c3 &lt;- b3\n    while (mean(abs(c3)) != 1) {\n      c3 &lt;- corr.dist(c3)\n    }\n    rs3 &lt;- corrMatOrder(c3, order = \"hclust\")\n    d3 &lt;- c3[rs3, rs3]\n    d3\n    \n    n &lt;- c(\"B\", \"A\", \"C\", \"S\")\n    b4 &lt;- b[n, n]\n    c4 &lt;- b4\n    while (mean(abs(c4)) != 1) {\n      c4 &lt;- corr.dist(c4)\n    }\n    rs4 &lt;- corrMatOrder(c4, order = \"hclust\")\n    d4 &lt;- c4[rs4, rs4]\n    d4\n\n\n\n\nTable 3: Subgraph Blockmodels\n\n\n\n\n\n\n\n(a) Blockmodel of a subgraph.\n\n\n\n\n\n\n\nS\nC\nB\nA\nV\nU\n\n\n\n\nS\n1\n1\n1\n1\n-1\n-1\n\n\nC\n1\n1\n1\n1\n-1\n-1\n\n\nB\n1\n1\n1\n1\n-1\n-1\n\n\nA\n1\n1\n1\n1\n-1\n-1\n\n\nV\n-1\n-1\n-1\n-1\n1\n1\n\n\nU\n-1\n-1\n-1\n-1\n1\n1\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Another Blockmodel of another subgraph\n\n\n\n\n\n\n\nB\nC\nA\nS\n\n\n\n\nB\n1\n1\n-1\n-1\n\n\nC\n1\n1\n-1\n-1\n\n\nA\n-1\n-1\n1\n1\n\n\nS\n-1\n-1\n1\n1\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s do a couple of final splits of the subgraph composed of nodes \\(A, B, C, S, U, V\\). This is shown in Table 3. The first split separates nodes in block \\(A, B, C, S\\) from those in block \\(U, V\\) (Table 3 (a)). The second splits the nodes in subgraph \\(A, B, C, S\\) into two blocks composed of \\(A, S\\) and \\(B, C\\), respectively (Table 3 (b)).\n\nnode.color &lt;- c(\"firebrick\", rep(\"steelblue\", 2), \"purple\", rep(\"tan3\", 5), rep(\"darkgreen\", 5), rep(\"#CC79A7\", 5), \"firebrick\", rep(\"darkturquoise\", 2))\n    p &lt;- ggraph(gr, layout = 'kk') \n    p &lt;- p + geom_edge_link(color = \"black\", width = 1.15) \n    p &lt;- p + geom_node_point(aes(x = x, y = y), color = node.color, size = 22)\n    p &lt;- p + geom_node_text(aes(label = name), size = 10, color = \"white\")\n    p &lt;- p + theme_graph()\n    p\n\n\n\n\n\n\n\nFigure 3: An undirected graph with block membership indicated by node color.\n\n\n\n\n\nFigure 3 shows the nodes in Figure 1 colored according to our final block partition. It is clear that the blockmodeling approach captures patterns of structural similarity. For instance, all the single-connection nodes connected to more central nodes get assigned to their own position: Block 1b: \\(E, F, G, H, T\\), Block 2a: \\(I, J, K, L, M\\), and Block 2b: \\(N, O, P, Q, R\\). The most central node \\(D\\) (in terms of Eigenvector centrality) occupies a unique position in the graph. Two of the three central nodes (in terms of degree centrality) \\(B, C\\) get assigned to their own position. Meanwhile \\(A, S\\) form their own structurally similar block. Finally, \\(U, V\\) also form their own structurally similar block as both are structurally equivalent in the orignal graph.\n\n\nThe Blocked Adjacency Matrix\nWhat happens if we were to go back fo the adjacency matrix corresponding to Figure 1, and then reshuffle the rows and columns to correspond to all these wonderful blocks we have uncovered? Well, we would en up with something like Table 4. This is called the blocked adjacency matrix. In the blocked adjacency matrix, the division between the nodes corresponding to each block of structurally similar nodes in Table 2 and Table 3 is marked by thick black lines going across the rows and columns.\nEach diagonal rectangle in Table 4 corresponds to within-block connections. Each off-diagonal rectangle corresponds to between block connections. There are two kinds of rectangles in the blocked adjacency matrix. First, there are rectangles that only contains zero entries. These are called zero blocks. For instance the top-left rectangle in Table 4 is a zero block. Then there rectangles that have some non-zero entries in them (ones, since this is a binary adjacency matrix). These are called one blocks. For instance, the fourth rectangle going down the rows (corresponding to the block that nodes \\(B, C\\) belong to) is a one block.\n\n\n\n\n\n\n\n\n\n\nI\nJ\nK\nL\nM\nN\nO\nP\nQ\nR\nT\nE\nF\nG\nH\nB\nC\nA\nS\nU\nV\nD\n\n\n\n\nI\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nJ\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nK\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nL\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nM\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nN\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nO\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nP\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nQ\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nR\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nT\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n\n\nE\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nF\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nG\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nH\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nB\n1\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\nC\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\nA\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n1\n\n\nS\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n\n\nU\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\nV\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\nD\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n\n\n\n\n\n\n\nTable 4: Blocked adjancency matrix.\n\n\n\n\nZero-blocks indicate that the members of the row block don’t have any connections with the members the column block (which can include themselves!). For instance, the zero-block in the top-left corner of the blocked adjacency matrix in Table 4 indicates that the members of this block are not connected to one another in the network (and we can verify from Figure 3 that this is indeed the case).\nOne blocks indicate that the members of the column block share some connections with the members of the column block (which can also include themselves!). For instance, the one-block in the fourth rectangle going down the rows (corresponding to the block that nodes \\(B, C\\) belong to) tells us that members of this block are connected to at least one member of the \\(I, J, K, L, M\\) block (and we can verify from Figure 3 that this is indeed the case, since \\(B\\) is connected to all of them).\n\n\nThe Image Matrix\nFrom this reshuffled adjacency matrix, we can get to a reduced image matrix containing the relations not between the nodes in the graph, but between the blocks in the graph. The way we proceed to construct the image matrix is as follows:\n\nFirst we create an empty matrix \\(\\mathbf{B}\\) of dimensions \\(b \\times b\\) where \\(B\\) is the number of blocks in the blockmodel. In our example, \\(b = 7\\) so the image matrix has seven rows and seven columns. The \\(ij^{th}\\) cell in the image matrix \\(\\mathbf{B}\\) records the relationship between row block i and column block j in the blockmodel.\nSecond, we put a zero in the image matrix if the rectangle corresponding to the relationship between blocks i and j in Table 4 is a zero-block.\nThird, we put a one in the image matrix if the rectangle corresponding to the relationship between blocks i and j in Table 4 is a one-block.\n\nThe result is Table 5:\n\nc &lt;- matrix(c(0, 0, 0, 1, 0, 0, 0,\n                0, 0, 0, 1, 0, 0, 0,\n                0, 0, 0, 0, 1, 0, 0,\n                1, 1, 0, 0, 0, 0, 1,\n                0, 0, 1, 0, 0, 1, 1,\n                0, 0, 0, 0, 1, 0, 0,\n                0, 0, 0, 1, 1, 0, 0), \n              nrow = 7)\nrownames(c) &lt;- c(\"I, J, K, L\", \"N, O, P, Q, R\", \"T, E, F, G, H\", \"B, C\", \"A, S\", \"U, V\", \"D\")\ncolnames(c) &lt;- c(\"I, J, K, L\", \"N, O, P, Q, R\", \"T, E, F, G, H\", \"B, C\", \"A, S\", \"U, V\", \"D\")\nc\n\n\n\n\n\n\n\n\n\n\n\nI, J, K, L\nN, O, P, Q, R\nT, E, F, G, H\nB, C\nA, S\nU, V\nD\n\n\n\n\nI, J, K, L\n0\n0\n0\n1\n0\n0\n0\n\n\nN, O, P, Q, R\n0\n0\n0\n1\n0\n0\n0\n\n\nT, E, F, G, H\n0\n0\n0\n0\n1\n0\n0\n\n\nB, C\n1\n1\n0\n0\n0\n0\n1\n\n\nA, S\n0\n0\n1\n0\n0\n1\n1\n\n\nU, V\n0\n0\n0\n0\n1\n0\n0\n\n\nD\n0\n0\n0\n1\n1\n0\n0\n\n\n\n\n\n\n\nTable 5: Image matrix corresponding to the blockmodel of an undirected graph.\n\n\n\n\nSo the big blocked adjacency matrix in Table 1 (c) can be reduced to the image matrix shown Table 5, summarizing the relations between the blocks in the graph. This matrix, can then even be represented as a graph, so that we can see the pattern of relations between blocks! This is shown in Figure 4\n\n    gr &lt;- graph_from_adjacency_matrix(c) %&gt;% \n      as_tbl_graph() %&gt;% \n      activate(nodes) %&gt;% \n      mutate(names =  c(\"I, J, K, L\", \"N, O, P, Q, R\", \"T, E, F, G, H\", \"B, C\", \"A, S\", \"U, V\", \"D\"))\n    p &lt;- ggraph(gr, layout = 'tree') + \n         geom_edge_link(color = \"black\", width = 1.15)  + \n         geom_node_label(aes(label = names), size = c(5, 5, 5, 7, 7, 5, 10))\n    p &lt;- p + theme_graph()\n    p\n\n\n\n\n\n\n\nFigure 4: Graph representation of reduced image matrix from a blockmodel.\n\n\n\n\n\nThis is how blockmodeling works!\nNote that the final plot is done using tidygraph. You can try yourself to plot it with ggraph instead."
  },
  {
    "objectID": "teaching/sna/material/04/04-descriptives-2-bm.html#footnotes",
    "href": "teaching/sna/material/04/04-descriptives-2-bm.html#footnotes",
    "title": "Social Network Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis worksheet is inspired and adapted from this source↩︎"
  },
  {
    "objectID": "teaching/sna/index.html",
    "href": "teaching/sna/index.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "The course text book can be found here: R4SNA (work in progress)\nSchedule\n\n\n\n\nslides\npractical\ndata\nworksheet\n\n\n\n\n1: Introduction\n\n\n\n.qmd\n\n\n2: The Language of Networks\n\n\n\n.qmd\n\n\n3: Network Concepts and Descriptives I\n\n\n.zip\n.qmd\n\n\n4: Network Concepts and Descriptives II\n\n \n\n.qmd\n\n\n5: Beyond ‘Standard’ Networks\n\n \n.zip\n.qmd\n\n\n6: Network Visualization I\n\n\n\n.qmd\n\n\n7: Network Visualization II\n\n\n\n.qmd\n\n\n8: Network Modelling: Introduction\n\n\n\n.qmd\n\n\n9: Random Graph Models\n\n\n\n.qmd\n\n\n10: Exponential Random Graph Models (ERGMs)\n\n \n.zip\n.qmd .qmd\n\n\n11: Stochastic Actor Oriented Models (SAOMs)\n\n \n.zip .zip\n.qmd .qmd\n\n\n\n\n\n\n\nR Packages\nThroughout the course we will use a variety of different packages of doing network analysis, modeling and visualization. Make sure to install them all and have them ready to load when needed:\n\ninstall.packages(\"igraph\")   \ninstall.packages(\"statnet\")  #installs ergm, network, and sna\ninstall.packages(\"snahelper\")\ninstall.packages(\"netUtils\")\ninstall.packages(\"ggraph\")\ninstall.packages(\"backbone\")\ninstall.packages(\"netrankr\")\ninstall.packages(\"signnet\")\ninstall.packages(\"egor\")\ninstall.packages(\"intergraph\")\ninstall.packages(\"graphlayouts\")\ninstall.packages(\"visNetwork\")\ninstall.packages(\"patchwork\")\ninstall.packages(\"edgebundle\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"gganimate\")\ninstall.packages(\"ggforce\")\ninstall.packages(\"rsiena\")\n\ninstall.packages(\"remotes\")\nremotes::install_github(\"schochastics/networkdata\")"
  },
  {
    "objectID": "project/index.html",
    "href": "project/index.html",
    "title": "Projects",
    "section": "",
    "text": "Statistical Entropy Analysis of Network Data\n\n\nIn this project, a general framework for using statistical entropies to capture interdependencies among node and tie variables in multivariate networks is developed.\n\n\n\n\n\n\n\n\n\n\n\n\nMultigraph Representation of Network Data\n\n\nThe exploratory and confirmatory statistical analysis of multivariate social networks represented as multigraphs.\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork of Interconnected Convoys\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGender Dependent Structures in Charachter Networks\n\n\nUsing network analysis to analyze gender representation in popular cinema.\n\n\n\n\n\n\n\n\n\n\n\n\nNEXUS1492 - Reconstructing Archaeological Networks\n\n\nReconstructing Archaeological Networks And Their Transformations Across The Historical Divide.\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "project/seand/index.html",
    "href": "project/seand/index.html",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "",
    "text": "Termeh Shafie"
  },
  {
    "objectID": "project/seand/index.html#project-summary",
    "href": "project/seand/index.html#project-summary",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "Project summary",
    "text": "Project summary\nIn multivariate statistics, there is an abundance of different measures of centrality and spread, many of which cannot be applied on variables measured on nominal or ordinal scale. Since network data in majority comprises such variables, alternative measures for analyzing spread, flatness and association is needed. This is also of particular relevance given the special feature of interdependent observations in networks.\nMultivariate entropy analysis is a general statistical method for analyzing and finding dependence structure in data consisting of repeated observations of variables with a common domain and with discrete finite range spaces. Only nominal scale is required for each variable, so only the size of the variable’s range space is important but not its actual values. Variables on ordinal or numerical scales, even continuous numerical scales, can be used, but they should be aggregated so that their ranges match the number of available repeated observations. By investigating the frequencies of occurrences of joint variable outcomes, complicated dependence structures, partial independence and conditional independence as well as redundancies and functional dependence can be found.\nSince 2015, I am working with Ove Frank and Krzysztof Nowicki on a project in which we build a systematic framework for using statistical entropy tools to analyze network data.\nThe proposed framework is implemented in the R package ‘netropy’ and a description of various functions implemented in the package are given in the following. More details are provided in the package vignettes and the references listed."
  },
  {
    "objectID": "project/seand/index.html#r-package-netropy",
    "href": "project/seand/index.html#r-package-netropy",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "R package netropy",
    "text": "R package netropy\n\n\nPackage overview\n  \nThis package introduces these entropy tools in the context of network data. Brief description of various functions implemented in the package are given in the following but more details are provided in the package vignettes and the references listed.\n\n\nInstallation\nYou can install the released version of netropy from CRAN with:\ninstall.packages(\"netropy\")\nThe development version from GitHub with:\n# install.packages(\"devtools\")\ndevtools::install_github(\"termehs/netropy\")\nTo load the package:\n\nlibrary('netropy')\n\n\n\nLoading internal data\nThe different entropy tools are explained and illustrated by exploring data from a network study of a corporate law firm, which has previously been analysed by several authors (link). The data set is included in the package as a list with objects representing adjacency matrices for each of the three networks advice (directed), friendship (directed) and co-work (undirected), together with a data frame comprising 8 attributes on each of the 71 lawyers.\nTo load the data, extract each object and assign the correct names to them:\n\ndata(lawdata) \nadj.advice &lt;- lawdata[[1]]\nadj.friend &lt;- lawdata[[2]]\nadj.cowork &lt;-lawdata[[3]]\ndf.att &lt;- lawdata[[4]]"
  },
  {
    "objectID": "project/seand/index.html#variable-domains-and-data-editing",
    "href": "project/seand/index.html#variable-domains-and-data-editing",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "Variable domains and data editing",
    "text": "Variable domains and data editing\nA requirement for the applicability of these entropy tools is the specification of discrete variables with finite range spaces on the same domain: either node attributes/vertex variables, edges/dyad variables or triad variables. These can be either observed or transformed as shown in the following using the above example data set.\nWe have 8 vertex variables with 71 observations, two of which (years and age) are numerical and needs categorization based on their cumulative distributions. This categorization is in details described in the vignette “variable domains and data editing”. Here we just show the new dataframe created (note that variable senior is omitted as it only comprises unique values and that we edit all variable to start from 0):\n\natt.var &lt;-\n  data.frame(\n    status   = df.att$status-1,\n    gender   = df.att$gender,\n    office   = df.att$office-1,\n    years    = ifelse(df.att$years &lt;= 3,0,\n                      ifelse(df.att$years &lt;= 13,1,2)),\n    age      = ifelse(df.att$age &lt;= 35,0,\n                      ifelse(df.att$age &lt;= 45,1,2)),\n    practice = df.att$practice,\n    lawschool= df.att$lawschool-1\n    )\nhead(att.var)\n\n  status gender office years age practice lawschool\n1      0      1      0     2   2        1         0\n2      0      1      0     2   2        0         0\n3      0      1      1     1   2        1         0\n4      0      1      0     2   2        0         2\n5      0      1      1     2   2        1         1\n6      0      1      1     2   2        1         0\n\n\nThese vertex variables can be transformed into dyad variables by using the function get_dyad_var(). Observed node attributes in the dataframe att_var are then transformed into pairs of individual attributes. For example, status with binary outcomes is transformed into dyads having 4 possible outcomes (0,0), (0,1), (1,0), (1,1):\n\ndyad.status    &lt;- get_dyad_var(att.var$status, type = 'att')\ndyad.gender    &lt;- get_dyad_var(att.var$gender, type = 'att')\ndyad.office    &lt;- get_dyad_var(att.var$office, type = 'att')\ndyad.years     &lt;- get_dyad_var(att.var$years, type = 'att')\ndyad.age       &lt;- get_dyad_var(att.var$age, type = 'att')\ndyad.practice  &lt;- get_dyad_var(att.var$practice, type = 'att')\ndyad.lawschool &lt;- get_dyad_var(att.var$lawschool, type = 'att')\n\nSimilarly, dyad variables can be created based on observed ties. For the undirected edges, we use indicator variables read directly from the adjacency matrix for the dyad in question, while for the directed ones (advice and friendship) we have pairs of indicators representing sending and receiving ties with 4 possible outcomes :\n\ndyad.cwk    &lt;- get_dyad_var(adj.cowork, type = 'tie')\ndyad.adv    &lt;- get_dyad_var(adj.advice, type = 'tie')\ndyad.frn    &lt;- get_dyad_var(adj.friend, type = 'tie')\n\nAll 10 dyad variables are merged into one data frame for subsequent entropy analysis:\n\ndyad.var &lt;-\n  data.frame(cbind(status   = dyad.status$var,\n                  gender    = dyad.gender$var,\n                  office    = dyad.office$var,\n                  years     = dyad.years$var,\n                  age       = dyad.age$var,\n                  practice  = dyad.practice$var,\n                  lawschool = dyad.lawschool$var,\n                  cowork    = dyad.cwk$var,\n                  advice    = dyad.adv$var,\n                  friend    = dyad.frn$var)\n                  )\nhead(dyad.var)\n\n  status gender office years age practice lawschool cowork advice friend\n1      3      3      0     8   8        1         0      0      3      2\n2      3      3      3     5   8        3         0      0      0      0\n3      3      3      3     5   8        2         0      0      1      0\n4      3      3      0     8   8        1         6      0      1      2\n5      3      3      0     8   8        0         6      0      1      1\n6      3      3      1     7   8        1         6      0      1      1\n\n\nA similar function get_triad_var() is implemented for transforming vertex variables and different relation types into triad variables. This is described in more detail in the vignette “variable domains and data editing”."
  },
  {
    "objectID": "project/seand/index.html#univariate-bivariate-and-trivariate-entropies",
    "href": "project/seand/index.html#univariate-bivariate-and-trivariate-entropies",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "Univariate, bivariate and trivariate entropies",
    "text": "Univariate, bivariate and trivariate entropies\nThe function entropy_bivar() computes the bivariate entropies of all pairs of variables in the dataframe. The output is given as an upper triangular matrix with cells giving the bivariate entropies of row and column variables. The diagonal thus gives the univariate entropies for each variable in the dataframe:\n\nH2 &lt;- entropy_bivar(dyad.var)\nH2\n\n          status gender office years   age practice lawschool cowork advice\nstatus     1.493  2.868  3.640 3.370 3.912    3.453     4.363  2.092  2.687\ngender        NA  1.547  3.758 3.939 4.274    3.506     4.439  2.158  2.785\noffice        NA     NA  2.239 4.828 4.901    4.154     5.058  2.792  3.388\nyears         NA     NA     NA 2.671 4.857    4.582     5.422  3.268  3.868\nage           NA     NA     NA    NA 2.801    4.743     5.347  3.411  4.028\npractice      NA     NA     NA    NA    NA    1.962     4.880  2.530  3.127\nlawschool     NA     NA     NA    NA    NA       NA     2.953  3.567  4.186\ncowork        NA     NA     NA    NA    NA       NA        NA  0.615  1.687\nadvice        NA     NA     NA    NA    NA       NA        NA     NA  1.248\nfriend        NA     NA     NA    NA    NA       NA        NA     NA     NA\n          friend\nstatus     2.324\ngender     2.415\noffice     3.044\nyears      3.483\nage        3.637\npractice   2.831\nlawschool  3.812\ncowork     1.456\nadvice     1.953\nfriend     0.881\n\n\nBivariate entropies can be used to detect redundant variables that should be omitted from the dataframe for further analysis. This occurs when the univariate entropy for a variable is equal to the bivariate entropies for pairs including that variable. As seen above, the dataframe dyad.var has no redundant variables. This can also be checked using the function redundancy() which yields a binary matrix as output indicating which row and column variables are hold the same information:\n\nredundancy(dyad.var)\n\nNULL\n\n\nMore examples of using the function redundancy() is given in the vignette “univariate bivariate and trivariate entropies”.\nTrivariate entropies can be computed using the function entropy_trivar() which returns a dataframe with the first three columns representing possible triples of variables V1,V2, and V3 from the dataframe in question, and their entropies H(V1,V2,V3) as the fourth column. We illustrated this on the dataframe dyad.var:\n\nH3 &lt;- entropy_trivar(dyad.var)\nhead(H3, 10) # view first 10 rows of dataframe\n\n       V1     V2        V3 H(V1,V2,V3)\n1  status gender    office       4.938\n2  status gender     years       4.609\n3  status gender       age       5.129\n4  status gender  practice       4.810\n5  status gender lawschool       5.664\n6  status gender    cowork       3.464\n7  status gender    advice       4.048\n8  status gender    friend       3.685\n9  status office     years       5.321\n10 status office       age       5.721"
  },
  {
    "objectID": "project/seand/index.html#joint-entropy-and-association-graphs",
    "href": "project/seand/index.html#joint-entropy-and-association-graphs",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "Joint entropy and association graphs",
    "text": "Joint entropy and association graphs\nJoint entropies is a non-negative measure of association among pairs of variables. It is equal to 0 if and only if two variables are completely independent of each other.\nThe function joint_entropy() computes the joint entropies between all pairs of variables in a given dataframe and returns a list consisting of the upper triangular joint entropy matrix (univariate entropies in the diagonal) and a dataframe giving the frequency distributions of unique joint entropy values. A function argument specifies the precision given in number of decimals for which the frequency distribution of unique entropy values is created (default is 3). Applying the function on the dataframe dyad.var with two decimals:\n\nJ &lt;- joint_entropy(dyad.var, 2)\nJ$matrix\n\n          status gender office years  age practice lawschool cowork advice\nstatus      1.49   0.17   0.09  0.79 0.38     0.00      0.08   0.02   0.05\ngender        NA   1.55   0.03  0.28 0.07     0.00      0.06   0.00   0.01\noffice        NA     NA   2.24  0.08 0.14     0.05      0.13   0.06   0.10\nyears         NA     NA     NA  2.67 0.61     0.05      0.20   0.02   0.05\nage           NA     NA     NA    NA 2.80     0.02      0.41   0.01   0.02\npractice      NA     NA     NA    NA   NA     1.96      0.04   0.05   0.08\nlawschool     NA     NA     NA    NA   NA       NA      2.95   0.00   0.01\ncowork        NA     NA     NA    NA   NA       NA        NA   0.62   0.18\nadvice        NA     NA     NA    NA   NA       NA        NA     NA   1.25\nfriend        NA     NA     NA    NA   NA       NA        NA     NA     NA\n          friend\nstatus      0.05\ngender      0.01\noffice      0.08\nyears       0.07\nage         0.05\npractice    0.01\nlawschool   0.02\ncowork      0.04\nadvice      0.18\nfriend      0.88\n\nJ$freq\n\n      j  #(J = j) #(J &gt;= j)\n1  0.79         1         1\n2  0.61         1         2\n3  0.41         1         3\n4  0.38         1         4\n5  0.28         1         5\n6   0.2         1         6\n7  0.18         2         8\n8  0.17         1         9\n9  0.14         1        10\n10 0.13         1        11\n11  0.1         1        12\n12 0.09         1        13\n13 0.08         4        17\n14 0.07         2        19\n15 0.06         2        21\n16 0.05         7        28\n17 0.04         2        30\n18 0.03         1        31\n19 0.02         5        36\n20 0.01         5        41\n21    0         4        45\n\n\nAs seen, the strongest association is between the variables status and years with joint entropy values of 0.79. We have independence (joint entropy value of 0) between two pairs of variables: (status,practice), (practise,gender), (cowork,gender),and (cowork,lawschool).\nThese results can be illustrated in a association graph using the function assoc_graph() which returns a ggraph object in which nodes represent variables and links represent strength of association (thicker links indicate stronger dependence). To use the function we need to load the ggraph library and to determine a threshold which the graph drawn is based on. We set it to 0.15 so that we only visualize the strongest associations\n\nlibrary(ggraph)\nassoc_graph(dyad.var, 0.15)\n\n\n\n\n\n\n\n\nGiven this threshold, we see isolated and disconnected nodes representing independent variables. We note strong dependence between the three dyadic variables status,years and age, but also a somewhat strong dependence among the three variables lawschool, years and age, and the three variables status, years and gender. The association graph can also be interpreted as a tendency for relations cowork and friend to be independent conditionally on relation advice, that is, any dependence between dyad variables cowork and friend is explained by advice.\nA threshold that gives a graph with reasonably many small independent or conditionally independent subsets of variables can be considered to represent a multivariate model for further testing.\nMore details and examples of joint entropies and association graphs are given in the vignette “joint entropies and association graphs”."
  },
  {
    "objectID": "project/seand/index.html#prediction-power-based-on-expected-conditional-entropies",
    "href": "project/seand/index.html#prediction-power-based-on-expected-conditional-entropies",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "Prediction power based on expected conditional entropies",
    "text": "Prediction power based on expected conditional entropies\nThe function prediction_power() computes prediction power when pairs of variables in a given dataframe are used to predict a third variable from the same dataframe. The variable to be predicted and the dataframe in which this variable also is part of is given as input arguments, and the output is an upper triangular matrix giving the expected conditional entropies of pairs of row and column variables (denoted \\(X\\) and \\(Y\\)) of the matrix, i.e. EH(Z|X,Y) where \\(Z\\) is the variable to be predicted. The diagonal gives EH(Z|X) , that is when only one variable as a predictor. Note that NA’s are in the row and column representing the variable being predicted.\nAssume we are interested in predicting variable status (that is whether a lawyer in the data set is an associate or partner). This is done by running the following syntax\n\nprediction_power('status', dyad.var)\n\n          status gender office years   age practice lawschool cowork advice\nstatus        NA     NA     NA    NA    NA       NA        NA     NA     NA\ngender        NA  1.375  1.180 0.670 0.855    1.304     1.225  1.306  1.263\noffice        NA     NA  2.147 0.493 0.820    1.374     1.245  1.373  1.325\nyears         NA     NA     NA 2.265 0.573    0.682     0.554  0.691  0.667\nage           NA     NA     NA    NA 1.877    1.089     0.958  1.087  1.052\npractice      NA     NA     NA    NA    NA    2.446     1.388  1.459  1.410\nlawschool     NA     NA     NA    NA    NA       NA     3.335  1.390  1.337\ncowork        NA     NA     NA    NA    NA       NA        NA  2.419  1.400\nadvice        NA     NA     NA    NA    NA       NA        NA     NA  2.781\nfriend        NA     NA     NA    NA    NA       NA        NA     NA     NA\n          friend\nstatus        NA\ngender     1.270\noffice     1.334\nyears      0.684\nage        1.058\npractice   1.427\nlawschool  1.350\ncowork     1.411\nadvice     1.407\nfriend     3.408\n\n\nFor better readability, the powers of different predictors can be conveniently compared by using prediction plots that display a color matrix with rows for \\(X\\) and columns for \\(Y\\) with darker colors in the cells when we have higher prediction power for \\(Z\\).\nMore details and examples of expected conditional entropies and prediction power are given in the package vignette."
  },
  {
    "objectID": "project/seand/index.html#divergence-tests-of-goodness-of-fit",
    "href": "project/seand/index.html#divergence-tests-of-goodness-of-fit",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "Divergence Tests of Goodness of Fit",
    "text": "Divergence Tests of Goodness of Fit\nOccurring cliques in association graphs represent connected components of dependent variables, and by comparing the graphs for different thresholds, specific structural models of multivariate dependence can be suggested and tested. The function div_gof() allows such hypothesis tests for pairwise independence of \\(X\\) and \\(Y\\): \\(X \\bot Y\\), and pairwise independence conditional a third variable \\(Z\\): \\(X\\bot Y|Z\\).\nTo test friend\\(\\bot\\) cowork\\(|\\)advice, that is whether dyad variable friend is independent of cowork given advice we use the function as shown below:\n\ndiv_gof(dat = dyad.var, var1 = \"friend\", var2 = \"cowork\", var_cond = \"advice\")\n\n     D df(D)\n1 0.94    12\n\n\nNot specifying argument var_cond would instead test friend\\(\\bot\\)cowork without any conditioning."
  },
  {
    "objectID": "project/seand/index.html#references",
    "href": "project/seand/index.html#references",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "References",
    "text": "References\nParts of the theoretical background is provided in the package vignettes, but for more details, consult the following literature:\n\nFrank, O., & Shafie, T. (2016). Multivariate entropy analysis of network data. Bulletin of Sociological Methodology/Bulletin de Méthodologie Sociologique, 129(1), 45-63. link"
  },
  {
    "objectID": "project/convoys/index.html",
    "href": "project/convoys/index.html",
    "title": "Network of Interconnected Convoys",
    "section": "",
    "text": "Termeh Shafie"
  },
  {
    "objectID": "project/convoys/index.html#more-info-coming-soon",
    "href": "project/convoys/index.html#more-info-coming-soon",
    "title": "Network of Interconnected Convoys",
    "section": "More info coming soon",
    "text": "More info coming soon"
  },
  {
    "objectID": "blog/posts/test/index.html",
    "href": "blog/posts/test/index.html",
    "title": "Hello World",
    "section": "",
    "text": "Termeh Shafie"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Termeh Shafie",
    "section": "",
    "text": "👋 Hi, I’m Termeh\nI’m a statistician with more than a decade of experience in developing methods and models to analyze multivariate social networks. I’m also the developer and maintainer of two R packages on the topic.\nInterdisciplinary applications are close to heart and I have had many such collaborations in different branches of computational social science and digital humanities. You can find information on some of the methodological and empirical projects that I have worked on or currently work on here.\nA bit of random trivia about me: my Erdős number is 3 and I have an irrational obsession with Pokémon."
  },
  {
    "objectID": "about/index.html#education",
    "href": "about/index.html#education",
    "title": "Termeh Shafie",
    "section": "Education",
    "text": "Education\n\nPhD in Statistics | 2013 | Stockholm University | Stockholm, Sweden\nLicentiate of Philosophy in Statistic | 2008 | Umeå University | Umeå Sweden\nMaster of Social Science (major in Statistics) | 2006 | Umeå University | Umeå Sweden\nMaster of Science in Public Administration and Economics | 2006 | Umeå University | Umeå Sweden"
  },
  {
    "objectID": "about/index.html#experience",
    "href": "about/index.html#experience",
    "title": "Termeh Shafie",
    "section": "Experience",
    "text": "Experience\n\nProfessor of Computational Social Science and Data Science (W2) | July 2023 – | Department of Politics and Public Administration | Center for Data and Methods | University of Konstanz\nInterim Professor | Apr 2023 – July 2023 | Department of Politics and Public Administration | University of Konstanz\nSenior Researcher | Jul 2022 – Mar 2023 | Department of Computational Social Science GESIS – Leibniz Institute for the Social Sciences\nLecturer in Social Statistics | Jul 2018 – Apr 2022 | Department of Social Statistics | The University of Manchester\nPostdoctoral Researcher | Nov 2017 – Jun 2018 | Department of Humanities, Social and Political Sciences | ETH Zürich\nPostdoctoral Researcher | Sep 2013 – Oct 2017 | Department of Computer and Information Science | University of Konstanz"
  },
  {
    "objectID": "talks/EUSN2021/index.html",
    "href": "talks/EUSN2021/index.html",
    "title": "Multiplexity Analysis of Networks using Multigraph Representations",
    "section": "",
    "text": "The 5th European Conference on Social Networks\nTermeh Shafie, David Schoch\n\nSep 7, 2021\n\nAbstract\nMultivariate networks comprising several compositional and structural variables can be represented as multigraphs by various forms of aggregations based on vertex attributes. We propose a framework to perform exploratory and confirmatory multiplexity analysis of aggregated multigraphs in order to find relevant associations between vertex and edge attributes. The exploration is performed by comparing the frequencies of the different edges within and between aggregated vertex categories, while the confirmatory analysis is performed using derived complexity or multiplexity statistics under different random multigraph models. These statistics are defined by the distribution of edge multiplicities and provide information on the covariation and dependencies of different edges given vertex attributes. The presented approach highlights the need to further analyse and model structural dependencies with respect to edge entrainment. We illustrate the approach by applying it on a well known multivariate network dataset which has previously been analysed in the context of multiplexity.\n\n\n\n\n\nSlides"
  },
  {
    "objectID": "talks/CMB2022/index.html",
    "href": "talks/CMB2022/index.html",
    "title": "“Analyzing Social Structure using Multigraph Representations”",
    "section": "",
    "text": "Presentation @ Centre Marc Bloch, Berlin\nTermeh Shafie\n\nNov 2, 2022\n\nAbstract\nMultivariate networks consist of a vertex set with at least one type of edge between pairs of vertices and with numerical and/or qualitative attributes on the vertices and the edges. These networks provide a more accurate representation of social structure than univariate networks, but analysing them introduce technical and computational complexity. In this presentation, we consider exploratory and confirmatory analysis of multivariate networks represented as multigraphs. Multigraph data structure is described with examples of their natural appearance, together with a description of the possibility to obtain multigraphs using blocking, aggregation and scaling. Two random multigraph models are presented and several statistics under these models are derived. It is shown how these statistics can be used to analyse local and global network properties in order to convey important social phenomena and processes. Applications are used to illustrate the applicability of the presented approach, including when analysing the gendered inequalities in popular cinema by using dialogue networks. Moreover, some examples are provided on how to use the package ‘multigraphr’ in the statistical software R to perform the analysis.\n\n\n\n\n\nSlides"
  },
  {
    "objectID": "talks/Inaugural-KN/index.html",
    "href": "talks/Inaugural-KN/index.html",
    "title": "Statistical Analysis & Modeling of Multivariate Networks",
    "section": "",
    "text": "Inaugural Lecture, University of Konstanz\nTermeh Shafie\n\nJun 5, 2024\n\n\n\n\nSlides"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "“The Interplay of Structural Features and Observed Dissimilarities Among Centrality Indices”\n\n\nThe XLIV Social Networks Conference of INSNA\n\n\n\nJun 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Analysis & Modeling of Multivariate Networks\n\n\nInaugural Lecture, University of Konstanz\n\n\n\nJun 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Analyzing Social Structure using Multigraph Representations”\n\n\nPresentation @ Centre Marc Bloch, Berlin\n\n\n\nNov 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Statistical Analysis of Multivariate Egocentric Networks”\n\n\nThe XLII Social Networks Conference of INSNA\n\n\n\nJul 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Statistical Entropy Analysis of Network Data”\n\n\nThe Women in Network Science (WiNS) seminar\n\n\n\nMay 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiplexity Analysis of Networks using Multigraph Representations\n\n\nThe 5th European Conference on Social Networks\n\n\n\nSep 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Goodness of Fit Tests for Random Multigraph Models”\n\n\nNetwork 2021 - A Joint Sunbelt and NetSci Conference\n\n\n\nJul 6, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Gender Dependent Structures of Dialogue Networks in Films”\n\n\nThe 4th European Conference on Social Networks\n\n\n\nSep 9, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/ontario_sna/index.html",
    "href": "publications/ontario_sna/index.html",
    "title": "Nation Building and Social Signaling in Southern Ontario AD 1350-1650",
    "section": "",
    "text": "John P Hart, Termeh Shafie, Jennifer Birch, Susan Dermarkar, Ronald F Williamson\n\n2016-05-25"
  },
  {
    "objectID": "publications/ontario_sna/index.html#abstract",
    "href": "publications/ontario_sna/index.html#abstract",
    "title": "Nation Building and Social Signaling in Southern Ontario AD 1350-1650",
    "section": "Abstract",
    "text": "Abstract\nPottery is a mainstay of archaeological analysis worldwide. Often, high proportions of the pottery recovered from a given site are decorated in some manner. In northern Iroquoia, late pre-contact pottery and early contact decoration commonly occur on collars—thick bands of clay that encircle a pot and extend several centimeters down from the lip. These decorations constitute signals that conveyed information about a pot’s user(s). In southern Ontario the period A.D. 1350 to 1650 witnessed substantial changes in socio-political and settlement systems that included population movement, coalescence of formerly separate communities into large villages and towns, waxing and waning of regional strife, the formation of nations, and finally the development of three confederacies that each occupied distinct, constricted areas. Social network analysis demonstrates that signaling practices changed to reflect these regional patterns. Networks become more consolidated through time ultimately resulting in a “small world” network with small degrees of separation between sites reflecting the integration of communities within and between the three confederacies.\nPLoS ONE 11(5), e0156178"
  },
  {
    "objectID": "publications/isotopes/index.html",
    "href": "publications/isotopes/index.html",
    "title": "Investigating Human Geographic Origins using Dual-Isotope (87Sr/86Sr, d18O) Assignment approaches",
    "section": "",
    "text": "Jason E Laffoon, Till F Sonnemann, Termeh Shafie, Corinne L Hofman, Ulrik Brandes, Gareth R Davies\n\n2017-02-25"
  },
  {
    "objectID": "publications/isotopes/index.html#abstract",
    "href": "publications/isotopes/index.html#abstract",
    "title": "Investigating Human Geographic Origins using Dual-Isotope (87Sr/86Sr, d18O) Assignment approaches",
    "section": "Abstract",
    "text": "Abstract\nSubstantial progress in the application of multiple isotope analyses has greatly improved the ability to identify nonlocal individuals amongst archaeological populations over the past decades. More recently the development of large scale models of spatial isotopic variation (isoscapes) has contributed to improved geographic assignments of human and animal origins. Persistent challenges remain, however, in the accurate identification of individual geographic origins from skeletal isotope data in studies of human (and animal) migration and provenance. In an attempt to develop and test more standardized and quantitative approaches to geographic assignment of individual origins using isotopic data two methods, combining 87Sr/86Sr and d18O isoscapes, are examined for the Circum-Caribbean region 1) an Interval approach using a defined range of fixed isotopic variation per location and 2) a Likelihood assignment approach using univariate and bivariate probability density functions. These two methods are tested with enamel isotope data from a modern sample of known origin from Caracas, Venezuela and further explored with two archaeological samples of unknown origin recovered from Cuba and Trinidad. The results emphasize both the potential and limitation of the different approaches. Validation tests on the known origin sample exclude most areas of the Circum-Caribbean region and correctly highlight Caracas as a possible place of origin with both approaches. The positive validation results clearly demonstrate the overall efficacy of a dual-isotope approach to geoprovenance. The accuracy and precision of geographic assignments may be further improved by better understanding of the relationships between environmental and biological isotope variation; continued development and refinement of relevant isoscapes; and the eventual incorporation of a broader array of isotope proxy data.\nPLoS ONE 12(2), e0172562"
  },
  {
    "objectID": "publications/data_protection/index.html",
    "href": "publications/data_protection/index.html",
    "title": "Data Protection for Online Social Networks and P-Stability for Graphs",
    "section": "",
    "text": "Vicenç Torra, Termeh Shafie, Julián Salas\n\n2016-01-01"
  },
  {
    "objectID": "publications/data_protection/index.html#abstract",
    "href": "publications/data_protection/index.html#abstract",
    "title": "Data Protection for Online Social Networks and P-Stability for Graphs",
    "section": "Abstract",
    "text": "Abstract\nGraphs can be used as a model for online social networks. In this framework, vertices represent individuals and edges relationships between individuals. In recent years, different approaches have been considered to offer data privacy to online social networks and for developing graph protection. Perturbative approaches are formally defined in terms of perturbation and modification of graphs. In this paper, we discuss the concept of P -stability on graphs and its relation to data privacy. The concept of P-stability is rooted in the number of graphs given a fixed degree sequence. In this paper, we show that for any graph there exists a class of P-stable graphs. This result implies that there is a fully polynomial randomized approximation for graph masking for the graphs in the class. In order to further refine the classification of a given graph, we introduce the concept of natural class of a graph. It is based on a class of scale-free networks.\nIEEE Transactions on Emerging Topics in Computing 4(3), 374-381"
  },
  {
    "objectID": "publications/global_local_multigraphs/index.html",
    "href": "publications/global_local_multigraphs/index.html",
    "title": "Analyzing Local and Global Properties of Multigraphs",
    "section": "",
    "text": "Termeh Shafie\n\n2016-09-25"
  },
  {
    "objectID": "publications/global_local_multigraphs/index.html#abstract",
    "href": "publications/global_local_multigraphs/index.html#abstract",
    "title": "Analyzing Local and Global Properties of Multigraphs",
    "section": "Abstract",
    "text": "Abstract\nThe local structure of undirected multigraphs under two random multigraph models is analyzed and compared. The first model generates multigraphs by randomly coupling pairs of stubs according to a fixed degree sequence so that edge assignments to vertex pair sites are dependent. The second model is a simplification that ignores the dependency between the edge assignments. It is investigated when this ignorance is justified so that the simplified model can be used as an approximation, thus facilitating the structural analysis of network data with multiple relations and loops. The comparison is based on the local properties of multigraphs given by marginal distribution of edge multiplicities and some local properties that are aggregations of global properties.\nJournal of Mathematical Sociology 40(4), 239-264"
  },
  {
    "objectID": "publications/pa_complexsystems/index.html",
    "href": "publications/pa_complexsystems/index.html",
    "title": "A complex systems view on physical activity with actionable insights for behaviour change",
    "section": "",
    "text": "Julia Schüler, Matti T. J. Heino, Natàlia Balagué, Angel M. Chater, Markus Gruber, Martina Kanning, Daniel Keim, Daniela Mier, Maria Moreno-Villanueva, Fridtjof W. Nussbeck, Jens Pruessner, Termeh Shafie, Michael Schwenk, Maik Bieleke\n\n2025-08-04"
  },
  {
    "objectID": "publications/pa_complexsystems/index.html#abstract",
    "href": "publications/pa_complexsystems/index.html#abstract",
    "title": "A complex systems view on physical activity with actionable insights for behaviour change",
    "section": "Abstract",
    "text": "Abstract\nPhysical inactivity and its associated health and economic burdens continue to rise despite decades of interdisciplinary research aimed at promoting physical activity. This Perspective takes a complex systems view on physical activity, proposing that at least two layers of complexity should be considered: (1) interactions between various physiological, psychological, social and environmental systems; and (2) their dynamic interactions across time. To address this complexity, all stages of the research process—from theory and measurement to study design, analysis and interventions—must be aligned with a complex systems perspective. This alignment requires intensive interdisciplinary collaboration and an integration of basic and applied research beyond current research practices to create transdisciplinary solutions. We offer actionable insights that bridge the gap between abstract theoretical approaches (for example, complex systems and attractor landscape frameworks of behaviour change) and practical research on physical activity, thereby laying a foundation for more effective behaviour change interventions.\nNature Human Behaviour"
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "A complex systems view on physical activity with actionable insights for behaviour change\n\n\nA perspective that takes a complex systems view on physical activity.\n\n\n\n\n\n\n\n\n\n\n\n\nThe interplay of structural features and observed dissimilarities among centrality indices\n\n\nThe association of network topology with dissimilarities of indices is assessed\n\n\n\n\n\n\n\n\n\n\n\n\nGoodness of fit tests for random multigraph models\n\n\nGoodness of fit tests for different probability models for random multigraphs.\n\n\n\n\n\n\n\n\n\n\n\n\nMultiplexity Analysis of Networks using Multigraph Representations\n\n\nA method for performing multiplexity analysis in social networks with several node covariates is presented.\n\n\n\n\n\n\n\n\n\n\n\n\nSocial Network Analysis\n\n\nA review chapter on social network analysis aimed towards undergraduate students.\n\n\n\n\n\n\n\n\n\n\n\n\nA Framework for Reconstructing Archaeological Networks using Exponential Random Graph Models\n\n\nWe present a general framework in which we combine exponential random graph models with archaeological substantiations of mechanisms for network formation.\n\n\n\n\n\n\n\n\n\n\n\n\nReconstructing Archaeological Networks with Structural Holes\n\n\nWe consider exponential random graph models, and show how they can be applied to reconstruct networks coherent with Burt’s arguments on closure and structural holes.\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Multigraphs and Aggregated Triads with Fixed Degrees\n\n\nNew combinatorial results are given for the global probability distribution of edge multiplicities and its marginal local distributions of loops and edges.\n\n\n\n\n\n\n\n\n\n\n\n\nHypergraph Representations: A Study of Carib Attacks on Colonial Forces, 1509-1700\n\n\nWe perform a centrality analysis of a directed hypergraph representing attacks by indigenous peoples from the Lesser Antilles on European colonial settlements, 1509-1700.\n\n\n\n\n\n\n\n\n\n\n\n\nInvestigating Human Geographic Origins using Dual-Isotope (87Sr/86Sr, d18O) Assignment approaches\n\n\nWe develop and test more standardized and quantitative approaches to geographic assignment of individual origins using multivariate isotopic data.\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Local and Global Properties of Multigraphs\n\n\nThe local and global structures of undirected multigraphs under two random multigraph models are analyzed and compared.\n\n\n\n\n\n\n\n\n\n\n\n\nNation Building and Social Signaling in Southern Ontario AD 1350-1650\n\n\nSocial network analysis is used to demonstrates the signaling practices reflecting regional patterns.\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariate Entropy Analysis of Network Data\n\n\nWe show how it is possible to systematically check for tendencies in data, such as independencies or conditional independencies, using multivariate entropies.\n\n\n\n\n\n\n\n\n\n\n\n\nData Protection for Online Social Networks and P-Stability for Graphs\n\n\nWe consider different approaches for data privacy in online social networks and for developing graph protection.\n\n\n\n\n\n\n\n\n\n\n\n\nA Multigraph Approach to Social Network Analysis\n\n\nThe theoretical background for analyzing multivariate social networks using multigraph representations is introduced.\n\n\n\n\n\n\n\n\n\n\n\n\nComplexity of Families of Multigraphs\n\n\nComplexity measured for multigraphs are specified and their applicability is discussed.\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/aggregated_triads/index.html",
    "href": "publications/aggregated_triads/index.html",
    "title": "Random Multigraphs and Aggregated Triads with Fixed Degrees",
    "section": "",
    "text": "Ove Frank, Termeh Shafie\n\n2018-01-25"
  },
  {
    "objectID": "publications/aggregated_triads/index.html#abstract",
    "href": "publications/aggregated_triads/index.html#abstract",
    "title": "Random Multigraphs and Aggregated Triads with Fixed Degrees",
    "section": "Abstract",
    "text": "Abstract\nRandom multigraphs with fixed degrees are obtained by the configuration model or by so called random stub matching. New combinatorial results are given for the global probability distribution of edge multiplicities and its marginal local distributions of loops and edges. The number of multigraphs on triads is determined for arbitrary degrees, and aggregated triads are shown to be useful for analyzing regular and almost regular multigraphs. Relationships between entropy and complexity are given and numerically illustrated for multigraphs with different number of vertices and specified average and variance for the degrees.\nNetwork Science 6(2), 232-250"
  },
  {
    "objectID": "publications/interplay_str_cent/index.html",
    "href": "publications/interplay_str_cent/index.html",
    "title": "The interplay of structural features and observed dissimilarities among centrality indices",
    "section": "",
    "text": "David Schoch, Termeh Shafie\n\n2023-12-06"
  },
  {
    "objectID": "publications/interplay_str_cent/index.html#abstract",
    "href": "publications/interplay_str_cent/index.html#abstract",
    "title": "The interplay of structural features and observed dissimilarities among centrality indices",
    "section": "Abstract",
    "text": "Abstract\nAn abundance of centrality indices has been proposed which capture the importance of nodes in a network based on different structural features. While there remains a persistent belief that similarities in outcomes of indices is contingent on their technical definitions, a growing body of research shows that structural features affect observed similarities more than technicalities. We conduct a series of experiments on artificial networks to trace the influence of specific structural features on the similarity of indices which confirm previous results in the literature. Our analysis on 1163 real-world networks, however, shows that little of the observations on synthetic networks convincingly carry over to empirical settings. Our findings suggest that although it seems clear that (dis)similarities among centralities depend on structural properties of the network, using correlation type analyses do not seem to be a promising approach to uncover such connections.\nSocial Networks"
  },
  {
    "objectID": "publications/reconstructing_arch_nets/index.html",
    "href": "publications/reconstructing_arch_nets/index.html",
    "title": "Reconstructing Archaeological Networks with Structural Holes",
    "section": "",
    "text": "Viviana Amati, Termeh Shafie, Ulrik Brandes\n\n2018-03-25"
  },
  {
    "objectID": "publications/reconstructing_arch_nets/index.html#abstract",
    "href": "publications/reconstructing_arch_nets/index.html#abstract",
    "title": "Reconstructing Archaeological Networks with Structural Holes",
    "section": "Abstract",
    "text": "Abstract\nModel-based reconstruction is an approach to infer network structures where they cannot be observed. For archaeological networks, several models based on assumptions concerning distance among sites, site size, or costs and benefits have been proposed to infer missing ties. Since these assumptions are formulated at a dyadic level, they do not provide means to express dependencies among ties and therefore include less plausible network scenarios. In this paper we investigate the use of network models that explicitly incorporate tie dependence. In particular, we consider exponential random graph models, and show how they can be applied to reconstruct networks coherent with Burt’s arguments on closure and structural holes (Burt 2001). The approach is illustrated on data from the Middle Bronze Age in the Aegean. authors:\nJournal of Archaeological Method and Theory 25(1), 226-253"
  },
  {
    "objectID": "publications/multivariate_entropy_analysis/index.html",
    "href": "publications/multivariate_entropy_analysis/index.html",
    "title": "Multivariate Entropy Analysis of Network Data",
    "section": "",
    "text": "Over Frank, Termeh Shafie\n\n2016-01-02"
  },
  {
    "objectID": "publications/multivariate_entropy_analysis/index.html#abstract",
    "href": "publications/multivariate_entropy_analysis/index.html#abstract",
    "title": "Multivariate Entropy Analysis of Network Data",
    "section": "Abstract",
    "text": "Abstract\nMultigraphs with numerical or qualitative attributes defined on vertices and edges can benefit from systematic methods based on multivariate entropies for describing and analysing the interdependencies that are present between vertex and edge attributes. This is here illustrated by application of these tools to a subset of data on the social relations among Renaissance Florentine families collected by John Padgett. Using multivariate entropies we show how it is possible to systematically check for tendencies in data that can be described as independencies or conditional independencies, or as dependencies allowing certain combinations of variables to predict other variables. We also show how different structural models can be tested by divergence measures obtained from the multivariate entropies.\nBulletin of Sociological Methodology/Bulletin de Méthodologie Sociologique 120(1), 45-63"
  },
  {
    "objectID": "publications/multiplexity/index.html",
    "href": "publications/multiplexity/index.html",
    "title": "Multiplexity Analysis of Networks using Multigraph Representations",
    "section": "",
    "text": "Termeh Shafie, David Schoch\n\n2021-09-30"
  },
  {
    "objectID": "publications/multiplexity/index.html#abstract",
    "href": "publications/multiplexity/index.html#abstract",
    "title": "Multiplexity Analysis of Networks using Multigraph Representations",
    "section": "Abstract",
    "text": "Abstract\nMultivariate networks comprising several compositional and structural variables can be represented as multigraphs by various forms of aggregations based on vertex attributes. We propose a framework to perform exploratory and confirmatory multiplexity analysis of aggregated multigraphs in order to find relevant associations between vertex and edge attributes. The exploration is performed by comparing frequencies of the different edges within and between aggregated vertex categories, while the confirmatory analysis is performed using derived complexity or multiplexity statistics under different random multigraph models. These statistics are defined by the distribution of edge multiplicities and provide information on the covariation and dependencies of different edges given vertex attributes. The presented approach highlights the need to further analyse and model structural dependencies with respect to edge entrainment. We illustrate the approach by applying it on a well known multivariate network dataset which has previously been analysed in the context of multiplexity.\npublication: Statistical Methods & Applications 30, 1425–1444"
  },
  {
    "objectID": "publications/hypergraph/index.html",
    "href": "publications/hypergraph/index.html",
    "title": "Hypergraph Representations: A Study of Carib Attacks on Colonial Forces, 1509-1700",
    "section": "",
    "text": "Termeh Shafie, David Schoch, Jimmy Mans, Corinne Hofman, Ulrik Brandes\n\n2017-10-25"
  },
  {
    "objectID": "publications/hypergraph/index.html#abstract",
    "href": "publications/hypergraph/index.html#abstract",
    "title": "Hypergraph Representations: A Study of Carib Attacks on Colonial Forces, 1509-1700",
    "section": "Abstract",
    "text": "Abstract\nNetwork data consisting of recorded historical events can be represented as hyper-graphs where the ties or events can connect any number of nodes or event related attributes. In this paper, we perform a centrality analysis of a directed hypergraph representing attacks by indigenous peoples from the Lesser Antilles on European colonial settlements, 1509-1700. The results of central attacks with respect to at- tacked colonial force, member of attack alliances, and year and location of attack are discussed and compared to a non-relational exploratory analysis of the data. This comparison points to the importance of a mixed methods approach to enhance the analysis and to obtain a complementary understanding of a network study.\npublication: ‘Journal of Historical Network Research 1(1), 52-70’"
  },
  {
    "objectID": "publications/gof_multigraph/index.html",
    "href": "publications/gof_multigraph/index.html",
    "title": "Goodness of fit tests for random multigraph models",
    "section": "",
    "text": "Termeh Shafie\n\n2022-07-21"
  },
  {
    "objectID": "publications/gof_multigraph/index.html#abstract",
    "href": "publications/gof_multigraph/index.html#abstract",
    "title": "Goodness of fit tests for random multigraph models",
    "section": "Abstract",
    "text": "Abstract\nGoodness of fit tests for two probabilistic multigraph models are presented. The first model is random stub matching given fixed degrees (RSM) so that edge assignments to vertex pair sites are dependent, and the second is independent edge assignments (IEA) according to a common probability distribution. Tests are performed using goodness of fit measures between the edge multiplicity sequence of an observed multigraph, and the expected one according to a simple or composite hypothesis. Test statistics of Pearson type and of likelihood ratio type are used, and the expected values of the Pearson statistic under the different models are derived. Test performances based on simulations indicate that even for small number of edges, the null distributions of both statistics are well approximated by their asymptotic χ2-distribution. The non-null distributions of the test statistics can be well approximated by proposed adjusted χ2-distributions used for power approximations. The influence of RSM on both test statistics is substantial for small number of edges and implies a shift of their distributions towards smaller values compared to what holds true for the null distributions under IEA. Two applications on social networks are included to illustrate how the tests can guide in the analysis of social structure.\nJournal of Applied Statistics"
  },
  {
    "objectID": "publications/complexity_multigraphs/index.html",
    "href": "publications/complexity_multigraphs/index.html",
    "title": "Complexity of Families of Multigraphs",
    "section": "",
    "text": "Ove Frank, Termeh Shafie\n\n2012-12-01"
  },
  {
    "objectID": "publications/complexity_multigraphs/index.html#abstract",
    "href": "publications/complexity_multigraphs/index.html#abstract",
    "title": "Complexity of Families of Multigraphs",
    "section": "Abstract",
    "text": "Abstract\nThis article describes families of finite multigraphs with labeled or unlabeled edges and vertices. It shows how size and complexity vary for different types of equivalence classes of graphs defined by ignoring only edge labels or ignoring both edge and vertex labels. Complexity is quantified by the distribution of edge multiplicities, and different complexity measures are discussed. Basic occupancy models for multigraphs are used to illustrate different graph distributions on isomorphism and complexity. The loss of information caused by ignoring edge and vertex labels is quantified by entropy and joint information that provide tools for studying properties of and relations between different graph families.\nIn 2012 JSM Proceedings: Papers Presented at the Joint Statistical Meetings, San Diego, California, July 28-August 2, 2012, and Other ASA-sponsored Conferences, American Statistical Association, 2012"
  },
  {
    "objectID": "publications/sna_chapter/index.html",
    "href": "publications/sna_chapter/index.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "Termeh Shafie\n\n2020-06-17"
  },
  {
    "objectID": "publications/sna_chapter/index.html#abstract",
    "href": "publications/sna_chapter/index.html#abstract",
    "title": "Social Network Analysis",
    "section": "Abstract",
    "text": "Abstract\nSocial networks comprise a set of nodes or actors (representing, e.g., individuals, groups, organisations) that are pairwise connected by edges or ties (representing, e.g., relationships, interactions, communication). The social systems arising exhibit patterns of interest, and social network analysis is the study of how and why these patterns emerge, sustain, and evolve. Of primary interest is thus to understand and describe the social processes that support the observed structure. These processes are founded in theories about network representation and theories about observed social phenomena. The benefit from network conceptualisation is thus obtained by outlining the association and distinction between these theories. This entry serves as an introduction to fundamental network concepts and analytical approaches, their potential for studying social phenomena, and a description of why they are central to theoretical constructs. This entry also provides a short introduction to statistical network modelling for cross-sectional and longitudinal network data.\nSAGE Research Methods Foundations"
  },
  {
    "objectID": "publications/multigraph_approach/index.html",
    "href": "publications/multigraph_approach/index.html",
    "title": "A Multigraph Approach to Social Network Analysis",
    "section": "",
    "text": "Termeh Shafie\n\n2015-06-01"
  },
  {
    "objectID": "publications/multigraph_approach/index.html#abstract",
    "href": "publications/multigraph_approach/index.html#abstract",
    "title": "A Multigraph Approach to Social Network Analysis",
    "section": "Abstract",
    "text": "Abstract\nMultigraphs are graphs where multiple edges and edge loops are permitted. The main purpose of this article is to show the versatility of a multigraph approach when analysing social networks. Multigraph data structures are described and it is exemplified how they naturally occur in many contexts but also how they can be constructed by different kinds of aggregation in graphs. Special attention is given to a random multigraph model based on independent edge assignments to sites of vertex pairs and some useful measures of the local and global structure under this model are presented. Further, it is shown how some general measures of simplicity and complexity of multigraphs are easily handled under the present model.\n‘Journal of Social Structure 16, 1-22’"
  },
  {
    "objectID": "publications/ergm_framework_arch/index.html",
    "href": "publications/ergm_framework_arch/index.html",
    "title": "A Framework for Reconstructing Archaeological Networks using Exponential Random Graph Models",
    "section": "",
    "text": "Viviana Amati, Angus Mol, Termeh Shafie, Corinne Hofman, Ulrik Brandes\n\n2019-08-25"
  },
  {
    "objectID": "publications/ergm_framework_arch/index.html#abstract",
    "href": "publications/ergm_framework_arch/index.html#abstract",
    "title": "A Framework for Reconstructing Archaeological Networks using Exponential Random Graph Models",
    "section": "Abstract",
    "text": "Abstract\nReconstructing ties between archaeological contexts may contribute to explain and describe a variety of past social phenomena. Several models have been formulated to infer the structure of such archaeological networks. The underlying propositions about mechanisms regulating the formation of ties in the past are often articulated on a dyadic basis and therefore rarely account for dependencies among ties. Here, we present a general framework in which we combine exponential random graph models with archaeological substantiations of mechanisms that may be responsible for network formation to account for tie dependence. We use data collected over a set of sites in the Caribbean during the period AD 100 - 400 to illustrate the steps to obtain a network reconstruction.\nJournal of Archaeological Method and Theory 27, 192-219"
  },
  {
    "objectID": "talks/WiNS2022/index.html",
    "href": "talks/WiNS2022/index.html",
    "title": "“Statistical Entropy Analysis of Network Data”",
    "section": "",
    "text": "The Women in Network Science (WiNS) seminar\nTermeh Shafie\n\nMay 5, 2022\n\nAbstract\nIn multivariate statistics, there is an abundance of different measures of centrality and spread, many of which cannot be applied on variables measured on nominal or ordinal scale. Since network data in majority comprises such variables, alternative measures for analysing spread, flatness and association is needed. This is also of particular relevance given the special feature of interdependent observations in networks. In this presentation, multivariate entropy analysis is introduced and demonstrated as a general statistical method for finding, analysing and testing complicated dependence structures such as partial and conditional independencies, redundancies and functional dependencies. For example, consider the joint entropies of all pairs of variables which are used to construct a sequence of association graphs that represent variables by nodes and pairwise dependences above decreasing thresholds by links (cf. graphical models). By successively lowering the threshold from the maximum joint entropy to smaller occurring values, the sequence of graphs get more and more links. Connected components that are cliques represent dependent subsets of variables, and different components represent independent subsets of variables. Conditional independence between subsets of variables can be identified by omitting the subset corresponding to the conditioning variables. By comparing such graphs given different thresholds and with different components and cliques, specific structural models of multivariate dependence can be suggested and tested by divergence measures of goodness of fit. The roles of various entropy based measures are further highlighted and illustrated by applications on social network data. These applications show that important social phenomena and processes are often identified using these tools. The proposed framework is implemented in the R package ‘netropy’ and examples of using functions from this package are also presented.\n\n\n\n\n\nSlides\n\n\nExamples\n\n\nVideo"
  },
  {
    "objectID": "talks/NETWORKS2021/index.html",
    "href": "talks/NETWORKS2021/index.html",
    "title": "“Goodness of Fit Tests for Random Multigraph Models”",
    "section": "",
    "text": "Network 2021 - A Joint Sunbelt and NetSci Conference\nTermeh Shafie\n\nJul 6, 2021\n\nAbstract\nGoodness of fit tests for two probabilistic multigraph models are presented. The first model is obtained by random stub matching given fixed degrees (RSM) so that edge assignments to vertex pair sites are dependent, and the second is obtained by independent edge assignments (IEA) according to a common probability distribution. The tests are performed using goodness of fit measures between the edge multiplicity sequence of an observed multigraph, and the expected multiplicity sequence according to a simple or composite hypothesis. Test statistics of Pearson type and of likelihood ratio type are used, and the expected values of the Pearson statistic under the different models are derived. Illustrations of test performances are given and the results indicate that even for small number of edges, the null distributions of both statistics are well approximated by their asymptotic χ2-distribution. The non-null distributions of the test statistics can be well approximated by proposed adjusted χ2-distributions used for power approximations. The influence of RSM on both test statistics is substantial for small number of edges and implies a shift of their distributions towards smaller values compared to what holds true for the null distributions under IEA.\n\n\n\n\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "talks/SUNBELT2022/index.html",
    "href": "talks/SUNBELT2022/index.html",
    "title": "“Statistical Analysis of Multivariate Egocentric Networks”",
    "section": "",
    "text": "The XLII Social Networks Conference of INSNA\nTermeh Shafie\n\nJul 15, 2022\n\nAbstract\nIn this presentation we take a closer look at how multigraph representations (where multiple relations and edge loops are permitted) can be used to analyse structural and compositional features of egocentric networks with perceived alter-to-alter ties included. We aggregate the ego nets based on single or combined exogenous variables and several measures are used to uncover underlying social processes and phenomena with respect to the endogenous variables. Using theoretical underpinnings, we specify hypotheses that are tested using statistics under different probability models for multigraphs. The framework is exemplified and applied to the well-known data on Krackhardt’s high tech managers.\n\n\n\n\n\nSlides"
  },
  {
    "objectID": "talks/EUSN2019/index.html",
    "href": "talks/EUSN2019/index.html",
    "title": "“Gender Dependent Structures of Dialogue Networks in Films”",
    "section": "",
    "text": "The 4th European Conference on Social Networks\nTermeh Shafie, Pete Jones\n\nSep 9, 2019\n\nAbstract\nWe present a novel approach to empirically analyse gender representation in films by using entropy tools. Multivariate entropy analysis is a general statistical method for analysing and testing compli- cated dependence structures in data consisting of repeated observations of variables with a common domain and discrete finite range spaces. Only nominal scale is required for each variable, thus making it very suitable for extracting semantic information from dialogues and scripts. Variables on ordinal or numerical scales can also be used, but they should be aggregated so that their ranges match the number of available repeated observations. Using a corpus of movie dialogue networks, we illustrate how these tools can enhance the analysis of gender representations in films by identifying factors associated with certain tropes of gendered conversation. The observed dialogue networks are transformed into a multidimensional data set comprising multiple node and edge attributes, where the latter is coded according to conversational content between pairs of same sex characters. Under the assumptions that associations between conversational content are shared by members of the same gender group, dialogues are used to reflect a meaning structure that characterize this group (stereotypical and non-stereotypical gender portrayals). Furthermore, we use the output of the entropy analyses to specify different structural models to be tested against null distributions according to stochastic blockmodels and random multigraph models. This application yields a deeper insight into the gendered nature of dialogue in film, an area in which little systematic empirical research exists. Moreover, the multivariate entropy analysis allows for the relationships between textual attributes, production-level attributes and endogenous network attributes to be explored together. This provides a novel opportunity to situate the character networks within their industrial context in a way that provides a new layer to the development of network tools for analysing fictional narrative texts.\n\n\n\n\n\nSlides"
  },
  {
    "objectID": "talks/SUNBELT2024/index.html",
    "href": "talks/SUNBELT2024/index.html",
    "title": "“The Interplay of Structural Features and Observed Dissimilarities Among Centrality Indices”",
    "section": "",
    "text": "The XLIV Social Networks Conference of INSNA\nTermeh Shafie\n\nJun 29, 2024\n\nAbstract\nAn abundance of centrality indices has been proposed which capture the importance of nodes in a network based on different structural features. While there remains a persistent belief that similarities in outcomes of indices is contingent on their technical definitions, a growing body of research shows that structural features affect observed similarities more than technicalities. We conduct a series of experiments on artificial networks to trace the influence of specific structural features on the similarity of indices which confirm previous results in the literature. Our analysis on 1163 real-world networks, however, shows that little of the observations on synthetic networks convincingly carry over to empirical settings. Our findings suggest that although it seems clear that (dis)similarities among centralities depend on structural properties of the network, using correlation type analyses do not seem to be a promising approach to uncover such connections.\n\n\n\n\n\nSlides"
  },
  {
    "objectID": "index.html#general",
    "href": "index.html#general",
    "title": "Termeh Shafie",
    "section": "General",
    "text": "General\n\n09.09.2024: Netropy 0.2.0\nThe new version of netropy is published on CRAN.\n\n\n05.06.2024: Inaugural Professorial Lecture 🤓\nI’ll be giving my inaugural professorial lecture. which is open for anyone who wants join. Expect a lot of big words, personal anecdotes, some mildly amusing jokes, and maybe even a technical hiccup or two 🤓📚✨  Join afterward for finger food and drinks 🥂  📅 Date: June 5th 📍 Venue: Data Theatre ZT1204, University of Konstanz\n\n\n20.02.2024: Multigraphr 0.2.0\nThe new version of multigraphr is published on CRAN. No user facing changes, just improved performance thanks to David Schoch.\n\n\n10.08.2023: Maternity Leave 🐣\nOn parental leave until end of the year.\n\n\n27.07.2023: Professor of Computational Social Science and Data Science\nToday I start my new position as a Professor at the Department of Politics and Public Administration, University of Konstanz."
  },
  {
    "objectID": "index.html#recent-publications",
    "href": "index.html#recent-publications",
    "title": "Termeh Shafie",
    "section": "Recent Publications",
    "text": "Recent Publications\n\n\n\n\n\n\nTitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nA complex systems view on physical activity with actionable insights for behaviour change\n\n\nAug 4, 2025\n\n\n\n\n\n\nThe interplay of structural features and observed dissimilarities among centrality indices\n\n\nDec 6, 2023\n\n\n\n\n\n\nGoodness of fit tests for random multigraph models\n\n\nJul 21, 2022\n\n\n\n\n\n\nNo matching items\n\n\nall publications"
  },
  {
    "objectID": "index.html#latest-talks",
    "href": "index.html#latest-talks",
    "title": "Termeh Shafie",
    "section": "Latest Talks",
    "text": "Latest Talks\n\n\n\n\n\n\n\n\n\n\n“The Interplay of Structural Features and Observed Dissimilarities Among Centrality Indices”\n\n\n\n\n\nJun 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Analysis & Modeling of Multivariate Networks\n\n\n\n\n\nJun 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n“Statistical Analysis of Multivariate Egocentric Networks”\n\n\n\n\n\nJul 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n“Statistical Entropy Analysis of Network Data”\n\n\n\n\n\nMay 5, 2022\n\n\n\n\n\nNo matching items\n\n\nmore talks"
  },
  {
    "objectID": "index.html#r-packages",
    "href": "index.html#r-packages",
    "title": "Termeh Shafie",
    "section": "R packages",
    "text": "R packages\n\n\n\n\n\n\n\n\n\nLatest published version 0.2.0 on CRAN is up to date.\n\n\n\n\n\n\n\n\n\n\n\nLatest published version 0.2.0 on CRAN is up to date.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#github-activity",
    "href": "index.html#github-activity",
    "title": "Termeh Shafie",
    "section": "GitHub Activity",
    "text": "GitHub Activity"
  },
  {
    "objectID": "index.html#social-media",
    "href": "index.html#social-media",
    "title": "Termeh Shafie",
    "section": "Social Media",
    "text": "Social Media\n\n \n\n    Bluesky\n\n\n\n \n\n\n\n\n    Toots"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "",
    "section": "",
    "text": "Hello World\n\n\n\ntest\n\n\n\nThis is a test entry…\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "project/movienetworks/index.html",
    "href": "project/movienetworks/index.html",
    "title": "Gender Dependent Structures in Charachter Networks",
    "section": "",
    "text": "Termeh Shafie"
  },
  {
    "objectID": "project/movienetworks/index.html#project-summary",
    "href": "project/movienetworks/index.html#project-summary",
    "title": "Gender Dependent Structures in Charachter Networks",
    "section": "Project Summary",
    "text": "Project Summary\nSince 2019, I’m sporadically working on a project with Pete Jones which aims to apply existing and novel quantitative methods (e.g. relational event models, entropy based natural language processing and multigraph representations), to study the gendered inequalities in popular cinema. See Pete’s website for more information and some awesome packages such as charinet which he has developed on the topic."
  },
  {
    "objectID": "project/rmm/index.html",
    "href": "project/rmm/index.html",
    "title": "Multigraph Representation of Network Data",
    "section": "",
    "text": "Termeh Shafie"
  },
  {
    "objectID": "project/rmm/index.html#project-summary",
    "href": "project/rmm/index.html#project-summary",
    "title": "Multigraph Representation of Network Data",
    "section": "Project summary",
    "text": "Project summary\nMultigraphs are network representations in which multiple edges and edge loops (self edges) are permitted. Multigraph data structure can be observed directly but the possibility to obtain multigraphs using blocking, aggregation and scaling makes them widely applicable.\nFor the past decade, I’ve been working on a statistical framework for analyzing network data using this representation. I have developed different multigraph multigraph models and derived several statistics under these models that can be used to analyse local and global network properties in order to convey important social phenomena and processes. The latest contribution in this framework is formal goodness of fit tests for the developed probability models for random multigraphs.\nThe proposed framework is in full implemented in the R package ‘multigraphr’ and a description of various functions implemented in the package are given in the following. More details are provided in the package vignetteand the references listed."
  },
  {
    "objectID": "project/rmm/index.html#r-package-multigraphr",
    "href": "project/rmm/index.html#r-package-multigraphr",
    "title": "Multigraph Representation of Network Data",
    "section": "R package multigraphr",
    "text": "R package multigraphr\n\nPackage overview\n  \nThis package introduces the multigraph framework for analyzing social network data. Brief description of various functions implemented in the package are given in the following but more details are provided in the package vignettes and the references listed.\n\n\nInstallation\nYou can install the released version of multigraphr from CRAN with:\ninstall.packages(\"multigraphr\")\nThe development version from GitHub with:\n# install.packages(\"devtools\")\ndevtools::install_github(\"termehs/multigraphr\")"
  },
  {
    "objectID": "project/rmm/index.html#multigraphs-and-applicability",
    "href": "project/rmm/index.html#multigraphs-and-applicability",
    "title": "Multigraph Representation of Network Data",
    "section": "Multigraphs and applicability",
    "text": "Multigraphs and applicability\nMultigraphs are network representations in which multiple edges and edge loops (self edges) are permitted. These data structures can be either directly observed or aggregated by classifying or cross-classifying node attributes into meta nodes. For the latter case, within group edges correspond to self-edges. See example below where the original graph with 15 nodes and 12 edges (left) is aggregated based on node categories into a small multigraph with 4 nodes (right).\n\nEdge aggregation can also be used to obtain multigraphs. Assume that we study a graph with three different types of relations over three periods of time: \nIf we aggregate over time periods, we obtain for each edge category a multigraph for the total time period of three days:\n\nFor more details on these kinds of aggregations, see Shafie (2015;2016)."
  },
  {
    "objectID": "project/rmm/index.html#multigraph-representation-of-network-data",
    "href": "project/rmm/index.html#multigraph-representation-of-network-data",
    "title": "Multigraph Representation of Network Data",
    "section": "Multigraph representation of network data",
    "text": "Multigraph representation of network data\nMultigraphs are represented by their edge multiplicity sequence M with elements M(i,j), denoting the number of edges at vertex pair sites (i,j) ordered according to (1,1) &lt; (1,2) &lt;···&lt; (1,n) &lt; (2,2) &lt; (2,3) &lt;···&lt; (n,n), where n is number of nodes. The number of vertex pair sites is given by r = n(n+1)/2.\n\nRandom multigraph models\nTwo probability models for generating undirected random multigraphs are implemented in the package together with several statistics under these two models. Moreover, functions for goodness of fit tests are available for the presented models.\nNote that some of the functions are only practical for small scale multigraphs.\nThe first model is obtained by random stub matching (RSM) given observed degree sequence of a multigraphs, so that edge assignments to vertex pair sites are dependent. The second is obtained by independent edge assignments (IEA) according to a common probability distribution. There are two ways in which an approximate IEA model can be obtained from an RSM model, thus facilitating the structural analysis. These two ways are\n\nindependent stub assignment (ISA)\nindependent edge assignment of stubs (IEAS)\n\n(Shafie, 2016).\n\n\nExample\n\nlibrary('multigraphr')\n\nConsider a small graph on 3 nodes and the following adjacency matrix:\n\nA &lt;-  matrix(c(1, 1, 0, \n               1, 2, 2, \n               0, 2, 0), \n             nrow = 3, ncol = 3)\nA\n\n     [,1] [,2] [,3]\n[1,]    1    1    0\n[2,]    1    2    2\n[3,]    0    2    0\n\n\nThe degree sequence of the multigraph has double counted diagonals (edge stubs for loops) and is given by\n\nD &lt;- get_degree_seq(adj = A, type = 'graph')\nD\n\n[1] 3 7 2\n\n\nso that number of edges in the multigraph is half the sum of the degree sequence which is equal to 6.\nThe RSM model given observed degree sequence shows the sample space consists of 7 possible multigraphs, as represented by their multiplicity sequence m.seq (each row correspond to the edge multiplicity sequence of a unique multigraph):\n\nrsm_1 &lt;- rsm_model(deg.seq = D)\nrsm_1$m.seq\n\n  M11 M12 M13 M22 M23 M33\n1   1   1   0   3   0   1\n2   1   1   0   2   2   0\n3   1   0   1   3   1   0\n4   0   3   0   2   0   1\n5   0   3   0   1   2   0\n6   0   2   1   2   1   0\n7   0   1   2   3   0   0\n\n\nwith probabilities associated with each multigraph, together with statistics ‘number of loops’, ‘number of multiple edges’ and ‘simple graphs or not’:\n\nrsm_1$prob.dists\n\n    prob.rsm loops multiedges simple\n1 0.03030303     5          1      0\n2 0.18181818     3          3      0\n3 0.06060606     4          2      0\n4 0.06060606     3          3      0\n5 0.24242424     1          5      0\n6 0.36363636     2          4      0\n7 0.06060606     3          3      0\n\n\nConsider using the IEA model to approximate the RSM model so that edge assignment probabilities are functions of observed degree sequence. Note that the sample space for multigraphs is much bigger than for the RSM model so the multiplicity sequences are not printed (they can be found using the function get_edgemultip_seq for very small multigraphs and their probabilities can be found using the multinomial distribution). The following shows the number of multigraphs under either of the IEA models:\n\nieas_1 &lt;-   iea_model(adj = A , type = 'graph',  model = 'IEAS', K = 0, apx = TRUE)\nieas_1$nr.multigraphs\n\n[1] 462"
  },
  {
    "objectID": "project/rmm/index.html#statistics-to-analyze-structural-properties",
    "href": "project/rmm/index.html#statistics-to-analyze-structural-properties",
    "title": "Multigraph Representation of Network Data",
    "section": "Statistics to analyze structural properties",
    "text": "Statistics to analyze structural properties\nThese statistics include number of loops (indicator of e.g. homophily) and number of multiple edges (indicator of e.g. multiplexity/interlocking), which are implemented in the package together with their probability distributions, moments and interval estimates under the different multigraph models.\n\nExample (cont’d)\nUnder the RSM model, the first two moments and interval estimates of the statistics M1 = ‘number of loops’ and M2 = ‘number of multiple edges’ are given by\n\nrsm_1$M\n\n             M1    M2\nExpected  2.273 3.727\nVariance  0.986 0.986\nUpper 95% 4.259 5.713\nLower 95% 0.287 1.741\n\n\nwhich are calculated using the numerically found probability distributions under RSM (no analytical solutions exist for these moments).\nUnder the IEA models (IEAS or ISA), moments of these statistics, together with the complexity statistic \\(R_k\\) representing the sequence of frequencies of edge sites with multiplicities 0,1,…,k, are found using derived formulas. Thus, there is no limit on multigraph size to use these. When the IEAS model is used to approximate the RSM model as shown above:\n\nieas_1$M\n\n              M1    M2\nObserved   3.000 3.000\nExpected   2.273 3.727\nVariance   1.412 1.412\nUpper 95%  4.649 6.104\nLower 95% -0.104 1.351\n\nieas_1$R\n\n             R0     R1     R2\nObserved  2.000  2.000  2.000\nExpected  2.674  1.588  1.030\nVariance  0.575  1.129  0.760\nUpper 95% 4.191  3.713  2.773\nLower 95% 1.156 -0.537 -0.713\n\n\nWhen the ISA model is used to approximate the RSM model (see above):\n\nisa_1 &lt;-   iea_model(adj = A , type = 'graph',  \n                     model = 'ISA', K = 0, apx = TRUE)\nisa_1$M\n\n             M1    M2\nObserved  3.000 3.000\nExpected  2.583 3.417\nVariance  1.471 1.471\nUpper 95% 5.009 5.842\nLower 95% 0.158 0.991\n\nisa_1$R\n\n             R0     R1     R2\nObserved  2.000  2.000  2.000\nExpected  2.599  1.703  1.018\nVariance  0.622  1.223  0.748\nUpper 95% 4.176  3.915  2.748\nLower 95% 1.021 -0.509 -0.711\n\n\nThe IEA models can also be used independent of the RSM model. For example, the IEAS model can be used where edge assignment probabilities are estimated using the observed edge multiplicities (maximum likelihood estimates):\n\nieas_2 &lt;-   iea_model(adj = A , type = 'graph',  \n                      model = 'IEAS', K = 0, apx = FALSE)\nieas_2$M\n\n             M1    M2\nObserved  3.000 3.000\nExpected  3.000 3.000\nVariance  1.500 1.500\nUpper 95% 5.449 5.449\nLower 95% 0.551 0.551\n\nieas_2$R\n\n             R0     R1     R2\nObserved  2.000  2.000  2.000\nExpected  2.845  1.331  1.060\nVariance  0.434  0.805  0.800\nUpper 95% 4.163  3.125  2.849\nLower 95% 1.528 -0.464 -0.729\n\n\nThe ISA model can also be used independent of the RSM model. Then, a sequence containing the stub assignment probabilities (for example based on prior belief) should be given as argument:\n\nisa_2 &lt;-   iea_model(adj = A , type = 'graph',  \n                     model = 'ISA', K = 0, apx = FALSE, p.seq = c(1/3, 1/3, 1/3))\nisa_2$M\n\n              M1    M2\nObserved   3.000 3.000\nExpected   2.000 4.000\nVariance   1.333 1.333\nUpper 95%  4.309 6.309\nLower 95% -0.309 1.691\n\nisa_2$R\n\n             R0     R1     R2\nObserved  2.000  2.000  2.000\nExpected  2.144  2.248  1.160\nVariance  0.632  1.487  0.710\nUpper 95% 3.734  4.687  2.845\nLower 95% 0.554 -0.190 -0.525\n\n\nThe interval estimates can then be visualized to detect discrepancies between observed and expected values thus indicating social mechanisms at play in the generation of edges, and to detect interval overlap and potential interdependence between different types of edges (see Shafie 2015,2016; Shafie & Schoch 2021)."
  },
  {
    "objectID": "project/rmm/index.html#goodness-of-fit-tests",
    "href": "project/rmm/index.html#goodness-of-fit-tests",
    "title": "Multigraph Representation of Network Data",
    "section": "Goodness of fit tests",
    "text": "Goodness of fit tests\nGoodness of fits tests of multigraph models using Pearson (S) and information divergence (A) test statistics under the random stub matching (RSM) and by independent edge assignments (IEA) model, where the latter is either independent edge assignments of stubs (IEAS) or independent stub assignment (ISA). The tests are performed using goodness-of-fit measures between the edge multiplicity sequence of a specified model or an observed multigraph, and the expected multiplicity sequence according to a simple or composite hypothesis."
  },
  {
    "objectID": "project/rmm/index.html#simulated-goodness-of-fit-tests",
    "href": "project/rmm/index.html#simulated-goodness-of-fit-tests",
    "title": "Multigraph Representation of Network Data",
    "section": "Simulated goodness of fit tests",
    "text": "Simulated goodness of fit tests\nProbability distributions of test statistics, summary of tests, moments of tests statistics, adjusted test statistics, critical values, significance level according to asymptotic distribution, and power of tests can be examined using gof_sim given a specified model from which we simulate observed values from, and a null or non-null hypothesis from which we calculate expected values from. This in order to investigate the behavior of the null and non-null distributions of the test statistics and their fit to to asymptotic chi-square distributions.\n\nExample\nSimulated goodness of fit tests for multigraphs with n=4 nodes and m=10 edges.\n(1) Testing a simple IEAS hypothesis with degree sequence (6,6,6,2) against a RSM model with degrees (8,8,2,2):\n\ngof1 &lt;- gof_sim(m = 10, model = 'IEAS', deg.mod = c(8,8,2,2), \n                hyp = 'IEAS', deg.hyp = c(6,6,6,2))\n\n(2) Testing a correctly specified simple IEAS hypothesis with degree sequence (14,2,2,2):\n\ngof2 &lt;- gof_sim(m = 10, model = 'IEAS', deg.mod = c(14,2,2,2), \n                hyp = 'IEAS', deg.hyp = c(14,2,2,2))\n\nThe non-null (gof1) and null (gof2) distributions of the test statistics together with their asymptotic chi2-distribution can be visualized using ggplot2:\n \n(3) Testing a composite IEAS hypothesis against a RSM model with degree sequence (14,2,2,2):\n\ngof3 &lt;- gof_sim(m = 10, model = 'RSM', deg.mod = c(14,2,2,2), \n                hyp = 'IEAS', deg.hyp = 0)\n\n(4) Testing a composite ISA hypothesis against a ISA model with degree sequence (14,2,2,2):\n\ngof4 &lt;- gof_sim(m = 10, model = 'ISA', deg.mod = c(14,2,2,2), \n                hyp = 'ISA', deg.hyp = 0)\n\nThe non-null (gof3) and null (gof4) distributions of the test statistics can then be visualized as shown above to check their fit to the asymptotic χ²-distribution."
  },
  {
    "objectID": "project/rmm/index.html#performing-the-goodness-of-fit-test-on-your-data",
    "href": "project/rmm/index.html#performing-the-goodness-of-fit-test-on-your-data",
    "title": "Multigraph Representation of Network Data",
    "section": "Performing the goodness of fit test on your data",
    "text": "Performing the goodness of fit test on your data\nUse function gof_test to test whether the observed data follows IEA approximations of the RSM model. The null hypotheses can be simple or composite, although the latter is not recommended for small multigraphs as it is difficult to detect a false composite hypothesis under an RSM model and under IEA models (this can be checked and verified using gof_sim to simulate these cases).\nNon-rejection of the null implies that the approximations fit the data, thus implying that above statistics under the IEA models can be used to further analyze the observed network. Consider the following multigraph from the well known Florentine family network with marital. This multigraphs is aggregated based on the three actor attributes wealth (W), number of priorates (P) and total number of ties (T) which are all dichotomized to reflect high or low economic, political and social influence (details on the aggregation can be found in Shafie, 2015):\n\nThe multiplicity sequence represented as an upper triangular matrix for this mutigrpah is given by\n\nflor_m &lt;- t(matrix(c (0, 0, 1, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 2, 0, 0, 1, 5,\n                      0, 0, 0, 0, 0, 0, 1, 1,\n                      0, 0, 0, 0, 0, 0, 1, 2,\n                      0, 0, 0, 0, 0, 0, 2, 1,\n                      0, 0, 0, 0, 0, 0, 0, 2,\n                      0, 0, 0, 0, 0, 0, 0, 1), nrow= 8, ncol=8))\n\nThe equivalence of adjacency matrix for the multigraph is given by\n\nflor_adj &lt;- flor_m+t(flor_m)\nflor_adj \n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    0    0    1    0    0    0    0    0\n[2,]    0    0    0    0    0    0    0    0\n[3,]    1    0    0    2    0    0    1    5\n[4,]    0    0    2    0    0    0    1    1\n[5,]    0    0    0    0    0    0    1    2\n[6,]    0    0    0    0    0    0    2    1\n[7,]    0    0    1    1    1    2    0    2\n[8,]    0    0    5    1    2    1    2    2\n\n\nwith the diagonal representing the loops double counted (Shafie, 2016). The function get_degree_seq can now be used to find the degree sequence for this multigraph:\n\nflor_d &lt;- get_degree_seq(adj = flor_adj, type = 'multigraph')\nflor_d\n\n[1]  1  0  9  4  3  3  7 13\n\n\nNow we test whether the observed network fits the IEAS or the ISA model. The \\(p\\)-values for testing whether there is a significant difference between observed and expected edge multiplicity values according to the two approximate IEA models are given in the output tables below. Note that the asymptotic χ²-distribution has \\(r-1 = (n(n+1)/2) - 1 =35\\) degrees of freedom.\n\nflor_ieas_test &lt;- gof_test(flor_adj, 'multigraph', 'IEAS', flor_d, 35)\nflor_ieas_test\n\n  Stat dof Stat(obs) p-value\n1    S  35    15.762   0.998\n2    A  35    18.905   0.988\n\n\n\nflor_isa_test &lt;- gof_test(flor_adj, 'multigraph', 'ISA', flor_d, 35)\nflor_isa_test \n\n  Stat dof Stat(obs) p-value\n1    S  35    16.572   0.997\n2    A  35    19.648   0.983\n\n\nThe results show that we have strong evidence for the null such that we fail to reject it. Thus, there is not a significant difference between the observed and the expected edge multiplicity sequence according on the two IEA models. Statistics derived under these models presented above can thus be used to analyze the structure of these multigraphs."
  },
  {
    "objectID": "project/rmm/index.html#references",
    "href": "project/rmm/index.html#references",
    "title": "Multigraph Representation of Network Data",
    "section": "References",
    "text": "References\n\nShafie, T. (2015). A multigraph approach to social network analysis. Journal of Social Structure, 16. Link\nShafie, T. (2016). Analyzing local and global properties of multigraphs. The Journal of Mathematical Sociology, 40(4), 239-264. Link\nFrank, O., Shafie, T., (2018). Random Multigraphs and Aggregated Triads with Fixed Degrees. Network Science, 6(2), 232-250. Link\nShafie, T., Schoch, D. (2021) Multiplexity analysis of networks using multigraph representations. Statistical Methods & Applications 30, 1425–1444. Link\nShafie, T. (2022). Goodness of fit tests for random multigraph models, Journal of Applied Statistics. Link"
  },
  {
    "objectID": "project/nexus/index.html",
    "href": "project/nexus/index.html",
    "title": "NEXUS1492 - Reconstructing Archaeological Networks",
    "section": "",
    "text": "Termeh Shafie"
  },
  {
    "objectID": "project/nexus/index.html#project-summary",
    "href": "project/nexus/index.html#project-summary",
    "title": "NEXUS1492 - Reconstructing Archaeological Networks",
    "section": "Project Summary",
    "text": "Project Summary\nBetween 2013-2018, I worked in international ERC-Synergy research project NEXUS1492 investigates the impacts of colonial encounters in the Caribbean through the reconstruction and modelling of archaeological networks.\nYou can read more about the project and its output here."
  },
  {
    "objectID": "project/nexus/index.html#references",
    "href": "project/nexus/index.html#references",
    "title": "NEXUS1492 - Reconstructing Archaeological Networks",
    "section": "References",
    "text": "References\n\nShafie T., Schoch D., Mans J., Hofman C., Brandes U., (2017). Hypergraph Representations: A Study of Carib Attacks on Colonial Forces, 1509-1700. Journal of Historical Network Research,1(1), 52-70. Link\nLaffoon, J.E., Sonnemann, T.F., Shafie, T., Hofman, C.L., Brandes, U. and Davies, G.R., (2017). Investigating human geographic origins using dual-isotope (87Sr/86Sr, δ18O) assignment approaches. PloS one, 12(2), p.e0172562. Link\nAmati, V., Shafie, T., Brandes U., (2018) Reconstructing Archaeological Networks with Structural Holes. Journal of Archaeological Method and Theory volume 25, 226–253. Link\nAmati, V., Mol, A., Shafie, T., Hofman, C., Brandes U., (2020). A Framework for Reconstructing Archaeological Networks Using Exponential Random Graph Models. Journal of Archaeological Method and Theory volume 27, 192–219. Link"
  },
  {
    "objectID": "teaching/sna/material/03/03-descriptives-1.html",
    "href": "teaching/sna/material/03/03-descriptives-1.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "library(igraph)\nlibrary(networkdata)\nlibrary(netUtils)"
  },
  {
    "objectID": "teaching/sna/material/03/03-descriptives-1.html#structural-features",
    "href": "teaching/sna/material/03/03-descriptives-1.html#structural-features",
    "title": "Social Network Analysis",
    "section": "Structural features",
    "text": "Structural features\n\nExercise: Degree Distribution\n\npa &lt;- sample_pa(1500, power = 1.5, m = 3, directed = FALSE)\ner &lt;- sample_gnp(1500, p = 0.003)\n\n\nVerify that the networks have approximately the same density\nPlot the degree distributions of both networks and check for skewness\ncalculate the degrees (degree()) and order the sequence decreasingly (order())\nFor both networks, create a loop (i=1,…, 50) which deletes the top i vertices (delete_vertices()) in the ordered degree sequence and compute the diameter of the resulting networks\nWhat can you observe?\n\nSolution\n\n\nShow solution\n#calculate the density (edge_density is the same as graph.density)\nedge_density(pa)\nedge_density(er)\n\n# for the preferential attachment network, \n# we plot the degree distribution on a log-log scale\n# a straight-ish line should appear \nplot(degree_distribution(pa), log=\"xy\")\nplot(degree_distribution(er))\n\n#calculate the degrees explicitly\ndeg_er &lt;- order(degree(er),decreasing = TRUE)\ndeg_pa &lt;- order(degree(pa),decreasing = TRUE)\n\n# iterate through the top 50 nodes, delete them and calculate the diameter\nres_er &lt;- rep(0,50)\nres_pa &lt;- rep(0,50)\n\nfor(i in 1:50){\n  g1 &lt;- delete_vertices(er,deg_er[1:i])\n  g2 &lt;- delete_vertices(pa,deg_pa[1:i])\n  res_er[i] &lt;- diameter(g1)\n  res_pa[i] &lt;- diameter(g2)\n}\n\nplot(res_pa,type = \"l\", col=\"red\",ylab=\"diameter\")\nlines(res_er,col=\"black\")\n\n\nYou’ll notice that the diameter increases very quickly for a preferential attachment network. The diameter of the random network on the other hand remains constant. If you would redo the analysis for the preferential attachment network and remove random nodes, you’ll notice that the diameter also remains constant. A preferential network is thus said to be fragile to targeted attacks but robust to random attacks.\n\n\nExercise: Density, Transitivity, Distances\nReal world networks tend to have a high transitivity (transitivity(type=\"global\")), low average distance (mean_distance()) and a low density (graph.density()).\n\nExperiment with sample_pa() and sample_gnp() and compute the three stats. Can all three criteria be fullfiled? Why/why not?\n\n\n\nHint\n# Parameters\nn &lt;- 1000       # number of nodes\nm &lt;- 5          # edges per new node in PA\np &lt;- 0.01       # connection probability for GNP\n\n# Preferential Attachment \ng_pa &lt;- sample_pa(n, power = 1, m = m, directed = FALSE)\n\n# Erdős–Rényi (GNP)\ng_gnp &lt;- sample_gnp(n, p, directed = FALSE)\n\n# Compute statistics\ncompute_stats &lt;- function(graph) {\n  trans &lt;- transitivity(graph, type = \"global\")\n  dist &lt;- mean_distance(graph, directed = FALSE, unconnected = TRUE)\n  dens &lt;- edge_density(graph)\n  return(c(transitivity = trans, mean_distance = dist, density = dens))\n}\n\n# Get stats\nstats_pa &lt;- compute_stats(g_pa)\nstats_gnp &lt;- compute_stats(g_gnp)\n\n# Combine into a data frame\nresults &lt;- data.frame(\n  Model = c(\"Preferential Attachment\", \"Erdős–Rényi (GNP)\"),\n  Transitivity = c(stats_pa[\"transitivity\"], stats_gnp[\"transitivity\"]),\n  Mean_Distance = c(stats_pa[\"mean_distance\"], stats_gnp[\"mean_distance\"]),\n  Density = c(stats_pa[\"density\"], stats_gnp[\"density\"])\n)\n\nresults"
  },
  {
    "objectID": "teaching/sna/material/04/04-descriptives-2.html",
    "href": "teaching/sna/material/04/04-descriptives-2.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "library(igraph)\nlibrary(networkdata)\nlibrary(netUtils)\nlibrary(netrankr)"
  },
  {
    "objectID": "teaching/sna/material/04/04-descriptives-2.html#reciprocity",
    "href": "teaching/sna/material/04/04-descriptives-2.html#reciprocity",
    "title": "Social Network Analysis",
    "section": "Reciprocity",
    "text": "Reciprocity\nLoad the “Hightech manager” dataset\n\ndata(\"ht_advice\")\ndata(\"ht_friends\")\ndata(\"ht_reports\")\n\nThese data were collected from the managers of a high-tech company. Each manager was asked “To whom do you go to for advice?” and “Who is your friend?”. Data for the item “To whom do you report?” was taken from company documents.\n\nExercise 1:\n\nBefore computing the reciprocity, think about where you expect high and where low reciprocity\nCompute the reciprocity of each network. Does it fit you intuition?\n\n\n\nShow solution\nreciprocity(ht_advice)\nreciprocity(ht_friends)\nreciprocity(ht_reports)"
  },
  {
    "objectID": "teaching/sna/material/04/04-descriptives-2.html#triad-census",
    "href": "teaching/sna/material/04/04-descriptives-2.html#triad-census",
    "title": "Social Network Analysis",
    "section": "Triad census",
    "text": "Triad census\n\nOne of the many applications of the triad census is to compare a set of networks. Networks with a similar triad census profile are said to be structurally similar.\n\nExercise 2:\nCompute the triad census for each of the three “Hightech manager” dataset and interpret the results.\n\n\nShow solution\ntriad_census(ht_advice)\ntriad_census(ht_friends)\ntriad_census(ht_reports)"
  },
  {
    "objectID": "teaching/sna/material/04/04-descriptives-2.html#centrality",
    "href": "teaching/sna/material/04/04-descriptives-2.html#centrality",
    "title": "Social Network Analysis",
    "section": "Centrality",
    "text": "Centrality\n\nExploring implemented indices\n(interactive session)\nigraph contains the following 10 indices:\n\ndegree (degree())\nweighted degree (strength())\nbetweenness (betweenness())\ncloseness (closeness())\neigenvector (eigen_centrality())\nalpha centrality (alpha_centrality())\npower centrality (power_centrality())\nPageRank (page_rank())\neccentricity (eccentricity())\nhubs and authorities (authority_score() and hub_score())\nsubgraph centrality (subgraph_centrality())\n\nTo illustrate some of the indices, we use the “dbces11” graph which is part of the netrankr package.\n\ndata(\"dbces11\")\n\n\n\nRise of the Medici\n\ndata(\"flo_marriage\")\n\nThe figure below shows part of the marriage network of Florentine families around 1430. The node size corresponds to the number of prior seats and the size of the label to their wealth. \nThe network includes families who were locked in a struggle for political control of the city of Florence. Two factions were dominant in this struggle: one revolved around the infamous Medicis, the other around the powerful Strozzis.\nThe network is frequently used to illustrate how a central position in a network can have beneficial outcomes for actors.\n\n\nExercise 3\n\nCalculate degree, betweenness, closeness, and eigenvector centrality\nArgue for each index, why it might be beneficial to have a high rank in this particular network and setting\n\n\n\nShow solution\nsort(degree(flo_marriage), decreasing = TRUE)\nsort(betweenness(flo_marriage), decreasing = TRUE)\nsort(closeness(flo_marriage), decreasing = TRUE)\nsort(eigen_centrality(flo_marriage)$vector, decreasing = TRUE)\n\n\nExtra: Choose one index and with the help of SNAhelper visualize the network such that the node sizes are proportional to centrality values.\n\n\nPageRank\nload the ATP or WTA dataset from the networkdata package. Both contain a list of igraph objects of tennis results from 1968 to 2021. The last season (2021) can be accessed via atp[[54]] or wta[[54]].\n\ndata(atp)\nstr(atp[[54]])\n\n-------------------------------------------------------\nATP SEASON 2021 (directed, weighted, one-mode network)\n-------------------------------------------------------\nNodes: 393, Edges: 2609, Density: 0.0169, Components: 14, Isolates: 0\n-Graph Attributes:\n  name(c)\n---\n-Vertex Attributes:\n name(c): Frances Tiafoe, Jan Lennard Struff, Sumit Nagal, John Millman, ...\n hand(c): R, R, R, R, R, R, L, R, R, R, R, R, R, R, L, R, R, R, L, R, R, ...\n age(n): 23, 31, 24, 32, 21, 34, 27, 22, 28, 29, 25, 29, 33, 34, 28, 29, ...\n country(c): USA, GER, IND, AUS, CZE, KAZ, GER, SRB, USA, BLR, COL, AUS, ...\n---\n-Edge Attributes:\n surface(c): Clay, Hard, Hard, Hard, Hard, Clay, Hard, Clay, Hard, Clay, ...\n weight(n): 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n---\n-Edges (first 10): \n Adolfo Daniel Vallejo-&gt;Tom Kocevar Desman Adrian Andreev-&gt;Alex De\nMinaur Adrian Andreev-&gt;Alexei Popyrin Adrian Andreev-&gt;Gerardo Lopez\nVillasenor Adrian Andreev-&gt;Miomir Kecmanovic Adrian Mannarino-&gt;Albert\nRamos Adrian Mannarino-&gt;Alexander Zverev Adrian Mannarino-&gt;Aljaz Bedene\nAdrian Mannarino-&gt;Andy Murray Adrian Mannarino-&gt;Arthur Cazaux\n\n\nA directed edge points from the loser to the winner of a match. The network is weighted and a weight corresponds to the number of times a player lost to another on a specific surface.\n\n\nExercise 4\n\nWhat do weighted in- and out-degree, and degree measure mean in the network?\nWhy could Page Rank be a good index to measure player strength?\nWho was the best player in 2021 according to Page Rank?\nPlot the relation between matches won and Page rank. What can you observe?\nTry to redo the analysis for a specific surface (Clay, Hard, Grass)\n\n\n\n\n\n\n\nTip\n\n\n\nthe igraph functions you need are page_rank() and strength(). Check the help for how they handle weights and direction.\nTo extract the network for a specific surface, check out subgraph_from_edges()\n\n\n\n\nShow solution\ng &lt;- atp[[54]]\n#top ten in 2021\nsort(page_rank(g)$vector,decreasing = TRUE)[1:10]\nplot(degree(g, mode = \"in\"),page_rank(g)$vector)\n# example: extract clay networks\ng1 &lt;- subgraph_from_edges(g,which(E(g)$surface==\"Clay\"))\n#top ten in 2021\nsort(page_rank(g1)$vector,decreasing = TRUE)[1:10]"
  },
  {
    "objectID": "teaching/sna/material/04/04-descriptives-2.html#exercise-clustering",
    "href": "teaching/sna/material/04/04-descriptives-2.html#exercise-clustering",
    "title": "Social Network Analysis",
    "section": "Exercise: Clustering",
    "text": "Exercise: Clustering\nPractice the clustering workflow with (a) network(s) from the networkdata package. When choosing a directed and/or weighted network, make sure to set parameters appropriately."
  },
  {
    "objectID": "teaching/sna/material/05/05-ego-nets.html",
    "href": "teaching/sna/material/05/05-ego-nets.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "library(egor)\nlibrary(igraph)\nlibrary(tidyverse)\nlibrary(patchwork)\n\nIn this practical we will\n\nbegin by covering the basics of ego network data, utilizing the egor package to manipulate, construct and visualize ego networks\n\nfocus on analyzing substantive questions related to homophily, the tendency for similar actors to interact at higher rates than dissimilar actors, on different demographic dimensions.\nconsider an example where the ego network properties are used to predict other outcomes of interest, like happiness\n\n\n\nThe example ego network data come from the 1985 General Social Survey (GSS), a national survey of American adults done face-to-face. The aim of the surveys are to\n\ntrack changes in social attitudes, behaviors, and attributes over time\nmeasure opinions on a wide range of social issues like race, religion, politics, crime, work, and family.\nprovide data for social science research\n\nWe work with ego network data from the GSS that has been preprocessed into three different files:\n\na file with the ego attributes\na file with the alter attributes\na file with the alter-alter ties (1 means that the alters know each other, 2 means they are especially close)\n\nLet’s go ahead and read in the three files, starting with the ego attribute data.\nRead in the three data file:\n\nego_dat &lt;- read.csv(file = \"data/gss1985_ego_dat.csv\" , stringsAsFactors = F) \nalter_dat &lt;- read.csv(file = \"data/gss1985_alter_dat.csv\", stringsAsFactors = F)\nalteralter_dat &lt;- read.csv(file = \"data/gss1985_alteralter_dat.csv\")\n\n\n\nExplore the three different data sets. Do you understand the structure and content of each one? Can you see similarities and differences between the data sets? Focus specifically on these columns:\n\n c(\"CASEID\", \"AGE\", \"EDUC\", \"RACE\", \"SEX\", \"HAPPY\", \"NUMGIVEN\")\n\n\n\nShow solution\nnrow(ego_dat)\nnrow(alter_dat)\nego_dat[1:10, c(\"CASEID\", \"AGE\", \"EDUC\", \"RACE\", \"SEX\", \"HAPPY\", \"NUMGIVEN\")]\n\n# CASEID, is the unique id for each respondent\n# for example: we can see that respondent 1 (CASEID = 19850001) names 5 alters. \n# The first alter (ALTERID = 1) is 32, has 18 years of education, and is not kin to ego. \n# NUMGIVEN is the number of alters given, there are NA's here that need to be removed\n# Note that the number of rows in the two data frames is not the same\n\nalter_dat[1:10, c(\"CASEID\", \"ALTERID\", \"AGE\", \"EDUC\", \"RACE\", \"SEX\", \"KIN\")] \n# Each row corresponds to a different named alter. \n# Each alter is denoted by an alter id (ALTERID), unique to that respondent (based on CASEID). \n# We see similar attributes as with the ego data.\n# There is also information on the relationship between ego and each alter. \n\nalteralter_dat[1:10, ]\n# this data frame captures the ties between the named alters (as reported on by the respondent)\n# We see four columns. The first column defines the relevant ego using CASEID. \n# ALTER1 defines the first alter in the dyad and ALTER2 defines the second. \n\n\nAs noted above, there are missing values above that need to be removed:\n\nego_dat &lt;- ego_dat[!is.na(ego_dat$NUMGIVEN), ]\n\n\n\n\n\nFirst challenge in analyzing ego network data is that we must transform traditional survey data into something that has the structure of a network, so that we can then utilize packages like igraph and sna. Our survey data will not look like traditional network inputs (matrices, edgelists, etc.) and each survey is likely to be different, complicating the task of putting together the ego networks.\nLuckily, the egor package has made the task of constructing ego networks from survey data much easier. We will utilize the basic functionality of this package throughout the tutorial.\nThe egor function assumes that you are inputting the data using three separate files. The main arguments are:\n\nalters = alter attributes data frame\negos = ego attributes data frame\naaties = alter-alter tie data frame\nalter_design = list of arguments to specify nomination information from survey\nego_design = list of arguments to specify survey design of study\nID.vars = list of variable names corresponding to key columns:\n\nego = variable name for id of ego\nalter = variable name for id of alter (in alter data)\nsource = variable name for ‘sender’ of tie in alter-alter data\ntarget = variable name for ‘receiver’ of tie in alter-alter data\n\n\nWe will use the three data frames read in above as the main inputs. We will also tell R that CASEID is the ego id variable and ALTERID is the id variable for alters, while ALTER1 and ALTER2 are the source/target variables in the alter-alter data frame. We also note that the maximum number of alters was set to 5:\n\negonetlist &lt;-  egor(alters = alter_dat, egos = ego_dat, \n                    aaties = alteralter_dat, alter_design = list(max = 5), \n                    ID.vars = list(ego = \"CASEID\", alter =\"ALTERID\", \n                                   source = \"ALTER1\", target = \"ALTER2\")) \negonetlist\n\n# EGO data (active): 1,531 × 13\n  .egoID   AGE  EDUC RACE  SEX   RELIG AGE_CATEGORICAL EDUC_CATEGORICAL NUMGIVEN\n*  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt;               &lt;int&gt;\n1 1.99e7    33    16 white male  jewi… 30s             College                 6\n2 1.99e7    49    19 white male  cath… 40s             Post Graduate           6\n3 1.99e7    23    16 white fema… jewi… 20s             College                 5\n4 1.99e7    26    20 white fema… jewi… 20s             Post Graduate           5\n5 1.99e7    24    17 white fema… cath… 20s             Post Graduate           5\n# ℹ 1,526 more rows\n# ℹ 4 more variables: HAPPY &lt;int&gt;, HEALTH &lt;int&gt;, PARTYID &lt;int&gt;, WTSSALL &lt;dbl&gt;\n# ALTER data: 4,483 × 12\n  .altID   .egoID   AGE  EDUC RACE  SEX   RELIG AGE_CATEGORICAL EDUC_CATEGORICAL\n*  &lt;int&gt;    &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt;           \n1      1 19850001    32    18 white male  jewi… 30s             Post Graduate   \n2      2 19850001    29    16 white fema… prot… 20s             College         \n3      3 19850001    32    18 white male  jewi… 30s             Post Graduate   \n# ℹ 4,480 more rows\n# ℹ 3 more variables: TALKTO &lt;int&gt;, SPOUSE &lt;int&gt;, KIN &lt;int&gt;\n# AATIE data: 4,880 × 4\n    .egoID .srcID .tgtID WEIGHT\n*    &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n1 19850001      1      2      2\n2 19850001      1      3      1\n3 19850001      1      4      1\n# ℹ 4,877 more rows\n\n\negor objects are constructed as tibbles, which are data frames built using the tidyverse logic. For those versed in the tidyverse, one can take advantage of all the functions, calls, etc. that go along with those kinds of objects. It is, however, not strictly necessary to know the syntax of the tidyverse to work with these objects. Let’s take a look at the egor object.\n\nnames(egonetlist) \n\n[1] \"ego\"   \"alter\" \"aatie\"\n\n\nWe see that the elements are made up of our three data frames. For example, we take the first five columns of the ego data and alter attributes, extracted from the egor object:\n\negonetlist[[\"ego\"]][, 1:5]\n\n# A tibble: 1,531 × 5\n     .egoID   AGE  EDUC RACE  SEX   \n      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n 1 19850001    33    16 white male  \n 2 19850002    49    19 white male  \n 3 19850003    23    16 white female\n 4 19850004    26    20 white female\n 5 19850005    24    17 white female\n 6 19850006    45    17 white male  \n 7 19850007    44    18 white female\n 8 19850008    56    12 white female\n 9 19850009    85     7 white female\n10 19850010    65    12 white female\n# ℹ 1,521 more rows\n\negonetlist[[\"alter\"]][, 1:5]\n\n# A tibble: 4,483 × 5\n   .altID   .egoID   AGE  EDUC RACE \n    &lt;int&gt;    &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;\n 1      1 19850001    32    18 white\n 2      2 19850001    29    16 white\n 3      3 19850001    32    18 white\n 4      4 19850001    35    16 white\n 5      5 19850001    29    13 white\n 6      1 19850002    42    12 white\n 7      2 19850002    44    18 white\n 8      3 19850002    45    16 white\n 9      4 19850002    40    12 white\n10      5 19850002    50    18 white\n# ℹ 4,473 more rows\n\n\nNote that the id variable for ego has been renamed to .egoID (it was CASEID on the original data). We also see see that the alter id has been renamed .altID (from ALTERID on the original data).\nAnd now we look at the alter-alter ties where we can see that the column names for the alter-alter ties have also been renamed from the input data. The variables are now .srcID and .tgtID (rather than ALTER1 and ALTER2, as on the original data).\n\negonetlist[[\"aatie\"]]\n\n# A tibble: 4,880 × 4\n     .egoID .srcID .tgtID WEIGHT\n      &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n 1 19850001      1      2      2\n 2 19850001      1      3      1\n 3 19850001      1      4      1\n 4 19850001      1      5      1\n 5 19850001      2      3      2\n 6 19850001      2      4      2\n 7 19850001      2      5      2\n 8 19850001      3      4      1\n 9 19850001      3      5      1\n10 19850001      4      5      1\n# ℹ 4,870 more rows\n\n\n\n\nCalculate density on the egor object using the function ego_density(). Note that all ego networks of size 0 or 1 will have NAs for density).\n\n\nShow solution\ndens &lt;- ego_density(egonetlist)\nhead(dens)\n\n# For example, respondent 1 (19850001) has 5 alters and all 10 possible ties exist \n# (density = 1), \n# while respondent 2 (1950002) has 5 alters but only 8 ties exist (density = .8). \n# To check this you can run the following:\nalteralter_dat[alteralter_dat$CASEID == 19850001, ]\nalteralter_dat[alteralter_dat$CASEID == 19850002, ]\n\n\n\n\n\n\nWe not plot the networks using igraph. First step is to convert the information in the egor object to igraph objects. We do this using the as_igraph() function. Let’s take a look at the first three ego networks.\n\nego_nets &lt;- as_igraph(egonetlist)\nego_nets[1:3] \n\n$`19850001`\nIGRAPH 5f36126 UN-- 5 10 -- \n+ attr: .egoID (g/n), name (v/c), AGE (v/n), EDUC (v/n), RACE (v/c),\n| SEX (v/c), RELIG (v/c), AGE_CATEGORICAL (v/c), EDUC_CATEGORICAL\n| (v/c), TALKTO (v/n), SPOUSE (v/n), KIN (v/n), WEIGHT (e/n)\n+ edges from 5f36126 (vertex names):\n [1] 1--2 1--3 1--4 1--5 2--3 2--4 2--5 3--4 3--5 4--5\n\n$`19850002`\nIGRAPH 15ba0c9 UN-- 5 8 -- \n+ attr: .egoID (g/n), name (v/c), AGE (v/n), EDUC (v/n), RACE (v/c),\n| SEX (v/c), RELIG (v/c), AGE_CATEGORICAL (v/c), EDUC_CATEGORICAL\n| (v/c), TALKTO (v/n), SPOUSE (v/n), KIN (v/n), WEIGHT (e/n)\n+ edges from 15ba0c9 (vertex names):\n[1] 1--2 1--3 1--4 1--5 2--4 3--4 3--5 4--5\n\n$`19850003`\nIGRAPH d2d7e9e UN-- 5 6 -- \n+ attr: .egoID (g/n), name (v/c), AGE (v/n), EDUC (v/n), RACE (v/c),\n| SEX (v/c), RELIG (v/c), AGE_CATEGORICAL (v/c), EDUC_CATEGORICAL\n| (v/c), TALKTO (v/n), SPOUSE (v/n), KIN (v/n), WEIGHT (e/n)\n+ edges from d2d7e9e (vertex names):\n[1] 1--2 1--3 1--4 2--3 2--4 3--4\n\n\nNote that we would use as_network() function if we wanted to construct networks in the network format.\nWe have a list of ego networks (in the igraph format), with each ego network in a different element in the list. We can see that the information on the alters was automatically passed to the igraph objects, as was the information on the weights for the alter-alter ties. Note that by default the igraph objects will not include ego. Ego is often (but not always) excluded from visualizations and calculations because ego is, by definition, tied to all alters. Including ego thus offers little additional structural information. We will consider measures below that incorporate both ego and alter information.\nAs with all igraph objects, we can extract useful information, like the attributes of the nodes. As an example, let’s extract information on sex of alters from the first ego network.\n\nvertex_attr(ego_nets[[1]], \"SEX\")\n\n[1] \"male\"   \"female\" \"male\"   \"male\"   \"female\"\n\n# Note that this is the same information as:\n# alter_dat[alter_dat$CASEID == 19850001, \"SEX\"]\n\nNow let’s plot a few networks focusing on the first 6 ego networks:\n\n# lapply() will perform a given function, here plot(), \n# over every element of an input list, here the first 6 elements of ego_nets.\n# note that you also can use purrr::walk()\npar(mfrow = c(2, 3))\nlapply(ego_nets[1:6], plot)\n\n\n\n\n\n\n\n\n\n\n\nNow that we have a list of networks, we can apply the same function to each network using a single line of code, again with the help of lapply().\n\n\nFind the mean number of nodes and ties using vcount() and ecount() function for all networks. Try reproducing the plots shown below. Hint: You can use lapply() again here.\n\n\n\n\n\n\n\n\n\n\n\nShow solution\n# nodes\nnetwork_sizes &lt;- lapply(ego_nets, vcount)\nnetwork_sizes &lt;- unlist(network_sizes)\nmean(network_sizes, na.rm = T)\n\n# ties\nnetwork_edge_counts &lt;- lapply(ego_nets, ecount)\nnetwork_edge_counts &lt;- unlist(network_edge_counts)\nmean(network_edge_counts, na.rm = T)\n\n# Create data frames\ndf1 &lt;- data.frame(NetworkSize = network_sizes)\ndf2 &lt;- data.frame(EdgeCount = network_edge_counts)\n\n# Create histograms (ggplot approach)\np1 &lt;- ggplot(df1, aes(x = NetworkSize)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"skyblue\") +\n  ggtitle(\"Histogram of Network Sizes\") +\n  xlab(\"# of Nodes\") +\n  theme_minimal()\n\np2 &lt;- ggplot(df2, aes(x = EdgeCount)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"tan2\") +\n  ggtitle(\"Histogram of Edge Counts\") +\n  xlab(\"# of Edges\") +\n  theme_minimal()\n\np1 + p2 # needs patchwork\n\n\n\n\n\nUse edge_density and apply to every ego network in the data to find the density of all networks. Create a histogram of these densities as in exercise 3.\n\n\nShow solution\ndensities &lt;- lapply(ego_nets, edge_density)\ndensities &lt;- unlist(densities)\nhist(densities)\n\n\n\n\n\nThere are also inbuilt functions in the egor package that can be used to analyze the ego networks, for example ego_density, composition, alts_diversity_count, count_dyads, and comp_ei. Explore these functions by yourself. Note that you then need to work with the egor object and not the igraph object.\n\n\n\n\nThe EI index (External-Internal index) in ego network compositional analysis is a measure of homophily (similarity) or heterophily (difference) in ego networks. It tells you whether ego’s alters are more similar or dissimilar to the ego based on a categorical attribute (e.g., gender, ethnicity, etc.). The formula is given as: \\[EI = \\frac{E - I}{E + I}\\] where:\n\nE = Number of external ties (alters with a different attribute value than ego)\nI = Number of internal ties (alters with the same attribute value as ego)\n\nThe measure is interpreted as follows:\n\n+1: all alters are different from ego (maximum heterophily)\n0: equal number of similar and different alters (neutral)\n–1: all alters are the same as ego (maximum homophily)\n\nLet’s start by looking at the level of homophily based on the attribute SEX:\n\ncomp_ei(egonetlist, alt.attr = \"SEX\", ego.attr = \"SEX\")\n\n# A tibble: 1,531 × 2\n     .egoID    ei\n      &lt;int&gt; &lt;dbl&gt;\n 1 19850001  -0.2\n 2 19850002  -0.2\n 3 19850003  -1  \n 4 19850004   0.2\n 5 19850005   0.2\n 6 19850006  -0.5\n 7 19850007  -0.6\n 8 19850008   0.2\n 9 19850009   0  \n10 19850010  -1  \n# ℹ 1,521 more rows\n\n\nFor each ego, you know have an index that tells you the level of homophily based on the attribute “SEX”. For example, the first ego 19850001 tends to have slightly more homophilious ties compared to heterophilous ties. To get a better idea of this measure over all networks you can compute the proportion of negative/positive values or condition on (group by) another attribute. For example:\n\ncomp_ei(egonetlist, alt.attr = \"SEX\", ego.attr = \"SEX\") %&gt;% count(ei &lt; 0)\n\n# A tibble: 3 × 2\n  `ei &lt; 0`     n\n  &lt;lgl&gt;    &lt;int&gt;\n1 FALSE      599\n2 TRUE       795\n3 NA         137\n\n\nNote that the NA’s are cause because of some missing values in the alter data set.\n\n\nDo the same thing, but only consider ties that are based on variable RACE. What can you conclude?\n\n\nShow solution\ncomp_ei(egonetlist, alt.attr = \"RACE\", ego.attr = \"RACE\")\n\n\nHomphily based on SEX and RACE offers very different stories. We see that race is a much more salient dimension than gender, with many respondents matching perfectly with all members of their network along racial lines, but much less so with gender, where differences between ego and alter are more common.\n\n\n\n\nAbove we examined the properties of the ego networks, focusing mostly on racial and gender homophily. There are a number of other properties we could explore in more detail, like density or network size. For example, we might want to predict network size as a function of race, gender or other demographic characteristics.\nWe can also use properties of the ego networks as predictors of other outcomes of interest. For example, let’s try and predict the variable HAPPY using the features of the ego networks. Are individuals with larger ego networks happier?\nHAPPY is coded as a 1 (very happy), 2 (pretty happy), 3 (not too happy). Let’s add a label to the variable and reorder it to run from not happy to happy:\n\nego_dat$HAPPY_FACTOR &lt;- factor(ego_dat$HAPPY, levels = c(3, 2, 1), \n                            labels = c(\"not too happy\", \"pretty happy\", \n                                     \"very happy\"))\n\nWe also turn our race and sex variables into factors. We set white as the first category in our race variable.\n\nego_dat$RACE_FACTOR &lt;- factor(ego_dat$RACE, levels = c(\"white\", \"asian\", \n                                                       \"black\", \"hispanic\", \n                                                       \"other\")) \nego_dat$SEX_FACTOR &lt;- factor(ego_dat$SEX)\n\nLet’s also save density\n\ndens &lt;- ego_density(egonetlist)\nego_dat$DENSITY &lt;- dens[[\"density\"]] #  getting values out of tibble format\n\nHAPPY is an ordinal variable. With ordinal outcome variables, it is best to utilize ordered logistic regression (or a similar model). We will need the polr() function in the MASS package to run these models.\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\ndens &lt;- ego_density(egonetlist)\nego_dat$DENSITY &lt;- dens[[\"density\"]] #  getting values out of tibble format\n\nLet’s create a data frame that has no missing data on any of the variables we want to include in the full model. The outcome of interest is HAPPY_FACTOR and the main predictors are ego network size (NUMGIVEN) and density (DENSITY) . We also include a number of demographic controls:\n\nego_dat_nomiss &lt;- na.omit(ego_dat[, c(\"HAPPY_FACTOR\", \"NUMGIVEN\", \"DENSITY\", \n                                     \"EDUC\", \"AGE\", \"RACE_FACTOR\", \n                                     \"SEX_FACTOR\")])\n\nNow we run the ordered logistic regression predicting happiness. For our first model we will predict happiness as a function of our two structural network features, ego network size and density.\n\nsummary(happy_mod1 &lt;- polr(HAPPY_FACTOR ~ NUMGIVEN + DENSITY, \n                           data = ego_dat_nomiss)) \n\n\nRe-fitting to get Hessian\n\n\nCall:\npolr(formula = HAPPY_FACTOR ~ NUMGIVEN + DENSITY, data = ego_dat_nomiss)\n\nCoefficients:\n           Value Std. Error t value\nNUMGIVEN 0.08399    0.04753   1.767\nDENSITY  0.47660    0.20388   2.338\n\nIntercepts:\n                           Value   Std. Error t value\nnot too happy|pretty happy -1.4375  0.2700    -5.3240\npretty happy|very happy     1.5709  0.2697     5.8236\n\nResidual Deviance: 2105.42 \nAIC: 2113.42 \n\n\nThe results suggest that respondents with dense networks report higher levels of happiness, while ego network size (NUMGIVEN) is not a significant predictor of happiness, controlling for density. The initial results would suggest that it is less about the number of people in your ego network that matters for happiness, and more about whether they know each other.\n\n\nAdd the control variables into the model and interpret the results.\n\n\nShow solution\nsummary(happy_mod2 &lt;- polr(HAPPY_FACTOR ~ NUMGIVEN + DENSITY + EDUC + AGE + \n                             RACE_FACTOR + SEX_FACTOR, \n                           data = ego_dat_nomiss))\n\n\nTo summarize the final model fit: density is still a significant predictor of happiness. Individuals with alters who are interconnected consistently report higher levels of happiness, showing the potential benefits of being part of an integrated social group. We also see that individuals with higher education tend to report higher levels of happiness, while those individuals identifying as black report lower levels of happiness."
  },
  {
    "objectID": "teaching/sna/material/05/05-ego-nets.html#data",
    "href": "teaching/sna/material/05/05-ego-nets.html#data",
    "title": "Social Network Analysis",
    "section": "",
    "text": "The example ego network data come from the 1985 General Social Survey (GSS), a national survey of American adults done face-to-face. The aim of the surveys are to\n\ntrack changes in social attitudes, behaviors, and attributes over time\nmeasure opinions on a wide range of social issues like race, religion, politics, crime, work, and family.\nprovide data for social science research\n\nWe work with ego network data from the GSS that has been preprocessed into three different files:\n\na file with the ego attributes\na file with the alter attributes\na file with the alter-alter ties (1 means that the alters know each other, 2 means they are especially close)\n\nLet’s go ahead and read in the three files, starting with the ego attribute data.\nRead in the three data file:\n\nego_dat &lt;- read.csv(file = \"data/gss1985_ego_dat.csv\" , stringsAsFactors = F) \nalter_dat &lt;- read.csv(file = \"data/gss1985_alter_dat.csv\", stringsAsFactors = F)\nalteralter_dat &lt;- read.csv(file = \"data/gss1985_alteralter_dat.csv\")\n\n\n\nExplore the three different data sets. Do you understand the structure and content of each one? Can you see similarities and differences between the data sets? Focus specifically on these columns:\n\n c(\"CASEID\", \"AGE\", \"EDUC\", \"RACE\", \"SEX\", \"HAPPY\", \"NUMGIVEN\")\n\n\n\nShow solution\nnrow(ego_dat)\nnrow(alter_dat)\nego_dat[1:10, c(\"CASEID\", \"AGE\", \"EDUC\", \"RACE\", \"SEX\", \"HAPPY\", \"NUMGIVEN\")]\n\n# CASEID, is the unique id for each respondent\n# for example: we can see that respondent 1 (CASEID = 19850001) names 5 alters. \n# The first alter (ALTERID = 1) is 32, has 18 years of education, and is not kin to ego. \n# NUMGIVEN is the number of alters given, there are NA's here that need to be removed\n# Note that the number of rows in the two data frames is not the same\n\nalter_dat[1:10, c(\"CASEID\", \"ALTERID\", \"AGE\", \"EDUC\", \"RACE\", \"SEX\", \"KIN\")] \n# Each row corresponds to a different named alter. \n# Each alter is denoted by an alter id (ALTERID), unique to that respondent (based on CASEID). \n# We see similar attributes as with the ego data.\n# There is also information on the relationship between ego and each alter. \n\nalteralter_dat[1:10, ]\n# this data frame captures the ties between the named alters (as reported on by the respondent)\n# We see four columns. The first column defines the relevant ego using CASEID. \n# ALTER1 defines the first alter in the dyad and ALTER2 defines the second. \n\n\nAs noted above, there are missing values above that need to be removed:\n\nego_dat &lt;- ego_dat[!is.na(ego_dat$NUMGIVEN), ]"
  },
  {
    "objectID": "teaching/sna/material/05/05-ego-nets.html#creating-the-network-using-egor",
    "href": "teaching/sna/material/05/05-ego-nets.html#creating-the-network-using-egor",
    "title": "Social Network Analysis",
    "section": "",
    "text": "First challenge in analyzing ego network data is that we must transform traditional survey data into something that has the structure of a network, so that we can then utilize packages like igraph and sna. Our survey data will not look like traditional network inputs (matrices, edgelists, etc.) and each survey is likely to be different, complicating the task of putting together the ego networks.\nLuckily, the egor package has made the task of constructing ego networks from survey data much easier. We will utilize the basic functionality of this package throughout the tutorial.\nThe egor function assumes that you are inputting the data using three separate files. The main arguments are:\n\nalters = alter attributes data frame\negos = ego attributes data frame\naaties = alter-alter tie data frame\nalter_design = list of arguments to specify nomination information from survey\nego_design = list of arguments to specify survey design of study\nID.vars = list of variable names corresponding to key columns:\n\nego = variable name for id of ego\nalter = variable name for id of alter (in alter data)\nsource = variable name for ‘sender’ of tie in alter-alter data\ntarget = variable name for ‘receiver’ of tie in alter-alter data\n\n\nWe will use the three data frames read in above as the main inputs. We will also tell R that CASEID is the ego id variable and ALTERID is the id variable for alters, while ALTER1 and ALTER2 are the source/target variables in the alter-alter data frame. We also note that the maximum number of alters was set to 5:\n\negonetlist &lt;-  egor(alters = alter_dat, egos = ego_dat, \n                    aaties = alteralter_dat, alter_design = list(max = 5), \n                    ID.vars = list(ego = \"CASEID\", alter =\"ALTERID\", \n                                   source = \"ALTER1\", target = \"ALTER2\")) \negonetlist\n\n# EGO data (active): 1,531 × 13\n  .egoID   AGE  EDUC RACE  SEX   RELIG AGE_CATEGORICAL EDUC_CATEGORICAL NUMGIVEN\n*  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt;               &lt;int&gt;\n1 1.99e7    33    16 white male  jewi… 30s             College                 6\n2 1.99e7    49    19 white male  cath… 40s             Post Graduate           6\n3 1.99e7    23    16 white fema… jewi… 20s             College                 5\n4 1.99e7    26    20 white fema… jewi… 20s             Post Graduate           5\n5 1.99e7    24    17 white fema… cath… 20s             Post Graduate           5\n# ℹ 1,526 more rows\n# ℹ 4 more variables: HAPPY &lt;int&gt;, HEALTH &lt;int&gt;, PARTYID &lt;int&gt;, WTSSALL &lt;dbl&gt;\n# ALTER data: 4,483 × 12\n  .altID   .egoID   AGE  EDUC RACE  SEX   RELIG AGE_CATEGORICAL EDUC_CATEGORICAL\n*  &lt;int&gt;    &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt;           \n1      1 19850001    32    18 white male  jewi… 30s             Post Graduate   \n2      2 19850001    29    16 white fema… prot… 20s             College         \n3      3 19850001    32    18 white male  jewi… 30s             Post Graduate   \n# ℹ 4,480 more rows\n# ℹ 3 more variables: TALKTO &lt;int&gt;, SPOUSE &lt;int&gt;, KIN &lt;int&gt;\n# AATIE data: 4,880 × 4\n    .egoID .srcID .tgtID WEIGHT\n*    &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n1 19850001      1      2      2\n2 19850001      1      3      1\n3 19850001      1      4      1\n# ℹ 4,877 more rows\n\n\negor objects are constructed as tibbles, which are data frames built using the tidyverse logic. For those versed in the tidyverse, one can take advantage of all the functions, calls, etc. that go along with those kinds of objects. It is, however, not strictly necessary to know the syntax of the tidyverse to work with these objects. Let’s take a look at the egor object.\n\nnames(egonetlist) \n\n[1] \"ego\"   \"alter\" \"aatie\"\n\n\nWe see that the elements are made up of our three data frames. For example, we take the first five columns of the ego data and alter attributes, extracted from the egor object:\n\negonetlist[[\"ego\"]][, 1:5]\n\n# A tibble: 1,531 × 5\n     .egoID   AGE  EDUC RACE  SEX   \n      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n 1 19850001    33    16 white male  \n 2 19850002    49    19 white male  \n 3 19850003    23    16 white female\n 4 19850004    26    20 white female\n 5 19850005    24    17 white female\n 6 19850006    45    17 white male  \n 7 19850007    44    18 white female\n 8 19850008    56    12 white female\n 9 19850009    85     7 white female\n10 19850010    65    12 white female\n# ℹ 1,521 more rows\n\negonetlist[[\"alter\"]][, 1:5]\n\n# A tibble: 4,483 × 5\n   .altID   .egoID   AGE  EDUC RACE \n    &lt;int&gt;    &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;\n 1      1 19850001    32    18 white\n 2      2 19850001    29    16 white\n 3      3 19850001    32    18 white\n 4      4 19850001    35    16 white\n 5      5 19850001    29    13 white\n 6      1 19850002    42    12 white\n 7      2 19850002    44    18 white\n 8      3 19850002    45    16 white\n 9      4 19850002    40    12 white\n10      5 19850002    50    18 white\n# ℹ 4,473 more rows\n\n\nNote that the id variable for ego has been renamed to .egoID (it was CASEID on the original data). We also see see that the alter id has been renamed .altID (from ALTERID on the original data).\nAnd now we look at the alter-alter ties where we can see that the column names for the alter-alter ties have also been renamed from the input data. The variables are now .srcID and .tgtID (rather than ALTER1 and ALTER2, as on the original data).\n\negonetlist[[\"aatie\"]]\n\n# A tibble: 4,880 × 4\n     .egoID .srcID .tgtID WEIGHT\n      &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n 1 19850001      1      2      2\n 2 19850001      1      3      1\n 3 19850001      1      4      1\n 4 19850001      1      5      1\n 5 19850001      2      3      2\n 6 19850001      2      4      2\n 7 19850001      2      5      2\n 8 19850001      3      4      1\n 9 19850001      3      5      1\n10 19850001      4      5      1\n# ℹ 4,870 more rows\n\n\n\n\nCalculate density on the egor object using the function ego_density(). Note that all ego networks of size 0 or 1 will have NAs for density).\n\n\nShow solution\ndens &lt;- ego_density(egonetlist)\nhead(dens)\n\n# For example, respondent 1 (19850001) has 5 alters and all 10 possible ties exist \n# (density = 1), \n# while respondent 2 (1950002) has 5 alters but only 8 ties exist (density = .8). \n# To check this you can run the following:\nalteralter_dat[alteralter_dat$CASEID == 19850001, ]\nalteralter_dat[alteralter_dat$CASEID == 19850002, ]"
  },
  {
    "objectID": "teaching/sna/material/05/05-ego-nets.html#plotting-the-ego-nets",
    "href": "teaching/sna/material/05/05-ego-nets.html#plotting-the-ego-nets",
    "title": "Social Network Analysis",
    "section": "",
    "text": "We not plot the networks using igraph. First step is to convert the information in the egor object to igraph objects. We do this using the as_igraph() function. Let’s take a look at the first three ego networks.\n\nego_nets &lt;- as_igraph(egonetlist)\nego_nets[1:3] \n\n$`19850001`\nIGRAPH 5f36126 UN-- 5 10 -- \n+ attr: .egoID (g/n), name (v/c), AGE (v/n), EDUC (v/n), RACE (v/c),\n| SEX (v/c), RELIG (v/c), AGE_CATEGORICAL (v/c), EDUC_CATEGORICAL\n| (v/c), TALKTO (v/n), SPOUSE (v/n), KIN (v/n), WEIGHT (e/n)\n+ edges from 5f36126 (vertex names):\n [1] 1--2 1--3 1--4 1--5 2--3 2--4 2--5 3--4 3--5 4--5\n\n$`19850002`\nIGRAPH 15ba0c9 UN-- 5 8 -- \n+ attr: .egoID (g/n), name (v/c), AGE (v/n), EDUC (v/n), RACE (v/c),\n| SEX (v/c), RELIG (v/c), AGE_CATEGORICAL (v/c), EDUC_CATEGORICAL\n| (v/c), TALKTO (v/n), SPOUSE (v/n), KIN (v/n), WEIGHT (e/n)\n+ edges from 15ba0c9 (vertex names):\n[1] 1--2 1--3 1--4 1--5 2--4 3--4 3--5 4--5\n\n$`19850003`\nIGRAPH d2d7e9e UN-- 5 6 -- \n+ attr: .egoID (g/n), name (v/c), AGE (v/n), EDUC (v/n), RACE (v/c),\n| SEX (v/c), RELIG (v/c), AGE_CATEGORICAL (v/c), EDUC_CATEGORICAL\n| (v/c), TALKTO (v/n), SPOUSE (v/n), KIN (v/n), WEIGHT (e/n)\n+ edges from d2d7e9e (vertex names):\n[1] 1--2 1--3 1--4 2--3 2--4 3--4\n\n\nNote that we would use as_network() function if we wanted to construct networks in the network format.\nWe have a list of ego networks (in the igraph format), with each ego network in a different element in the list. We can see that the information on the alters was automatically passed to the igraph objects, as was the information on the weights for the alter-alter ties. Note that by default the igraph objects will not include ego. Ego is often (but not always) excluded from visualizations and calculations because ego is, by definition, tied to all alters. Including ego thus offers little additional structural information. We will consider measures below that incorporate both ego and alter information.\nAs with all igraph objects, we can extract useful information, like the attributes of the nodes. As an example, let’s extract information on sex of alters from the first ego network.\n\nvertex_attr(ego_nets[[1]], \"SEX\")\n\n[1] \"male\"   \"female\" \"male\"   \"male\"   \"female\"\n\n# Note that this is the same information as:\n# alter_dat[alter_dat$CASEID == 19850001, \"SEX\"]\n\nNow let’s plot a few networks focusing on the first 6 ego networks:\n\n# lapply() will perform a given function, here plot(), \n# over every element of an input list, here the first 6 elements of ego_nets.\n# note that you also can use purrr::walk()\npar(mfrow = c(2, 3))\nlapply(ego_nets[1:6], plot)"
  },
  {
    "objectID": "teaching/sna/material/05/05-ego-nets.html#descriptives-on-all-ego-nets",
    "href": "teaching/sna/material/05/05-ego-nets.html#descriptives-on-all-ego-nets",
    "title": "Social Network Analysis",
    "section": "",
    "text": "Now that we have a list of networks, we can apply the same function to each network using a single line of code, again with the help of lapply().\n\n\nFind the mean number of nodes and ties using vcount() and ecount() function for all networks. Try reproducing the plots shown below. Hint: You can use lapply() again here.\n\n\n\n\n\n\n\n\n\n\n\nShow solution\n# nodes\nnetwork_sizes &lt;- lapply(ego_nets, vcount)\nnetwork_sizes &lt;- unlist(network_sizes)\nmean(network_sizes, na.rm = T)\n\n# ties\nnetwork_edge_counts &lt;- lapply(ego_nets, ecount)\nnetwork_edge_counts &lt;- unlist(network_edge_counts)\nmean(network_edge_counts, na.rm = T)\n\n# Create data frames\ndf1 &lt;- data.frame(NetworkSize = network_sizes)\ndf2 &lt;- data.frame(EdgeCount = network_edge_counts)\n\n# Create histograms (ggplot approach)\np1 &lt;- ggplot(df1, aes(x = NetworkSize)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"skyblue\") +\n  ggtitle(\"Histogram of Network Sizes\") +\n  xlab(\"# of Nodes\") +\n  theme_minimal()\n\np2 &lt;- ggplot(df2, aes(x = EdgeCount)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"tan2\") +\n  ggtitle(\"Histogram of Edge Counts\") +\n  xlab(\"# of Edges\") +\n  theme_minimal()\n\np1 + p2 # needs patchwork\n\n\n\n\n\nUse edge_density and apply to every ego network in the data to find the density of all networks. Create a histogram of these densities as in exercise 3.\n\n\nShow solution\ndensities &lt;- lapply(ego_nets, edge_density)\ndensities &lt;- unlist(densities)\nhist(densities)\n\n\n\n\n\nThere are also inbuilt functions in the egor package that can be used to analyze the ego networks, for example ego_density, composition, alts_diversity_count, count_dyads, and comp_ei. Explore these functions by yourself. Note that you then need to work with the egor object and not the igraph object."
  },
  {
    "objectID": "teaching/sna/material/05/05-ego-nets.html#homophily-ego-alter-attributes",
    "href": "teaching/sna/material/05/05-ego-nets.html#homophily-ego-alter-attributes",
    "title": "Social Network Analysis",
    "section": "",
    "text": "The EI index (External-Internal index) in ego network compositional analysis is a measure of homophily (similarity) or heterophily (difference) in ego networks. It tells you whether ego’s alters are more similar or dissimilar to the ego based on a categorical attribute (e.g., gender, ethnicity, etc.). The formula is given as: \\[EI = \\frac{E - I}{E + I}\\] where:\n\nE = Number of external ties (alters with a different attribute value than ego)\nI = Number of internal ties (alters with the same attribute value as ego)\n\nThe measure is interpreted as follows:\n\n+1: all alters are different from ego (maximum heterophily)\n0: equal number of similar and different alters (neutral)\n–1: all alters are the same as ego (maximum homophily)\n\nLet’s start by looking at the level of homophily based on the attribute SEX:\n\ncomp_ei(egonetlist, alt.attr = \"SEX\", ego.attr = \"SEX\")\n\n# A tibble: 1,531 × 2\n     .egoID    ei\n      &lt;int&gt; &lt;dbl&gt;\n 1 19850001  -0.2\n 2 19850002  -0.2\n 3 19850003  -1  \n 4 19850004   0.2\n 5 19850005   0.2\n 6 19850006  -0.5\n 7 19850007  -0.6\n 8 19850008   0.2\n 9 19850009   0  \n10 19850010  -1  \n# ℹ 1,521 more rows\n\n\nFor each ego, you know have an index that tells you the level of homophily based on the attribute “SEX”. For example, the first ego 19850001 tends to have slightly more homophilious ties compared to heterophilous ties. To get a better idea of this measure over all networks you can compute the proportion of negative/positive values or condition on (group by) another attribute. For example:\n\ncomp_ei(egonetlist, alt.attr = \"SEX\", ego.attr = \"SEX\") %&gt;% count(ei &lt; 0)\n\n# A tibble: 3 × 2\n  `ei &lt; 0`     n\n  &lt;lgl&gt;    &lt;int&gt;\n1 FALSE      599\n2 TRUE       795\n3 NA         137\n\n\nNote that the NA’s are cause because of some missing values in the alter data set.\n\n\nDo the same thing, but only consider ties that are based on variable RACE. What can you conclude?\n\n\nShow solution\ncomp_ei(egonetlist, alt.attr = \"RACE\", ego.attr = \"RACE\")\n\n\nHomphily based on SEX and RACE offers very different stories. We see that race is a much more salient dimension than gender, with many respondents matching perfectly with all members of their network along racial lines, but much less so with gender, where differences between ego and alter are more common."
  },
  {
    "objectID": "teaching/sna/material/05/05-ego-nets.html#ego-networks-as-predictors",
    "href": "teaching/sna/material/05/05-ego-nets.html#ego-networks-as-predictors",
    "title": "Social Network Analysis",
    "section": "",
    "text": "Above we examined the properties of the ego networks, focusing mostly on racial and gender homophily. There are a number of other properties we could explore in more detail, like density or network size. For example, we might want to predict network size as a function of race, gender or other demographic characteristics.\nWe can also use properties of the ego networks as predictors of other outcomes of interest. For example, let’s try and predict the variable HAPPY using the features of the ego networks. Are individuals with larger ego networks happier?\nHAPPY is coded as a 1 (very happy), 2 (pretty happy), 3 (not too happy). Let’s add a label to the variable and reorder it to run from not happy to happy:\n\nego_dat$HAPPY_FACTOR &lt;- factor(ego_dat$HAPPY, levels = c(3, 2, 1), \n                            labels = c(\"not too happy\", \"pretty happy\", \n                                     \"very happy\"))\n\nWe also turn our race and sex variables into factors. We set white as the first category in our race variable.\n\nego_dat$RACE_FACTOR &lt;- factor(ego_dat$RACE, levels = c(\"white\", \"asian\", \n                                                       \"black\", \"hispanic\", \n                                                       \"other\")) \nego_dat$SEX_FACTOR &lt;- factor(ego_dat$SEX)\n\nLet’s also save density\n\ndens &lt;- ego_density(egonetlist)\nego_dat$DENSITY &lt;- dens[[\"density\"]] #  getting values out of tibble format\n\nHAPPY is an ordinal variable. With ordinal outcome variables, it is best to utilize ordered logistic regression (or a similar model). We will need the polr() function in the MASS package to run these models.\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\ndens &lt;- ego_density(egonetlist)\nego_dat$DENSITY &lt;- dens[[\"density\"]] #  getting values out of tibble format\n\nLet’s create a data frame that has no missing data on any of the variables we want to include in the full model. The outcome of interest is HAPPY_FACTOR and the main predictors are ego network size (NUMGIVEN) and density (DENSITY) . We also include a number of demographic controls:\n\nego_dat_nomiss &lt;- na.omit(ego_dat[, c(\"HAPPY_FACTOR\", \"NUMGIVEN\", \"DENSITY\", \n                                     \"EDUC\", \"AGE\", \"RACE_FACTOR\", \n                                     \"SEX_FACTOR\")])\n\nNow we run the ordered logistic regression predicting happiness. For our first model we will predict happiness as a function of our two structural network features, ego network size and density.\n\nsummary(happy_mod1 &lt;- polr(HAPPY_FACTOR ~ NUMGIVEN + DENSITY, \n                           data = ego_dat_nomiss)) \n\n\nRe-fitting to get Hessian\n\n\nCall:\npolr(formula = HAPPY_FACTOR ~ NUMGIVEN + DENSITY, data = ego_dat_nomiss)\n\nCoefficients:\n           Value Std. Error t value\nNUMGIVEN 0.08399    0.04753   1.767\nDENSITY  0.47660    0.20388   2.338\n\nIntercepts:\n                           Value   Std. Error t value\nnot too happy|pretty happy -1.4375  0.2700    -5.3240\npretty happy|very happy     1.5709  0.2697     5.8236\n\nResidual Deviance: 2105.42 \nAIC: 2113.42 \n\n\nThe results suggest that respondents with dense networks report higher levels of happiness, while ego network size (NUMGIVEN) is not a significant predictor of happiness, controlling for density. The initial results would suggest that it is less about the number of people in your ego network that matters for happiness, and more about whether they know each other.\n\n\nAdd the control variables into the model and interpret the results.\n\n\nShow solution\nsummary(happy_mod2 &lt;- polr(HAPPY_FACTOR ~ NUMGIVEN + DENSITY + EDUC + AGE + \n                             RACE_FACTOR + SEX_FACTOR, \n                           data = ego_dat_nomiss))\n\n\nTo summarize the final model fit: density is still a significant predictor of happiness. Individuals with alters who are interconnected consistently report higher levels of happiness, showing the potential benefits of being part of an integrated social group. We also see that individuals with higher education tend to report higher levels of happiness, while those individuals identifying as black report lower levels of happiness."
  },
  {
    "objectID": "teaching/sna/material/05/05-ego-nets.html#footnotes",
    "href": "teaching/sna/material/05/05-ego-nets.html#footnotes",
    "title": "Social Network Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis worksheet is inspired and adapted from this source↩︎"
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms2.html",
    "href": "teaching/sna/material/10/10-ergms2.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "Following the same modeling outline as before, we will here fit ERGMs on two-mode networks. Here, the model terms are slightly different than for the case of one-mode data. We will use the data used here, but I have already cleaned the data so you can import it as an .RDS file.\nThe data is on students joining clubs in a high school and we have basic information about the students, the clubs, and which students are members of which clubs. Familiarize yourself with this data set.\nRecall that this page is useful for looking up different model terms: ERGM terms.\n\n\n\nlibrary(statnet) # also loads the ERGM package\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(intergraph)\nlibrary(patchwork)\nlibrary(networkdata)\n\n# download data (network object):\nbnet96 &lt;- readRDS(\"bnet96.rds\")\n\n\n\n\nWe start with a simple model and just include a term for edges. The edges term is directly analogous to the edges term for one mode networks, except here we must remember that edges can only exist between actors of different modes, i.e. edges are only possible between women and dates for social events.\n\nmod1 &lt;- ergm(bnet96 ~ edges)\nsummary(mod1)\n\nCall:\nergm(formula = bnet96 ~ edges)\n\nMaximum Likelihood Results:\n\n      Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges -3.44427    0.01977      0  -174.3   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 118331  on 85358  degrees of freedom\n Residual Deviance:  23558  on 85357  degrees of freedom\n \nAIC: 23560  BIC: 23569  (Smaller is better. MC Std. Err. = 0)\n\n\nQ1 How do you interpret the coefficient here? Recall that the probability of a tie is then given by exp(-3.44427) / (1 + exp(-3.44427)) = .0309402.\n\n\n\nWe have some structural conditions to consider: some student-club combinations cannot occur, or occur at extremely low rates. We need to adjust our model for these cases, or run the risk of biasing the estimates. For example, girls are very unlikely to join boys sports teams (e.g., wrestling or football), while boys are unlikely to join girls sports teams (volleyball). There are similar structural issues with grade, as some clubs are restricted to certain grades. For example, students in grade 9, 10, 11 or 12 do not join eighth grade sports teams (e.g., 8th grade football).\nWe will include nodemix terms in the model for gender and grade, capturing the student-club combinations that are unlikely to exist. The two attributes of interest are studentgender_clubgender and studentgrade96_clubgrad.\nWe will start with gender. Remember that student gender is measured as male or female, while club gender is measured as boys, boys_girls, or girls. We want to print out the combinations of student-club genders (e.g., “male.boys”). By default, nodemix will print the terms sorted alphabetically, first by the second level (clubs) and then by the first level (students). Let’s create our vector of names (student-club combinations) with that ordering in mind.\n\nstudent_gender &lt;- sort(as.character(c(\"female\", \"male\")))\nclub_gender &lt;- sort(c(\"boys\", \"boys_girls\", \"girls\"))\n\ngender_levels &lt;- paste(rep(student_gender, times = length(club_gender)), \n                      rep(club_gender, each = length(student_gender)),\n                      sep = \".\" )\ndata.frame(gender_levels)\n\n      gender_levels\n1       female.boys\n2         male.boys\n3 female.boys_girls\n4   male.boys_girls\n5      female.girls\n6        male.girls\n\n\nThe terms of interest are female.boys and male.girls; as these are the rare events we want to adjust for, girls joining boys clubs and boys joining girls clubs. This corresponds to spots 1 and 6 from the summary statistics, so let’s create a vector holding that information.\n\ngender_terms &lt;- c(1, 6)\n\nAnd now we look at the grade attribute, studentgrade96_clubgrade. Student grade is measured as 8, 9, 10, 11 or 12. Club grade is measured as: eighth (just eighth graders), ninth (just ninth graders), ninth_tenth_eleventh (just ninth, tenth or eleventh graders), and ninth+ (no eighth graders). The value is all_grades if there are no restrictions on membership, in terms of grade. Let’s get all student-club combinations for grade and put them together to be consistent with the nodemix term (sorted by the second level and then by the first level):\n\nstudent_grades &lt;- sort(as.character(c(8:12)))\nclub_grades &lt;- sort(c(\"all_grades\", \"eight\", \"ninth\", \"ninth_tenth_eleventh\", \"ninth+\"))\n\ngrade_levels &lt;- paste(rep(student_grades, times = length(club_grades)), \n                      rep(club_grades, each = length(student_grades)),\n                      sep = \".\" )\ndata.frame(grade_levels)\n\n              grade_levels\n1            10.all_grades\n2            11.all_grades\n3            12.all_grades\n4             8.all_grades\n5             9.all_grades\n6                 10.eight\n7                 11.eight\n8                 12.eight\n9                  8.eight\n10                 9.eight\n11                10.ninth\n12                11.ninth\n13                12.ninth\n14                 8.ninth\n15                 9.ninth\n16 10.ninth_tenth_eleventh\n17 11.ninth_tenth_eleventh\n18 12.ninth_tenth_eleventh\n19  8.ninth_tenth_eleventh\n20  9.ninth_tenth_eleventh\n21               10.ninth+\n22               11.ninth+\n23               12.ninth+\n24                8.ninth+\n25                9.ninth+\n\n\nWe want to include terms for any student-club combination that should not exist; like 12th graders in a ninth grade club, 12.ninth. Based on the order from the summary statistics, this corresponds to: 6 (10.eighth), 7 (11.eighth), 8 (12.eighth), 10 (9.eighth), 11 (10.ninth), 12 (11.ninth), 13 (12.ninth), 14 (8.ninth), 18 (12.ninth_tenth_eleventh), 19 (8.ninth_tenth_eleventh) and 24 (8.ninth+).\n\ngrade_terms &lt;- c(6, 7, 8, 10, 11, 12, 13, 14, 18, 19, 24)\n\nWe an now estimate our model, including nodemix terms for studentgender_clubgender and studentgrade96_clubgrade. We will specify which terms to include using the levels2 argument, setting it to the vectors defined above. To simplify the estimation of the model, we will specify these terms using an Offset() function (although we could have estimated the coefficients within the model). When using Offset(), the coefficients are not estimated and are instead set using the values supplied in the coef argument. This is appropriate in our case as the coefficients are based on logical conditions (e.g., 12th graders do not join 9th grade clubs) and can be set a priori by the researcher. Here, we set the coefficients to -10 for every term. We set the coefficients to -10 as we want to estimate the models conditioned on the fact that these student-club combinations are very rare.\n\noffset_coefs_gender &lt;- rep(-10, length(gender_terms))\noffset_coefs_grade &lt;- rep(-10, length(grade_terms))\n\nmod2 &lt;- ergm(bnet96 ~ edges + \n               Offset(~ nodemix(\"studentgender_clubgender\", \n                              levels2 = gender_terms), \n                      coef = offset_coefs_gender) + \n               Offset(~ nodemix(\"studentgrade96_clubgrade\", \n                              levels2 = grade_terms), \n                      coef = offset_coefs_grade))\nsummary(mod2)\n\nCall:\nergm(formula = bnet96 ~ edges + Offset(~nodemix(\"studentgender_clubgender\", \n    levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(\"studentgrade96_clubgrade\", \n    levels2 = grade_terms), coef = offset_coefs_grade))\n\nMaximum Likelihood Results:\n\n      Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges -2.99179    0.01994      0    -150   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 118331  on 85358  degrees of freedom\n Residual Deviance:  21515  on 85357  degrees of freedom\n \nAIC: 21517  BIC: 21526  (Smaller is better. MC Std. Err. = 0)\n\n\nAll of the offset terms are set to -10, although they are not printed out here. We see that the edge coefficient is different than with mod1, suggesting the importance of adjusting our model for structural/logical constraints. Note also that the model fit should only be compared to other models with the same set of offset terms.\nWe used nodemix terms to capture structural conditions in the data, but we could imagine using nodemix terms to answer more substantive questions. We could test if certain types of students (e.g., girls) are more likely to join certain types of clubs (academic, leadership, etc.), and this is left as an exercise.\n\n\n\nWe now add nodefactor and homophily terms to our model. With two-mode networks, these terms can take two forms, with different terms for the first and second mode. Here we focus on the second mode, the clubs.\n\n\nnodefactor terms capture differences in degree across nodes, here clubs, with different attributes. We are interested in the main effect of club type (sports, academic, etc.) and club profile (low, moderate, high, very high) on membership. For example, do high profile clubs have more members than low profile clubs?\nThe term of interest is b2factor (b2 indicating the second mode of a bipartite network). We will include b2factor terms for each club attribute of interest, club_type_detailed and club_profile. We include a levels argument for club_profile to set the order that the results are printed. By default, the results are printed in alphabetical order. In this case, that would correspond to high (1), low (2), moderate (3), very high (4), with high excluded as the reference. But we want the results to run from moderate (3) to high (1) to very high (4), with low (2) excluded (as this is easier to interpret). For club_type_detailed, we use the levels argument to set the second category, academic interest, as the reference. We set levels to -2 to exclude only the second category.\n\nmod3 &lt;- ergm(bnet96 ~ edges + \n               Offset(~nodemix(\"studentgender_clubgender\", \n                              levels2 = gender_terms), \n                      coef = offset_coefs_gender) + \n               Offset(~nodemix(\"studentgrade96_clubgrade\", \n                              levels2 = grade_terms), \n                      coef = offset_coefs_grade) + \n                b2factor(\"club_type_detailed\", levels = -2) + \n                b2factor(\"club_profile\", levels = c(3, 1, 4)))\nsummary(mod3)\n\nCall:\nergm(formula = bnet96 ~ edges + Offset(~nodemix(\"studentgender_clubgender\", \n    levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(\"studentgrade96_clubgrade\", \n    levels2 = grade_terms), coef = offset_coefs_grade) + b2factor(\"club_type_detailed\", \n    levels = -2) + b2factor(\"club_profile\", levels = c(3, 1, \n    4)))\n\nMaximum Likelihood Results:\n\n                                                 Estimate Std. Error MCMC %\nedges                                            -2.82059    0.04035      0\nb2factor.club_type_detailed.Academic Competition -0.44051    0.08242      0\nb2factor.club_type_detailed.Ethnic Interest      -1.03174    0.16677      0\nb2factor.club_type_detailed.Individual Sports    -0.41835    0.08391      0\nb2factor.club_type_detailed.Leadership           -0.36924    0.18887      0\nb2factor.club_type_detailed.Media                -0.16730    0.14109      0\nb2factor.club_type_detailed.Performance Art       0.10725    0.06436      0\nb2factor.club_type_detailed.Service               0.31118    0.06262      0\nb2factor.club_type_detailed.Team Sports           0.45881    0.07798      0\nb2factor.club_profile.moderate                   -0.29070    0.05741      0\nb2factor.club_profile.high                       -0.94788    0.09396      0\nb2factor.club_profile.very_high                  -0.48185    0.11402      0\n                                                 z value Pr(&gt;|z|)    \nedges                                            -69.902   &lt;1e-04 ***\nb2factor.club_type_detailed.Academic Competition  -5.345   &lt;1e-04 ***\nb2factor.club_type_detailed.Ethnic Interest       -6.186   &lt;1e-04 ***\nb2factor.club_type_detailed.Individual Sports     -4.986   &lt;1e-04 ***\nb2factor.club_type_detailed.Leadership            -1.955   0.0506 .  \nb2factor.club_type_detailed.Media                 -1.186   0.2357    \nb2factor.club_type_detailed.Performance Art        1.666   0.0956 .  \nb2factor.club_type_detailed.Service                4.970   &lt;1e-04 ***\nb2factor.club_type_detailed.Team Sports            5.883   &lt;1e-04 ***\nb2factor.club_profile.moderate                    -5.064   &lt;1e-04 ***\nb2factor.club_profile.high                       -10.088   &lt;1e-04 ***\nb2factor.club_profile.very_high                   -4.226   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 118331  on 85358  degrees of freedom\n Residual Deviance:  21116  on 85346  degrees of freedom\n \nAIC: 21140  BIC: 21252  (Smaller is better. MC Std. Err. = 0)\n\n\nWe see, for example, that competitive academic clubs and individual sports tend to have fewer members than academic interest clubs (the reference), while service clubs (like National Honors Society) and team sports tend to be large. We also see that high profile clubs have, if anything, fewer members than low profile clubs.\nMore formally, we can interpret the coefficient on individual sports (for example) as follows: the odds of a student being part of an individual sports team is exp(-0.41835) times lower than the odds of being part of an academic interest club. It is worth emphasizing that b2factor is based only on club attributes, and is thus different from nodemix (see above), which incorporates attributes from both modes.\n\n\n\nHomophily is more complicated with two-mode networks than with one-mode networks. This is the case as there are no direct ties from nodes of the same type; in our case, there are no ties from students to students or from clubs to clubs. So, if we are interested in homophily on a club attribute, say club type, we cannot ask if team sports are tied to other team sports, as there are no ties between clubs. Instead, we must ask if similar clubs are linked together through students; e.g., do students in team sports tend to be members of other team sports?\nWe must counts two-paths, based on homophily on the attribute of interest. The basic idea is to sum up the number of times that we see A-\\(i\\)-B, where A and B are clubs with the same attribute (e.g., both team sports) and \\(i\\) is a student in both A and B. The count is technically over half the number of two-paths, to avoid double counting (as A-\\(i\\)-B is substantively the same as B-\\(i\\)-A). The term is b2nodematch. A positive coefficient on b2nodematch would indicate that students are members of similar kinds of clubs, above what can be explained by other terms in the model.\nBut how do we sum up the homophilous two-paths? In the simplest case, we can sum up all of the two-paths that match on the attribute of interest. This is the default specification. We may, however, have good reason to incorporate some discounting, so that adding one more two-path (for a given edge) only marginally increases the count, once we reach a certain threshold. For example, if student \\(i\\) is a member of the football team, (\\(i\\)-football edge), then if \\(i\\) is also a member of the wrestling team, that would be a strong signal of matching on club type (both team sports). But adding another team sport membership, say \\(i\\) is also a member of the baseball team, may not offer quite as much information; as we already know that \\(i\\) joins team sports. We may then want to count the second two-path less than the first.\nWe can control the discounting using a beta parameter, which raises the count of two-paths (for a given edge) to beta. Setting beta to 1 would yield the number of two-paths (for an edge) where there is a match on the club attribute of interest (so the \\(i\\)-football edge would contribute a count of 2). Setting beta to 0 gives us the other extreme: showing if the given edge is involved in at least one homophilous two-path (so the \\(i\\)-football edge would contribute a count of 1). The count of two-paths, raised to beta, is then summed over all edges and divided by 2.\nWe set beta to .25, but we could imagine using a range of values (estimating the model each time), using model fit statistics to evaluate the choice of beta.\n\nset.seed(1007)\n\nmod4 &lt;- ergm(bnet96 ~ edges + \n                Offset(~nodemix(\"studentgender_clubgender\", \n                              levels2 = gender_terms), \n                      coef = offset_coefs_gender) + \n                Offset(~nodemix(\"studentgrade96_clubgrade\", \n                              levels2 = grade_terms), \n                      coef = offset_coefs_grade) + \n                b2factor(\"club_type_detailed\", levels = -2) + \n                b2nodematch(\"club_type_detailed\", beta = .25) + \n                b2factor(\"club_profile\", levels = c(3, 1, 4)) + \n                b2nodematch(\"club_profile\", beta = .25), \n              control = control.ergm(MCMC.burnin = 20000,\n                                     MCMC.samplesize = 3000))\n\nThe model is now being fit using MCMC techniques. This means that we should check if the model is fitting well. Use the mcmc.diagnostics() function.\nQ2 How would you intepret the below results (not plotted here)?\n\nmcmc.diagnostics(mod4)\n\nGo ahead a summarize model 4:\n\nsummary(mod4)\n\nCall:\nergm(formula = bnet96 ~ edges + Offset(~nodemix(\"studentgender_clubgender\", \n    levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(\"studentgrade96_clubgrade\", \n    levels2 = grade_terms), coef = offset_coefs_grade) + b2factor(\"club_type_detailed\", \n    levels = -2) + b2nodematch(\"club_type_detailed\", beta = 0.25) + \n    b2factor(\"club_profile\", levels = c(3, 1, 4)) + b2nodematch(\"club_profile\", \n    beta = 0.25), control = control.ergm(MCMC.burnin = 20000, \n    MCMC.samplesize = 3000))\n\nMonte Carlo Maximum Likelihood Results:\n\n                                                  Estimate Std. Error MCMC %\nedges                                            -3.596083   0.075080      0\nb2factor.club_type_detailed.Academic Competition -0.323232   0.085331      0\nb2factor.club_type_detailed.Ethnic Interest      -0.844453   0.169750      0\nb2factor.club_type_detailed.Individual Sports    -0.296443   0.083610      0\nb2factor.club_type_detailed.Leadership           -0.171296   0.193888      0\nb2factor.club_type_detailed.Media                 0.028381   0.138718      0\nb2factor.club_type_detailed.Performance Art       0.129749   0.059862      0\nb2factor.club_type_detailed.Service               0.361836   0.059245      0\nb2factor.club_type_detailed.Team Sports           0.502725   0.075382      0\nb2nodematch.club_type_detailed                    0.413830   0.070045      0\nb2factor.club_profile.moderate                   -0.075008   0.059536      0\nb2factor.club_profile.high                       -0.538820   0.098625      0\nb2factor.club_profile.very_high                  -0.003789   0.124166      0\nb2nodematch.club_profile                          0.779436   0.088837      0\n                                                 z value Pr(&gt;|z|)    \nedges                                            -47.897  &lt; 1e-04 ***\nb2factor.club_type_detailed.Academic Competition  -3.788 0.000152 ***\nb2factor.club_type_detailed.Ethnic Interest       -4.975  &lt; 1e-04 ***\nb2factor.club_type_detailed.Individual Sports     -3.546 0.000392 ***\nb2factor.club_type_detailed.Leadership            -0.883 0.376978    \nb2factor.club_type_detailed.Media                  0.205 0.837889    \nb2factor.club_type_detailed.Performance Art        2.167 0.030198 *  \nb2factor.club_type_detailed.Service                6.107  &lt; 1e-04 ***\nb2factor.club_type_detailed.Team Sports            6.669  &lt; 1e-04 ***\nb2nodematch.club_type_detailed                     5.908  &lt; 1e-04 ***\nb2factor.club_profile.moderate                    -1.260 0.207713    \nb2factor.club_profile.high                        -5.463  &lt; 1e-04 ***\nb2factor.club_profile.very_high                   -0.031 0.975654    \nb2nodematch.club_profile                           8.774  &lt; 1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 118331  on 85358  degrees of freedom\n Residual Deviance:  20965  on 85344  degrees of freedom\n \nAIC: 20993  BIC: 21124  (Smaller is better. MC Std. Err. = 0.9958)\n\n\n\n\n\n\nWe now add the analogous terms (nodefactor and nodematch) for the first mode, students. Here we focus on student attributes, specifically for race. We will add b1factor and b1nodematch terms to the model (b1 indicating the first mode of a bipartite network). For b1factor, we test if there are differences in degree by racial groups (do white students join clubs at lower rates than Asian students?).\nFor b1nodematch, we test if clubs are segregated along racial lines. We will count the number of two paths, \\(i\\)-A-\\(j\\), where \\(i\\) and \\(j\\) are students of the same race and A is a club that both \\(i\\) and \\(j\\) are members of. Again, we can use the beta argument to set the discounting when summing up the two-paths that match on the attribute of interest. We will set beta to .15.\nTo help with estimation convergence, we will also set the reference category for the b1factor term to include Asian (1), Hispanic (3) and Native American (4) (basically collapsing some of the smaller categories into a single ‘other’ category). Finally, we will tweak the control parameters, increasing the burnin and `sample size. This can take a little while to run, so we might want to include parallel processing options to speed things up (here we set the number of processors to 4).\n\nmod5 &lt;- ergm(bnet96 ~ edges + \n               Offset(~nodemix(\"studentgender_clubgender\", \n                              levels2 = gender_terms), \n                      coef = offset_coefs_gender) + \n                Offset(~nodemix(\"studentgrade96_clubgrade\", \n                              levels2 = grade_terms), \n                      coef = offset_coefs_grade) + \n               b2factor(\"club_type_detailed\", levels = -2) + \n               b2nodematch(\"club_type_detailed\", beta = .25) + \n               b2factor(\"club_profile\", levels = c(3, 1, 4)) + \n               b2nodematch(\"club_profile\", beta = .25) +\n               b1factor(\"race\", levels = -c(1, 3, 4)) + \n               b1nodematch(\"race\", beta = .15), \n             control = control.ergm(MCMC.burnin = 30000, \n                                    MCMC.samplesize = 5000, \n                                    parallel = 4, \n                                    parallel.type = \"PSOCK\"))\nsummary(mod5)\n\nCall:\nergm(formula = bnet96 ~ edges + Offset(~nodemix(\"studentgender_clubgender\", \n    levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(\"studentgrade96_clubgrade\", \n    levels2 = grade_terms), coef = offset_coefs_grade) + b2factor(\"club_type_detailed\", \n    levels = -2) + b2nodematch(\"club_type_detailed\", beta = 0.25) + \n    b2factor(\"club_profile\", levels = c(3, 1, 4)) + b2nodematch(\"club_profile\", \n    beta = 0.25) + b1factor(\"race\", levels = -c(1, 3, 4)) + b1nodematch(\"race\", \n    beta = 0.15), control = control.ergm(MCMC.burnin = 30000, \n    MCMC.samplesize = 5000, parallel = 4, parallel.type = \"PSOCK\"))\n\nMonte Carlo Maximum Likelihood Results:\n\n                                                 Estimate Std. Error MCMC %\nedges                                            -6.83027    0.14712      0\nb2factor.club_type_detailed.Academic Competition -0.05914    0.05116      0\nb2factor.club_type_detailed.Ethnic Interest      -0.24224    0.11227      0\nb2factor.club_type_detailed.Individual Sports     0.39357    0.05757      0\nb2factor.club_type_detailed.Leadership            0.10532    0.13252      0\nb2factor.club_type_detailed.Media                 0.15606    0.09393      0\nb2factor.club_type_detailed.Performance Art       0.17197    0.03435      0\nb2factor.club_type_detailed.Service               0.14154    0.03296      0\nb2factor.club_type_detailed.Team Sports           0.95282    0.05173      0\nb2nodematch.club_type_detailed                    0.33851    0.06648      0\nb2factor.club_profile.moderate                    0.07439    0.04003      0\nb2factor.club_profile.high                       -0.10308    0.07348      0\nb2factor.club_profile.very_high                   0.16866    0.09232      0\nb2nodematch.club_profile                          0.69575    0.08628      0\nb1factor.race.black                              -1.11809    0.05122      0\nb1factor.race.white                              -1.51223    0.06228      0\nb1nodematch.race                                  5.04005    0.18534      0\n                                                 z value Pr(&gt;|z|)    \nedges                                            -46.426   &lt;1e-04 ***\nb2factor.club_type_detailed.Academic Competition  -1.156   0.2477    \nb2factor.club_type_detailed.Ethnic Interest       -2.158   0.0310 *  \nb2factor.club_type_detailed.Individual Sports      6.836   &lt;1e-04 ***\nb2factor.club_type_detailed.Leadership             0.795   0.4268    \nb2factor.club_type_detailed.Media                  1.661   0.0966 .  \nb2factor.club_type_detailed.Performance Art        5.006   &lt;1e-04 ***\nb2factor.club_type_detailed.Service                4.294   &lt;1e-04 ***\nb2factor.club_type_detailed.Team Sports           18.420   &lt;1e-04 ***\nb2nodematch.club_type_detailed                     5.092   &lt;1e-04 ***\nb2factor.club_profile.moderate                     1.858   0.0632 .  \nb2factor.club_profile.high                        -1.403   0.1607    \nb2factor.club_profile.very_high                    1.827   0.0677 .  \nb2nodematch.club_profile                           8.064   &lt;1e-04 ***\nb1factor.race.black                              -21.830   &lt;1e-04 ***\nb1factor.race.white                              -24.281   &lt;1e-04 ***\nb1nodematch.race                                  27.194   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 118331  on 85358  degrees of freedom\n Residual Deviance:  20437  on 85341  degrees of freedom\n \nAIC: 20471  BIC: 20630  (Smaller is better. MC Std. Err. = 1.304)\n\n\nInterpretation: It looks like black and white students are members of fewer clubs than Asian, Native American or Hispanic students (the reference), while there is segregation along racial lines (looking at the b1nodematch term). Students are unlikely to find themselves in clubs where there are few (or even no) students of the same race. For example, by chance we might expect Asian students (a small racial group) to often be in clubs with few other Asian students, but empirically this rarely happens. Looking at AIC and BIC, we see that the model fit is improved quite a bit from the previous model.\n\n\n\n\nDownload the same darta for 1997 and perform a similar analysis, i.e. fitting ERGMs with the same first and second mode terms. Can you note differences between the two years?\n\n\nbnet97 &lt;- readRDS(\"bnet97.rds\")\n\n\nPerform MCMC and GoF assessment on the models fitted for both years."
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms2.html#packages-needed",
    "href": "teaching/sna/material/10/10-ergms2.html#packages-needed",
    "title": "Social Network Analysis",
    "section": "",
    "text": "library(statnet) # also loads the ERGM package\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(intergraph)\nlibrary(patchwork)\nlibrary(networkdata)\n\n# download data (network object):\nbnet96 &lt;- readRDS(\"bnet96.rds\")"
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms2.html#model-1-only-edge-term",
    "href": "teaching/sna/material/10/10-ergms2.html#model-1-only-edge-term",
    "title": "Social Network Analysis",
    "section": "",
    "text": "We start with a simple model and just include a term for edges. The edges term is directly analogous to the edges term for one mode networks, except here we must remember that edges can only exist between actors of different modes, i.e. edges are only possible between women and dates for social events.\n\nmod1 &lt;- ergm(bnet96 ~ edges)\nsummary(mod1)\n\nCall:\nergm(formula = bnet96 ~ edges)\n\nMaximum Likelihood Results:\n\n      Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges -3.44427    0.01977      0  -174.3   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 118331  on 85358  degrees of freedom\n Residual Deviance:  23558  on 85357  degrees of freedom\n \nAIC: 23560  BIC: 23569  (Smaller is better. MC Std. Err. = 0)\n\n\nQ1 How do you interpret the coefficient here? Recall that the probability of a tie is then given by exp(-3.44427) / (1 + exp(-3.44427)) = .0309402."
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms2.html#model-2-gender-and-grade-specific-clubs",
    "href": "teaching/sna/material/10/10-ergms2.html#model-2-gender-and-grade-specific-clubs",
    "title": "Social Network Analysis",
    "section": "",
    "text": "We have some structural conditions to consider: some student-club combinations cannot occur, or occur at extremely low rates. We need to adjust our model for these cases, or run the risk of biasing the estimates. For example, girls are very unlikely to join boys sports teams (e.g., wrestling or football), while boys are unlikely to join girls sports teams (volleyball). There are similar structural issues with grade, as some clubs are restricted to certain grades. For example, students in grade 9, 10, 11 or 12 do not join eighth grade sports teams (e.g., 8th grade football).\nWe will include nodemix terms in the model for gender and grade, capturing the student-club combinations that are unlikely to exist. The two attributes of interest are studentgender_clubgender and studentgrade96_clubgrad.\nWe will start with gender. Remember that student gender is measured as male or female, while club gender is measured as boys, boys_girls, or girls. We want to print out the combinations of student-club genders (e.g., “male.boys”). By default, nodemix will print the terms sorted alphabetically, first by the second level (clubs) and then by the first level (students). Let’s create our vector of names (student-club combinations) with that ordering in mind.\n\nstudent_gender &lt;- sort(as.character(c(\"female\", \"male\")))\nclub_gender &lt;- sort(c(\"boys\", \"boys_girls\", \"girls\"))\n\ngender_levels &lt;- paste(rep(student_gender, times = length(club_gender)), \n                      rep(club_gender, each = length(student_gender)),\n                      sep = \".\" )\ndata.frame(gender_levels)\n\n      gender_levels\n1       female.boys\n2         male.boys\n3 female.boys_girls\n4   male.boys_girls\n5      female.girls\n6        male.girls\n\n\nThe terms of interest are female.boys and male.girls; as these are the rare events we want to adjust for, girls joining boys clubs and boys joining girls clubs. This corresponds to spots 1 and 6 from the summary statistics, so let’s create a vector holding that information.\n\ngender_terms &lt;- c(1, 6)\n\nAnd now we look at the grade attribute, studentgrade96_clubgrade. Student grade is measured as 8, 9, 10, 11 or 12. Club grade is measured as: eighth (just eighth graders), ninth (just ninth graders), ninth_tenth_eleventh (just ninth, tenth or eleventh graders), and ninth+ (no eighth graders). The value is all_grades if there are no restrictions on membership, in terms of grade. Let’s get all student-club combinations for grade and put them together to be consistent with the nodemix term (sorted by the second level and then by the first level):\n\nstudent_grades &lt;- sort(as.character(c(8:12)))\nclub_grades &lt;- sort(c(\"all_grades\", \"eight\", \"ninth\", \"ninth_tenth_eleventh\", \"ninth+\"))\n\ngrade_levels &lt;- paste(rep(student_grades, times = length(club_grades)), \n                      rep(club_grades, each = length(student_grades)),\n                      sep = \".\" )\ndata.frame(grade_levels)\n\n              grade_levels\n1            10.all_grades\n2            11.all_grades\n3            12.all_grades\n4             8.all_grades\n5             9.all_grades\n6                 10.eight\n7                 11.eight\n8                 12.eight\n9                  8.eight\n10                 9.eight\n11                10.ninth\n12                11.ninth\n13                12.ninth\n14                 8.ninth\n15                 9.ninth\n16 10.ninth_tenth_eleventh\n17 11.ninth_tenth_eleventh\n18 12.ninth_tenth_eleventh\n19  8.ninth_tenth_eleventh\n20  9.ninth_tenth_eleventh\n21               10.ninth+\n22               11.ninth+\n23               12.ninth+\n24                8.ninth+\n25                9.ninth+\n\n\nWe want to include terms for any student-club combination that should not exist; like 12th graders in a ninth grade club, 12.ninth. Based on the order from the summary statistics, this corresponds to: 6 (10.eighth), 7 (11.eighth), 8 (12.eighth), 10 (9.eighth), 11 (10.ninth), 12 (11.ninth), 13 (12.ninth), 14 (8.ninth), 18 (12.ninth_tenth_eleventh), 19 (8.ninth_tenth_eleventh) and 24 (8.ninth+).\n\ngrade_terms &lt;- c(6, 7, 8, 10, 11, 12, 13, 14, 18, 19, 24)\n\nWe an now estimate our model, including nodemix terms for studentgender_clubgender and studentgrade96_clubgrade. We will specify which terms to include using the levels2 argument, setting it to the vectors defined above. To simplify the estimation of the model, we will specify these terms using an Offset() function (although we could have estimated the coefficients within the model). When using Offset(), the coefficients are not estimated and are instead set using the values supplied in the coef argument. This is appropriate in our case as the coefficients are based on logical conditions (e.g., 12th graders do not join 9th grade clubs) and can be set a priori by the researcher. Here, we set the coefficients to -10 for every term. We set the coefficients to -10 as we want to estimate the models conditioned on the fact that these student-club combinations are very rare.\n\noffset_coefs_gender &lt;- rep(-10, length(gender_terms))\noffset_coefs_grade &lt;- rep(-10, length(grade_terms))\n\nmod2 &lt;- ergm(bnet96 ~ edges + \n               Offset(~ nodemix(\"studentgender_clubgender\", \n                              levels2 = gender_terms), \n                      coef = offset_coefs_gender) + \n               Offset(~ nodemix(\"studentgrade96_clubgrade\", \n                              levels2 = grade_terms), \n                      coef = offset_coefs_grade))\nsummary(mod2)\n\nCall:\nergm(formula = bnet96 ~ edges + Offset(~nodemix(\"studentgender_clubgender\", \n    levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(\"studentgrade96_clubgrade\", \n    levels2 = grade_terms), coef = offset_coefs_grade))\n\nMaximum Likelihood Results:\n\n      Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges -2.99179    0.01994      0    -150   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 118331  on 85358  degrees of freedom\n Residual Deviance:  21515  on 85357  degrees of freedom\n \nAIC: 21517  BIC: 21526  (Smaller is better. MC Std. Err. = 0)\n\n\nAll of the offset terms are set to -10, although they are not printed out here. We see that the edge coefficient is different than with mod1, suggesting the importance of adjusting our model for structural/logical constraints. Note also that the model fit should only be compared to other models with the same set of offset terms.\nWe used nodemix terms to capture structural conditions in the data, but we could imagine using nodemix terms to answer more substantive questions. We could test if certain types of students (e.g., girls) are more likely to join certain types of clubs (academic, leadership, etc.), and this is left as an exercise."
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms2.html#second-mode-terms-node-factor-homphily",
    "href": "teaching/sna/material/10/10-ergms2.html#second-mode-terms-node-factor-homphily",
    "title": "Social Network Analysis",
    "section": "",
    "text": "We now add nodefactor and homophily terms to our model. With two-mode networks, these terms can take two forms, with different terms for the first and second mode. Here we focus on the second mode, the clubs.\n\n\nnodefactor terms capture differences in degree across nodes, here clubs, with different attributes. We are interested in the main effect of club type (sports, academic, etc.) and club profile (low, moderate, high, very high) on membership. For example, do high profile clubs have more members than low profile clubs?\nThe term of interest is b2factor (b2 indicating the second mode of a bipartite network). We will include b2factor terms for each club attribute of interest, club_type_detailed and club_profile. We include a levels argument for club_profile to set the order that the results are printed. By default, the results are printed in alphabetical order. In this case, that would correspond to high (1), low (2), moderate (3), very high (4), with high excluded as the reference. But we want the results to run from moderate (3) to high (1) to very high (4), with low (2) excluded (as this is easier to interpret). For club_type_detailed, we use the levels argument to set the second category, academic interest, as the reference. We set levels to -2 to exclude only the second category.\n\nmod3 &lt;- ergm(bnet96 ~ edges + \n               Offset(~nodemix(\"studentgender_clubgender\", \n                              levels2 = gender_terms), \n                      coef = offset_coefs_gender) + \n               Offset(~nodemix(\"studentgrade96_clubgrade\", \n                              levels2 = grade_terms), \n                      coef = offset_coefs_grade) + \n                b2factor(\"club_type_detailed\", levels = -2) + \n                b2factor(\"club_profile\", levels = c(3, 1, 4)))\nsummary(mod3)\n\nCall:\nergm(formula = bnet96 ~ edges + Offset(~nodemix(\"studentgender_clubgender\", \n    levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(\"studentgrade96_clubgrade\", \n    levels2 = grade_terms), coef = offset_coefs_grade) + b2factor(\"club_type_detailed\", \n    levels = -2) + b2factor(\"club_profile\", levels = c(3, 1, \n    4)))\n\nMaximum Likelihood Results:\n\n                                                 Estimate Std. Error MCMC %\nedges                                            -2.82059    0.04035      0\nb2factor.club_type_detailed.Academic Competition -0.44051    0.08242      0\nb2factor.club_type_detailed.Ethnic Interest      -1.03174    0.16677      0\nb2factor.club_type_detailed.Individual Sports    -0.41835    0.08391      0\nb2factor.club_type_detailed.Leadership           -0.36924    0.18887      0\nb2factor.club_type_detailed.Media                -0.16730    0.14109      0\nb2factor.club_type_detailed.Performance Art       0.10725    0.06436      0\nb2factor.club_type_detailed.Service               0.31118    0.06262      0\nb2factor.club_type_detailed.Team Sports           0.45881    0.07798      0\nb2factor.club_profile.moderate                   -0.29070    0.05741      0\nb2factor.club_profile.high                       -0.94788    0.09396      0\nb2factor.club_profile.very_high                  -0.48185    0.11402      0\n                                                 z value Pr(&gt;|z|)    \nedges                                            -69.902   &lt;1e-04 ***\nb2factor.club_type_detailed.Academic Competition  -5.345   &lt;1e-04 ***\nb2factor.club_type_detailed.Ethnic Interest       -6.186   &lt;1e-04 ***\nb2factor.club_type_detailed.Individual Sports     -4.986   &lt;1e-04 ***\nb2factor.club_type_detailed.Leadership            -1.955   0.0506 .  \nb2factor.club_type_detailed.Media                 -1.186   0.2357    \nb2factor.club_type_detailed.Performance Art        1.666   0.0956 .  \nb2factor.club_type_detailed.Service                4.970   &lt;1e-04 ***\nb2factor.club_type_detailed.Team Sports            5.883   &lt;1e-04 ***\nb2factor.club_profile.moderate                    -5.064   &lt;1e-04 ***\nb2factor.club_profile.high                       -10.088   &lt;1e-04 ***\nb2factor.club_profile.very_high                   -4.226   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 118331  on 85358  degrees of freedom\n Residual Deviance:  21116  on 85346  degrees of freedom\n \nAIC: 21140  BIC: 21252  (Smaller is better. MC Std. Err. = 0)\n\n\nWe see, for example, that competitive academic clubs and individual sports tend to have fewer members than academic interest clubs (the reference), while service clubs (like National Honors Society) and team sports tend to be large. We also see that high profile clubs have, if anything, fewer members than low profile clubs.\nMore formally, we can interpret the coefficient on individual sports (for example) as follows: the odds of a student being part of an individual sports team is exp(-0.41835) times lower than the odds of being part of an academic interest club. It is worth emphasizing that b2factor is based only on club attributes, and is thus different from nodemix (see above), which incorporates attributes from both modes.\n\n\n\nHomophily is more complicated with two-mode networks than with one-mode networks. This is the case as there are no direct ties from nodes of the same type; in our case, there are no ties from students to students or from clubs to clubs. So, if we are interested in homophily on a club attribute, say club type, we cannot ask if team sports are tied to other team sports, as there are no ties between clubs. Instead, we must ask if similar clubs are linked together through students; e.g., do students in team sports tend to be members of other team sports?\nWe must counts two-paths, based on homophily on the attribute of interest. The basic idea is to sum up the number of times that we see A-\\(i\\)-B, where A and B are clubs with the same attribute (e.g., both team sports) and \\(i\\) is a student in both A and B. The count is technically over half the number of two-paths, to avoid double counting (as A-\\(i\\)-B is substantively the same as B-\\(i\\)-A). The term is b2nodematch. A positive coefficient on b2nodematch would indicate that students are members of similar kinds of clubs, above what can be explained by other terms in the model.\nBut how do we sum up the homophilous two-paths? In the simplest case, we can sum up all of the two-paths that match on the attribute of interest. This is the default specification. We may, however, have good reason to incorporate some discounting, so that adding one more two-path (for a given edge) only marginally increases the count, once we reach a certain threshold. For example, if student \\(i\\) is a member of the football team, (\\(i\\)-football edge), then if \\(i\\) is also a member of the wrestling team, that would be a strong signal of matching on club type (both team sports). But adding another team sport membership, say \\(i\\) is also a member of the baseball team, may not offer quite as much information; as we already know that \\(i\\) joins team sports. We may then want to count the second two-path less than the first.\nWe can control the discounting using a beta parameter, which raises the count of two-paths (for a given edge) to beta. Setting beta to 1 would yield the number of two-paths (for an edge) where there is a match on the club attribute of interest (so the \\(i\\)-football edge would contribute a count of 2). Setting beta to 0 gives us the other extreme: showing if the given edge is involved in at least one homophilous two-path (so the \\(i\\)-football edge would contribute a count of 1). The count of two-paths, raised to beta, is then summed over all edges and divided by 2.\nWe set beta to .25, but we could imagine using a range of values (estimating the model each time), using model fit statistics to evaluate the choice of beta.\n\nset.seed(1007)\n\nmod4 &lt;- ergm(bnet96 ~ edges + \n                Offset(~nodemix(\"studentgender_clubgender\", \n                              levels2 = gender_terms), \n                      coef = offset_coefs_gender) + \n                Offset(~nodemix(\"studentgrade96_clubgrade\", \n                              levels2 = grade_terms), \n                      coef = offset_coefs_grade) + \n                b2factor(\"club_type_detailed\", levels = -2) + \n                b2nodematch(\"club_type_detailed\", beta = .25) + \n                b2factor(\"club_profile\", levels = c(3, 1, 4)) + \n                b2nodematch(\"club_profile\", beta = .25), \n              control = control.ergm(MCMC.burnin = 20000,\n                                     MCMC.samplesize = 3000))\n\nThe model is now being fit using MCMC techniques. This means that we should check if the model is fitting well. Use the mcmc.diagnostics() function.\nQ2 How would you intepret the below results (not plotted here)?\n\nmcmc.diagnostics(mod4)\n\nGo ahead a summarize model 4:\n\nsummary(mod4)\n\nCall:\nergm(formula = bnet96 ~ edges + Offset(~nodemix(\"studentgender_clubgender\", \n    levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(\"studentgrade96_clubgrade\", \n    levels2 = grade_terms), coef = offset_coefs_grade) + b2factor(\"club_type_detailed\", \n    levels = -2) + b2nodematch(\"club_type_detailed\", beta = 0.25) + \n    b2factor(\"club_profile\", levels = c(3, 1, 4)) + b2nodematch(\"club_profile\", \n    beta = 0.25), control = control.ergm(MCMC.burnin = 20000, \n    MCMC.samplesize = 3000))\n\nMonte Carlo Maximum Likelihood Results:\n\n                                                  Estimate Std. Error MCMC %\nedges                                            -3.596083   0.075080      0\nb2factor.club_type_detailed.Academic Competition -0.323232   0.085331      0\nb2factor.club_type_detailed.Ethnic Interest      -0.844453   0.169750      0\nb2factor.club_type_detailed.Individual Sports    -0.296443   0.083610      0\nb2factor.club_type_detailed.Leadership           -0.171296   0.193888      0\nb2factor.club_type_detailed.Media                 0.028381   0.138718      0\nb2factor.club_type_detailed.Performance Art       0.129749   0.059862      0\nb2factor.club_type_detailed.Service               0.361836   0.059245      0\nb2factor.club_type_detailed.Team Sports           0.502725   0.075382      0\nb2nodematch.club_type_detailed                    0.413830   0.070045      0\nb2factor.club_profile.moderate                   -0.075008   0.059536      0\nb2factor.club_profile.high                       -0.538820   0.098625      0\nb2factor.club_profile.very_high                  -0.003789   0.124166      0\nb2nodematch.club_profile                          0.779436   0.088837      0\n                                                 z value Pr(&gt;|z|)    \nedges                                            -47.897  &lt; 1e-04 ***\nb2factor.club_type_detailed.Academic Competition  -3.788 0.000152 ***\nb2factor.club_type_detailed.Ethnic Interest       -4.975  &lt; 1e-04 ***\nb2factor.club_type_detailed.Individual Sports     -3.546 0.000392 ***\nb2factor.club_type_detailed.Leadership            -0.883 0.376978    \nb2factor.club_type_detailed.Media                  0.205 0.837889    \nb2factor.club_type_detailed.Performance Art        2.167 0.030198 *  \nb2factor.club_type_detailed.Service                6.107  &lt; 1e-04 ***\nb2factor.club_type_detailed.Team Sports            6.669  &lt; 1e-04 ***\nb2nodematch.club_type_detailed                     5.908  &lt; 1e-04 ***\nb2factor.club_profile.moderate                    -1.260 0.207713    \nb2factor.club_profile.high                        -5.463  &lt; 1e-04 ***\nb2factor.club_profile.very_high                   -0.031 0.975654    \nb2nodematch.club_profile                           8.774  &lt; 1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 118331  on 85358  degrees of freedom\n Residual Deviance:  20965  on 85344  degrees of freedom\n \nAIC: 20993  BIC: 21124  (Smaller is better. MC Std. Err. = 0.9958)"
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms2.html#first-mode-terms-node-factor-homphily",
    "href": "teaching/sna/material/10/10-ergms2.html#first-mode-terms-node-factor-homphily",
    "title": "Social Network Analysis",
    "section": "",
    "text": "We now add the analogous terms (nodefactor and nodematch) for the first mode, students. Here we focus on student attributes, specifically for race. We will add b1factor and b1nodematch terms to the model (b1 indicating the first mode of a bipartite network). For b1factor, we test if there are differences in degree by racial groups (do white students join clubs at lower rates than Asian students?).\nFor b1nodematch, we test if clubs are segregated along racial lines. We will count the number of two paths, \\(i\\)-A-\\(j\\), where \\(i\\) and \\(j\\) are students of the same race and A is a club that both \\(i\\) and \\(j\\) are members of. Again, we can use the beta argument to set the discounting when summing up the two-paths that match on the attribute of interest. We will set beta to .15.\nTo help with estimation convergence, we will also set the reference category for the b1factor term to include Asian (1), Hispanic (3) and Native American (4) (basically collapsing some of the smaller categories into a single ‘other’ category). Finally, we will tweak the control parameters, increasing the burnin and `sample size. This can take a little while to run, so we might want to include parallel processing options to speed things up (here we set the number of processors to 4).\n\nmod5 &lt;- ergm(bnet96 ~ edges + \n               Offset(~nodemix(\"studentgender_clubgender\", \n                              levels2 = gender_terms), \n                      coef = offset_coefs_gender) + \n                Offset(~nodemix(\"studentgrade96_clubgrade\", \n                              levels2 = grade_terms), \n                      coef = offset_coefs_grade) + \n               b2factor(\"club_type_detailed\", levels = -2) + \n               b2nodematch(\"club_type_detailed\", beta = .25) + \n               b2factor(\"club_profile\", levels = c(3, 1, 4)) + \n               b2nodematch(\"club_profile\", beta = .25) +\n               b1factor(\"race\", levels = -c(1, 3, 4)) + \n               b1nodematch(\"race\", beta = .15), \n             control = control.ergm(MCMC.burnin = 30000, \n                                    MCMC.samplesize = 5000, \n                                    parallel = 4, \n                                    parallel.type = \"PSOCK\"))\nsummary(mod5)\n\nCall:\nergm(formula = bnet96 ~ edges + Offset(~nodemix(\"studentgender_clubgender\", \n    levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(\"studentgrade96_clubgrade\", \n    levels2 = grade_terms), coef = offset_coefs_grade) + b2factor(\"club_type_detailed\", \n    levels = -2) + b2nodematch(\"club_type_detailed\", beta = 0.25) + \n    b2factor(\"club_profile\", levels = c(3, 1, 4)) + b2nodematch(\"club_profile\", \n    beta = 0.25) + b1factor(\"race\", levels = -c(1, 3, 4)) + b1nodematch(\"race\", \n    beta = 0.15), control = control.ergm(MCMC.burnin = 30000, \n    MCMC.samplesize = 5000, parallel = 4, parallel.type = \"PSOCK\"))\n\nMonte Carlo Maximum Likelihood Results:\n\n                                                 Estimate Std. Error MCMC %\nedges                                            -6.83027    0.14712      0\nb2factor.club_type_detailed.Academic Competition -0.05914    0.05116      0\nb2factor.club_type_detailed.Ethnic Interest      -0.24224    0.11227      0\nb2factor.club_type_detailed.Individual Sports     0.39357    0.05757      0\nb2factor.club_type_detailed.Leadership            0.10532    0.13252      0\nb2factor.club_type_detailed.Media                 0.15606    0.09393      0\nb2factor.club_type_detailed.Performance Art       0.17197    0.03435      0\nb2factor.club_type_detailed.Service               0.14154    0.03296      0\nb2factor.club_type_detailed.Team Sports           0.95282    0.05173      0\nb2nodematch.club_type_detailed                    0.33851    0.06648      0\nb2factor.club_profile.moderate                    0.07439    0.04003      0\nb2factor.club_profile.high                       -0.10308    0.07348      0\nb2factor.club_profile.very_high                   0.16866    0.09232      0\nb2nodematch.club_profile                          0.69575    0.08628      0\nb1factor.race.black                              -1.11809    0.05122      0\nb1factor.race.white                              -1.51223    0.06228      0\nb1nodematch.race                                  5.04005    0.18534      0\n                                                 z value Pr(&gt;|z|)    \nedges                                            -46.426   &lt;1e-04 ***\nb2factor.club_type_detailed.Academic Competition  -1.156   0.2477    \nb2factor.club_type_detailed.Ethnic Interest       -2.158   0.0310 *  \nb2factor.club_type_detailed.Individual Sports      6.836   &lt;1e-04 ***\nb2factor.club_type_detailed.Leadership             0.795   0.4268    \nb2factor.club_type_detailed.Media                  1.661   0.0966 .  \nb2factor.club_type_detailed.Performance Art        5.006   &lt;1e-04 ***\nb2factor.club_type_detailed.Service                4.294   &lt;1e-04 ***\nb2factor.club_type_detailed.Team Sports           18.420   &lt;1e-04 ***\nb2nodematch.club_type_detailed                     5.092   &lt;1e-04 ***\nb2factor.club_profile.moderate                     1.858   0.0632 .  \nb2factor.club_profile.high                        -1.403   0.1607    \nb2factor.club_profile.very_high                    1.827   0.0677 .  \nb2nodematch.club_profile                           8.064   &lt;1e-04 ***\nb1factor.race.black                              -21.830   &lt;1e-04 ***\nb1factor.race.white                              -24.281   &lt;1e-04 ***\nb1nodematch.race                                  27.194   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 118331  on 85358  degrees of freedom\n Residual Deviance:  20437  on 85341  degrees of freedom\n \nAIC: 20471  BIC: 20630  (Smaller is better. MC Std. Err. = 1.304)\n\n\nInterpretation: It looks like black and white students are members of fewer clubs than Asian, Native American or Hispanic students (the reference), while there is segregation along racial lines (looking at the b1nodematch term). Students are unlikely to find themselves in clubs where there are few (or even no) students of the same race. For example, by chance we might expect Asian students (a small racial group) to often be in clubs with few other Asian students, but empirically this rarely happens. Looking at AIC and BIC, we see that the model fit is improved quite a bit from the previous model."
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms2.html#exercise",
    "href": "teaching/sna/material/10/10-ergms2.html#exercise",
    "title": "Social Network Analysis",
    "section": "",
    "text": "Download the same darta for 1997 and perform a similar analysis, i.e. fitting ERGMs with the same first and second mode terms. Can you note differences between the two years?\n\n\nbnet97 &lt;- readRDS(\"bnet97.rds\")\n\n\nPerform MCMC and GoF assessment on the models fitted for both years."
  },
  {
    "objectID": "teaching/sna/material/07/07-netviz-2.html",
    "href": "teaching/sna/material/07/07-netviz-2.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "library(igraph)\nlibrary(graphlayouts)\nlibrary(ggraph)\nlibrary(networkdata)"
  },
  {
    "objectID": "teaching/sna/material/07/07-netviz-2.html#load-packages",
    "href": "teaching/sna/material/07/07-netviz-2.html#load-packages",
    "title": "Social Network Analysis",
    "section": "",
    "text": "library(igraph)\nlibrary(graphlayouts)\nlibrary(ggraph)\nlibrary(networkdata)"
  },
  {
    "objectID": "teaching/sna/material/07/07-netviz-2.html#dynamic-layouts",
    "href": "teaching/sna/material/07/07-netviz-2.html#dynamic-layouts",
    "title": "Social Network Analysis",
    "section": "Dynamic layouts",
    "text": "Dynamic layouts\n\nlibrary(gganimate)\nlibrary(ggplot2)\nlibrary(patchwork)\n# also install package 'gifski' to create gifs\n\n\n# downloaded from https://www.stats.ox.ac.uk/~snijders/siena/siena_datasets.htm\ndata(\"s50\")\ns50\n\n[[1]]\nIGRAPH dd2aeb6 UN-- 50 74 -- \n+ attr: name (v/c), smoke (v/n)\n+ edges from dd2aeb6 (vertex names):\n [1] V1 --V11 V1 --V14 V2 --V7  V2 --V11 V3 --V4  V3 --V9  V4 --V9  V5 --V32\n [9] V6 --V8  V7 --V12 V7 --V26 V7 --V42 V7 --V44 V10--V11 V10--V14 V10--V15\n[17] V10--V33 V11--V14 V11--V15 V11--V16 V11--V19 V11--V30 V12--V42 V12--V44\n[25] V15--V16 V17--V18 V17--V19 V17--V21 V17--V22 V17--V24 V18--V19 V18--V35\n[33] V19--V24 V19--V26 V19--V30 V21--V22 V21--V24 V21--V31 V21--V32 V22--V24\n[41] V22--V25 V22--V31 V22--V34 V22--V43 V23--V24 V25--V31 V25--V32 V26--V29\n[49] V26--V30 V26--V44 V27--V28 V27--V29 V27--V30 V29--V30 V29--V33 V30--V33\n[57] V31--V32 V31--V34 V31--V37 V32--V37 V34--V37 V36--V38 V36--V41 V38--V41\n+ ... omitted several edges\n\n[[2]]\nIGRAPH 6e28c77 UN-- 50 81 -- \n+ attr: name (v/c), smoke (v/n)\n+ edges from 6e28c77 (vertex names):\n [1] V1 --V10 V1 --V11 V1 --V14 V1 --V33 V2 --V26 V3 --V4  V3 --V9  V4 --V5 \n [9] V4 --V17 V4 --V34 V5 --V17 V6 --V8  V6 --V35 V7 --V26 V7 --V44 V10--V11\n[17] V10--V14 V10--V33 V11--V14 V11--V19 V11--V26 V11--V30 V12--V15 V12--V26\n[25] V12--V42 V12--V44 V15--V16 V15--V36 V15--V42 V16--V26 V16--V42 V16--V44\n[33] V17--V22 V17--V24 V17--V27 V17--V32 V18--V35 V19--V21 V19--V23 V19--V30\n[41] V19--V36 V19--V41 V21--V31 V21--V37 V21--V40 V22--V24 V23--V50 V24--V25\n[49] V24--V28 V25--V27 V25--V28 V25--V32 V26--V42 V27--V28 V28--V35 V29--V30\n[57] V29--V33 V29--V42 V30--V33 V30--V36 V30--V41 V31--V32 V31--V37 V32--V37\n+ ... omitted several edges\n\n[[3]]\nIGRAPH 9285b7f UN-- 50 77 -- \n+ attr: name (v/c), smoke (v/n)\n+ edges from 9285b7f (vertex names):\n [1] V1 --V10 V1 --V11 V1 --V14 V1 --V41 V2 --V7  V2 --V23 V2 --V26 V3 --V4 \n [9] V3 --V9  V3 --V34 V4 --V32 V4 --V34 V5 --V17 V5 --V32 V6 --V24 V6 --V27\n[17] V6 --V28 V7 --V16 V7 --V26 V7 --V42 V7 --V44 V8 --V25 V10--V11 V10--V12\n[25] V10--V14 V10--V33 V11--V14 V11--V15 V11--V33 V12--V15 V12--V33 V14--V33\n[33] V15--V29 V15--V33 V15--V36 V16--V26 V16--V42 V16--V44 V17--V22 V17--V27\n[41] V19--V29 V19--V30 V19--V36 V21--V31 V21--V37 V21--V40 V21--V45 V24--V27\n[49] V24--V28 V25--V50 V26--V44 V27--V28 V29--V30 V29--V33 V30--V33 V30--V36\n[57] V31--V37 V35--V37 V35--V50 V36--V38 V36--V41 V37--V47 V38--V41 V39--V43\n+ ... omitted several edges\n\n\nThe dataset consists of three networks with 50 actors each and a vertex attribute for the smoking behaviour of students. As a first step, we need to create a layout for all three networks. You can basically use any type of layout for each network, but I’d recommend layout_as_dynamic() from the package {{graphlayouts}}. The algorithm calculates a reference layout which is a layout of the union of all networks and individual layouts based on stress minimization and combines those in a linear combination which is controlled by the alpha parameter. For alpha=1, only the reference layout is used and all graphs have the same layout. For alpha=0, the stress layout of each individual graph is used. Values in-between interpolate between the two layouts.\n\n# Try other values for alpha\nxy &lt;- layout_as_dynamic(s50, alpha = 0.2)\npList &lt;- vector(\"list\", length(s50))\n\n\n#static plots\nfor (i in 1:length(s50)) {\n  pList[[i]] &lt;- ggraph(s50[[i]], layout = \"manual\", x = xy[[i]][, 1], y = xy[[i]][, 2]) +\n    geom_edge_link0(edge_width = 0.6, edge_colour = \"grey66\") +\n    geom_node_point(shape = 21, aes(fill = as.factor(smoke)), size = 6) +\n    geom_node_text(label = 1:50, repel = FALSE, color = \"white\", size = 4) +\n    scale_fill_manual(\n      values = c(\"forestgreen\", \"grey25\", \"firebrick\"),\n      guide = ifelse(i != 2, \"none\", \"legend\"),\n      name = \"smoking\",\n      labels = c(\"never\", \"occasionally\", \"regularly\")\n    ) +\n    theme_graph() +\n    theme(legend.position = \"bottom\") +\n    labs(title = paste0(\"Wave \", i))\n}\n# Reduce(\"+\", pList)\npList[[1]] + pList[[2]] + pList[[3]]\n\n\n\n\n\n\n\n\nThis is nice but of course we want to animate the changes. This is where we say goodbye to ggraph and hello to good-old ggplot2. First, we create a list of data frames for all nodes and add the layout to it.\n\n# create a list which contains all nodes and layout\nnodes_lst &lt;- lapply(1:length(s50), function(i) {\n  cbind(igraph::as_data_frame(s50[[i]], \"vertices\"),\n    x = xy[[i]][, 1], y = xy[[i]][, 2], frame = i\n  )\n})\n\n\nedges_lst &lt;- lapply(1:length(s50), function(i) {\n  cbind(igraph::as_data_frame(s50[[i]], \"edges\"), frame = i)\n})\n\nedges_lst &lt;- lapply(1:length(s50), function(i) {\n  edges_lst[[i]]$x &lt;- nodes_lst[[i]]$x[match(edges_lst[[i]]$from, nodes_lst[[i]]$name)]\n  edges_lst[[i]]$y &lt;- nodes_lst[[i]]$y[match(edges_lst[[i]]$from, nodes_lst[[i]]$name)]\n  edges_lst[[i]]$xend &lt;- nodes_lst[[i]]$x[match(edges_lst[[i]]$to, nodes_lst[[i]]$name)]\n  edges_lst[[i]]$yend &lt;- nodes_lst[[i]]$y[match(edges_lst[[i]]$to, nodes_lst[[i]]$name)]\n  edges_lst[[i]]$id &lt;- paste0(edges_lst[[i]]$from, \"-\", edges_lst[[i]]$to)\n  edges_lst[[i]]$status &lt;- TRUE\n  edges_lst[[i]]\n})\n\nhead(edges_lst[[1]])\n\n  from  to frame         x         y      xend       yend     id status\n1   V1 V11     1  1.803046  0.247353  2.123787 -0.7457415 V1-V11   TRUE\n2   V1 V14     1  1.803046  0.247353  2.391336  0.2295657 V1-V14   TRUE\n3   V2  V7     1  3.623264 -1.313577  3.870197 -1.9488863  V2-V7   TRUE\n4   V2 V11     1  3.623264 -1.313577  2.123787 -0.7457415 V2-V11   TRUE\n5   V3  V4     1 -4.877254 -2.719843 -3.854924 -2.8766208  V3-V4   TRUE\n6   V3  V9     1 -4.877254 -2.719843 -5.416373 -3.4092101  V3-V9   TRUE\n\n\nWe have expanded the edge data frame in a way that also includes the coordinates of the endpoints from the layout that we calculated earlier.\nNow we create a helper matrix which includes all edges that are present in any of the networks.\n\nall_edges &lt;- do.call(\"rbind\", lapply(s50, get.edgelist))\nall_edges &lt;- all_edges[!duplicated(all_edges), ]\nall_edges &lt;- cbind(all_edges, paste0(all_edges[, 1], \"-\", all_edges[, 2]))\n\nThis is used to impute the edges into all networks. So any edge that is not present in time frame two and three gets added to time frame one. But to keep track of these, we set there status to FALSE.\n\nedges_lst &lt;- lapply(1:length(s50), function(i) {\n  idx &lt;- which(!all_edges[, 3] %in% edges_lst[[i]]$id)\n  if (length(idx != 0)) {\n    tmp &lt;- data.frame(from = all_edges[idx, 1], to = all_edges[idx, 2], id = all_edges[idx, 3])\n    tmp$x &lt;- nodes_lst[[i]]$x[match(tmp$from, nodes_lst[[i]]$name)]\n    tmp$y &lt;- nodes_lst[[i]]$y[match(tmp$from, nodes_lst[[i]]$name)]\n    tmp$xend &lt;- nodes_lst[[i]]$x[match(tmp$to, nodes_lst[[i]]$name)]\n    tmp$yend &lt;- nodes_lst[[i]]$y[match(tmp$to, nodes_lst[[i]]$name)]\n    tmp$frame &lt;- i\n    tmp$status &lt;- FALSE\n    edges_lst[[i]] &lt;- rbind(edges_lst[[i]], tmp)\n  }\n  edges_lst[[i]]\n})\n\nWhy are we doing this? After a lot of experimenting, I came to the conclusion that it is always best to draw all edges, but use zero opacity if status = FALSE. In that way, one gets a smoother transition for edges that (dis)appear. There are probably other workarounds though.\nIn the last step, we create a data frame out of the lists.\n\nedges_df &lt;- do.call(\"rbind\", edges_lst)\nnodes_df &lt;- do.call(\"rbind\", nodes_lst)\n\nhead(edges_df)\n\n  from  to frame         x         y      xend       yend     id status\n1   V1 V11     1  1.803046  0.247353  2.123787 -0.7457415 V1-V11   TRUE\n2   V1 V14     1  1.803046  0.247353  2.391336  0.2295657 V1-V14   TRUE\n3   V2  V7     1  3.623264 -1.313577  3.870197 -1.9488863  V2-V7   TRUE\n4   V2 V11     1  3.623264 -1.313577  2.123787 -0.7457415 V2-V11   TRUE\n5   V3  V4     1 -4.877254 -2.719843 -3.854924 -2.8766208  V3-V4   TRUE\n6   V3  V9     1 -4.877254 -2.719843 -5.416373 -3.4092101  V3-V9   TRUE\n\nhead(nodes_df)\n\n   name smoke         x         y frame\nV1   V1     2  1.803046  0.247353     1\nV2   V2     3  3.623264 -1.313577     1\nV3   V3     1 -4.877254 -2.719843     1\nV4   V4     1 -3.854924 -2.876621     1\nV5   V5     1 -2.824439 -3.368968     1\nV6   V6     1 -1.529341 -5.292054     1\n\n\nAnd that’s it in terms of data wrangling. All that is left is to plot/animate the data.\n\nggplot() +\n  geom_segment(\n    data = edges_df,\n    aes(x = x, xend = xend, y = y, yend = yend, group = id, alpha = status),\n    show.legend = FALSE\n  ) +\n  geom_point(\n    data = nodes_df, aes(x, y, group = name, fill = as.factor(smoke)),\n    shape = 21, size = 4, show.legend = FALSE\n  ) +\n  scale_fill_manual(values = c(\"forestgreen\", \"grey25\", \"firebrick\")) +\n  scale_alpha_manual(values = c(0, 1)) +\n  ease_aes(\"quadratic-in-out\") +\n  transition_states(frame, state_length = 0.5, wrap = FALSE) +\n  labs(title = \"Wave {closest_state}\") +\n  theme_void()"
  },
  {
    "objectID": "teaching/sna/material/07/07-netviz-2.html#interactive-plots-with-visnetwork",
    "href": "teaching/sna/material/07/07-netviz-2.html#interactive-plots-with-visnetwork",
    "title": "Social Network Analysis",
    "section": "Interactive plots with visNetwork",
    "text": "Interactive plots with visNetwork\n\nlibrary(visNetwork)\ndata(\"karate\")\n\n\nvisIgraph(karate)\n\n\n\n\n\n\nkarate_df &lt;- toVisNetworkData(karate)\nvisNetwork(nodes = karate_df$nodes, \n           edges = karate_df$edges, height = \"300px\")"
  },
  {
    "objectID": "teaching/sna/material/07/07-netviz-2.html#gimmicks",
    "href": "teaching/sna/material/07/07-netviz-2.html#gimmicks",
    "title": "Social Network Analysis",
    "section": "Gimmicks",
    "text": "Gimmicks\nThe ggforce package works pretty nicely with ggraph. You can, for instance, use the geom_mark_*() functions to highlight clusters.\n\nlibrary(ggforce)\n\n\nset.seed(665)\n\n#create network with a group structure\ng &lt;- sample_islands(9, 40, 0.4, 15)\ng &lt;- simplify(g)\nV(g)$grp &lt;- as.character(rep(1:9, each = 40))\n\n\nggraph(g, layout = \"backbone\", keep = 0.4) +\n  geom_edge_link0(edge_color = \"grey66\", edge_width = 0.2) +\n  geom_node_point(aes(fill = grp), shape = 21, size = 3) +\n  geom_mark_hull(\n    aes(x, y, group = grp, fill = grp),\n    concavity = 4,\n    expand = unit(2, \"mm\"),\n    alpha = 0.25\n  ) +\n  scale_color_brewer(palette = \"Set1\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme_graph()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nOf course you can also add a label to your clusters.\n\nggraph(g, layout = \"backbone\", keep = 0.4) +\n  geom_edge_link0(edge_color = \"grey66\", edge_width = 0.2) +\n  geom_node_point(aes(fill = grp), shape = 21, size = 3) +\n  geom_mark_hull(\n    aes(x, y, group = grp, fill = grp, label=grp),\n    concavity = 4,\n    expand = unit(2, \"mm\"),\n    alpha = 0.25\n  ) +\n  scale_color_brewer(palette = \"Set1\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme_graph()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n“How can I achieve that my directed edges stop at the node border, independent from the node size?”\n\nOut of the box you will probably end up with something like this\n\n# create a random network\nset.seed(1071)\ng &lt;- sample_pa(30, 1)\nV(g)$degree &lt;- degree(g, mode = \"in\")\n\nggraph(g, \"stress\") +\n  geom_edge_link(\n    aes(end_cap = circle(node2.degree + 2, \"pt\")),\n    edge_colour = \"black\",\n    arrow = arrow(\n      angle = 10,\n      length = unit(0.15, \"inches\"),\n      ends = \"last\",\n      type = \"closed\"\n    )\n  ) +\n  geom_node_point(aes(size = degree), col = \"grey66\", show.legend = FALSE) +\n  scale_size(range = c(3, 11)) +\n  theme_graph()\n\n\n\n\n\n\n\n\nThe overlap can be avoided by using the I() function from base R, which treats the entries of a vector “as is”. So we know that if a node has degree 5, it will be mapped to a circle with radius (or diameter?) “5pt”. Since this means, that you have no control over the scaling, you need to do that beforehand.\n\nnormalise &lt;- function(x, from = range(x), to = c(0, 1)) {\n  x &lt;- (x - from[1]) / (from[2] - from[1])\n  if (!identical(to, c(0, 1))) {\n    x &lt;- x * (to[2] - to[1]) + to[1]\n  }\n  x\n}\n\n# map to the range you want\nV(g)$degree &lt;- normalise(V(g)$degree, to = c(3, 11))\n\nggraph(g, \"stress\") +\n  geom_edge_link(\n    aes(end_cap = circle(node2.degree + 2, \"pt\")),\n    edge_colour = \"grey25\",\n    arrow = arrow(\n      angle = 10,\n      length = unit(0.15, \"inches\"),\n      ends = \"last\",\n      type = \"closed\"\n    )\n  ) +\n  geom_node_point(aes(size = I(degree)), col = \"grey66\") +\n  theme_graph()\n\n\n\n\n\n\n\n\n\n“How can I lower the opacity of nodes without making edges visible underneath?”\n\nOne of the rules you should try to follow is that edges should not be visible on top of nodes. Usually that is easy to achieve by drawing the edges before the nodes. But if you want to lower the opacity of nodes, they do become visible again.\n\ng &lt;- sample_gnp(20, 0.5)\nV(g)$degree &lt;- degree(g)\n\nggraph(g, \"stress\") +\n  geom_edge_link(edge_colour = \"grey66\") +\n  geom_node_point(\n    size = 8,\n    aes(alpha = degree),\n    col = \"red\",\n    show.legend = FALSE\n  ) +\n  theme_graph()\n\n\n\n\n\n\n\n\nThe solution is rather simple. Just add a node layer with the same aesthetics below with alpha=1 (default) and color=\"white\" (or the background color of the plot).\n\nggraph(g, \"stress\") +\n  geom_edge_link(edge_colour = \"grey66\") +\n  geom_node_point(size = 8, col = \"white\") +\n  geom_node_point(\n    aes(alpha = degree),\n    size = 8,\n    col = \"red\",\n    show.legend = FALSE\n  ) +\n  theme_graph()\n\n\n\n\n\n\n\n\nOf course you could also use start_cap and end_cap here, but you may have to fiddle again as in the last example.\n\n“How can I enhance readability of node labels in hairball graphs?”\n\nSometimes it is really hard to make labels readable when the network is very cluttered\n\ng &lt;- sample_gnp(50, 0.7)\nV(g)$name &lt;- sapply(1:50, function(x) paste0(sample(LETTERS, 4), collapse = \"\"))\nE(g)$weight &lt;- runif(ecount(g))\n\nggraph(g) +\n  geom_edge_link0(aes(edge_color = weight, edge_width = weight), show.legend = FALSE) +\n  geom_node_point(size = 8, color = \"#44a6c6\") +\n  geom_node_text(aes(label = name), fontface = \"bold\") +\n  scale_edge_color_continuous(low = \"grey66\", high = \"black\") +\n  scale_edge_width(range = c(0.1, 0.5)) +\n  theme_graph() +\n  coord_fixed()\n\nUsing \"stress\" as default layout\n\n\n\n\n\n\n\n\n\nHere you can make use of the fact that the layout of the nodes are stored in a “hidden” data frame when a ggraph object is constructed (this is what we made use of with geom_mark_hull() too). That means you can use other geoms from other packages. In this case, the shadowtext package as shown below.\n\nggraph(g,\"stress\") +\n  geom_edge_link0(aes(edge_color = weight, edge_width = weight), show.legend = FALSE) +\n  geom_node_point(size = 8, color = \"#44a6c6\") +\n  shadowtext::geom_shadowtext(aes(x, y, label = name), color = \"black\", size = 4, bg.colour = \"white\") +\n  scale_edge_color_continuous(low = \"grey66\", high = \"black\") +\n  scale_edge_width(range = c(0.1, 0.5)) +\n  theme_graph() +\n  coord_fixed()"
  },
  {
    "objectID": "teaching/sna/material/08/08-cugd.html",
    "href": "teaching/sna/material/08/08-cugd.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "In this session, we will be using conditional uniform graph distributions to simulate random networks. These random networks correspond to the null model and generate the null distribution to which we can compare our observed features to. Thus, we can conclude whether or not an observed feature of interested is significantly different than those from the null model. Most of the examples here are those presented in the lecture.\n\n\n\nlibrary(statnet)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(intergraph)\nlibrary(patchwork)\nlibrary(networkdata)\n\n\n\n\nWe will be primarily be working with matrix, network and graph objects. It is important that you can understand and pay attention to these since some functions only work with graph objects, and others with network/matrix objects. We try to keep it clear here by using suffix g, net and mat to clarify object assignment."
  },
  {
    "objectID": "teaching/sna/material/08/08-cugd.html#packages-needed",
    "href": "teaching/sna/material/08/08-cugd.html#packages-needed",
    "title": "Social Network Analysis",
    "section": "",
    "text": "library(statnet)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(intergraph)\nlibrary(patchwork)\nlibrary(networkdata)"
  },
  {
    "objectID": "teaching/sna/material/08/08-cugd.html#object-types",
    "href": "teaching/sna/material/08/08-cugd.html#object-types",
    "title": "Social Network Analysis",
    "section": "",
    "text": "We will be primarily be working with matrix, network and graph objects. It is important that you can understand and pay attention to these since some functions only work with graph objects, and others with network/matrix objects. We try to keep it clear here by using suffix g, net and mat to clarify object assignment."
  },
  {
    "objectID": "teaching/sna/material/08/08-cugd.html#load-a-dataset-and-extract-adjacency-matrix",
    "href": "teaching/sna/material/08/08-cugd.html#load-a-dataset-and-extract-adjacency-matrix",
    "title": "Social Network Analysis",
    "section": "Load a dataset and extract adjacency matrix",
    "text": "Load a dataset and extract adjacency matrix\nWe are going to use a data set, coleman, which is automatically loaded with the package statnet. To get information about it type ?coleman and select Colemans High School Friendship Data. This should open a help file with information about the data set. Read the description of the data in the help file in order to know what you are working with. To load the data in your session:\n\ndata(coleman, package = \"sna\")\n\nAs described in the help file, the data set is an array with 2 observations on the friendship nominations of 73 students (one for fall and one for spring). We will start by focusing on the fall network here, and create the adjacency matrix for the network:\n\nfall_mat &lt;- coleman[1,,] \n\nQ1: How can you check whether the network is directed or undirected?\nQ2: How can you calculate the number of ties you have in the fall network?"
  },
  {
    "objectID": "teaching/sna/material/08/08-cugd.html#visualize-the-network",
    "href": "teaching/sna/material/08/08-cugd.html#visualize-the-network",
    "title": "Social Network Analysis",
    "section": "Visualize the network",
    "text": "Visualize the network\nCreate a graph object from the adjacency matrix and visualize the network:\n\nfall_g &lt;- graph_from_adjacency_matrix(fall_mat, \"directed\")\nfall_p &lt;- ggraph(fall_g , layout = \"nicely\") + \n          geom_edge_link(edge_colour = \"#666060\", end_cap = circle(9,\"pt\"), \n                         n = 2, edge_width = 0.4, edge_alpha = 1, \n                         arrow = arrow(angle = 15, \n                         length = unit(0.1, \"inches\"), \n                         ends = \"last\", type = \"closed\"))  +\n            geom_node_point(fill = \"#525252\",colour = \"#FFFFFF\", \n                           size = 5, stroke = 1.1, shape = 21) + \n            theme_graph() + \n          ggtitle(\"fall friendship network\") +\n            theme(legend.position = \"none\")\nfall_p"
  },
  {
    "objectID": "teaching/sna/material/08/08-cugd.html#dyad-census-and-triad-census",
    "href": "teaching/sna/material/08/08-cugd.html#dyad-census-and-triad-census",
    "title": "Social Network Analysis",
    "section": "Dyad census and triad census",
    "text": "Dyad census and triad census\nTabulate the number of dyads that do not have any ties, have exactly one tie, and that have two ties (i.e. are reciprocated or mutual dyads). We calculate these numbers for both the observed and the random network using igraph:\n\ndyad_census(fall_g) \ndyad_census(sim2_g)\n# using sna package with matrix objects instead\nsna::dyad.census(fall_mat) \nsna::dyad.census(sim2_mat)\n\nQ4: Where do you note the strongest difference between the observed and simulated network?\nNow we do the same to compare the number of transitive triads. Using the sna function for calculating triad census might be easier since it also includes the triad labels:\n\ntriad_census(fall_g) \ntriad_census(sim2_g)\n# using sna package with matrix objects instead\nsna::triad.census(fall_mat) \nsna::triad.census(sim2_mat)\n\nQ4: Where do you note the strongest difference between the observed and simulated network in terms of transitivity?\nNote that the number of complete triads (MAN: 300) is 22 in the observed data and 0 for the random network. However, this might be an unfair comparison as the complete 300 triangle contains three reciprocated ties and we already saw that the Coleman data had a much higher number mutual dyads than the random network. But is this just a coincidence? To answer this we need to generate a world of hypothetical networks by generating many many random networks.\nTo see just how unusual mutual ties are in the alternative world, we can generate 1000 random networks while conditioning on the observed number of ties (the exact density):\n\nsim2.1000_mat &lt;- rgnm(n = 1000, nv = dim(fall_mat)[1], m = sum(fall_mat), mode = \"digraph\")\nsim2.1000_dc &lt;- as.data.frame(sna::dyad.census(sim2.1000_mat))\n\nNow we can draw the histogram for the distribution of mutual dyads through:\n\np_sim2.1000_dc &lt;- ggplot(sim2.1000_dc, aes(x= Mut))  +\n  geom_histogram(binwidth = 1, color=\"darkgrey\", fill=\"lightgrey\") +\n  coord_cartesian(ylim=c(0,200)) +\n  labs(title = \"\", x = \"number of mutual ties\") \np_sim2.1000_dc\n\n\n\n\n\n\n\n\nQ5: Do any of the 1000 random networks have as large a number of mutual dyads as in the observed fall network?"
  },
  {
    "objectID": "teaching/sna/material/08/08-cugd.html#data",
    "href": "teaching/sna/material/08/08-cugd.html#data",
    "title": "Social Network Analysis",
    "section": "Data",
    "text": "Data\nThis data set comes from a network study of corporate law partnership that was carried out in a Northeastern US corporate law firm. You can read about this data set here. We will go through two examples for testing homophily using this data set. Thus, the null and alternative hypotheses for both tests are: \\(H_0\\): observed homophily effect is from \\(\\mathcal{U}|L\\) model\n\\(H_1\\): observed homophily effect is not from \\(\\mathcal{U}|L\\) model\nbut we will use different relations and check for social selection based on different attributes."
  },
  {
    "objectID": "teaching/sna/material/08/08-cugd.html#analysing-homophily-using-non-parametric-null-distribution",
    "href": "teaching/sna/material/08/08-cugd.html#analysing-homophily-using-non-parametric-null-distribution",
    "title": "Social Network Analysis",
    "section": "Analysing homophily using non-parametric null distribution",
    "text": "Analysing homophily using non-parametric null distribution\n\nTest 1: Friendship based on gender\nWe start with a simple homophily test: do lawyers befriend those with the same gender? To load the friendship network:\n\ndata(\"law_friends\")\n\nWe then create an adjacency matrix from the directed graph, calculate number of ties and nodes:\n\nlaw_mat.frn &lt;- as_adjacency_matrix(law_friends, sparse = FALSE)\nlaw_nodes &lt;- dim(law_mat.frn)[1]\nlaw_ties.frn &lt;- sum(law_mat.frn)\n\nNext we save the binary attribute ‘gender’ from the loaded graph object as a vector, which we then convert into a data frame:\n\nlaw_attr.gend &lt;- as.data.frame(vertex_attr(law_cowork)$gender)\n\nTo calculate the number of observed homophilous ties:\n\nhomoph_obs.frn &lt;- sum(law_mat.frn[law_attr.gend == 1, law_attr.gend == 1]) + \n              sum(law_mat.frn[law_attr.gend == 2, law_attr.gend == 2])\n\nNext, generate 1000 random graphs with the same number of ties as the observed one, i.e. the null model \\({\\cal{U}}|L\\):\n\nlaw_sim1000.frn &lt;- rgnm(1000, law_nodes, law_ties.frn, mode='digraph')\n\nFor each random network generated, calculate the number of homophilous ties in the same way:\n\nhomoph_sim.frn &lt;- apply(law_sim1000.frn, 1, function(x) {\n                sum(x[law_attr.gend == 1,law_attr.gend == 1]) + \n                sum(x[law_attr.gend == 2, law_attr.gend == 2])})\n\nTo plot the distribution of homophilous ties under the null hypothesis \\(H_0\\):\n\nhomoph_sim.frn &lt;- as.data.frame(homoph_sim.frn)\np_lawsim1000.frn &lt;- ggplot(homoph_sim.frn, aes(x= homoph_sim.frn))  +\n  geom_histogram(binwidth = 5, color=\"darkgrey\", fill=\"lightgrey\") +\n  coord_cartesian(ylim=c(0,200)) +\n  labs(title = \"\", x = \"number of homophilious ties\")\np_lawsim1000.frn \n\n\n\n\n\n\n\n\nQ8: Can you reject the null? Why or why not?\n\n\nTest 1: Cowork based on law practice\nWe want to check whether or not the partners of the firm more frequently work together with other partners having the same practice. We import the data as a graph object from the networkdata package:\n\ndata(\"law_cowork\")\n\nWe then create an adjacency matrix from the directed graph for the first 36 lawyers in the network corresponding to the partners of the firm (see attribute ‘status’). To test homophily now, we only consider the reciprocal ties so we need to symmetrize the matrix to create and undirected graph:\n\nlaw_mat_cwdir &lt;- as_adjacency_matrix(law_cowork, sparse = FALSE)\nlaw_mat_cwdir &lt;- law_mat_cwdir[1:36,1:36]\nlaw_mat_cw &lt;- (law_mat_cwdir == t(law_mat_cwdir) & law_mat_cwdir ==1) + 0\nlaw_nodes_cw &lt;- dim(law_mat_cw)[1]\nlaw_ties_cw &lt;- sum(law_mat_cw)/2\n\nNext we save the binary attribute ‘practice’ (1 = litigation, 2 = corporate) from the graph object as a vector, which is then in turn converted into a data frame (again only for the first 36 lawyers who are partners):\n\nlaw_attr.pract &lt;- as.data.frame(vertex_attr(law_cowork)$pract[1:36])\n\nTo calculate the number of observed homophilous ties:\n\nhomoph_obs.cw &lt;- sum(\n  law_mat_cw[law_attr.pract == 1, law_attr.pract == 1])/2 + \n  sum(law_mat_cw[law_attr.pract == 2, law_attr.pract == 2])/2\nhomoph_obs.cw\n\n[1] 72\n\n\nNext, generate 1000 random graphs with the same number of ties as the observed one, i.e. the null model \\({\\cal{U}}|L\\):\n\nset.seed(7722) # so that we all get the same results\nlaw_sim1000.cw &lt;- rgnm(1000, law_nodes_cw, law_ties_cw, mode='graph')\n\nFor each random network generated, calculate the number of homophilous ties in the same way:\n\nhomoph_sim.cw &lt;- apply(law_sim1000.cw, 1, function(x) {\n                sum(x[law_attr.pract == 1,law_attr.pract == 1])/2 + \n                sum(x[law_attr.pract == 2, law_attr.pract == 2])/2})\n\nThe distribution of homophilous ties under the null hypothesis \\(H_0\\) is plotted below with a red line indicating where the observed value falls in this distribution:\n\nhomoph_sim.cw &lt;- as.data.frame(homoph_sim.cw)\np_lawsim1000.cw &lt;- ggplot(homoph_sim.cw , aes(x= homoph_sim.cw))  +\n  geom_histogram(binwidth = 1, color=\"darkgrey\", fill=\"lightgrey\") +\n  coord_cartesian(ylim=c(0,100)) +\n  geom_vline(xintercept = homoph_obs.cw, lwd=0.5, colour=\"red\") +\n  labs(title = \"\", x = \"number of homophilious ties\")\np_lawsim1000.cw\n\n\n\n\n\n\n\n\nQ9: Can you reject the null? Why or why not? We can also calculate the \\(p\\)-value for this test. Thus, you can use this distribution to calculate probability \\(P(\\text{test statistic} &gt; \\textrm{ observed value } |H_0 \\textrm{ true })\\):\n\nsum(homoph_sim.cw  &gt; homoph_obs.cw)/1000\n\n[1] 0.002\n\n\nQ10: How do you interpret this value?"
  },
  {
    "objectID": "teaching/sna/material/06/06-netviz-1.html",
    "href": "teaching/sna/material/06/06-netviz-1.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "library(igraph)\nlibrary(graphlayouts)\nlibrary(ggraph)\nlibrary(networkdata)"
  },
  {
    "objectID": "teaching/sna/material/06/06-netviz-1.html#load-packages",
    "href": "teaching/sna/material/06/06-netviz-1.html#load-packages",
    "title": "Social Network Analysis",
    "section": "",
    "text": "library(igraph)\nlibrary(graphlayouts)\nlibrary(ggraph)\nlibrary(networkdata)"
  },
  {
    "objectID": "teaching/sna/material/06/06-netviz-1.html#prepare-data",
    "href": "teaching/sna/material/06/06-netviz-1.html#prepare-data",
    "title": "Social Network Analysis",
    "section": "Prepare data",
    "text": "Prepare data\n\n# load the game of thrones dataset\ndata(got)\ngotS1 &lt;- got[[1]]"
  },
  {
    "objectID": "teaching/sna/material/06/06-netviz-1.html#basic-plotting",
    "href": "teaching/sna/material/06/06-netviz-1.html#basic-plotting",
    "title": "Social Network Analysis",
    "section": "Basic plotting",
    "text": "Basic plotting\n\nplot(gotS1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis plot is not representative for the capabilities of igraph. Check out this tutorial\n\n\n\n# quick plot function of ggraph\nautograph(gotS1)\n\n\n\n\n\n\n\n\nautograph() allows you to specify node/edge colours too but it really is only meant to give you a quick overview without writing a massive amount of code. Think of it as the plot() function for ggraph.\nBefore we continue, we add some more node attributes to the GoT network that can be used during visualization.\n\n# define a custom color palette\ngot_palette &lt;- c(\n  \"#1A5878\", \"#C44237\", \"#AD8941\", \"#E99093\",\n  \"#50594B\", \"#8968CD\", \"#9ACD32\"\n)\n\n# compute a clustering for node colors\nV(gotS1)$clu &lt;- as.character(membership(cluster_louvain(gotS1)))\n\n# compute degree as node size\nV(gotS1)$size &lt;- degree(gotS1)\n\n\nAesthetics and scales examples\n\nggraph(gotS1, layout = \"stress\") +\n  geom_edge_link0(aes(edge_width = weight), edge_colour = \"grey66\") +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  geom_node_text(aes(filter = (size &gt;= 25), label = name), size = 6) +\n  scale_fill_manual(values = got_palette) +\n  scale_edge_width(range = c(0.2, 3)) +\n  scale_size(range = c(3, 12)) +\n  theme_graph() +\n  theme(legend.position = \"none\")+\n  coord_fixed()\n\n\n\n\n\n\n\n\nUsing geom_edge_link instead of geom_edge_link0 for gradients along edges\n\nggraph(gotS1, layout = \"stress\") +\n  geom_edge_link(aes(alpha = after_stat(index)), edge_colour = \"black\") +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  scale_fill_manual(values = got_palette) +\n  scale_edge_width_continuous(range = c(0.2, 3)) +\n  scale_size_continuous(range = c(1, 6)) +\n  theme_graph() +\n  theme(legend.position = \"none\")+\n  coord_fixed()\n\n\n\n\n\n\n\n\nUsing geom_edge_link2 instead of geom_edge_link0 for more advanced gradients along edges\n\nggraph(gotS1, layout = \"stress\") +\n  geom_edge_link2(aes(edge_colour = node.clu),edge_width = 0.5)+\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  scale_fill_manual(values = got_palette) +\n  scale_edge_color_manual(values = got_palette) +\n  scale_edge_width_continuous(range = c(0.2, 3)) +\n  scale_size_continuous(range = c(1, 6)) +\n  theme_graph() +\n  theme(legend.position = \"none\")+\n  coord_fixed()\n\n\n\n\n\n\n\n\nUsing a different geom\n\nggraph(gotS1, layout = \"stress\") +\n  geom_edge_arc0(aes(edge_width = weight), edge_colour = \"grey66\",strength = 0.1) +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  scale_fill_manual(values = got_palette) +\n  scale_edge_width(range = c(0.2, 3)) +\n  scale_size(range = c(1, 6)) +\n  theme_graph() +\n  theme(legend.position = \"none\")+\n  coord_fixed()\n\n\n\n\n\n\n\n\nNot specifying any scales:\n\nggraph(gotS1, layout = \"stress\") +\n  geom_edge_link0(aes(edge_width = weight), edge_colour = \"grey66\") +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  theme_graph() +\n  theme(legend.position = \"none\")+\n  coord_fixed()\n\n\n\n\n\n\n\n\n\nggraph(gotS1, layout = \"stress\") +\n  geom_edge_link0(aes(edge_width = weight), edge_colour = \"grey66\") +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  scale_edge_width_continuous(range = c(0.2, 3)) +\n  scale_size_continuous(range = c(1, 6)) +\n  theme_graph() +\n  theme(legend.position = \"none\")+\n  coord_fixed()\n\n\n\n\n\n\n\n\n\nggraph(gotS1, layout = \"stress\") +\n  geom_edge_link0(aes(edge_width = weight), edge_colour = \"grey66\") +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  scale_fill_manual(values = got_palette) +\n  scale_edge_width_continuous(range = c(0.2, 3), guide = \"none\") +\n  scale_size_continuous(range = c(1, 6), guide = \"none\") +\n  theme_graph() +\n  theme(legend.position = \"bottom\") +\n  coord_fixed()"
  },
  {
    "objectID": "teaching/sna/material/06/06-netviz-1.html#contentric-layouts",
    "href": "teaching/sna/material/06/06-netviz-1.html#contentric-layouts",
    "title": "Social Network Analysis",
    "section": "Contentric layouts",
    "text": "Contentric layouts\nPlot that focuses on Ned Stark:\n\nggraph(gotS1, layout = \"focus\", focus = 1) +\n  geom_edge_link0(aes(edge_width = weight), edge_colour = \"grey66\") +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  geom_node_text(aes(filter = (name == \"Ned\"), size = size, label = name),\n    family = \"serif\"\n  ) +\n  scale_edge_width_continuous(range = c(0.2, 1.2)) +\n  scale_size_continuous(range = c(1, 5)) +\n  scale_fill_manual(values = got_palette) +\n  theme_graph() +\n  theme(legend.position = \"none\") + \n  coord_fixed()\n\n\n\n\n\n\n\n\nAdding draw_circle:\n\nggraph(gotS1, layout = \"focus\", focus = 2) +\n  draw_circle(col = \"#00BFFF\", use = \"focus\", max.circle = 3) +\n  geom_edge_link0(aes(width = weight), edge_colour = \"grey66\") +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  geom_node_text(aes(filter = (name == \"Daenerys\"), size = size, label = name),\n    family = \"serif\"\n  ) +\n  scale_edge_width_continuous(range = c(0.2, 1.2)) +\n  scale_size_continuous(range = c(1, 5)) +\n  scale_fill_manual(values = got_palette) +\n  theme_graph() +\n  theme(legend.position = \"none\") + \n  coord_fixed()\n\n\n\n\n\n\n\n\nConcentric layout based on a centrality index:\n\nggraph(gotS1, layout = \"centrality\", cent = strength(gotS1)) +\n  geom_edge_link0(aes(edge_width = weight), edge_colour = \"grey66\") +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  geom_node_text(aes(size = size, label = name), family = \"serif\") +\n  scale_edge_width_continuous(range = c(0.2, 0.9)) +\n  scale_size_continuous(range = c(1, 8)) +\n  scale_fill_manual(values = got_palette) +\n  theme_graph() +\n  theme(legend.position = \"none\") + \n  coord_fixed()"
  },
  {
    "objectID": "teaching/sna/material/06/06-netviz-1.html#exercise-1",
    "href": "teaching/sna/material/06/06-netviz-1.html#exercise-1",
    "title": "Social Network Analysis",
    "section": "Exercise 1",
    "text": "Exercise 1\nUse the network of a different season to produce a plot by yourself\nOR choose a dataset from the networkdata package data(package = \"networkdata\")\nThere are no constrains, just try out and play around"
  },
  {
    "objectID": "teaching/sna/material/06/06-netviz-1.html#exercise-2",
    "href": "teaching/sna/material/06/06-netviz-1.html#exercise-2",
    "title": "Social Network Analysis",
    "section": "Exercise 2",
    "text": "Exercise 2\nRecreate the iconic “polblogs” network visualization \nThe network shows the linking between political blogs during the 2004 election in the US. Red nodes are conservative leaning blogs and blue ones liberal.\n\ndata(\"polblogs\")"
  },
  {
    "objectID": "teaching/tidyverse-II/index.html",
    "href": "teaching/tidyverse-II/index.html",
    "title": "Data Science with Tidyverse II",
    "section": "",
    "text": "Make sure to install and load Tidymodels:\n\ninstall.packages(\"tidymodels\")\nlibrary(tidymodels)\n\n\nCourse Material\n\n\n\n\n\nSlides\nWorksheet\n\n\n\n\nIntroduction\n\n\n\n\nPart I\n\n1.Your Data Budget \n\n\n\n\n2. Build a Model \n\n\n\n\n3. Evaluate the Model \n\n\n\n\n4. Model Tuning \n\n\nPart II\n\n1. Feature Engineering \n\n\n\n\n2. Tuning Hyperparameters \n\n\n\n\n3. Grid Search via Racing \n\n\n\n\n4. Iterative Search \n\n\nCase Study: Chicago L-Train\n\n\n\n\n\n\nExtensions & Hands-On Exercises\n\n\n\n\nPractical\nWorksheet\n\n\n\n\nK-means\n\n\n\n\nBootstrapping\n\n\n\n\nCoefficients\n\n\n\n\nInference"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/xtabs.html",
    "href": "teaching/tidyverse-II/material/case-studies/xtabs.html",
    "title": "Statistical analysis of contingency tables",
    "section": "",
    "text": "This article only requires that you have the tidymodels package installed.\nHere, we’ll walk through conducting a \\(\\chi^2\\) (chi-squared) test of independence and a chi-squared goodness of fit test using infer. We’ll start out with a chi-squared test of independence, which can be used to test the association between two categorical variables. Then, we’ll move on to a chi-squared goodness of fit test, which tests how well the distribution of one categorical variable can be approximated by some theoretical distribution.\nThroughout this vignette, we’ll make use of the ad_data data set (available in the modeldata package, which is part of tidymodels). This data set is related to cognitive impairment in 333 patients from Craig-Schapiro et al (2011). See ?ad_data for more information on the variables included and their source. One of the main research questions in these data were how a person’s genetics related to the Apolipoprotein E gene affect their cognitive skills. The data shows:\n\nlibrary(tidymodels) # Includes the infer package\nset.seed(1234)\n\ndata(ad_data, package = \"modeldata\")\nad_data %&gt;%\n  select(Genotype, Class)\n#&gt; # A tibble: 333 × 2\n#&gt;    Genotype Class   \n#&gt;    &lt;fct&gt;    &lt;fct&gt;   \n#&gt;  1 E3E3     Control \n#&gt;  2 E3E4     Control \n#&gt;  3 E3E4     Control \n#&gt;  4 E3E4     Control \n#&gt;  5 E3E3     Control \n#&gt;  6 E4E4     Impaired\n#&gt;  7 E2E3     Control \n#&gt;  8 E2E3     Control \n#&gt;  9 E3E3     Control \n#&gt; 10 E2E3     Impaired\n#&gt; # ℹ 323 more rows\n\nThe three main genetic variants are called E2, E3, and E4. The values in Genotype represent the genetic makeup of patients based on what they inherited from their parents (i.e, a value of “E2E4” means E2 from one parent and E4 from the other)."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/xtabs.html#introduction",
    "href": "teaching/tidyverse-II/material/case-studies/xtabs.html#introduction",
    "title": "Statistical analysis of contingency tables",
    "section": "",
    "text": "This article only requires that you have the tidymodels package installed.\nHere, we’ll walk through conducting a \\(\\chi^2\\) (chi-squared) test of independence and a chi-squared goodness of fit test using infer. We’ll start out with a chi-squared test of independence, which can be used to test the association between two categorical variables. Then, we’ll move on to a chi-squared goodness of fit test, which tests how well the distribution of one categorical variable can be approximated by some theoretical distribution.\nThroughout this vignette, we’ll make use of the ad_data data set (available in the modeldata package, which is part of tidymodels). This data set is related to cognitive impairment in 333 patients from Craig-Schapiro et al (2011). See ?ad_data for more information on the variables included and their source. One of the main research questions in these data were how a person’s genetics related to the Apolipoprotein E gene affect their cognitive skills. The data shows:\n\nlibrary(tidymodels) # Includes the infer package\nset.seed(1234)\n\ndata(ad_data, package = \"modeldata\")\nad_data %&gt;%\n  select(Genotype, Class)\n#&gt; # A tibble: 333 × 2\n#&gt;    Genotype Class   \n#&gt;    &lt;fct&gt;    &lt;fct&gt;   \n#&gt;  1 E3E3     Control \n#&gt;  2 E3E4     Control \n#&gt;  3 E3E4     Control \n#&gt;  4 E3E4     Control \n#&gt;  5 E3E3     Control \n#&gt;  6 E4E4     Impaired\n#&gt;  7 E2E3     Control \n#&gt;  8 E2E3     Control \n#&gt;  9 E3E3     Control \n#&gt; 10 E2E3     Impaired\n#&gt; # ℹ 323 more rows\n\nThe three main genetic variants are called E2, E3, and E4. The values in Genotype represent the genetic makeup of patients based on what they inherited from their parents (i.e, a value of “E2E4” means E2 from one parent and E4 from the other)."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/xtabs.html#test-of-independence",
    "href": "teaching/tidyverse-II/material/case-studies/xtabs.html#test-of-independence",
    "title": "Statistical analysis of contingency tables",
    "section": "Test of independence",
    "text": "Test of independence\nTo carry out a chi-squared test of independence, we’ll examine the association between their cognitive ability (impaired and healthy) and the genetic makeup. This is what the relationship looks like in the sample data:\n\n\n\n\n\n\n\n\n\nIf there were no relationship, we would expect to see the purple bars reaching to the same length, regardless of cognitive ability. Are the differences we see here, though, just due to random noise?\nFirst, to calculate the observed statistic, we can use specify() and calculate().\n\n# calculate the observed statistic\nobserved_indep_statistic &lt;- ad_data %&gt;%\n  specify(Genotype ~ Class) %&gt;%\n  calculate(stat = \"Chisq\")\n\nThe observed \\(\\chi^2\\) statistic is 21.5774809. Now, we want to compare this statistic to a null distribution, generated under the assumption that these variables are not actually related, to get a sense of how likely it would be for us to see this observed statistic if there were actually no association between cognitive ability and genetics.\nWe can generate() the null distribution in one of two ways: using randomization or theory-based methods. The randomization approach permutes the response and explanatory variables, so that each person’s genetics is matched up with a random cognitive rating from the sample in order to break up any association between the two.\n\n# generate the null distribution using randomization\nnull_distribution_simulated &lt;- ad_data %&gt;%\n  specify(Genotype ~ Class) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 5000, type = \"permute\") %&gt;%\n  calculate(stat = \"Chisq\")\n\nNote that, in the line specify(Genotype ~ Class) above, we could use the equivalent syntax specify(response = Genotype, explanatory = Class). The same goes in the code below, which generates the null distribution using theory-based methods instead of randomization.\n\n# generate the null distribution by theoretical approximation\nnull_distribution_theoretical &lt;- ad_data %&gt;%\n  specify(Genotype ~ Class) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  # note that we skip the generation step here!\n  calculate(stat = \"Chisq\")\n\nTo get a sense for what these distributions look like, and where our observed statistic falls, we can use visualize():\n\n# visualize the null distribution and test statistic!\nnull_distribution_simulated %&gt;%\n  visualize() + \n  shade_p_value(observed_indep_statistic,\n                direction = \"greater\")\n\n\n\n\n\n\n\n\nWe could also visualize the observed statistic against the theoretical null distribution. Note that we skip the generate() and calculate() steps when using the theoretical approach, and that we now need to provide method = \"theoretical\" to visualize().\n\n# visualize the theoretical null distribution and test statistic!\nad_data %&gt;%\n  specify(Genotype ~ Class) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  visualize(method = \"theoretical\") + \n  shade_p_value(observed_indep_statistic,\n                direction = \"greater\")\n\n\n\n\n\n\n\n\nTo visualize both the randomization-based and theoretical null distributions to get a sense of how the two relate, we can pipe the randomization-based null distribution into visualize(), and further provide method = \"both\".\n\n# visualize both null distributions and the test statistic!\nnull_distribution_simulated %&gt;%\n  visualize(method = \"both\") + \n  shade_p_value(observed_indep_statistic,\n                direction = \"greater\")\n\n\n\n\n\n\n\n\nEither way, it looks like our observed test statistic would be fairly unlikely if there were actually no association between cognition and genotype. More exactly, we can calculate the p-value:\n\n# calculate the p value from the observed statistic and null distribution\np_value_independence &lt;- null_distribution_simulated %&gt;%\n  get_p_value(obs_stat = observed_indep_statistic,\n              direction = \"greater\")\n\np_value_independence\n#&gt; # A tibble: 1 × 1\n#&gt;   p_value\n#&gt;     &lt;dbl&gt;\n#&gt; 1  0.0006\n\nThus, if there were really no relationship between cognition and genotype, the probability that we would see a statistic as or more extreme than 21.5774809 is approximately 6^{-4}.\nNote that, equivalently to the steps shown above, the package supplies a wrapper function, chisq_test, to carry out Chi-Squared tests of independence on tidy data. The syntax goes like this:\n\nchisq_test(ad_data, Genotype ~ Class)\n#&gt; # A tibble: 1 × 3\n#&gt;   statistic chisq_df  p_value\n#&gt;       &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;\n#&gt; 1      21.6        5 0.000630"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/xtabs.html#goodness-of-fit",
    "href": "teaching/tidyverse-II/material/case-studies/xtabs.html#goodness-of-fit",
    "title": "Statistical analysis of contingency tables",
    "section": "Goodness of fit",
    "text": "Goodness of fit\nNow, moving on to a chi-squared goodness of fit test, we’ll take a look at just the genotype data. Many papers have investigated the relationship of Apolipoprotein E to diseases. For example, Song et al (2004) conducted a meta-analysis of numerous studies that looked at this gene and heart disease. In their paper, they describe the frequency of the different genotypes across many samples. For the cognition study, it might be interesting to see if our sample of genotypes was consistent with this literature (treating the rates, for this analysis, as known).\nThe rates of the meta-analysis and our observed data are:\n\n# Song, Y., Stampfer, M. J., & Liu, S. (2004). Meta-Analysis: Apolipoprotein E \n# Genotypes and Risk for Coronary Heart Disease. Annals of Internal Medicine, \n# 141(2), 137.\nmeta_rates &lt;- c(\"E2E2\" = 0.71, \"E2E3\" = 11.4, \"E2E4\" = 2.32,\n                \"E3E3\" = 61.0, \"E3E4\" = 22.6, \"E4E4\" = 2.22)\nmeta_rates &lt;- meta_rates/sum(meta_rates) # these add up to slightly &gt; 100%\n\nobs_rates &lt;- table(ad_data$Genotype)/nrow(ad_data)\nround(cbind(obs_rates, meta_rates) * 100, 2)\n#&gt;      obs_rates meta_rates\n#&gt; E2E2      0.60       0.71\n#&gt; E2E3     11.11      11.37\n#&gt; E2E4      2.40       2.31\n#&gt; E3E3     50.15      60.85\n#&gt; E3E4     31.83      22.54\n#&gt; E4E4      3.90       2.21\n\nSuppose our null hypothesis is that Genotype follows the same frequency distribution as the meta-analysis. Lets now test whether this difference in distributions is statistically significant.\nFirst, to carry out this hypothesis test, we would calculate our observed statistic.\n\n# calculating the null distribution\nobserved_gof_statistic &lt;- ad_data %&gt;%\n  specify(response = Genotype) %&gt;%\n  hypothesize(null = \"point\", p = meta_rates) %&gt;%\n  calculate(stat = \"Chisq\")\n\nThe observed statistic is 23.3838483. Now, generating a null distribution, by just dropping in a call to generate():\n\n# generating a null distribution\nnull_distribution_gof &lt;- ad_data %&gt;%\n  specify(response = Genotype) %&gt;%\n  hypothesize(null = \"point\", p = meta_rates) %&gt;%\n  generate(reps = 5000, type = \"simulate\") %&gt;%\n  calculate(stat = \"Chisq\")\n\nAgain, to get a sense for what these distributions look like, and where our observed statistic falls, we can use visualize():\n\n# visualize the null distribution and test statistic!\nnull_distribution_gof %&gt;%\n  visualize() + \n  shade_p_value(observed_gof_statistic,\n                direction = \"greater\")\n\n\n\n\n\n\n\n\nThis statistic seems like it would be unlikely if our rates were the same as the rates from the meta-analysis! How unlikely, though? Calculating the p-value:\n\n# calculate the p-value\np_value_gof &lt;- null_distribution_gof %&gt;%\n  get_p_value(observed_gof_statistic,\n              direction = \"greater\")\n\np_value_gof\n#&gt; # A tibble: 1 × 1\n#&gt;   p_value\n#&gt;     &lt;dbl&gt;\n#&gt; 1   0.001\n\nThus, if each genotype occurred at the same rate as the Song paper, the probability that we would see a distribution like the one we did is approximately 0.001.\nAgain, equivalently to the steps shown above, the package supplies a wrapper function, chisq_test, to carry out chi-squared goodness of fit tests on tidy data. The syntax goes like this:\n\nchisq_test(ad_data, response = Genotype, p = meta_rates)\n#&gt; # A tibble: 1 × 3\n#&gt;   statistic chisq_df  p_value\n#&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1      23.4        5 0.000285"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/xtabs.html#session-info",
    "href": "teaching/tidyverse-II/material/case-studies/xtabs.html#session-info",
    "title": "Statistical analysis of contingency tables",
    "section": "Session information",
    "text": "Session information\n\n#&gt; ─ Session info ─────────────────────────────────────────────────────\n#&gt;  version  R version 4.5.0 (2025-04-11)\n#&gt;  language (EN)\n#&gt;  date     2025-07-07\n#&gt;  pandoc   3.2\n#&gt;  quarto   1.7.29\n#&gt; \n#&gt; ─ Packages ─────────────────────────────────────────────────────────\n#&gt;  package      version date (UTC) source\n#&gt;  broom        1.0.8   2025-03-28 CRAN (R 4.5.0)\n#&gt;  dials        1.4.0   2025-02-13 CRAN (R 4.5.0)\n#&gt;  dplyr        1.1.4   2023-11-17 CRAN (R 4.5.0)\n#&gt;  ggplot2      3.5.2   2025-04-09 CRAN (R 4.5.0)\n#&gt;  infer        1.0.8   2025-04-14 CRAN (R 4.5.0)\n#&gt;  parsnip      1.3.1   2025-03-12 CRAN (R 4.5.0)\n#&gt;  purrr        1.0.4   2025-02-05 CRAN (R 4.5.0)\n#&gt;  recipes      1.3.0   2025-04-17 CRAN (R 4.5.0)\n#&gt;  rlang        1.1.6   2025-04-11 CRAN (R 4.5.0)\n#&gt;  rsample      1.3.0   2025-04-02 CRAN (R 4.5.0)\n#&gt;  tibble       3.2.1   2023-03-20 CRAN (R 4.5.0)\n#&gt;  tidymodels   1.3.0   2025-02-21 CRAN (R 4.5.0)\n#&gt;  tune         1.3.0   2025-02-21 CRAN (R 4.5.0)\n#&gt;  workflows    1.2.0   2025-02-19 CRAN (R 4.5.0)\n#&gt;  yardstick    1.3.2   2025-01-22 CRAN (R 4.5.0)\n#&gt; \n#&gt; ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/xtabs-ws.html",
    "href": "teaching/tidyverse-II/material/case-studies/xtabs-ws.html",
    "title": "Statistical analysis of contingency tables",
    "section": "",
    "text": "This article only requires that you have the tidymodels package installed.\nHere, we’ll walk through conducting a \\(\\chi^2\\) (chi-squared) test of independence and a chi-squared goodness of fit test using infer. We’ll start out with a chi-squared test of independence, which can be used to test the association between two categorical variables. Then, we’ll move on to a chi-squared goodness of fit test, which tests how well the distribution of one categorical variable can be approximated by some theoretical distribution.\nThroughout this vignette, we’ll make use of the ad_data data set (available in the modeldata package, which is part of tidymodels). This data set is related to cognitive impairment in 333 patients from Craig-Schapiro et al (2011). See ?ad_data for more information on the variables included and their source. One of the main research questions in these data were how a person’s genetics related to the Apolipoprotein E gene affect their cognitive skills. The data shows:\n\nlibrary(tidymodels) # Includes the infer package\nset.seed(1234)\n\ndata(ad_data, package = \"modeldata\")\nad_data %&gt;%\n  select(Genotype, Class)\n\n# A tibble: 333 × 2\n   Genotype Class   \n   &lt;fct&gt;    &lt;fct&gt;   \n 1 E3E3     Control \n 2 E3E4     Control \n 3 E3E4     Control \n 4 E3E4     Control \n 5 E3E3     Control \n 6 E4E4     Impaired\n 7 E2E3     Control \n 8 E2E3     Control \n 9 E3E3     Control \n10 E2E3     Impaired\n# ℹ 323 more rows\n\n\nThe three main genetic variants are called E2, E3, and E4. The values in Genotype represent the genetic makeup of patients based on what they inherited from their parents (i.e, a value of “E2E4” means E2 from one parent and E4 from the other)."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/xtabs-ws.html#introduction",
    "href": "teaching/tidyverse-II/material/case-studies/xtabs-ws.html#introduction",
    "title": "Statistical analysis of contingency tables",
    "section": "",
    "text": "This article only requires that you have the tidymodels package installed.\nHere, we’ll walk through conducting a \\(\\chi^2\\) (chi-squared) test of independence and a chi-squared goodness of fit test using infer. We’ll start out with a chi-squared test of independence, which can be used to test the association between two categorical variables. Then, we’ll move on to a chi-squared goodness of fit test, which tests how well the distribution of one categorical variable can be approximated by some theoretical distribution.\nThroughout this vignette, we’ll make use of the ad_data data set (available in the modeldata package, which is part of tidymodels). This data set is related to cognitive impairment in 333 patients from Craig-Schapiro et al (2011). See ?ad_data for more information on the variables included and their source. One of the main research questions in these data were how a person’s genetics related to the Apolipoprotein E gene affect their cognitive skills. The data shows:\n\nlibrary(tidymodels) # Includes the infer package\nset.seed(1234)\n\ndata(ad_data, package = \"modeldata\")\nad_data %&gt;%\n  select(Genotype, Class)\n\n# A tibble: 333 × 2\n   Genotype Class   \n   &lt;fct&gt;    &lt;fct&gt;   \n 1 E3E3     Control \n 2 E3E4     Control \n 3 E3E4     Control \n 4 E3E4     Control \n 5 E3E3     Control \n 6 E4E4     Impaired\n 7 E2E3     Control \n 8 E2E3     Control \n 9 E3E3     Control \n10 E2E3     Impaired\n# ℹ 323 more rows\n\n\nThe three main genetic variants are called E2, E3, and E4. The values in Genotype represent the genetic makeup of patients based on what they inherited from their parents (i.e, a value of “E2E4” means E2 from one parent and E4 from the other)."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/xtabs-ws.html#test-of-independence",
    "href": "teaching/tidyverse-II/material/case-studies/xtabs-ws.html#test-of-independence",
    "title": "Statistical analysis of contingency tables",
    "section": "Test of independence",
    "text": "Test of independence\nTo carry out a chi-squared test of independence, we’ll examine the association between their cognitive ability (impaired and healthy) and the genetic makeup. This is what the relationship looks like in the sample data:\n\n\n\n\n\n\n\n\n\nIf there were no relationship, we would expect to see the purple bars reaching to the same length, regardless of cognitive ability. Are the differences we see here, though, just due to random noise?\nFirst, to calculate the observed statistic, we can use specify() and calculate().\n\n# calculate the observed statistic\nobserved_indep_statistic &lt;- ad_data %&gt;%\n  specify(Genotype ~ Class) %&gt;%\n  calculate(stat = \"Chisq\")\n\nThe observed \\(\\chi^2\\) statistic is 21.5774809. Now, we want to compare this statistic to a null distribution, generated under the assumption that these variables are not actually related, to get a sense of how likely it would be for us to see this observed statistic if there were actually no association between cognitive ability and genetics.\nWe can generate() the null distribution in one of two ways: using randomization or theory-based methods. The randomization approach permutes the response and explanatory variables, so that each person’s genetics is matched up with a random cognitive rating from the sample in order to break up any association between the two.\n\n# generate the null distribution using randomization\nnull_distribution_simulated &lt;- ad_data %&gt;%\n  specify(Genotype ~ Class) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 5000, type = \"permute\") %&gt;%\n  calculate(stat = \"Chisq\")\n\nNote that, in the line specify(Genotype ~ Class) above, we could use the equivalent syntax specify(response = Genotype, explanatory = Class). The same goes in the code below, which generates the null distribution using theory-based methods instead of randomization.\n\n# generate the null distribution by theoretical approximation\nnull_distribution_theoretical &lt;- ad_data %&gt;%\n  specify(Genotype ~ Class) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  # note that we skip the generation step here!\n  calculate(stat = \"Chisq\")\n\nTo get a sense for what these distributions look like, and where our observed statistic falls, we can use visualize():\n\n# visualize the null distribution and test statistic!\nnull_distribution_simulated %&gt;%\n  visualize() + \n  shade_p_value(observed_indep_statistic,\n                direction = \"greater\")\n\n\n\n\n\n\n\n\nWe could also visualize the observed statistic against the theoretical null distribution. Note that we skip the generate() and calculate() steps when using the theoretical approach, and that we now need to provide method = \"theoretical\" to visualize().\n\n# visualize the theoretical null distribution and test statistic!\nad_data %&gt;%\n  specify(Genotype ~ Class) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  visualize(method = \"theoretical\") + \n  shade_p_value(observed_indep_statistic,\n                direction = \"greater\")\n\n\n\n\n\n\n\n\nTo visualize both the randomization-based and theoretical null distributions to get a sense of how the two relate, we can pipe the randomization-based null distribution into visualize(), and further provide method = \"both\".\n\n# visualize both null distributions and the test statistic!\nnull_distribution_simulated %&gt;%\n  visualize(method = \"both\") + \n  shade_p_value(observed_indep_statistic,\n                direction = \"greater\")\n\n\n\n\n\n\n\n\nEither way, it looks like our observed test statistic would be fairly unlikely if there were actually no association between cognition and genotype. More exactly, we can calculate the p-value:\n\n# calculate the p value from the observed statistic and null distribution\np_value_independence &lt;- null_distribution_simulated %&gt;%\n  get_p_value(obs_stat = observed_indep_statistic,\n              direction = \"greater\")\n\np_value_independence\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1  0.0006\n\n\nThus, if there were really no relationship between cognition and genotype, the probability that we would see a statistic as or more extreme than 21.5774809 is approximately 6^{-4}.\nNote that, equivalently to the steps shown above, the package supplies a wrapper function, chisq_test, to carry out Chi-Squared tests of independence on tidy data. The syntax goes like this:\n\nchisq_test(ad_data, Genotype ~ Class)\n\n# A tibble: 1 × 3\n  statistic chisq_df  p_value\n      &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;\n1      21.6        5 0.000630"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/xtabs-ws.html#goodness-of-fit",
    "href": "teaching/tidyverse-II/material/case-studies/xtabs-ws.html#goodness-of-fit",
    "title": "Statistical analysis of contingency tables",
    "section": "Goodness of fit",
    "text": "Goodness of fit\nNow, moving on to a chi-squared goodness of fit test, we’ll take a look at just the genotype data. Many papers have investigated the relationship of Apolipoprotein E to diseases. For example, Song et al (2004) conducted a meta-analysis of numerous studies that looked at this gene and heart disease. In their paper, they describe the frequency of the different genotypes across many samples. For the cognition study, it might be interesting to see if our sample of genotypes was consistent with this literature (treating the rates, for this analysis, as known).\nThe rates of the meta-analysis and our observed data are:\n\n# Song, Y., Stampfer, M. J., & Liu, S. (2004). Meta-Analysis: Apolipoprotein E \n# Genotypes and Risk for Coronary Heart Disease. Annals of Internal Medicine, \n# 141(2), 137.\nmeta_rates &lt;- c(\"E2E2\" = 0.71, \"E2E3\" = 11.4, \"E2E4\" = 2.32,\n                \"E3E3\" = 61.0, \"E3E4\" = 22.6, \"E4E4\" = 2.22)\nmeta_rates &lt;- meta_rates/sum(meta_rates) # these add up to slightly &gt; 100%\n\nobs_rates &lt;- table(ad_data$Genotype)/nrow(ad_data)\nround(cbind(obs_rates, meta_rates) * 100, 2)\n\n     obs_rates meta_rates\nE2E2      0.60       0.71\nE2E3     11.11      11.37\nE2E4      2.40       2.31\nE3E3     50.15      60.85\nE3E4     31.83      22.54\nE4E4      3.90       2.21\n\n\nSuppose our null hypothesis is that Genotype follows the same frequency distribution as the meta-analysis. Lets now test whether this difference in distributions is statistically significant.\nFirst, to carry out this hypothesis test, we would calculate our observed statistic.\n\n# calculating the null distribution\nobserved_gof_statistic &lt;- ad_data %&gt;%\n  specify(response = Genotype) %&gt;%\n  hypothesize(null = \"point\", p = meta_rates) %&gt;%\n  calculate(stat = \"Chisq\")\n\nThe observed statistic is 23.3838483. Now, generating a null distribution, by just dropping in a call to generate():\n\n# generating a null distribution\nnull_distribution_gof &lt;- ad_data %&gt;%\n  specify(response = Genotype) %&gt;%\n  hypothesize(null = \"point\", p = meta_rates) %&gt;%\n  generate(reps = 5000, type = \"simulate\") %&gt;%\n  calculate(stat = \"Chisq\")\n\nAgain, to get a sense for what these distributions look like, and where our observed statistic falls, we can use visualize():\n\n# visualize the null distribution and test statistic!\nnull_distribution_gof %&gt;%\n  visualize() + \n  shade_p_value(observed_gof_statistic,\n                direction = \"greater\")\n\n\n\n\n\n\n\n\nThis statistic seems like it would be unlikely if our rates were the same as the rates from the meta-analysis! How unlikely, though? Calculating the p-value:\n\n# calculate the p-value\np_value_gof &lt;- null_distribution_gof %&gt;%\n  get_p_value(observed_gof_statistic,\n              direction = \"greater\")\n\np_value_gof\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.001\n\n\nThus, if each genotype occurred at the same rate as the Song paper, the probability that we would see a distribution like the one we did is approximately 0.001.\nAgain, equivalently to the steps shown above, the package supplies a wrapper function, chisq_test, to carry out chi-squared goodness of fit tests on tidy data. The syntax goes like this:\n\nchisq_test(ad_data, response = Genotype, p = meta_rates)\n\n# A tibble: 1 × 3\n  statistic chisq_df  p_value\n      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1      23.4        5 0.000285"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/coefficients-ws.html",
    "href": "teaching/tidyverse-II/material/case-studies/coefficients-ws.html",
    "title": "Working with model coefficients",
    "section": "",
    "text": "There are many types of statistical models with diverse kinds of structure. Some models have coefficients (a.k.a. weights) for each term in the model. Familiar examples of such models are linear or logistic regression, but more complex models (e.g. neural networks, MARS) can also have model coefficients. When we work with models that use weights or coefficients, we often want to examine the estimated coefficients."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/coefficients-ws.html#introduction",
    "href": "teaching/tidyverse-II/material/case-studies/coefficients-ws.html#introduction",
    "title": "Working with model coefficients",
    "section": "",
    "text": "There are many types of statistical models with diverse kinds of structure. Some models have coefficients (a.k.a. weights) for each term in the model. Familiar examples of such models are linear or logistic regression, but more complex models (e.g. neural networks, MARS) can also have model coefficients. When we work with models that use weights or coefficients, we often want to examine the estimated coefficients."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/coefficients-ws.html#linear-regression",
    "href": "teaching/tidyverse-II/material/case-studies/coefficients-ws.html#linear-regression",
    "title": "Working with model coefficients",
    "section": "Linear regression",
    "text": "Linear regression\nLet’s start with a linear regression model:\n\\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\ldots + \\hat{\\beta}_px_p\\]\nThe \\(\\beta\\) values are the coefficients and the \\(x_j\\) are model predictors, or features.\nLet’s use the Chicago train data where we predict the ridership at the Clark and Lake station (column name: ridership) with the previous ridership data 14 days prior at three of the stations.\nThe data are in the modeldata package:\n\nlibrary(tidymodels)\ntidymodels_prefer()\ntheme_set(theme_bw())\n\ndata(Chicago)\n\nChicago &lt;- Chicago %&gt;% select(ridership, Clark_Lake, Austin, Harlem)\n\n\nA single model\nLet’s start by fitting only a single parsnip model object. We’ll create a model specification using linear_reg().\n\n\n\n\n\n\nNote\n\n\n\nThe default engine is \"lm\" so no call to set_engine() is required.\n\n\nThe fit() function estimates the model coefficients, given a formula and data set.\n\nlm_spec &lt;- linear_reg()\nlm_fit &lt;- fit(lm_spec, ridership ~ ., data = Chicago)\nlm_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = ridership ~ ., data = data)\n\nCoefficients:\n(Intercept)   Clark_Lake       Austin       Harlem  \n     1.6778       0.9035       0.6123      -0.5550  \n\n\nThe best way to retrieve the fitted parameters is to use the tidy() method. This function, in the broom package, returns the coefficients and their associated statistics in a data frame with standardized column names:\n\ntidy(lm_fit)\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    1.68     0.156      10.7  1.11e- 26\n2 Clark_Lake     0.904    0.0280     32.3  5.14e-210\n3 Austin         0.612    0.320       1.91 5.59e-  2\n4 Harlem        -0.555    0.165      -3.36 7.85e-  4\n\n\nWe’ll use this function in subsequent sections.\n\n\nResampled or tuned models\nThe tidymodels framework emphasizes the use of resampling methods to evaluate and characterize how well a model works. While time series resampling methods are appropriate for these data, we can also use the bootstrap to resample the data. This is a standard resampling approach when evaluating the uncertainty in statistical estimates.\nWe’ll use five bootstrap resamples of the data to simplify the plots and output (normally, we would use a larger number of resamples for more reliable estimates).\n\nset.seed(123)\nbt &lt;- bootstraps(Chicago, times = 5)\n\nWith resampling, we fit the same model to the different simulated versions of the data set produced by resampling. The tidymodels function fit_resamples() is the recommended approach for doing so.\n\n\n\n\n\n\nWarning\n\n\n\nThe fit_resamples() function does not automatically save the model objects for each resample since these can be quite large and its main purpose is estimating performance. However, we can pass a function to fit_resamples() that can save the model object or any other aspect of the fit.\n\n\nThis function takes a single argument that represents the fitted workflow object (even if you don’t give fit_resamples() a workflow).\nFrom this, we can extract the model fit. There are two “levels” of model objects that are available:\n\nThe parsnip model object, which wraps the underlying model object. We retrieve this using the extract_fit_parsnip() function.\nThe underlying model object (a.k.a. the engine fit) via the extract_fit_engine().\n\nWe’ll use the latter option and then tidy this model object as we did in the previous section. Let’s add this to the control function so that we can re-use it.\n\nget_lm_coefs &lt;- function(x) {\n  x %&gt;% \n    # get the lm model object\n    extract_fit_engine() %&gt;% \n    # transform its format\n    tidy()\n}\ntidy_ctrl &lt;- control_grid(extract = get_lm_coefs)\n\nThis argument is then passed to fit_resamples():\n\nlm_res &lt;- \n  lm_spec %&gt;% \n  fit_resamples(ridership ~ ., resamples = bt, control = tidy_ctrl)\nlm_res\n\n# Resampling results\n# Bootstrap sampling \n# A tibble: 5 × 5\n  splits              id         .metrics         .notes           .extracts\n  &lt;list&gt;              &lt;chr&gt;      &lt;list&gt;           &lt;list&gt;           &lt;list&gt;   \n1 &lt;split [5698/2076]&gt; Bootstrap1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n2 &lt;split [5698/2098]&gt; Bootstrap2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n3 &lt;split [5698/2064]&gt; Bootstrap3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n4 &lt;split [5698/2082]&gt; Bootstrap4 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n5 &lt;split [5698/2088]&gt; Bootstrap5 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n\n\nNote that there is a .extracts column in our resampling results. This object contains the output of our get_lm_coefs() function for each resample. The structure of the elements of this column is a little complex. Let’s start by looking at the first element (which corresponds to the first resample):\n\nlm_res$.extracts[[1]]\n\n# A tibble: 1 × 2\n  .extracts        .config             \n  &lt;list&gt;           &lt;chr&gt;               \n1 &lt;tibble [4 × 5]&gt; Preprocessor1_Model1\n\n\nThere is another column in this element called .extracts that has the results of the tidy() function call:\n\nlm_res$.extracts[[1]]$.extracts[[1]]\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    1.40     0.157       8.90 7.23e- 19\n2 Clark_Lake     0.842    0.0280     30.1  2.39e-184\n3 Austin         1.46     0.320       4.54 5.70e-  6\n4 Harlem        -0.637    0.163      -3.92 9.01e-  5\n\n\nThese nested columns can be flattened via the purrr unnest() function:\n\nlm_res %&gt;% \n  select(id, .extracts) %&gt;% \n  unnest(.extracts) \n\n# A tibble: 5 × 3\n  id         .extracts        .config             \n  &lt;chr&gt;      &lt;list&gt;           &lt;chr&gt;               \n1 Bootstrap1 &lt;tibble [4 × 5]&gt; Preprocessor1_Model1\n2 Bootstrap2 &lt;tibble [4 × 5]&gt; Preprocessor1_Model1\n3 Bootstrap3 &lt;tibble [4 × 5]&gt; Preprocessor1_Model1\n4 Bootstrap4 &lt;tibble [4 × 5]&gt; Preprocessor1_Model1\n5 Bootstrap5 &lt;tibble [4 × 5]&gt; Preprocessor1_Model1\n\n\nWe still have a column of nested tibbles, so we can run the same command again to get the data into a more useful format:\n\nlm_coefs &lt;- \n  lm_res %&gt;% \n  select(id, .extracts) %&gt;% \n  unnest(.extracts) %&gt;% \n  unnest(.extracts)\n\nlm_coefs %&gt;% select(id, term, estimate, p.value)\n\n# A tibble: 20 × 4\n   id         term        estimate   p.value\n   &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n 1 Bootstrap1 (Intercept)    1.40  7.23e- 19\n 2 Bootstrap1 Clark_Lake     0.842 2.39e-184\n 3 Bootstrap1 Austin         1.46  5.70e-  6\n 4 Bootstrap1 Harlem        -0.637 9.01e-  5\n 5 Bootstrap2 (Intercept)    1.69  2.87e- 28\n 6 Bootstrap2 Clark_Lake     0.911 1.06e-219\n 7 Bootstrap2 Austin         0.595 5.93e-  2\n 8 Bootstrap2 Harlem        -0.580 3.88e-  4\n 9 Bootstrap3 (Intercept)    1.27  3.43e- 16\n10 Bootstrap3 Clark_Lake     0.859 5.03e-194\n11 Bootstrap3 Austin         1.09  6.77e-  4\n12 Bootstrap3 Harlem        -0.470 4.34e-  3\n13 Bootstrap4 (Intercept)    1.95  2.91e- 34\n14 Bootstrap4 Clark_Lake     0.974 1.47e-233\n15 Bootstrap4 Austin        -0.116 7.21e-  1\n16 Bootstrap4 Harlem        -0.620 2.11e-  4\n17 Bootstrap5 (Intercept)    1.87  1.98e- 33\n18 Bootstrap5 Clark_Lake     0.901 1.16e-210\n19 Bootstrap5 Austin         0.494 1.15e-  1\n20 Bootstrap5 Harlem        -0.512 1.73e-  3\n\n\nThat’s better! Now, let’s plot the model coefficients for each resample:\n\nlm_coefs %&gt;%\n  filter(term != \"(Intercept)\") %&gt;% \n  ggplot(aes(x = term, y = estimate, group = id, col = id)) +  \n  geom_hline(yintercept = 0, lty = 3) + \n  geom_line(alpha = 0.3, lwd = 1.2) + \n  labs(y = \"Coefficient\", x = NULL) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nThere seems to be a lot of uncertainty in the coefficient for the Austin station data, but less for the other two.\nLooking at the code for unnesting the results, you may find the double-nesting structure excessive or cumbersome. However, the extraction functionality is flexible, and a simpler structure would prevent many use cases."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/coefficients-ws.html#more-complex-a-glmnet-model",
    "href": "teaching/tidyverse-II/material/case-studies/coefficients-ws.html#more-complex-a-glmnet-model",
    "title": "Working with model coefficients",
    "section": "More complex: a glmnet model",
    "text": "More complex: a glmnet model\nThe glmnet model can fit the same linear regression model structure shown above. It uses regularization (a.k.a penalization) to estimate the model parameters. This has the benefit of shrinking the coefficients towards zero, important in situations where there are strong correlations between predictors or if some feature selection is required. Both of these cases are true for our Chicago train data set.\nThere are two types of penalization that this model uses:\n\nLasso (a.k.a. \\(L_1\\)) penalties can shrink the model terms so much that they are absolute zero (i.e. their effect is entirely removed from the model).\nWeight decay (a.k.a ridge regression or \\(L_2\\)) uses a different type of penalty that is most useful for highly correlated predictors.\n\nThe glmnet model has two primary tuning parameters, the total amount of penalization and the mixture of the two penalty types. For example, this specification:\n\nglmnet_spec &lt;- \n  linear_reg(penalty = 0.1, mixture = 0.95) %&gt;% \n  set_engine(\"glmnet\")\n\nhas a penalty that is 95% lasso and 5% weight decay. The total amount of these two penalties is 0.1 (which is fairly high).\n\n\n\n\n\n\nNote\n\n\n\nModels with regularization require that predictors are all on the same scale. The ridership at our three stations are very different, but glmnet automatically centers and scales the data. You can use recipes to center and scale your data yourself.\n\n\nLet’s combine the model specification with a formula in a model workflow() and then fit the model to the data:\n\nglmnet_wflow &lt;- \n  workflow() %&gt;% \n  add_model(glmnet_spec) %&gt;% \n  add_formula(ridership ~ .)\n\nglmnet_fit &lt;- fit(glmnet_wflow, Chicago)\nglmnet_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nridership ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~0.95) \n\n   Df  %Dev Lambda\n1   0  0.00 6.1040\n2   1 12.75 5.5620\n3   1 23.45 5.0680\n4   1 32.43 4.6180\n5   1 39.95 4.2070\n6   1 46.25 3.8340\n7   1 51.53 3.4930\n8   1 55.94 3.1830\n9   1 59.62 2.9000\n10  1 62.70 2.6420\n11  2 65.28 2.4080\n12  2 67.44 2.1940\n13  2 69.23 1.9990\n14  2 70.72 1.8210\n15  2 71.96 1.6600\n16  2 73.00 1.5120\n17  2 73.86 1.3780\n18  2 74.57 1.2550\n19  2 75.17 1.1440\n20  2 75.66 1.0420\n21  2 76.07 0.9496\n22  2 76.42 0.8653\n23  2 76.70 0.7884\n24  2 76.94 0.7184\n25  2 77.13 0.6545\n26  2 77.30 0.5964\n27  2 77.43 0.5434\n28  2 77.55 0.4951\n29  2 77.64 0.4512\n30  2 77.72 0.4111\n31  2 77.78 0.3746\n32  2 77.84 0.3413\n33  2 77.88 0.3110\n34  2 77.92 0.2833\n35  2 77.95 0.2582\n36  2 77.98 0.2352\n37  2 78.00 0.2143\n38  2 78.01 0.1953\n39  2 78.03 0.1779\n40  2 78.04 0.1621\n41  2 78.05 0.1477\n42  2 78.06 0.1346\n43  2 78.07 0.1226\n44  2 78.07 0.1118\n45  2 78.08 0.1018\n46  2 78.08 0.0928\n\n...\nand 9 more lines.\n\n\nIn this output, the term lambda is used to represent the penalty.\nNote that the output shows many values of the penalty despite our specification of penalty = 0.1. It turns out that this model fits a “path” of penalty values. Even though we are interested in a value of 0.1, we can get the model coefficients for many associated values of the penalty from the same model object.\nLet’s look at two different approaches to obtaining the coefficients. Both will use the tidy() method. One will tidy a glmnet object and the other will tidy a tidymodels object.\n\nUsing glmnet penalty values\nThis glmnet fit contains multiple penalty values which depend on the data set; changing the data (or the mixture amount) often produces a different set of values. For this data set, there are 55 penalties available. To get the set of penalties produced for this data set, we can extract the engine fit and tidy:\n\nglmnet_fit %&gt;% \n  extract_fit_engine() %&gt;% \n  tidy() %&gt;% \n  rename(penalty = lambda) %&gt;%   # &lt;- for consistent naming\n  filter(term != \"(Intercept)\")\n\n# A tibble: 99 × 5\n   term        step estimate penalty dev.ratio\n   &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 Clark_Lake     2   0.0753    5.56     0.127\n 2 Clark_Lake     3   0.145     5.07     0.234\n 3 Clark_Lake     4   0.208     4.62     0.324\n 4 Clark_Lake     5   0.266     4.21     0.400\n 5 Clark_Lake     6   0.319     3.83     0.463\n 6 Clark_Lake     7   0.368     3.49     0.515\n 7 Clark_Lake     8   0.413     3.18     0.559\n 8 Clark_Lake     9   0.454     2.90     0.596\n 9 Clark_Lake    10   0.491     2.64     0.627\n10 Clark_Lake    11   0.526     2.41     0.653\n# ℹ 89 more rows\n\n\nThis works well but, it turns out that our penalty value (0.1) is not in the list produced by the model! The underlying package has functions that use interpolation to produce coefficients for this specific value, but the tidy() method for glmnet objects does not use it.\n\n\nUsing specific penalty values\nIf we run the tidy() method on the workflow or parsnip object, a different function is used that returns the coefficients for the penalty value that we specified:\n\ntidy(glmnet_fit)\n\n# A tibble: 4 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    1.69      0.1\n2 Clark_Lake     0.846     0.1\n3 Austin         0.271     0.1\n4 Harlem         0         0.1\n\n\nFor any another (single) penalty, we can use an additional argument:\n\ntidy(glmnet_fit, penalty = 5.5620)  # A value from above\n\n# A tibble: 4 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  12.6       5.56\n2 Clark_Lake    0.0753    5.56\n3 Austin        0         5.56\n4 Harlem        0         5.56\n\n\nThe reason for having two tidy() methods is that, with tidymodels, the focus is on using a specific penalty value.\n\n\nTuning a glmnet model\nIf we know a priori acceptable values for penalty and mixture, we can use the fit_resamples() function as we did before with linear regression. Otherwise, we can tune those parameters with the tidymodels tune_*() functions.\nLet’s tune our glmnet model over both parameters with this grid:\n\npen_vals &lt;- 10^seq(-3, 0, length.out = 10)\ngrid &lt;- crossing(penalty = pen_vals, mixture = c(0.1, 1.0))\n\nHere is where more glmnet-related complexity comes in: we know that each resample and each value of mixture will probably produce a different set of penalty values contained in the model object. How can we look at the coefficients at the specific penalty values that we are using to tune?\nThe approach that we suggest is to use the special path_values option for glmnet. Details are described in the technical documentation about glmnet and tidymodels but in short, this parameter will assign the collection of penalty values used by each glmnet fit (regardless of the data or value of mixture).\nWe can pass these as an engine argument and then update our previous workflow object:\n\nglmnet_tune_spec &lt;- \n  linear_reg(penalty = tune(), mixture = tune()) %&gt;% \n  set_engine(\"glmnet\", path_values = pen_vals)\n\nglmnet_wflow &lt;- \n  glmnet_wflow %&gt;% \n  update_model(glmnet_tune_spec)\n\nNow we will use an extraction function similar to when we used ordinary least squares. We add an additional argument to retain coefficients that are shrunk to zero by the lasso penalty:\n\nget_glmnet_coefs &lt;- function(x) {\n  x %&gt;% \n    extract_fit_engine() %&gt;% \n    tidy(return_zeros = TRUE) %&gt;% \n    rename(penalty = lambda)\n}\nparsnip_ctrl &lt;- control_grid(extract = get_glmnet_coefs)\n\nglmnet_res &lt;- \n  glmnet_wflow %&gt;% \n  tune_grid(\n    resamples = bt,\n    grid = grid,\n    control = parsnip_ctrl\n  )\nglmnet_res\n\n# Tuning results\n# Bootstrap sampling \n# A tibble: 5 × 5\n  splits              id         .metrics          .notes           .extracts\n  &lt;list&gt;              &lt;chr&gt;      &lt;list&gt;            &lt;list&gt;           &lt;list&gt;   \n1 &lt;split [5698/2076]&gt; Bootstrap1 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n2 &lt;split [5698/2098]&gt; Bootstrap2 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n3 &lt;split [5698/2064]&gt; Bootstrap3 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n4 &lt;split [5698/2082]&gt; Bootstrap4 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n5 &lt;split [5698/2088]&gt; Bootstrap5 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n\n\nAs noted before, the elements of the main .extracts column have an embedded list column with the results of get_glmnet_coefs():\n\nglmnet_res$.extracts[[1]] %&gt;% head()\n\n# A tibble: 6 × 4\n  penalty mixture .extracts         .config              \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;list&gt;            &lt;chr&gt;                \n1       1     0.1 &lt;tibble [40 × 5]&gt; Preprocessor1_Model01\n2       1     0.1 &lt;tibble [40 × 5]&gt; Preprocessor1_Model02\n3       1     0.1 &lt;tibble [40 × 5]&gt; Preprocessor1_Model03\n4       1     0.1 &lt;tibble [40 × 5]&gt; Preprocessor1_Model04\n5       1     0.1 &lt;tibble [40 × 5]&gt; Preprocessor1_Model05\n6       1     0.1 &lt;tibble [40 × 5]&gt; Preprocessor1_Model06\n\nglmnet_res$.extracts[[1]]$.extracts[[1]] %&gt;% head()\n\n# A tibble: 6 × 5\n  term         step estimate penalty dev.ratio\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     1    0.568  1          0.769\n2 (Intercept)     2    0.432  0.464      0.775\n3 (Intercept)     3    0.607  0.215      0.779\n4 (Intercept)     4    0.846  0.1        0.781\n5 (Intercept)     5    1.06   0.0464     0.782\n6 (Intercept)     6    1.22   0.0215     0.783\n\n\nAs before, we’ll have to use a double unnest(). Since the penalty value is in both the top-level and lower-level .extracts, we’ll use select() to get rid of the first version (but keep mixture):\n\nglmnet_res %&gt;% \n  select(id, .extracts) %&gt;% \n  unnest(.extracts) %&gt;% \n  select(id, mixture, .extracts) %&gt;%  # &lt;- removes the first penalty column\n  unnest(.extracts)\n\nBut wait! We know that each glmnet fit contains all of the coefficients. This means, for a specific resample and value of mixture, the results are the same:\n\nall.equal(\n  # First bootstrap, first `mixture`, first `penalty`\n  glmnet_res$.extracts[[1]]$.extracts[[1]],\n  # First bootstrap, first `mixture`, second `penalty`\n  glmnet_res$.extracts[[1]]$.extracts[[2]]\n)\n\n[1] TRUE\n\n\nFor this reason, we’ll add a slice(1) when grouping by id and mixture. This will get rid of the replicated results.\n\nglmnet_coefs &lt;- \n  glmnet_res %&gt;% \n  select(id, .extracts) %&gt;% \n  unnest(.extracts) %&gt;% \n  select(id, mixture, .extracts) %&gt;% \n  group_by(id, mixture) %&gt;%          # ┐\n  slice(1) %&gt;%                       # │ Remove the redundant results\n  ungroup() %&gt;%                      # ┘\n  unnest(.extracts)\n\nglmnet_coefs %&gt;% \n  select(id, penalty, mixture, term, estimate) %&gt;% \n  filter(term != \"(Intercept)\")\n\n# A tibble: 300 × 5\n   id         penalty mixture term       estimate\n   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1 Bootstrap1 1           0.1 Clark_Lake    0.391\n 2 Bootstrap1 0.464       0.1 Clark_Lake    0.485\n 3 Bootstrap1 0.215       0.1 Clark_Lake    0.590\n 4 Bootstrap1 0.1         0.1 Clark_Lake    0.680\n 5 Bootstrap1 0.0464      0.1 Clark_Lake    0.746\n 6 Bootstrap1 0.0215      0.1 Clark_Lake    0.793\n 7 Bootstrap1 0.01        0.1 Clark_Lake    0.817\n 8 Bootstrap1 0.00464     0.1 Clark_Lake    0.828\n 9 Bootstrap1 0.00215     0.1 Clark_Lake    0.834\n10 Bootstrap1 0.001       0.1 Clark_Lake    0.837\n# ℹ 290 more rows\n\n\nNow we have the coefficients. Let’s look at how they behave as more regularization is used:\n\nglmnet_coefs %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(mixture = format(mixture)) %&gt;% \n  ggplot(aes(x = penalty, y = estimate, col = mixture, groups = id)) + \n  geom_hline(yintercept = 0, lty = 3) +\n  geom_line(alpha = 0.5, lwd = 1.2) + \n  facet_wrap(~ term) + \n  scale_x_log10() +\n  scale_color_brewer(palette = \"Accent\") +\n  labs(y = \"coefficient\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nNotice a couple of things:\n\nWith a pure lasso model (i.e., mixture = 1), the Austin station predictor is selected out in each resample. With a mixture of both penalties, its influence increases. Also, as the penalty increases, the uncertainty in this coefficient decreases.\nThe Harlem predictor is either quickly selected out of the model or goes from negative to positive."
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nSubtitle\n\n\n\n\n\n\n\n\nData Science with Tidyverse I\n\n\nData Wrangling & Visualization\n\n\n\n\n\n\nData Science with Tidyverse II\n\n\nTidymodeling\n\n\n\n\n\n\nIntroductory Statistics\n\n\nBSc course\n\n\n\n\n\n\nMathematics for Social Scientists\n\n\nMSc course\n\n\n\n\n\n\nSocial Network Analysis\n\n\nMSc course\n\n\n\n\n\n\nStatistical Learning\n\n\nMSc course\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/stat-learn/material/03/03-linear-regression-II.html",
    "href": "teaching/stat-learn/material/03/03-linear-regression-II.html",
    "title": "Linear Regression II",
    "section": "",
    "text": "Our goal is to run a few models of different complexity and evaluate the performance based on a test-train split of the data.\n\n\nCan you understand why we do not use the full data to test the different models?\nWe will start by creating a function that allows us to assess model performance:\n\nrmse = function(actual, predicted) {\n  sqrt(mean((actual - predicted) ^ 2))\n}\n\nWe will also use a function that gives the complexity of the linear model as number of independent variables defined.\n\nget_complexity = function(model) {\n  length(coef(model)) - 1\n}\n\n\n\n\nCan you understand why we take -1 in the above function?\n\n\n\nWe will use the Advertisement data from ISLR2 which is in a compressed folder for download. Make sure you have set the correct working directory for reading the data. You can also load the data from the ISLR2 package. The Advertising data set consists of the sales of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: TV, radio, and newspaper.\n\nlibrary(readr) # install if not in your library\nAdvertising = read_csv(\"03-data/Advertising.csv\") \nknitr::kable(head(Advertising,10)) # look at 10 first rows of data\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n230.1\n37.8\n69.2\n22.1\n\n\n44.5\n39.3\n45.1\n10.4\n\n\n17.2\n45.9\n69.3\n9.3\n\n\n151.5\n41.3\n58.5\n18.5\n\n\n180.8\n10.8\n58.4\n12.9\n\n\n8.7\n48.9\n75.0\n7.2\n\n\n57.5\n32.8\n23.5\n11.8\n\n\n120.2\n19.6\n11.6\n13.2\n\n\n8.6\n2.1\n1.0\n4.8\n\n\n199.8\n2.6\n21.2\n10.6\n\n\n\n\n\nExplore the data by doing some plots. Example shown below.\n\nplot(Sales ~ TV, data = Advertising, col = \"dodgerblue\", pch = 20, cex = 1.5,\n     main = \"Sales vs Television Advertising\")\n\n\n\n\n\n\n\n\nWe can also explore the data by using a correlation matrix plot:\n\npairs(Advertising)\n\n\n\n\n\n\n\n\n\n\n\nWhat are the main patterns you detect?\n\n\n\nWe will now split the data in two. One part of the datasets will be used to fit (train) a model, which we will call the training data. The remainder of the original data will be used to assess how well the model is predicting, which we will call the test data.\nWe use the sample() function to obtain a random sample of the rows of the original data. We then use those row numbers (and remaining row numbers) to split the data accordingly. Notice we used the set.seed() function to reproduce the same random split each time we perform this analysis.\n\nset.seed(1)\nnum_obs = nrow(Advertising)\n\ntrain_index = sample(num_obs, size = trunc(0.50 * num_obs))\ntrain_data = Advertising[train_index, ]\ntest_data = Advertising[-train_index, ]\n\n\n\n\nWe start by fitting the simplest linear model, a model with no predictors:\n\nfit_0 = lm(Sales ~ 1, data = train_data)\nget_complexity(fit_0)\n\n[1] 0\n\n\nAs seen, the complexity of this model is 0. We compute the Test and Train RMSE for this model, via the function specified above and a direct formula:\n\n# train RMSE\nrmse(actual = train_data$Sales, predicted = predict(fit_0, train_data))\n\n[1] 5.297333\n\nsqrt(mean((train_data$Sales - predict(fit_0, train_data)) ^ 2))\n\n[1] 5.297333\n\n# test RMSE\nrmse(actual = test_data$Sales, predicted = predict(fit_0, test_data))\n\n[1] 5.112073\n\nsqrt(mean((test_data$Sales - predict(fit_0, test_data)) ^ 2)) \n\n[1] 5.112073\n\n\nLet’s make the computations of RMSE even more easy by using the below function:\n\n# train RMSE\nget_rmse = function(model, data, response) {\n  rmse(actual = subset(data, select = response, drop = TRUE),\n       predicted = predict(model, data))\n}\n\nCan you figure out how to use the above function?\nWe will fit 5 models, compute the train and test MSE for each model to evaluate it and visualize how it relates to the complexity (i.e. model size in terms of parameters) of the model. Furthermore, we will interpret the results in terms of underfitting or overfitting. We will conclude with dertemrineing which model to choose. The five models to fit are listed below:\n\nSales ~ Radio + Newspaper + TV\nSales ~ Radio * Newspaper * TV\nSales ~ Radio * Newspaper * TV + I(TV ^ 2)\nSales ~ Radio * Newspaper * TV + I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2)\nSales ~ Radio * Newspaper * TV + I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2)\n\nNote that the specification first*second indicates the cross of first and second. This is the same as first + second + first:second.\nWe fit all five models and print the output by creating a list of the model fits. We then obtain train RMSE, test RMSE, and model complexity for each. Finally, plot the results. The train RMSE can be seen in blue, while the test RMSE is given in orange.\n\nfit_1 = lm(Sales ~ Radio + Newspaper + TV, data = train_data)\nfit_2 = lm(Sales ~ Radio * Newspaper * TV, data = train_data)\nfit_3 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2), data = train_data)\nfit_4 = lm(Sales ~ Radio * Newspaper * TV + \n             I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2), data = train_data)\nfit_5 = lm(Sales ~ Radio * Newspaper * TV +\n             I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2), data = train_data)\n\nmodel_list = list(fit_1, fit_2, fit_3, fit_4, fit_5)\n#model_list #uncomment if you wish to see this list object\n\ntrain_rmse = sapply(model_list, get_rmse, data = train_data, response = \"Sales\")\ntest_rmse = sapply(model_list, get_rmse, data = test_data, response = \"Sales\")\nmodel_complexity = sapply(model_list, get_complexity)\n\ntrain_rmse = sapply(model_list, get_rmse, data = train_data, response = \"Sales\")\ntest_rmse = sapply(model_list, get_rmse, data = test_data, response = \"Sales\")\nmodel_complexity = sapply(model_list, get_complexity)\n\nplot(model_complexity, train_rmse, type = \"b\", \n     ylim = c(min(c(train_rmse, test_rmse)) - 0.02, \n              max(c(train_rmse, test_rmse)) + 0.02), \n     col = \"blue\", \n     lwd = 2,\n     xlab = \"Complexity (Model Size)\",\n     ylab = \"RMSE\")\nlines(model_complexity, test_rmse, type = \"b\", col = \"red\", lwd = 2)\ngrid()\n\n\n\n\n\n\n\n\n\n\n\nWhat do you conclude?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nUnderfitting models: In general High Train RMSE, High Test RMSE. Seen in fit_1 and fit_2.\nOverfitting models: In general Low Train RMSE, High Test RMSE. Seen in fit_4 and fit_5.\n\nWe see that the Test RMSE is smallest for fit_3, and it is the model we believe will perform the best on future data not used to train the model.\n\n\n\n\n\n\nWrite functions for the some of the other loss functions we covered in the lecture (e.g. MAE, MAPE). Apply them to asses the same five models above. Do you see similar or deviant results in terms of overfitting/underfitting?"
  },
  {
    "objectID": "teaching/stat-learn/material/03/03-linear-regression-II.html#question",
    "href": "teaching/stat-learn/material/03/03-linear-regression-II.html#question",
    "title": "Linear Regression II",
    "section": "",
    "text": "Can you understand why we do not use the full data to test the different models?\nWe will start by creating a function that allows us to assess model performance:\n\nrmse = function(actual, predicted) {\n  sqrt(mean((actual - predicted) ^ 2))\n}\n\nWe will also use a function that gives the complexity of the linear model as number of independent variables defined.\n\nget_complexity = function(model) {\n  length(coef(model)) - 1\n}"
  },
  {
    "objectID": "teaching/stat-learn/material/03/03-linear-regression-II.html#question-1",
    "href": "teaching/stat-learn/material/03/03-linear-regression-II.html#question-1",
    "title": "Linear Regression II",
    "section": "",
    "text": "Can you understand why we take -1 in the above function?"
  },
  {
    "objectID": "teaching/stat-learn/material/03/03-linear-regression-II.html#data",
    "href": "teaching/stat-learn/material/03/03-linear-regression-II.html#data",
    "title": "Linear Regression II",
    "section": "",
    "text": "We will use the Advertisement data from ISLR2 which is in a compressed folder for download. Make sure you have set the correct working directory for reading the data. You can also load the data from the ISLR2 package. The Advertising data set consists of the sales of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: TV, radio, and newspaper.\n\nlibrary(readr) # install if not in your library\nAdvertising = read_csv(\"03-data/Advertising.csv\") \nknitr::kable(head(Advertising,10)) # look at 10 first rows of data\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n230.1\n37.8\n69.2\n22.1\n\n\n44.5\n39.3\n45.1\n10.4\n\n\n17.2\n45.9\n69.3\n9.3\n\n\n151.5\n41.3\n58.5\n18.5\n\n\n180.8\n10.8\n58.4\n12.9\n\n\n8.7\n48.9\n75.0\n7.2\n\n\n57.5\n32.8\n23.5\n11.8\n\n\n120.2\n19.6\n11.6\n13.2\n\n\n8.6\n2.1\n1.0\n4.8\n\n\n199.8\n2.6\n21.2\n10.6\n\n\n\n\n\nExplore the data by doing some plots. Example shown below.\n\nplot(Sales ~ TV, data = Advertising, col = \"dodgerblue\", pch = 20, cex = 1.5,\n     main = \"Sales vs Television Advertising\")\n\n\n\n\n\n\n\n\nWe can also explore the data by using a correlation matrix plot:\n\npairs(Advertising)"
  },
  {
    "objectID": "teaching/stat-learn/material/03/03-linear-regression-II.html#question-2",
    "href": "teaching/stat-learn/material/03/03-linear-regression-II.html#question-2",
    "title": "Linear Regression II",
    "section": "",
    "text": "What are the main patterns you detect?"
  },
  {
    "objectID": "teaching/stat-learn/material/03/03-linear-regression-II.html#test-train-split",
    "href": "teaching/stat-learn/material/03/03-linear-regression-II.html#test-train-split",
    "title": "Linear Regression II",
    "section": "",
    "text": "We will now split the data in two. One part of the datasets will be used to fit (train) a model, which we will call the training data. The remainder of the original data will be used to assess how well the model is predicting, which we will call the test data.\nWe use the sample() function to obtain a random sample of the rows of the original data. We then use those row numbers (and remaining row numbers) to split the data accordingly. Notice we used the set.seed() function to reproduce the same random split each time we perform this analysis.\n\nset.seed(1)\nnum_obs = nrow(Advertising)\n\ntrain_index = sample(num_obs, size = trunc(0.50 * num_obs))\ntrain_data = Advertising[train_index, ]\ntest_data = Advertising[-train_index, ]"
  },
  {
    "objectID": "teaching/stat-learn/material/03/03-linear-regression-II.html#the-model",
    "href": "teaching/stat-learn/material/03/03-linear-regression-II.html#the-model",
    "title": "Linear Regression II",
    "section": "",
    "text": "We start by fitting the simplest linear model, a model with no predictors:\n\nfit_0 = lm(Sales ~ 1, data = train_data)\nget_complexity(fit_0)\n\n[1] 0\n\n\nAs seen, the complexity of this model is 0. We compute the Test and Train RMSE for this model, via the function specified above and a direct formula:\n\n# train RMSE\nrmse(actual = train_data$Sales, predicted = predict(fit_0, train_data))\n\n[1] 5.297333\n\nsqrt(mean((train_data$Sales - predict(fit_0, train_data)) ^ 2))\n\n[1] 5.297333\n\n# test RMSE\nrmse(actual = test_data$Sales, predicted = predict(fit_0, test_data))\n\n[1] 5.112073\n\nsqrt(mean((test_data$Sales - predict(fit_0, test_data)) ^ 2)) \n\n[1] 5.112073\n\n\nLet’s make the computations of RMSE even more easy by using the below function:\n\n# train RMSE\nget_rmse = function(model, data, response) {\n  rmse(actual = subset(data, select = response, drop = TRUE),\n       predicted = predict(model, data))\n}\n\nCan you figure out how to use the above function?\nWe will fit 5 models, compute the train and test MSE for each model to evaluate it and visualize how it relates to the complexity (i.e. model size in terms of parameters) of the model. Furthermore, we will interpret the results in terms of underfitting or overfitting. We will conclude with dertemrineing which model to choose. The five models to fit are listed below:\n\nSales ~ Radio + Newspaper + TV\nSales ~ Radio * Newspaper * TV\nSales ~ Radio * Newspaper * TV + I(TV ^ 2)\nSales ~ Radio * Newspaper * TV + I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2)\nSales ~ Radio * Newspaper * TV + I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2)\n\nNote that the specification first*second indicates the cross of first and second. This is the same as first + second + first:second.\nWe fit all five models and print the output by creating a list of the model fits. We then obtain train RMSE, test RMSE, and model complexity for each. Finally, plot the results. The train RMSE can be seen in blue, while the test RMSE is given in orange.\n\nfit_1 = lm(Sales ~ Radio + Newspaper + TV, data = train_data)\nfit_2 = lm(Sales ~ Radio * Newspaper * TV, data = train_data)\nfit_3 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2), data = train_data)\nfit_4 = lm(Sales ~ Radio * Newspaper * TV + \n             I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2), data = train_data)\nfit_5 = lm(Sales ~ Radio * Newspaper * TV +\n             I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2), data = train_data)\n\nmodel_list = list(fit_1, fit_2, fit_3, fit_4, fit_5)\n#model_list #uncomment if you wish to see this list object\n\ntrain_rmse = sapply(model_list, get_rmse, data = train_data, response = \"Sales\")\ntest_rmse = sapply(model_list, get_rmse, data = test_data, response = \"Sales\")\nmodel_complexity = sapply(model_list, get_complexity)\n\ntrain_rmse = sapply(model_list, get_rmse, data = train_data, response = \"Sales\")\ntest_rmse = sapply(model_list, get_rmse, data = test_data, response = \"Sales\")\nmodel_complexity = sapply(model_list, get_complexity)\n\nplot(model_complexity, train_rmse, type = \"b\", \n     ylim = c(min(c(train_rmse, test_rmse)) - 0.02, \n              max(c(train_rmse, test_rmse)) + 0.02), \n     col = \"blue\", \n     lwd = 2,\n     xlab = \"Complexity (Model Size)\",\n     ylab = \"RMSE\")\nlines(model_complexity, test_rmse, type = \"b\", col = \"red\", lwd = 2)\ngrid()"
  },
  {
    "objectID": "teaching/stat-learn/material/03/03-linear-regression-II.html#question-3",
    "href": "teaching/stat-learn/material/03/03-linear-regression-II.html#question-3",
    "title": "Linear Regression II",
    "section": "",
    "text": "What do you conclude?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nUnderfitting models: In general High Train RMSE, High Test RMSE. Seen in fit_1 and fit_2.\nOverfitting models: In general Low Train RMSE, High Test RMSE. Seen in fit_4 and fit_5.\n\nWe see that the Test RMSE is smallest for fit_3, and it is the model we believe will perform the best on future data not used to train the model."
  },
  {
    "objectID": "teaching/stat-learn/material/03/03-linear-regression-II.html#exercise",
    "href": "teaching/stat-learn/material/03/03-linear-regression-II.html#exercise",
    "title": "Linear Regression II",
    "section": "",
    "text": "Write functions for the some of the other loss functions we covered in the lecture (e.g. MAE, MAPE). Apply them to asses the same five models above. Do you see similar or deviant results in terms of overfitting/underfitting?"
  },
  {
    "objectID": "teaching/stat-learn/material/03/03-linear-regression-II.html#real-life-extrapolation",
    "href": "teaching/stat-learn/material/03/03-linear-regression-II.html#real-life-extrapolation",
    "title": "Linear Regression II",
    "section": "2.1 Real Life Extrapolation",
    "text": "2.1 Real Life Extrapolation\nIronman Data"
  },
  {
    "objectID": "teaching/stat-learn/material/03/03-linear-regression-II.html#the-bias-variance-tradeoff",
    "href": "teaching/stat-learn/material/03/03-linear-regression-II.html#the-bias-variance-tradeoff",
    "title": "Linear Regression II",
    "section": "2.2 The Bias Variance Tradeoff",
    "text": "2.2 The Bias Variance Tradeoff\n\\[ \\underbrace{E(y_0 - \\hat{f}(x_0))^2}_\\text{expected MSE at $x_0$} = \\overbrace{Var(\\hat{f}(x_0))}^\\text{variance of the model when fit on different samples} + \\underbrace{[Bias(\\hat{f}(x_0))]^2}_\\text{ability of the model to \"get it right\" on average} + \\overbrace{Var(\\epsilon)}^\\text{irreducible error}\\]\n\nWhat are we looking for when trying to see if there is overfitting?\nWhat is Overfitting and how is it different from extrapolation?"
  },
  {
    "objectID": "teaching/stat-learn/material/03/03-linear-regression-II.html#exercise-1",
    "href": "teaching/stat-learn/material/03/03-linear-regression-II.html#exercise-1",
    "title": "Linear Regression II",
    "section": "3.1 Exercise",
    "text": "3.1 Exercise\nReplicate the simulation example but try other models based on different order polynomials."
  },
  {
    "objectID": "teaching/stat-learn/material/05/05_Classification_2.html",
    "href": "teaching/stat-learn/material/05/05_Classification_2.html",
    "title": "Classification II",
    "section": "",
    "text": "Once we build a classification model, it’s important to make sure that we know how it performs! We talked about a few metrics you can calculate to help you assess how well your model is doing.\n\nAccuracy: \\(\\frac{TP + TN}{TP + TN + FP + FN}\\)\nConfusion Matrix Patterns\n\n\n\n\n\nActually 1\nActually 0\n\n\n\n\nPredicted 1\nTrue Positive (TP)\nFalse Positive (FP)\n\n\nPredicted 0\nFalse Negative (FN)\nTrue Negative (TN)\n\n\n\n\nPrecision: \\(\\frac{TP}{TP + FP}\\), how many of the predicted positives are true positives?\nRecall/Sensitivity: \\(\\frac{TP}{TP + FN}\\), how many of the actual positives did we accurately predict?\nSpecificity: \\(\\frac{TN}{TN + FP}\\), how many of the actual negatives did we accurately predict?\nF1 Score: \\(\\frac{2 * Precision * Recall}{Precision + Recall}\\), a combination of precision and recall.\nROC AUC: The area under the ROC curve which puts the False Positive Rate (FPR) on the x-axis, and the True Positive Rate on the y-axis.\n\n\n\nIf you were designing a Flu test, which of these metrics would be most important to you (there’s no one right answer) and why?"
  },
  {
    "objectID": "teaching/stat-learn/material/05/05_Classification_2.html#question",
    "href": "teaching/stat-learn/material/05/05_Classification_2.html#question",
    "title": "Classification II",
    "section": "",
    "text": "If you were designing a Flu test, which of these metrics would be most important to you (there’s no one right answer) and why?"
  },
  {
    "objectID": "teaching/stat-learn/material/05/05_Classification_2.html#naive",
    "href": "teaching/stat-learn/material/05/05_Classification_2.html#naive",
    "title": "Classification II",
    "section": "2.1 Naive",
    "text": "2.1 Naive\nNaive Bayes is Naive because it makes the assumption of conditional independence. This means that within classes (the groups we’re trying to predict), we assume the predictors are independent. However we know that’s not true, so it’s a Naive assumption to make.\nHowever, it simplifies our computation. When events are independent we can calculate their joint probability just by multiplying them:\n\\[\\underbrace{P(A,B,C)}_\\text{probability of A, B, and C} = P(A) * P(B) * P(C)\\]\nRather than calculating the joint probability \\(P(A,B,C)\\) by taking into account any relationships between the predictors."
  },
  {
    "objectID": "teaching/stat-learn/material/05/05_Classification_2.html#bayes",
    "href": "teaching/stat-learn/material/05/05_Classification_2.html#bayes",
    "title": "Classification II",
    "section": "2.2 Bayes",
    "text": "2.2 Bayes\nWhen classifying a data point, we use Bayes’ Theorem (at least the numerator of it) to calculate the score for each potential category. Then we choose the category with the highest score. Because we’re comparing scores that have the same denominator, we can ignore it (which is nice, as it’s difficiult to calculate).\n\\[ \\overbrace{P(\\text{group}_i | \\mathbf{X}) = \\frac{P(\\mathbf{X} | \\text{group}_i) * P(\\text{group}_i)}{P(\\mathbf{X})}}^\\text{Bayes' Theorem}\\]\n\n\\(P(\\text{group}_i | \\mathbf{X})\\) is the probability of our data point being in group \\(i\\) based on their predictor values \\(\\mathbf{X}\\)\n\\(P(\\mathbf{X} | \\text{group}_i)\\) is the likelihood of seeing features like \\(\\mathbf{X}\\) in group \\(i\\) (if our features are commonly seen in group \\(i\\), we’re more likely to predict you’re in group \\(i\\))\n\\(P(\\text{group}_i)\\) is the probability of being in group \\(i\\) overall (if a group is very rare, we don’t want to predict it often)\n\\(P(\\mathbf{X})\\) is the probability of seeing features like \\(\\mathbf{X}\\) overall, in any group (but we ignore this term)"
  },
  {
    "objectID": "teaching/stat-learn/material/05/05_Classification_2.html#naive-bayes-in-r",
    "href": "teaching/stat-learn/material/05/05_Classification_2.html#naive-bayes-in-r",
    "title": "Classification II",
    "section": "2.3 Naive Bayes in R",
    "text": "2.3 Naive Bayes in R\nFrom a training collection of sms text messages, that are labeled Spam or Ham, a set of used words can be extracted. After a couple of data cleaning operations (see below) it is counted how often words occur in the text messages in the training set. Words with a frequency equal or higher than a chosen threshold, e.g. five, are collected in a dictionary. Only these words are used to distinguish Spam from Ham. As the next step, each word is converted into a binary variable with the length of the number of messages in the training set, with value 1 in cell \\(j\\) if the word occurs one or more times in the \\(j\\)th text message and 0 otherwise. These binary variables are mutated into factor variables which can be used to generate NaiveBayes model that distinguishes Spam from Ham.\n\n2.3.1 Packages needed\n\nlibrary(tidyverse)\nlibrary(flextable) # install if not on your machine\nlibrary(tm) # install if not on your machine\nlibrary(e1071) # install if not on your machine\nlibrary(caret) # install if not on your machine\nlibrary(pROC) # install if not on your machine\n\n\n\n2.3.2 Analysis\n\ndf &lt;- read.csv(\"05-data/sms_spam.csv\")\n\n#transform df$type into a factor variable\ndf$type &lt;- factor(df$type, levels = c(\"ham\", \"spam\"),\n                       labels=c(\"ham\", \"spam\"))\n\nflextable(head(df)) %&gt;% autofit()\n\ntypetexthamGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...hamOk lar... Joking wif u oni...spamFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18'shamU dun say so early hor... U c already then say...hamNah I don't think he goes to usf, he lives around here thoughspamFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n\n\nThe R package tm (text mining) comes with a couple of helpful functions for Text Mining. In order to use the text mining functions, the data to be investigated must be in a so-called Corpus of text documents. The first step in this analysis is to convert the set of SMS messages into such a corpus. First the text messages are transformed into a vector source this is an R vector that interprets every element as a text document.\nFirst Five Elements of SMS message in a Vector Source and create a Corpus with all the sms messages as text documents. This is in fact a list of lists, every list contains the text of the document and metadata about the document.\n\nsms_vector_source &lt;- VectorSource(df$text)\nsms_vector_source[1:5]\n\n[1] \"Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\"                                            \n[2] \"Ok lar... Joking wif u oni...\"                                                                                                                              \n[3] \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"\n[4] \"U dun say so early hor... U c already then say...\"                                                                                                          \n[5] \"Nah I don't think he goes to usf, he lives around here though\"                                                                                              \n\nsms_corpus &lt;- Corpus(sms_vector_source)\n\nNow it is time to perform a couple of text cleaning preparation steps. The tm package has functions for these actions:\n\nBecause it is assumed that capitalised letters will not be used to distinguish Ham from Spam, replace all uppercase letters by lowercase letters.\nRemove numbers from the text messages. If it is assumed that numbers in the messages can be helpful to diferentiate between Spam and Ham, a more advanced way to deal with numbers is required\nLots of short words, like and, or, if, on, to etc. are not useful to distinct Ham from Spam; they can be removed from the texts before generating a model.\nRemove punctuation.\nRemove unnecessary spaces.\n\n\n#transform to lower case text messages\n#the tm::tm_map function applies a function on a corpus object\ncorpus_clean &lt;- tm_map(sms_corpus, tolower) %&gt;% \n#remove numbers\n  tm_map(removeNumbers) %&gt;% \n#remove stopwords\n  tm_map(removeWords, stopwords()) %&gt;% \n#remove punctuation\n  tm_map(removePunctuation) %&gt;% \n#remove additional spaces\n  tm_map(stripWhitespace)\n\ncorpus_clean[1]$content\n\n[1] \"go jurong point crazy available bugis n great world la e buffet cine got amore wat\"\n\n\nAssuming that splitting the data in a training and a test set will be used to assess the Naive Bayes model, the next step is splitting the data, e.g. 70% in the training set and 30% in the test set.\n\nset.seed(123)\ntrain &lt;- sample(1:length(corpus_clean),\n                size = .7*length(corpus_clean),\n                replace=FALSE)\n\nsms_train &lt;- df[train,]\nsms_test &lt;- df[-train,]\n\ncorpus_train &lt;- corpus_clean[train]\ncorpus_test &lt;- corpus_clean[-train]\n\nsum(train)\n\n[1] 10780878\n\n\nLet’s now create a document term matrix:\n\ndtm_train &lt;- DocumentTermMatrix(corpus_train)\ninspect(dtm_train)\n\n&lt;&lt;DocumentTermMatrix (documents: 3901, terms: 6476)&gt;&gt;\nNon-/sparse entries: 30139/25232737\nSparsity           : 100%\nMaximal term length: 40\nWeighting          : term frequency (tf)\nSample             :\n      Terms\nDocs   call can free get just know like ltgt now will\n  17      0   0    1   0    0    0    1    0   0    0\n  2155    0   3    1   1    0    0    0    6   0    0\n  2252    0   1    0   0    0    0    0    1   0    0\n  277     0   0    1   0    0    0    1    0   0    0\n  2933    0   0    0   0    0    0    0    2   0    0\n  3313    0   0    0   1    0    0    1    0   0   11\n  3368    0   0    0   0    0    0    0    0   0    0\n  3652    0   0    0   0    0    0    0    0   0    0\n  489     0   0    0   0    0    1    1    0   0    0\n  66      0   0    0   0    0    0    0    1   0    0\n\n\nTo distinguish Ham from Spam not every word in the corpus are useful. Words must appear in a couple of messages to be useful. A choice must be made for the threshold of the number of messages in which a word appears to be used in the model, e.g. 5 times.\nFirst construct a vector with words with a frequency of at least 5:\n\nfrequent_terms_5 &lt;- findFreqTerms(dtm_train, lowfreq=5)\nfrequent_terms_5[1:10]\n\n [1] \"anything\" \"lar\"      \"already\"  \"ard\"      \"can\"      \"check\"   \n [7] \"dat\"      \"later\"    \"like\"     \"picking\" \n\n\nThe Naive Bayes model uses as features not the number of times a term appears in a message, but only whether a term appears in a message.\nIt is possible to construct a Binary DTM in which the cells indicate whether a document contains the term (cell value = 1) or not (cell value = 0). It is this Binary DTM that is used in the Naive Bayes model. The Binary DTM is constructed for the words with frequency at least.\n\n#Binary DTM for training data\ndtm_train_bin &lt;- DocumentTermMatrix(\n                      corpus_train,\n                      control=list(weighting=weightBin,\n                                   dictionary=frequent_terms_5))\n\nWarning in TermDocumentMatrix.SimpleCorpus(x, control): custom functions are\nignored\n\n#Binary DTM for test data; needed to asses the model\ndtm_test_bin &lt;- DocumentTermMatrix(\n                      corpus_test,\n                      control=list(weighting=weightBin,\n                                   dictionary=frequent_terms_5))\n\nWarning in TermDocumentMatrix.SimpleCorpus(x, control): custom functions are\nignored\n\ninspect(dtm_train_bin)\n\n&lt;&lt;DocumentTermMatrix (documents: 3901, terms: 1179)&gt;&gt;\nNon-/sparse entries: 22345/4576934\nSparsity           : 100%\nMaximal term length: 19\nWeighting          : binary (bin)\nSample             :\n      Terms\nDocs   call can free get got just know like now will\n  2229    0   0    0   0   0    0    0    0   0    0\n  2252    0   1    0   0   0    0    0    0   0    0\n  327     1   1    0   1   0    1    0    0   1    0\n  3313    0   0    0   1   0    0    0    1   0    1\n  3368    0   0    0   0   0    0    0    0   0    0\n  3602    0   0    0   0   0    0    0    0   0    0\n  3652    0   0    0   0   0    0    0    0   0    0\n  489     0   0    0   0   0    0    1    1   0    0\n  514     0   1    0   0   0    0    0    0   0    0\n  66      0   0    0   0   0    0    0    0   0    0\n\n\nThe columns in the Binary DTM must be transformed into factor variables to use them in a Naive Bayes model. Then the Binary DTM is ready to generate a Naive Bayes model.\n\n#first use as matrix() to convert DTM matrix from a list into a matrix \ndtm_train_bin_matrix &lt;- as.matrix(dtm_train_bin)\ndtm_test_bin_matrix &lt;- as.matrix(dtm_test_bin)\n\n#convert the columns into factor\ndtm_train_bin_matrix &lt;- apply(dtm_train_bin_matrix, 2, factor)\ndtm_test_bin_matrix &lt;- apply(dtm_test_bin_matrix, 2, factor)\n\n#generate model\nnb_model &lt;- naiveBayes(x=dtm_train_bin_matrix,\n                       y=sms_train$type)\n\nsummary(nb_model)\n\n          Length Class  Mode     \napriori      2   table  numeric  \ntables    1179   -none- list     \nlevels       2   -none- character\nisnumeric 1179   -none- logical  \ncall         3   -none- call     \n\n\nNest we are assessing the model:\n\nUse the model to make predictions on the test data\nAssess the model using a confusion matrix\n\n\npreds &lt;- predict(nb_model, dtm_test_bin_matrix)\n\ncf &lt;- table(preds, sms_test$type)\n\ncf\n\n      \npreds   ham spam\n  ham  1448   24\n  spam    3  198\n\n\nAs can be seen in the Confusion Matrix, the model makes a good distinction between Spam and Ham. Only 3 of the 1451 Ham messages are classified as Spam while 24 of the 198 Spam messages are classified as Ham.\nThe caret::confusionMatrix() function gives a lot of metrics which can be used to assess a classification model. Which metric is most applicable depends on the context of the problem in question.\n\nconfusionMatrix(preds, sms_test$type)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  ham spam\n      ham  1448   24\n      spam    3  198\n                                          \n               Accuracy : 0.9839          \n                 95% CI : (0.9766, 0.9893)\n    No Information Rate : 0.8673          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.927           \n                                          \n Mcnemar's Test P-Value : 0.0001186       \n                                          \n            Sensitivity : 0.9979          \n            Specificity : 0.8919          \n         Pos Pred Value : 0.9837          \n         Neg Pred Value : 0.9851          \n             Prevalence : 0.8673          \n         Detection Rate : 0.8655          \n   Detection Prevalence : 0.8799          \n      Balanced Accuracy : 0.9449          \n                                          \n       'Positive' Class : ham             \n                                          \n\n\n\n\n2.3.3 Question\nWhich metric would you choose to evaluate the model here?"
  },
  {
    "objectID": "teaching/stat-learn/material/05/05_Classification_2.html#lizzo",
    "href": "teaching/stat-learn/material/05/05_Classification_2.html#lizzo",
    "title": "Classification II",
    "section": "3.1 Lizzo",
    "text": "3.1 Lizzo\nBuild a Logistic Regression Model to predict the mode of different Lizzo songs. Then check the performance of the model. Fill int he msising parts in the code below:\n\n# Read data\nd &lt;- read.csv(\"05-Data/Lizzo_data.csv\")\n\n# Convert boolean/logical TRUE/FALSE to 1/0\n# (works even if column is already logical)\nd$explicit &lt;- as.integer(d$explicit)\n\n# View first rows\nflextable(head(d)) %&gt;% autofit()\n\nXartist_nameartist_idalbum_idalbum_typealbum_release_datealbum_release_yearalbum_release_date_precisiondanceabilityenergykeyloudnessmodespeechinessacousticnessinstrumentalnesslivenessvalencetempotrack_idanalysis_urltime_signaturedisc_numberduration_msexplicittrack_hrefis_localtrack_nametrack_preview_urltrack_numbertypetrack_uriexternal_urls.spotifyalbum_namekey_namemode_namekey_mode1Lizzo56oDRnqbIiwx4mymNEv7dS6dFFcYQ8VhifgdKgYY5LYLalbum2019-04-192,019day0.5660.6601-4.03910.04790.015600.000000.23400.520162.1596YdQgWSpsxhVeX6Xmv3IFJhttps://api.spotify.com/v1/audio-analysis/6YdQgWSpsxhVeX6Xmv3IFJ31179,9781https://api.spotify.com/v1/tracks/6YdQgWSpsxhVeX6Xmv3IFJfalseCuz I Love Youhttps://p.scdn.co/mp3-preview/04472d81bc5a9669b8de9bd000085ac6456abc98?cid=841bc9c803cd42d7ae6230b2f464867f1trackspotify:track:6YdQgWSpsxhVeX6Xmv3IFJhttps://open.spotify.com/track/6YdQgWSpsxhVeX6Xmv3IFJCuz I Love YouC#majorC# major2Lizzo56oDRnqbIiwx4mymNEv7dS6dFFcYQ8VhifgdKgYY5LYLalbum2019-04-192,019day0.6940.8072-3.18310.08880.016700.000000.54700.797144.0315I7sGubUsKo4mVJpBoSVUrhttps://api.spotify.com/v1/audio-analysis/5I7sGubUsKo4mVJpBoSVUr41184,8571https://api.spotify.com/v1/tracks/5I7sGubUsKo4mVJpBoSVUrfalseLike a Girlhttps://p.scdn.co/mp3-preview/4f917112d7f75e2ca776c61b6dc8ab72fb9f8d06?cid=841bc9c803cd42d7ae6230b2f464867f2trackspotify:track:5I7sGubUsKo4mVJpBoSVUrhttps://open.spotify.com/track/5I7sGubUsKo4mVJpBoSVUrCuz I Love YouDmajorD major3Lizzo56oDRnqbIiwx4mymNEv7dS6dFFcYQ8VhifgdKgYY5LYLalbum2019-04-192,019day0.7670.8897-2.98810.09630.006110.000000.40200.843119.9300k664IuFwVP557Gnx7RhIlhttps://api.spotify.com/v1/audio-analysis/0k664IuFwVP557Gnx7RhIl41195,1441https://api.spotify.com/v1/tracks/0k664IuFwVP557Gnx7RhIlfalseJuicehttps://p.scdn.co/mp3-preview/7ca1598cec36c49118562b02491b9a030e233d88?cid=841bc9c803cd42d7ae6230b2f464867f3trackspotify:track:0k664IuFwVP557Gnx7RhIlhttps://open.spotify.com/track/0k664IuFwVP557Gnx7RhIlCuz I Love YouGmajorG major4Lizzo56oDRnqbIiwx4mymNEv7dS6dFFcYQ8VhifgdKgYY5LYLalbum2019-04-192,019day0.6930.8497-4.51710.08920.004660.001270.50300.76799.0216h2wpo2pshM8QnAvRySEO0https://api.spotify.com/v1/audio-analysis/6h2wpo2pshM8QnAvRySEO041175,2341https://api.spotify.com/v1/tracks/6h2wpo2pshM8QnAvRySEO0falseSoulmatehttps://p.scdn.co/mp3-preview/467bc6a734bfb66e77f069ed62c40598c057c083?cid=841bc9c803cd42d7ae6230b2f464867f4trackspotify:track:6h2wpo2pshM8QnAvRySEO0https://open.spotify.com/track/6h2wpo2pshM8QnAvRySEO0Cuz I Love YouGmajorG major5Lizzo56oDRnqbIiwx4mymNEv7dS6dFFcYQ8VhifgdKgYY5LYLalbum2019-04-192,019day0.6740.5428-6.98310.25300.047600.000000.06820.534150.9793kxsEF30mM0TZWfkOv4XsShttps://api.spotify.com/v1/audio-analysis/3kxsEF30mM0TZWfkOv4XsS31231,5700https://api.spotify.com/v1/tracks/3kxsEF30mM0TZWfkOv4XsSfalseJeromehttps://p.scdn.co/mp3-preview/8f662de51ad3f613063baba18babe9c508e84be2?cid=841bc9c803cd42d7ae6230b2f464867f5trackspotify:track:3kxsEF30mM0TZWfkOv4XsShttps://open.spotify.com/track/3kxsEF30mM0TZWfkOv4XsSCuz I Love YouG#majorG# major6Lizzo56oDRnqbIiwx4mymNEv7dS6dFFcYQ8VhifgdKgYY5LYLalbum2019-04-192,019day0.6230.71911-3.33100.04030.038300.000000.24700.722131.0276pRLJSrcskYSKYuKgJtDgDhttps://api.spotify.com/v1/audio-analysis/6pRLJSrcskYSKYuKgJtDgD41175,9520https://api.spotify.com/v1/tracks/6pRLJSrcskYSKYuKgJtDgDfalseCrybabyhttps://p.scdn.co/mp3-preview/d9ef0675e835123cbb253a6a96bc37036306990e?cid=841bc9c803cd42d7ae6230b2f464867f6trackspotify:track:6pRLJSrcskYSKYuKgJtDgDhttps://open.spotify.com/track/6pRLJSrcskYSKYuKgJtDgDCuz I Love YouBminorB minor\n\n\nFit the model:\n\n# --- 1. Define predictors and outcome ---\n\npredictors &lt;- c(\"danceability\", \"energy\", \"instrumentalness\", \"explicit\")\ncontin     &lt;- c(\"danceability\", \"energy\", \"instrumentalness\")\n\n# Make sure outcome is a factor (binary classification)\nd$mode &lt;- as.factor(d$mode)\n\n# --- 2. Train / test split (80/20, seed = 123) ---\nset.seed(123)\ntrain_index &lt;- createDataPartition(d$mode, p = 0.8, list = FALSE) # from caret\n\ntrain &lt;- \ntest  &lt;- \n\n\n# --- 3. Scale only continuous predictors ---\n# fit preprocessing on training data only\npreproc &lt;- preProcess(train[, contin], method = c(\"center\", \"scale\"))\n\n# apply to train and test\ntrain_scaled &lt;- train\ntrain_scaled[, contin] &lt;- predict(preproc, train[, contin])\n\ntest_scaled &lt;- test\ntest_scaled[, contin] &lt;- predict(preproc, test[, contin])\n\n# --- 4. Fit logistic regression model  ---\n\n# formula: mode ~ danceability + energy + instrumentalness + explicit\nmodel &lt;- glm(  \n                \n              )\n\n  \n# --- 5. Predictions: class and probabilities ---\n\n# Training set\ntrain_prob &lt;- predict(model, newdata = train_scaled, type = \"response\")\n# glm with factor response: prob is for the *second* level of mode\n# assume \"1\" is the positive class:\ntrain_pred &lt;- ifelse(train_prob &gt; 0.5, \"1\", \"0\")\ntrain_pred &lt;- factor(train_pred, levels = levels(train$mode))\n\n# Test set\ntest_prob &lt;- predict(model, newdata = test_scaled, type = \"response\")\ntest_pred &lt;- ifelse(test_prob &gt; 0.5, \"1\", \"0\")\ntest_pred &lt;- factor(test_pred, levels = levels(test$mode))\n\n  \n# --- 6. Metrics: accuracy, precision, recall, F1, ROC AUC ---\n\n# helper to compute metrics\nmetrics_fun &lt;- function(y_true, y_pred, y_prob) {\n  cm &lt;- table(truth = y_true, pred = y_pred)\n  \n  TP &lt;- cm[\"1\", \"1\"]\n  TN &lt;- cm[\"0\", \"0\"]\n  FP &lt;- cm[\"0\", \"1\"]\n  FN &lt;- cm[\"1\", \"0\"]\n  \n  acc  &lt;- (TP + TN) / sum(cm)\n  prec &lt;- TP / (TP + FP)\n  rec  &lt;- TP / (TP + FN)\n  f1   &lt;- 2 * prec * rec / (prec + rec)\n  \n  # ROC AUC\n  roc_obj &lt;- roc(response = y_true, predictor = y_prob, levels = c(\"0\", \"1\"))\n  auc_val &lt;- auc(roc_obj)\n  \n  c(Accuracy = acc,\n    Precision = prec,\n    Recall = rec,\n    F1 = f1,\n    ROC_AUC = as.numeric(auc_val))\n}\n\n# Train metrics\ntrain_metrics &lt;- \ntest_metrics  &lt;- \n\nVisualize a confusion matrix:\n\n# Build confusion matrix table to visualize\ncm &lt;- table(Predicted = train_pred, Actual = train$mode)\n\nggplot(cm_df, aes(x = Predicted, y = Actual, fill = Freq)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = Freq), size = 6) +\n  scale_fill_gradient(low = \"white\", high = \"steelblue\") +\n  labs(title = \"Confusion Matrix (Train Set)\", fill = \"Count\") +\n  theme_minimal(base_size = 14)\n\nor create a direct table:\n\n# table\ntable(Actual = train$mode, Predicted = train_pred)\n\nFinally plot a ROC curve:\n\nroc_train &lt;- roc(response = train$mode,\n                 predictor = train_prob,\n                 levels   = c(\"0\", \"1\"))   # \"0\" = negative, \"1\" = positive\n\n# Basic ROC plot with AUC\nplot(roc_train, print.auc = TRUE, main = \"ROC Curve (Train)\")\n\n\n3.1.1 Question\nGrab and diplay the coefficients fromt he above model. How do you interpret them?"
  },
  {
    "objectID": "teaching/stat-learn/material/11/11-neural-nets.html",
    "href": "teaching/stat-learn/material/11/11-neural-nets.html",
    "title": "Neural Networks",
    "section": "",
    "text": "Neural Networks are great. Their flexibility (layers…connections…activation functions…and more!) allows you to build complex models that can accurately model complex relationships between predictors and outcomes. But I want to caution you: Neural Networks aren’t magic. I often see people using them unnecessarily, just because they sound cool. If you’re going to use NN’s, make sure they’re the right tool for your problem.\nWhen building a neural network you need to think about 2 (main) things:\n\nThe Structure of the model (nodes/connections/activation functions)\nThe Loss Function (how do we measure how well our model is doing?)\n\n\n\nWhen working with neural networks in R, you may encounter two packages: keras and keras3. The older keras package is an R interface to the TensorFlow-specific version of Keras (often called Keras 2), which means it only works with TensorFlow. The newer keras3 package connects R to Keras 3, the modern version of Keras, which is designed to work with multiple backends (such as TensorFlow, JAX, or PyTorch). Because Keras 3 represents the current and future direction of the framework, keras3 is the recommended choice for new neural network work.\nWhen learning neural networks, you should generally use keras3 with the TensorFlow backend. This setup is actively maintained, aligns with up-to-date tutorials, and is well supported in both R and Python ecosystems. You should only use the older keras package if you are working with legacy code that specifically depends on TensorFlow-only Keras. To get started, first install the R package and then install a backend. If you are new to neural networks, TensorFlow is the recommended backend:\n\ninstall.packages(\"keras3\")\nkeras3::install_keras()\n\nThis command automatically sets up a compatible Python environment and installs TensorFlow for you. After completing these steps, you can immediately begin building and training neural networks in R using keras3.\n\n\n\nThe code below loads a music dataset, selects four audio features to predict valence, and standardizes the inputs so they are on the same scale. It then builds and trains a simple neural network with one linear output node, equivalent in form to a linear regression model, using mean squared error and stochastic gradient descent over five training epochs.\n\nlibrary(tidyverse)  \n\n# Read the data\ndf &lt;- read.csv(\"11-data/Music_data.csv\")\n\n# Define feature columns and target\nfeats &lt;- c(\"danceability\", \"energy\", \"loudness\", \"acousticness\")\npredict &lt;- \"valence\"\n\n# Print the shape of the data frame (rows, columns)\nprint(dim(df))\n\n[1] 2553   14\n\n# Select features and target\nX &lt;- df[, feats]\ny &lt;- df[, predict]\n\n# Standardize the features (mean = 0, sd = 1)\nX &lt;- scale(X)\n\nThe model below has the same shape as a simple linear regression, like we talked about in lecture. It has an input layer with 4 inputs (“danceability”, “energy”, “loudness”,“acousticness”), and 1 output layer for “valence”.\nWe will use the package keras3.\n\nlibrary(keras3)\n\n# structure of the model\nnn_model &lt;- keras_model_sequential() %&gt;%\n  layer_dense(\n    units = 1,\n    input_shape = c(4)   # same as input_shape=[4]\n  )\n\n# how to train the model\nnn_model %&gt;% compile(\n  loss = \"mean_squared_error\",\n  optimizer = optimizer_sgd()\n)\n\n# fit the model (same idea as sklearn / Python Keras)\nnn_model %&gt;% fit(\n  x = X,\n  y = y,\n  epochs = 5\n)\n\nEpoch 1/5\n80/80 - 0s - 2ms/step - loss: 0.6473\nEpoch 2/5\n80/80 - 0s - 412us/step - loss: 0.1129\nEpoch 3/5\n80/80 - 0s - 406us/step - loss: 0.0529\nEpoch 4/5\n80/80 - 0s - 400us/step - loss: 0.0397\nEpoch 5/5\n80/80 - 0s - 406us/step - loss: 0.0363\n\n\nWe next fit linear regression model is using the selected features to predict the target variable. The model estimates the relationship between each predictor and the response by finding the coefficients that minimize the sum of squared errors, using the entire dataset without any validation or train–test split.\n\n# Convert to data frame for lm()\ndf_lm &lt;- data.frame(X, y = y)\n\n# Build and fit the linear regression model\nmodel &lt;- lm(y ~ ., data = df_lm)\n\n# View model summary\nsummary(model)\n\n\nCall:\nlm(formula = y ~ ., data = df_lm)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55286 -0.13372 -0.00401  0.13306  0.53369 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.471304   0.003703 127.288  &lt; 2e-16 ***\ndanceability  0.108905   0.003828  28.446  &lt; 2e-16 ***\nenergy        0.097514   0.006552  14.883  &lt; 2e-16 ***\nloudness     -0.034970   0.005981  -5.847 5.66e-09 ***\nacousticness  0.035117   0.005127   6.850 9.23e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1871 on 2548 degrees of freedom\nMultiple R-squared:  0.2829,    Adjusted R-squared:  0.2818 \nF-statistic: 251.3 on 4 and 2548 DF,  p-value: &lt; 2.2e-16\n\n\nNow we extract the coefficients and intercept from the linear model:\n\n# get coefficients (including intercept)\ncoef(model)\n\n (Intercept) danceability       energy     loudness acousticness \n  0.47130376   0.10890452   0.09751431  -0.03497002   0.03511732 \n\n\nWe get the weights from the neural net, which for this model with one dense layer will be a list containing:\n\nA weight matrix (coefficients for each input feature)\nA bias term\n\n\n# get weights from Neural Network\nweights &lt;- get_weights(nn_model) \n\nweights\n\n[[1]]\n            [,1]\n[1,]  0.10631825\n[2,]  0.11201244\n[3,] -0.06878705\n[4,]  0.01332233\n\n[[2]]\n[1] 0.473378\n\n\nThe output lists the intercept and feature coefficients of the linear regression model, which directly correspond to the learned weights indicating how strongly and in what direction each input feature influences the predicted target value.\n\nWhat happens to the weights from our neural net as you increase the number of epochs (compare to the coefs from the linear regression model)?\n\n\n# ----- Linear regression coefficients -----\nlm_model &lt;- lm(y ~ ., data = data.frame(X, y))\nlm_coefs &lt;- coef(lm_model)[-1]   # exclude intercept\nlm_intercept &lt;- coef(lm_model)[1]\n\n# ----- Train neural nets with increasing epochs -----\nepoch_list &lt;- c(1, 5, 20, 100, 200)\n\nnn_weights &lt;- lapply(epoch_list, function(e) {\n  \n  nn_model &lt;- keras_model_sequential() %&gt;%\n    layer_dense(units = 1, input_shape = c(ncol(X)))\n  \n  nn_model %&gt;% compile(\n    loss = \"mean_squared_error\",\n    optimizer = optimizer_sgd()\n  )\n  \n  nn_model %&gt;% fit(\n    X, y,\n    epochs = e,\n    verbose = 0\n  )\n  \n  # extract weights (matrix) and bias\n  w &lt;- get_weights(nn_model)\n  list(\n    epochs = e,\n    weights = as.vector(w[[1]]),\n    bias = w[[2]]\n  )\n})\n\n# ----- View results -----\nnn_weights\n\n[[1]]\n[[1]]$epochs\n[1] 1\n\n[[1]]$weights\n[1] -0.116555884 -0.009368724  0.182410553  0.070211098\n\n[[1]]$bias\n[1] 0.3754594\n\n\n[[2]]\n[[2]]$epochs\n[1] 5\n\n[[2]]$weights\n[1]  0.11654302  0.17983818 -0.12111755  0.02648513\n\n[[2]]$bias\n[1] 0.472273\n\n\n[[3]]\n[[3]]$epochs\n[1] 20\n\n[[3]]$weights\n[1]  0.10762285  0.09810586 -0.03503770  0.03266637\n\n[[3]]$bias\n[1] 0.4706939\n\n\n[[4]]\n[[4]]$epochs\n[1] 100\n\n[[4]]$weights\n[1]  0.10737380  0.09639432 -0.03592658  0.03644631\n\n[[4]]$bias\n[1] 0.4691057\n\n\n[[5]]\n[[5]]$epochs\n[1] 200\n\n[[5]]$weights\n[1]  0.10761727  0.09660057 -0.03519304  0.03625473\n\n[[5]]$bias\n[1] 0.4701196\n\nlm_coefs\n\ndanceability       energy     loudness acousticness \n  0.10890452   0.09751431  -0.03497002   0.03511732 \n\n\nNote the following:\n\nAs epochs increase, the neural network weights stabilize and converge\nWith a single dense layer and MSE loss, the neural net is effectively learning a linear regression\nThe final neural network weights become very close to the linear regression coefficients\nDifferences at low epochs are due to incomplete optimization\n\n\n\n\nRemember that a densely connected layer is connected to EVERY node in the layer before and after it. The parameters can add up QUICKLY.\n\nWhat do you think can happen when you have a ton of parameters and only a little data?\n\nWhen you have many parameters but very little data, the model is likely to overfit, meaning it learns noise and random fluctuations instead of the true underlying pattern, resulting in poor performance on new, unseen data."
  },
  {
    "objectID": "teaching/stat-learn/material/11/11-neural-nets.html#installing-necessary-packages",
    "href": "teaching/stat-learn/material/11/11-neural-nets.html#installing-necessary-packages",
    "title": "Neural Networks",
    "section": "",
    "text": "When working with neural networks in R, you may encounter two packages: keras and keras3. The older keras package is an R interface to the TensorFlow-specific version of Keras (often called Keras 2), which means it only works with TensorFlow. The newer keras3 package connects R to Keras 3, the modern version of Keras, which is designed to work with multiple backends (such as TensorFlow, JAX, or PyTorch). Because Keras 3 represents the current and future direction of the framework, keras3 is the recommended choice for new neural network work.\nWhen learning neural networks, you should generally use keras3 with the TensorFlow backend. This setup is actively maintained, aligns with up-to-date tutorials, and is well supported in both R and Python ecosystems. You should only use the older keras package if you are working with legacy code that specifically depends on TensorFlow-only Keras. To get started, first install the R package and then install a backend. If you are new to neural networks, TensorFlow is the recommended backend:\n\ninstall.packages(\"keras3\")\nkeras3::install_keras()\n\nThis command automatically sets up a compatible Python environment and installs TensorFlow for you. After completing these steps, you can immediately begin building and training neural networks in R using keras3."
  },
  {
    "objectID": "teaching/stat-learn/material/11/11-neural-nets.html#simple-nn",
    "href": "teaching/stat-learn/material/11/11-neural-nets.html#simple-nn",
    "title": "Neural Networks",
    "section": "",
    "text": "The code below loads a music dataset, selects four audio features to predict valence, and standardizes the inputs so they are on the same scale. It then builds and trains a simple neural network with one linear output node, equivalent in form to a linear regression model, using mean squared error and stochastic gradient descent over five training epochs.\n\nlibrary(tidyverse)  \n\n# Read the data\ndf &lt;- read.csv(\"11-data/Music_data.csv\")\n\n# Define feature columns and target\nfeats &lt;- c(\"danceability\", \"energy\", \"loudness\", \"acousticness\")\npredict &lt;- \"valence\"\n\n# Print the shape of the data frame (rows, columns)\nprint(dim(df))\n\n[1] 2553   14\n\n# Select features and target\nX &lt;- df[, feats]\ny &lt;- df[, predict]\n\n# Standardize the features (mean = 0, sd = 1)\nX &lt;- scale(X)\n\nThe model below has the same shape as a simple linear regression, like we talked about in lecture. It has an input layer with 4 inputs (“danceability”, “energy”, “loudness”,“acousticness”), and 1 output layer for “valence”.\nWe will use the package keras3.\n\nlibrary(keras3)\n\n# structure of the model\nnn_model &lt;- keras_model_sequential() %&gt;%\n  layer_dense(\n    units = 1,\n    input_shape = c(4)   # same as input_shape=[4]\n  )\n\n# how to train the model\nnn_model %&gt;% compile(\n  loss = \"mean_squared_error\",\n  optimizer = optimizer_sgd()\n)\n\n# fit the model (same idea as sklearn / Python Keras)\nnn_model %&gt;% fit(\n  x = X,\n  y = y,\n  epochs = 5\n)\n\nEpoch 1/5\n80/80 - 0s - 2ms/step - loss: 0.6473\nEpoch 2/5\n80/80 - 0s - 412us/step - loss: 0.1129\nEpoch 3/5\n80/80 - 0s - 406us/step - loss: 0.0529\nEpoch 4/5\n80/80 - 0s - 400us/step - loss: 0.0397\nEpoch 5/5\n80/80 - 0s - 406us/step - loss: 0.0363\n\n\nWe next fit linear regression model is using the selected features to predict the target variable. The model estimates the relationship between each predictor and the response by finding the coefficients that minimize the sum of squared errors, using the entire dataset without any validation or train–test split.\n\n# Convert to data frame for lm()\ndf_lm &lt;- data.frame(X, y = y)\n\n# Build and fit the linear regression model\nmodel &lt;- lm(y ~ ., data = df_lm)\n\n# View model summary\nsummary(model)\n\n\nCall:\nlm(formula = y ~ ., data = df_lm)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55286 -0.13372 -0.00401  0.13306  0.53369 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.471304   0.003703 127.288  &lt; 2e-16 ***\ndanceability  0.108905   0.003828  28.446  &lt; 2e-16 ***\nenergy        0.097514   0.006552  14.883  &lt; 2e-16 ***\nloudness     -0.034970   0.005981  -5.847 5.66e-09 ***\nacousticness  0.035117   0.005127   6.850 9.23e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1871 on 2548 degrees of freedom\nMultiple R-squared:  0.2829,    Adjusted R-squared:  0.2818 \nF-statistic: 251.3 on 4 and 2548 DF,  p-value: &lt; 2.2e-16\n\n\nNow we extract the coefficients and intercept from the linear model:\n\n# get coefficients (including intercept)\ncoef(model)\n\n (Intercept) danceability       energy     loudness acousticness \n  0.47130376   0.10890452   0.09751431  -0.03497002   0.03511732 \n\n\nWe get the weights from the neural net, which for this model with one dense layer will be a list containing:\n\nA weight matrix (coefficients for each input feature)\nA bias term\n\n\n# get weights from Neural Network\nweights &lt;- get_weights(nn_model) \n\nweights\n\n[[1]]\n            [,1]\n[1,]  0.10631825\n[2,]  0.11201244\n[3,] -0.06878705\n[4,]  0.01332233\n\n[[2]]\n[1] 0.473378\n\n\nThe output lists the intercept and feature coefficients of the linear regression model, which directly correspond to the learned weights indicating how strongly and in what direction each input feature influences the predicted target value.\n\nWhat happens to the weights from our neural net as you increase the number of epochs (compare to the coefs from the linear regression model)?\n\n\n# ----- Linear regression coefficients -----\nlm_model &lt;- lm(y ~ ., data = data.frame(X, y))\nlm_coefs &lt;- coef(lm_model)[-1]   # exclude intercept\nlm_intercept &lt;- coef(lm_model)[1]\n\n# ----- Train neural nets with increasing epochs -----\nepoch_list &lt;- c(1, 5, 20, 100, 200)\n\nnn_weights &lt;- lapply(epoch_list, function(e) {\n  \n  nn_model &lt;- keras_model_sequential() %&gt;%\n    layer_dense(units = 1, input_shape = c(ncol(X)))\n  \n  nn_model %&gt;% compile(\n    loss = \"mean_squared_error\",\n    optimizer = optimizer_sgd()\n  )\n  \n  nn_model %&gt;% fit(\n    X, y,\n    epochs = e,\n    verbose = 0\n  )\n  \n  # extract weights (matrix) and bias\n  w &lt;- get_weights(nn_model)\n  list(\n    epochs = e,\n    weights = as.vector(w[[1]]),\n    bias = w[[2]]\n  )\n})\n\n# ----- View results -----\nnn_weights\n\n[[1]]\n[[1]]$epochs\n[1] 1\n\n[[1]]$weights\n[1] -0.116555884 -0.009368724  0.182410553  0.070211098\n\n[[1]]$bias\n[1] 0.3754594\n\n\n[[2]]\n[[2]]$epochs\n[1] 5\n\n[[2]]$weights\n[1]  0.11654302  0.17983818 -0.12111755  0.02648513\n\n[[2]]$bias\n[1] 0.472273\n\n\n[[3]]\n[[3]]$epochs\n[1] 20\n\n[[3]]$weights\n[1]  0.10762285  0.09810586 -0.03503770  0.03266637\n\n[[3]]$bias\n[1] 0.4706939\n\n\n[[4]]\n[[4]]$epochs\n[1] 100\n\n[[4]]$weights\n[1]  0.10737380  0.09639432 -0.03592658  0.03644631\n\n[[4]]$bias\n[1] 0.4691057\n\n\n[[5]]\n[[5]]$epochs\n[1] 200\n\n[[5]]$weights\n[1]  0.10761727  0.09660057 -0.03519304  0.03625473\n\n[[5]]$bias\n[1] 0.4701196\n\nlm_coefs\n\ndanceability       energy     loudness acousticness \n  0.10890452   0.09751431  -0.03497002   0.03511732 \n\n\nNote the following:\n\nAs epochs increase, the neural network weights stabilize and converge\nWith a single dense layer and MSE loss, the neural net is effectively learning a linear regression\nThe final neural network weights become very close to the linear regression coefficients\nDifferences at low epochs are due to incomplete optimization"
  },
  {
    "objectID": "teaching/stat-learn/material/11/11-neural-nets.html#parameter-bloat",
    "href": "teaching/stat-learn/material/11/11-neural-nets.html#parameter-bloat",
    "title": "Neural Networks",
    "section": "",
    "text": "Remember that a densely connected layer is connected to EVERY node in the layer before and after it. The parameters can add up QUICKLY.\n\nWhat do you think can happen when you have a ton of parameters and only a little data?\n\nWhen you have many parameters but very little data, the model is likely to overfit, meaning it learns noise and random fluctuations instead of the true underlying pattern, resulting in poor performance on new, unseen data."
  },
  {
    "objectID": "teaching/stat-learn/material/11/11-neural-nets.html#backpropagation",
    "href": "teaching/stat-learn/material/11/11-neural-nets.html#backpropagation",
    "title": "Neural Networks",
    "section": "2.1 Backpropagation",
    "text": "2.1 Backpropagation\nOn the first run (or forward pass), the DNN will select a batch of observations, randomly assign weights across all the node connections, and predict the output. The engine of neural networks is how it assesses its own accuracy and automatically adjusts the weights across all the node connections to improve that accuracy. This process is called backpropagation. To perform backpropagation we need two things:\n\nAn objective function;\nAn optimizer.\n\nFirst, you need to establish an objective (loss) function to measure performance. For regression problems this might be mean squared error (MSE) and for classification problems it is commonly binary and multi-categorical cross entropy. DNNs can have multiple loss functions but we’ll just focus on using one.\nOn each forward pass the DNN will measure its performance based on the loss function chosen. The DNN will then work backwards through the layers, compute the gradient of the loss with regards to the network weights, adjust the weights a little in the opposite direction of the gradient, grab another batch of observations to run through the model,… rinse and repeat until the loss function is minimized. This process is known as mini-batch stochastic gradient descent (mini-batch SGD). There are several variants of mini-batch SGD algorithms; they primarily differ in how fast they descend the gradient (controlled by the learning rate). These different variations make up the different optimizers that can be used.\nTo incorporate the backpropagation piece of our DNN we include compile() in our code sequence. In addition to the optimizer and loss function arguments, we can also identify one or more metrics in addition to our loss function to track and report.\n\nmodel &lt;- keras_model_sequential() %&gt;%\n  \n  # Network architecture\n  layer_dense(units = 128, activation = \"relu\", input_shape = ncol(mnist_x)) %&gt;%\n  layer_dense(units = 64, activation = \"relu\") %&gt;%\n  layer_dense(units = 10, activation = \"softmax\") %&gt;%\n  \n  # Backpropagation\n  compile(\n    loss = 'categorical_crossentropy',\n    optimizer = optimizer_rmsprop(),\n    metrics = c('accuracy')\n  )\n\nWe’ve created a base model, now we just need to train it with some data. To do so we feed our model into a fit() function along with our training data. We also provide a few other arguments that are worth mentioning:\n\nbatch_size: As we mentioned in the last section, the DNN will take a batch of data to run through the mini-batch SGD process. Batch sizes can be between one and several hundred. Small values will be more computationally burdensome while large values provide less feedback signal. Values are typically provided as a power of two that fit nicely into the memory requirements of the GPU or CPU hardware like 32, 64, 128, 256, and so on.\nepochs: An epoch describes the number of times the algorithm sees the entire data set. So, each time the algorithm has seen all samples in the data set, an epoch has completed. In our training set, we have 60,000 observations so running batches of 128 will require 469 passes for one epoch. The more complex the features and relationships in your data, the more epochs you’ll require for your model to learn, adjust the weights, and minimize the loss function.\nvalidation_split: The model will hold out XX% of the data so that we can compute a more accurate estimate of an out-of-sample error rate.\nverbose: We set this to FALSE for brevity; however, when TRUE you will see a live update of the loss function in your RStudio IDE.\n\nPlotting the output shows how our loss function (and specified metrics) improve for each epoch. We see that our model’s performance is optimized at 5–10 epochs and then proceeds to overfit, which results in a flatlined accuracy rate.\n\n# Train the model\nfit1 &lt;- model %&gt;%\n  fit(\n    x = mnist_x,\n    y = mnist_y,\n    epochs = 25,\n    batch_size = 128,\n    validation_split = 0.2,\n    verbose = FALSE\n  )\n\n# Display output\nfit1\n\n\nFinal epoch (plot to see history):\n    accuracy: 0.9996\n        loss: 0.002017\nval_accuracy: 0.9767\n    val_loss: 0.1442 \n\nplot(fit1)\n\n\n\n\n\n\n\n\nThis plot shows that as training progresses, training accuracy continues to increase and training loss keeps decreasing, while validation accuracy plateaus and validation loss begins to rise, indicating that the model starts to overfit after a certain number of epochs; learning the training data very well but no longer improving (and even worsening) its performance on unseen data."
  },
  {
    "objectID": "teaching/stat-learn/material/11/11-neural-nets.html#model-tuning",
    "href": "teaching/stat-learn/material/11/11-neural-nets.html#model-tuning",
    "title": "Neural Networks",
    "section": "2.2 Model Tuning",
    "text": "2.2 Model Tuning\nNow that we have an understanding of producing and running a DNN model, the next task is to find an optimal one by tuning different hyperparameters. There are many ways to tune a DNN. Typically, the tuning process follows these general steps; however, there is often a lot of iteration among these:\n\nAdjust model capacity (layers & nodes);\nAdd batch normalization;\nAdd regularization;\nAdjust learning rate.\n\n\n2.2.1 Model Capacity\nWe aim to maximize predictive performance while keeping model capacity as low as possible, since higher capacity allows a model to learn more patterns but also increases the risk of overfitting. Therefore, we focus on improving validation performance rather than training performance, and compare multiple model capacity settings with different numbers of layers and nodes while keeping all other parameters fixed.\n\n2.2.1.1 Exercise\nThe table below summarizes different model capacities you should evaluate, defined by the number of hidden layers and nodes per layer.\n\nModel capacities assessed, represented by the number of hidden layers and nodes per layer.\n\n\nSize\n1 Hidden Layer\n2 Hidden Layers\n3 Hidden Layers\n\n\n\n\nSmall\n16\n16, 8\n16, 8, 4\n\n\nMedium\n64\n64, 32\n64, 32, 16\n\n\nLarge\n256\n256, 128\n256, 128, 64\n\n\n\n\n# -------------------------------------------------\n# Table variants to run (size x # hidden layers)\n# -------------------------------------------------\nvariants &lt;- tribble(\n  ~size,   ~layers, ~units,\n  \"small\",  1,      c(16),\n  \"small\",  2,      c(16, 8),\n  \"small\",  3,      c(16, 8, 4),\n  \"medium\", 1,      c(64),\n  \"medium\", 2,      c(64, 32),\n  \"medium\", 3,      c(64, 32, 16),\n  \"large\",  1,      c(256),\n  \"large\",  2,      c(256, 128),\n  \"large\",  3,      c(256, 128, 64)\n) %&gt;%\n  mutate(layers_lab = paste(layers, \"layer\"))\n\n\n### Continue code here.....\n\n\n\n\n\n\n\nNote\n\n\n\nIf your models do not reach a stable (flatlined) validation error, increase the number of training epochs. Conversely, if validation error stabilizes early, continuing to train wastes computational resources without improving performance. To address this, you can use callbacks within fit() to automate training decisions. One commonly used callback is early stopping, which halts training when the loss function fails to improve for a specified number of epochs.\n\n\n\n\n\n2.2.2 Batch Normalization\nAlthough we normalized the input data before feeding it into the model, normalization remains important throughout the entire network, not just at the input stage. As data passes through each layer, its distribution can change during training. Batch normalization addresses this issue by adaptively normalizing layer outputs as their mean and variance shift over time. The primary benefit of batch normalization is improved gradient propagation, which makes training deeper neural networks more stable and efficient. As a result, the deeper your network becomes, the more important batch normalization is, and it can lead to better overall performance.\n\nmodel_w_norm &lt;- keras_model_sequential() %&gt;%\n  \n  # Network architecture with batch normalization\n  layer_dense(units = 256, activation = \"relu\", input_shape = ncol(mnist_x)) %&gt;%\n  layer_batch_normalization() %&gt;%\n  layer_dense(units = 128, activation = \"relu\") %&gt;%\n  layer_batch_normalization() %&gt;%\n  layer_dense(units = 64, activation = \"relu\") %&gt;%\n  layer_batch_normalization() %&gt;%\n  layer_dense(units = 10, activation = \"softmax\") %&gt;%\n\n  # Backpropagation\n  compile(\n    loss = \"categorical_crossentropy\",\n    optimizer = optimizer_rmsprop(),\n    metrics = c(\"accuracy\")\n  )\n\nNow try to add batch normalization to each of the previously assessed models, you should see a couple patterns emerge. One, batch normalization often helps to minimize the validation loss sooner, which increases efficiency of model training. Two, we see that for the larger, more complex models (3-layer medium and 2- and 3-layer large), batch normalization helps to reduce the overall amount of overfitting.\n\n\n2.2.3 Regularization\nPlacing constraints on a model’s complexity through regularization is a common way to reduce overfitting, and deep neural networks are no exception. Two widely used regularization approaches are the (L_1) and (L_2) penalties, which add a cost based on the magnitude of the model’s weights. In practice, the (L_2) norm, often referred to as weight decay in neural networks, is the most commonly used. Weight regularization encourages small, noisy signals to have weights close to zero while allowing consistently strong signals to retain larger weights.\n\n\n\n\n\n\nNote\n\n\n\nAs the number of layers and nodes increases, regularization using (L_1) or (L_2) penalties tends to have a greater impact on model performance. Because large models are more prone to overparameterization, these penalties help shrink unnecessary weights toward zero, reducing the risk of overfitting.\n\n\nWe can apply (L_1), (L_2), or a combination of both penalties by adding regularizer_XX() to each layer.\n\n# L2 (weight decay) regularizer\nl2_reg &lt;- regularizer_l2(0.001)\n\nmodel_w_reg &lt;- keras_model_sequential()  %&gt;% \n  layer_dense(\n    units = 256,\n    activation = \"relu\",\n    input_shape = c(ncol(mnist_x)),\n    kernel_regularizer = l2_reg\n  )  %&gt;% \n  layer_batch_normalization()  %&gt;% \n  layer_dense(\n    units = 128,\n    activation = \"relu\",\n    kernel_regularizer = l2_reg\n  )  %&gt;% \n  layer_batch_normalization()  %&gt;% \n  layer_dense(\n    units = 64,\n    activation = \"relu\",\n    kernel_regularizer = l2_reg\n  )  %&gt;% \n  layer_batch_normalization()  %&gt;% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel_w_reg  %&gt;% \n  compile(\n    optimizer = optimizer_rmsprop(),\n    loss = \"categorical_crossentropy\",\n    metrics = \"accuracy\"\n  )\n\n# Fit (same settings you used)\nfit2 &lt;- model_w_reg  %&gt;% \n  fit(\n    x = mnist_x,\n    y = mnist_y,\n    epochs = 25,\n    batch_size = 128,\n    validation_split = 0.2,\n    verbose = 0\n  )\n\nfit2\n\n\nFinal epoch (plot to see history):\n    accuracy: 0.985\n        loss: 0.1021\nval_accuracy: 0.975\n    val_loss: 0.1495 \n\nplot(fit2)\n\n\n\n\n\n\n\n\nCompared to the model before regularization, this figure shows that regularization reduces overfitting but slightly limits peak performance.\nIn the unregularized model, training accuracy quickly approaches 100% and training loss goes to near zero, while validation loss begins to increase after several epochs, clear evidence that the model is memorizing the training data. After adding regularization, training improves more gradually and does not reach the same extreme levels, but validation loss stabilizes instead of rising and validation accuracy remains more consistent. This indicates that regularization constrains the model’s complexity, preventing it from fitting noise and leading to better generalization to unseen data.\nDropout is another widely used regularization technique for reducing overfitting in neural networks. During training, dropout randomly sets a proportion of a layer’s output units to zero, which prevents the model from relying too heavily on any single feature or accidental patterns in the data. Typical dropout rates range from 0.2 to 0.5, though the optimal value depends on the dataset and must be tuned. Dropout is applied between layers using layer_dropout()."
  },
  {
    "objectID": "teaching/stat-learn/material/11/11-neural-nets.html#adjust-learning-rate",
    "href": "teaching/stat-learn/material/11/11-neural-nets.html#adjust-learning-rate",
    "title": "Neural Networks",
    "section": "2.3 Adjust learning rate",
    "text": "2.3 Adjust learning rate\nAnother important consideration is whether the optimization process converges to a global minimum or becomes trapped in a local minimum. Mini-batch stochastic gradient descent updates the model by taking small steps along the loss gradient, and the learning rate controls the size of these steps. If the learning rate is poorly chosen, the optimizer may stall in a local minimum rather than progressing toward the global minimum.\nThere are two main ways to address this issue. First, different optimizers (such as RMSProp, Adam, and Adagrad) use distinct strategies for adapting the learning rate, so we can either switch optimizers or manually tune the learning rate for a given optimizer. Second, the learning rate can be reduced automatically, often by a factor of 2 to 10, once the validation loss stops improving. Building on an optimal model, we switch to the Adam optimizer and decrease the learning rate by a factor of 0.05 as loss improvements begin to stall, while also incorporating early stopping to avoid unnecessary training time.\n\nmodel_w_adj_lrn &lt;- keras_model_sequential() %&gt;%\n  layer_dense(units = 256, activation = \"relu\", input_shape = ncol(mnist_x)) %&gt;%\n  layer_batch_normalization() %&gt;%\n  layer_dropout(rate = 0.4) %&gt;%\n  layer_dense(units = 128, activation = \"relu\") %&gt;%\n  layer_batch_normalization() %&gt;%\n  layer_dropout(rate = 0.3) %&gt;%\n  layer_dense(units = 64, activation = \"relu\") %&gt;%\n  layer_batch_normalization() %&gt;%\n  layer_dropout(rate = 0.2) %&gt;%\n  layer_dense(units = 10, activation = \"softmax\") %&gt;%\n  compile(\n    loss = 'categorical_crossentropy',\n    optimizer = optimizer_adam(),\n    metrics = c('accuracy')\n  ) %&gt;%\n  fit(\n    x = mnist_x,\n    y = mnist_y,\n    epochs = 35,\n    batch_size = 128,\n    validation_split = 0.2,\n    callbacks = list(\n      callback_early_stopping(patience = 5),\n      callback_reduce_lr_on_plateau(factor = 0.05)\n      ),\n    verbose = FALSE\n  )\n\nmodel_w_adj_lrn\n\n\nFinal epoch (plot to see history):\n     accuracy: 0.9828\n         loss: 0.05595\n val_accuracy: 0.9795\n     val_loss: 0.06854\nlearning_rate: 0.001 \n\n# Optimal\nmin(model_w_adj_lrn$metrics$val_loss)\n\n[1] 0.06786273\n\nmax(model_w_adj_lrn$metrics$val_acc)\n\n[1] 0.98025\n\n# Learning rate\nplot(model_w_adj_lrn)\n\n\n\n\n\n\n\n\nThis plot shows training behavior that is consistent with a well-regularized model using an adaptive optimizer and early stopping. The training loss decreases rapidly and then levels off, while the validation loss follows a similar trajectory and stabilizes without increasing, indicating that the model stops training before overfitting occurs. Training and validation accuracy increase together and converge to nearly the same value, suggesting strong generalization and minimal performance gap between the two. The learning rate remains constant throughout training, implying that learning-rate reduction was either not triggered or stabilized early, which is consistent with steady improvement in validation loss. Overall, the model training appears stable, efficient, and well controlled.\nOverall, we observe a modest improvement in performance, and the loss curve shows that training is halted at the point where overfitting begins to emerge."
  },
  {
    "objectID": "teaching/stat-learn/material/11/11-neural-nets.html#hyperparameter",
    "href": "teaching/stat-learn/material/11/11-neural-nets.html#hyperparameter",
    "title": "Neural Networks",
    "section": "2.4 Hyperparameter",
    "text": "2.4 Hyperparameter\nTuning Hyperparameter tuning for deep neural networks is often more involved than for other machine learning models because of the large number of hyperparameters and the dependencies between them. In practice, this requires deciding in advance on aspects such as the number of hidden layers and then defining a search grid over relevant parameters (e.g., number of units, learning rate, regularization strength). This process is similar in spirit to grid searches used for other models, but typically requires more manual coordination.\nIn the following example, we demonstrate a grid search by defining a set of hyperparameter combinations and iteratively training models on the MNIST dataset, recording performance metrics for comparison.\n\nThis run takes a very long time so by default eval:false and you need to change it when you want to perform the grid search.\n\n\n# -----------------------------\n# Hyperparameter grid\n# -----------------------------\ngrid &lt;- crossing(\n  nodes1 = c(128, 256),\n  nodes2 = c(64, 128),\n  nodes3 = c(32, 64),\n  dropout1 = c(0.3, 0.4),\n  dropout2 = c(0.2, 0.3),\n  dropout3 = c(0.1, 0.2),\n  lr_annealing = c(0.1, 0.5)\n)\n\n# -----------------------------\n# Model training function\n# -----------------------------\ntrain_model &lt;- function(nodes1, nodes2, nodes3,\n                        dropout1, dropout2, dropout3,\n                        lr_annealing) {\n\n  model &lt;- keras_model_sequential()  %&gt;% \n    layer_dense(nodes1, activation = \"relu\",\n                input_shape = c(ncol(mnist_x)))  %&gt;% \n    layer_batch_normalization()  %&gt;% \n    layer_dropout(dropout1)  %&gt;% \n    layer_dense(nodes2, activation = \"relu\")  %&gt;% \n    layer_batch_normalization()  %&gt;% \n    layer_dropout(dropout2)  %&gt;% \n    layer_dense(nodes3, activation = \"relu\")  %&gt;% \n    layer_batch_normalization()  %&gt;% \n    layer_dropout(dropout3)  %&gt;% \n    layer_dense(10, activation = \"softmax\")\n\n  model  %&gt;% \n    compile(\n      optimizer = optimizer_rmsprop(),\n      loss = \"categorical_crossentropy\",\n      metrics = \"accuracy\"\n    )\n\n  history &lt;- model  %&gt;% \n    fit(\n      mnist_x,\n      mnist_y,\n      epochs = 35,\n      batch_size = 128,\n      validation_split = 0.2,\n      callbacks = list(\n        callback_early_stopping(patience = 5, restore_best_weights = TRUE),\n        callback_reduce_lr_on_plateau(factor = lr_annealing)\n      ),\n      verbose = 0\n    )\n\n  tibble(\n    best_val_loss = min(history$metrics$val_loss),\n    best_val_acc  = max(history$metrics$val_accuracy),\n    epochs_run    = length(history$metrics$loss)\n  )\n}\n\n# -----------------------------\n# Run grid search\n# -----------------------------\nresults &lt;- grid  %&gt;% \n  mutate(\n    metrics = pmap(\n      list(nodes1, nodes2, nodes3,\n           dropout1, dropout2, dropout3,\n           lr_annealing),\n      train_model\n    )\n  )  %&gt;% \n  unnest(metrics)\n\n# -----------------------------\n# View best models\n# -----------------------------\nresults  %&gt;% \n  arrange(desc(best_val_acc))  %&gt;% \n  slice_head(n = 5)\n\n# Best model overall\nresults  %&gt;%  arrange(desc(best_val_acc))  %&gt;%  slice(1)\n\n# Plot accuracy vs dropout\nresults  %&gt;%  \n  ggplot(aes(dropout1, best_val_acc)) +\n  geom_point()\n\nThis plot shows the relationship between the first-layer dropout rate (dropout1) and the best validation accuracy achieved across all combinations of the other hyperparameters (layer sizes, additional dropout rates, and learning-rate annealing). Each point represents a different model configuration from the grid search.\nThe results indicate that increasing dropout1 from 0.30 to 0.40 does not substantially change overall validation performance, as both values produce models with similar peak accuracies clustered around 98%. However, the models with dropout1 = 0.30 show slightly less variability and fewer low-performing runs, suggesting more stable learning. In contrast, dropout1 = 0.40 occasionally leads to reduced performance, likely because stronger regularization removes too much signal early in the network.\nOverall, this suggests that the model is not highly sensitive to modest changes in first-layer dropout, but slightly lower dropout provides more consistent results within this hyperparameter range."
  },
  {
    "objectID": "teaching/stat-learn/material/07/07-model-selection-regularization.html",
    "href": "teaching/stat-learn/material/07/07-model-selection-regularization.html",
    "title": "Model Selection & Regularization",
    "section": "",
    "text": "Now that we know how to use model validation to check for overfitting, we might want to help fix overfitting. We do this with regularization which helps take a model thats to the righthand side of our Bias Variance graph and move it towards the center by making our model simpler.\nOne way we can do that is by penalizing the coefficients of our model if they’re too large in magnitude (far away from 0 in either direction).\n\n\nLASSO penalizes the sum of the absolute value of the coefficients by adding a penalty term to the loss function:\n\\[ \\underbrace{\\text{RSS}}_{\\sum(x_i - \\hat{x_i})^2} + \\lambda \\sum | \\beta_i |\\]\nLASSO has the benefit of tending to drag coefficients that don’t “pull their weight” to exactly 0, thus removing them from the model.\n\n\n\nRidge penalizes the sum of the squared coefficients by adding a penalty term to the loss function:\n\\[ \\underbrace{\\text{RSS}}_{\\sum(x_i - \\hat{x_i})^2} + \\lambda \\sum  \\beta_i^2\\]\nUnlike LASSO, Ridge tends to drag coefficients that don’t “pull their weight” to near 0, thus NOT removing them from the model.\n\n\n\nBoth models create a tug-of-war where coefficients (\\(\\beta_i\\)) need to “pull their weight” by reducing the Sum of Squared Residuals (\\(\\text{RSS}\\)) in order to be “worth” having a large value in the penalty.\nThe hyperparameter \\(\\lambda\\) controls how much coefficients are penalized.\n\n\nIf \\(\\lambda = 0\\), what happens to our model?\n\n\n\n\nYou can also think of Ridge and LASSO as having a “budget” for how big the sum of your (squared or absolute valued) coefficients can be.\n\nThis graph shows the concept of LASSO and RIDGE in a simple 2-parameter situation. The teal/blue circle (right) and square (left) represent the values for \\(\\beta_1\\) and \\(\\beta_2\\) that satisfy the “budget” for our coefficients. The dot in the middle of the red rings represents what the coefficients would be if we did not add a penalty at all. As you move from the inner to the outer red rings, the Residual Sums of Squares (RSS) goes up, meaning that our model is worse at accurately predicting data in our sample (remember that we’re giving up accuracy in the current sample, to hopefully gain accuracy out-of-sample).\nYou can see that the coefficients chosen by LASSO/Ridge occur at the point of the teal/blue area that meets the red rings. This is because 1) when we penalize our model, it HAS to fit within the contraints we give it (the teal/blue area) but 2) we still want the RSS to be as small as possible (a model that’s bad on ALL data sets isn’t useful).\n\n\n\nIn general: if you want to do variable selection (i.e. completely remove some variables), choose LASSO. But in the real world, people often use something called Elastic Net, which adds both LASSO (L1) and Ridge (L2) penalties to the loss function (see here)."
  },
  {
    "objectID": "teaching/stat-learn/material/07/07-model-selection-regularization.html#lasso",
    "href": "teaching/stat-learn/material/07/07-model-selection-regularization.html#lasso",
    "title": "Model Selection & Regularization",
    "section": "",
    "text": "LASSO penalizes the sum of the absolute value of the coefficients by adding a penalty term to the loss function:\n\\[ \\underbrace{\\text{RSS}}_{\\sum(x_i - \\hat{x_i})^2} + \\lambda \\sum | \\beta_i |\\]\nLASSO has the benefit of tending to drag coefficients that don’t “pull their weight” to exactly 0, thus removing them from the model."
  },
  {
    "objectID": "teaching/stat-learn/material/07/07-model-selection-regularization.html#ridge",
    "href": "teaching/stat-learn/material/07/07-model-selection-regularization.html#ridge",
    "title": "Model Selection & Regularization",
    "section": "",
    "text": "Ridge penalizes the sum of the squared coefficients by adding a penalty term to the loss function:\n\\[ \\underbrace{\\text{RSS}}_{\\sum(x_i - \\hat{x_i})^2} + \\lambda \\sum  \\beta_i^2\\]\nUnlike LASSO, Ridge tends to drag coefficients that don’t “pull their weight” to near 0, thus NOT removing them from the model."
  },
  {
    "objectID": "teaching/stat-learn/material/07/07-model-selection-regularization.html#penalties-in-general",
    "href": "teaching/stat-learn/material/07/07-model-selection-regularization.html#penalties-in-general",
    "title": "Model Selection & Regularization",
    "section": "",
    "text": "Both models create a tug-of-war where coefficients (\\(\\beta_i\\)) need to “pull their weight” by reducing the Sum of Squared Residuals (\\(\\text{RSS}\\)) in order to be “worth” having a large value in the penalty.\nThe hyperparameter \\(\\lambda\\) controls how much coefficients are penalized.\n\n\nIf \\(\\lambda = 0\\), what happens to our model?"
  },
  {
    "objectID": "teaching/stat-learn/material/07/07-model-selection-regularization.html#alternative-regularization-explanation",
    "href": "teaching/stat-learn/material/07/07-model-selection-regularization.html#alternative-regularization-explanation",
    "title": "Model Selection & Regularization",
    "section": "",
    "text": "You can also think of Ridge and LASSO as having a “budget” for how big the sum of your (squared or absolute valued) coefficients can be.\n\nThis graph shows the concept of LASSO and RIDGE in a simple 2-parameter situation. The teal/blue circle (right) and square (left) represent the values for \\(\\beta_1\\) and \\(\\beta_2\\) that satisfy the “budget” for our coefficients. The dot in the middle of the red rings represents what the coefficients would be if we did not add a penalty at all. As you move from the inner to the outer red rings, the Residual Sums of Squares (RSS) goes up, meaning that our model is worse at accurately predicting data in our sample (remember that we’re giving up accuracy in the current sample, to hopefully gain accuracy out-of-sample).\nYou can see that the coefficients chosen by LASSO/Ridge occur at the point of the teal/blue area that meets the red rings. This is because 1) when we penalize our model, it HAS to fit within the contraints we give it (the teal/blue area) but 2) we still want the RSS to be as small as possible (a model that’s bad on ALL data sets isn’t useful)."
  },
  {
    "objectID": "teaching/stat-learn/material/07/07-model-selection-regularization.html#how-to-choose-between-lasso-and-ridge",
    "href": "teaching/stat-learn/material/07/07-model-selection-regularization.html#how-to-choose-between-lasso-and-ridge",
    "title": "Model Selection & Regularization",
    "section": "",
    "text": "In general: if you want to do variable selection (i.e. completely remove some variables), choose LASSO. But in the real world, people often use something called Elastic Net, which adds both LASSO (L1) and Ridge (L2) penalties to the loss function (see here)."
  },
  {
    "objectID": "teaching/stat-learn/material/07/07-model-selection-regularization.html#lasso-1",
    "href": "teaching/stat-learn/material/07/07-model-selection-regularization.html#lasso-1",
    "title": "Model Selection & Regularization",
    "section": "2.1 LASSO",
    "text": "2.1 LASSO\n\n# ---- Load & clean data ----\nama &lt;- read_tsv(\"07-data/amazon-books.txt\") |&gt;\n  drop_na()\n\nRows: 325 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (5): Title, Author, Hard/ Paper, Publisher, ISBN-10\ndbl (8): List Price, Amazon Price, NumPages, Pub year, Height, Width, Thick,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncolnames(ama) &lt;- c(\n  \"Title\", \"Author\", \"List.Price\", \"Amazon.Price\", \"Hard.Paper\",\n  \"NumPages\", \"Publisher\", \"Pub.year\", \"ISBN.10\",\n  \"Height\", \"Width\", \"Thick\", \"Weight.oz\"\n)\n\n# ---- Set up X and y ----\npredictors &lt;- c(\"List.Price\", \"NumPages\", \"Weight.oz\", \"Thick\", \"Height\", \"Width\")\n\nX &lt;- ama[, predictors]\ny &lt;- ama$Amazon.Price\n\n# ---- Train/Test Split (80/20) ----\nset.seed(12345)\n\nn &lt;- nrow(ama)\ntrain_idx &lt;- sample(seq_len(n), size = floor(0.8 * n))\n\nX_train &lt;- X[train_idx, ]\nX_test  &lt;- X[-train_idx, ]\ny_train &lt;- y[train_idx]\ny_test  &lt;- y[-train_idx]\n\n# Model matrices (glmnet will standardize internally)\nX_train_mm &lt;- model.matrix(~ . - 1, data = X_train)\nX_test_mm  &lt;- model.matrix(~ . - 1, data = X_test)\n\n# ---- Lasso with Cross-Validation (alpha = 1) ----\ncv_fit &lt;- cv.glmnet(\n  x = X_train_mm,\n  y = y_train,\n  alpha = 1,          # Lasso\n  standardize = TRUE  # default; explicit for clarity\n)\n\n# ---- Predict ----\ny_pred_train &lt;- as.numeric(predict(cv_fit, newx = X_train_mm, s = \"lambda.min\"))\ny_pred_test  &lt;- as.numeric(predict(cv_fit, newx = X_test_mm,  s = \"lambda.min\"))\n\n# ---- Metrics ----\nmse  &lt;- function(y, yhat) mean((y - yhat)^2)\nmae  &lt;- function(y, yhat) mean(abs(y - yhat))\nmape &lt;- function(y, yhat) mean(abs((y - yhat) / y)) * 100\nr2   &lt;- function(y, yhat) 1 - sum((y - yhat)^2) / sum((y - mean(y))^2)\n\n# ---- Summary ----\nresultsL &lt;- data.frame(\n  Dataset = c(\"Train\", \"Test\"),\n  MSE  = c(mse(y_train, y_pred_train),\n           mse(y_test,  y_pred_test)),\n  MAE  = c(mae(y_train, y_pred_train),\n           mae(y_test,  y_pred_test)),\n  MAPE = c(mape(y_train, y_pred_train),\n           mape(y_test,  y_pred_test)),\n  R2   = c(r2(y_train, y_pred_train),\n           r2(y_test,  y_pred_test))\n)\n\n# ---- Pretty Table ----\nresultsL |&gt;\n  kable(digits = 4, align = \"c\",\n        caption = \"LASSO Regression Performance Summary\") |&gt;\n  kable_styling(full_width = FALSE, bootstrap_options = c(\"striped\", \"hover\"))\n\n\nLASSO Regression Performance Summary\n\n\nDataset\nMSE\nMAE\nMAPE\nR2\n\n\n\n\nTrain\n12.0562\n2.3216\n20.4430\n0.9259\n\n\nTest\n5.3945\n1.7597\n18.1162\n0.6238"
  },
  {
    "objectID": "teaching/stat-learn/material/07/07-model-selection-regularization.html#ridge-1",
    "href": "teaching/stat-learn/material/07/07-model-selection-regularization.html#ridge-1",
    "title": "Model Selection & Regularization",
    "section": "2.2 Ridge",
    "text": "2.2 Ridge\n\n# ---- Train/Test Split (80/20) ----\nset.seed(12345)  # use same seed as LASSO if you want identical split\n\nn &lt;- nrow(ama)\ntrain_idx &lt;- sample(seq_len(n), size = floor(0.8 * n))\n\nX_train &lt;- X[train_idx, ]\nX_test  &lt;- X[-train_idx, ]\ny_train &lt;- y[train_idx]\ny_test  &lt;- y[-train_idx]\n\n# Model matrices (glmnet will standardize internally)\nX_train_mm &lt;- model.matrix(~ . - 1, data = X_train)\nX_test_mm  &lt;- model.matrix(~ . - 1, data = X_test)\n\n# ---- Ridge with Cross-Validation (alpha = 0) ----\ncv_ridge &lt;- cv.glmnet(\n  x = X_train_mm,\n  y = y_train,\n  alpha = 0,          # Ridge\n  standardize = TRUE  # default; explicit for clarity\n)\n\n# ---- Predict ----\ny_pred_train &lt;- as.numeric(predict(cv_ridge, newx = X_train_mm, s = \"lambda.min\"))\ny_pred_test  &lt;- as.numeric(predict(cv_ridge, newx = X_test_mm,  s = \"lambda.min\"))\n\n# ---- Metrics ----\nmse  &lt;- function(y, yhat) mean((y - yhat)^2)\nmae  &lt;- function(y, yhat) mean(abs(y - yhat))\nmape &lt;- function(y, yhat) mean(abs((y - yhat) / y)) * 100\nr2   &lt;- function(y, yhat) 1 - sum((y - yhat)^2) / sum((y - mean(y))^2)\n\nresultsR &lt;- data.frame(\n  Dataset = c(\"Train\", \"Test\"),\n  MSE  = c(mse(y_train, y_pred_train),\n           mse(y_test,  y_pred_test)),\n  MAE  = c(mae(y_train, y_pred_train),\n           mae(y_test,  y_pred_test)),\n  MAPE = c(mape(y_train, y_pred_train),\n           mape(y_test,  y_pred_test)),\n  R2   = c(r2(y_train, y_pred_train),\n           r2(y_test,  y_pred_test))\n)\n\n# ---- Pretty Table ----\nresultsR |&gt;\n  kable(digits = 4, align = \"c\",\n        caption = \"Ridge Regression Performance Summary\") |&gt;\n  kable_styling(full_width = FALSE, bootstrap_options = c(\"striped\", \"hover\"))\n\n\nRidge Regression Performance Summary\n\n\nDataset\nMSE\nMAE\nMAPE\nR2\n\n\n\n\nTrain\n13.5612\n2.2791\n19.8514\n0.9166\n\n\nTest\n4.9377\n1.7572\n20.6326\n0.6557\n\n\n\n\n\n\n2.2.1 Question\nInterpret the results from LASSO and Rdige Regression.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nRidge Regression shows better test performance than LASSO, with a lower Test MSE (4.94 vs 5.39) and higher Test R² (0.656 vs 0.624).\n\nLASSO fits the training data slightly better, but this advantage does not translate into improved test accuracy.\n\nThe similarity in MAE and MAPE across models indicates both methods capture overall price patterns comparably.\n\nThe stronger generalization of Ridge suggests that, for this dataset, Ridge is the preferred regularization method, producing more stable estimates without overshrinking important predictors. In other words, Ridge generalizes slightly better than LASSO on this dataset. Ridge has lower Test MSE (4.94 vs 5.39) and higher Test R² (0.656 vs 0.624)."
  },
  {
    "objectID": "teaching/stat-learn/material/07/07-model-selection-regularization.html#plotting-regularization-paths",
    "href": "teaching/stat-learn/material/07/07-model-selection-regularization.html#plotting-regularization-paths",
    "title": "Model Selection & Regularization",
    "section": "2.3 Plotting Regularization Paths",
    "text": "2.3 Plotting Regularization Paths\n\n# Fit models\nridge_fit &lt;- glmnet(X, y, alpha = 0)\nlasso_fit &lt;- glmnet(X, y, alpha = 1)\n\n# Convert glmnet output to long data frame \nextract_paths &lt;- function(fit, model_name) {\n  coefs &lt;- as.matrix(fit$beta)   # p x K (rows = features, cols = lambdas)\n  lambdas &lt;- fit$lambda          # length K\n  \n  # transpose so rows = lambdas, cols = features\n  df &lt;- as.data.frame(t(coefs))  # now nrow = length(lambda), ncol = p\n  \n  df$lambda &lt;- lambdas\n  df$model &lt;- model_name\n  \n  df_long &lt;- df %&gt;%\n    tidyr::pivot_longer(\n      cols = -c(lambda, model),\n      names_to = \"term\",\n      values_to = \"estimate\"\n    ) %&gt;%\n    dplyr::mutate(log_lambda = log(lambda))\n  \n  df_long\n}\n\nridge_df &lt;- extract_paths(ridge_fit, \"Ridge\")\nlasso_df &lt;- extract_paths(lasso_fit, \"Lasso\")\n\npaths &lt;- bind_rows(ridge_df, lasso_df)\n\n# Ridge plot\ng_ridge &lt;- paths %&gt;%\n  filter(model == \"Ridge\") %&gt;%\n  ggplot(aes(x = log_lambda, y = estimate, color = term)) +\n  geom_line(linewidth = 1) +\n  labs(title = \"Ridge Regularization Paths\",\n       x = \"log(lambda)\", y = \"Coefficient\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\")\n\n# Lasso plot\ng_lasso &lt;- paths %&gt;%\n  filter(model == \"Lasso\") %&gt;%\n  ggplot(aes(x = log_lambda, y = estimate, color = term)) +\n  geom_line(linewidth = 1) +\n  labs(title = \"Lasso Regularization Paths\",\n       x = \"log(lambda)\", y = \"Coefficient\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\")\n\n\n# now let's put norms as x axes\npaths_norm &lt;- paths %&gt;%\n  group_by(model, lambda) %&gt;%\n  mutate(\n    l1_norm = sum(abs(estimate)),            # For Lasso\n    l2_norm = sqrt(sum(estimate^2))          # For Ridge\n  ) %&gt;%\n  ungroup()\n\ng_ridgenorms &lt;- paths_norm %&gt;%\n  filter(model == \"Ridge\") %&gt;%\n  ggplot(aes(x = l2_norm, y = estimate, color = term)) +\n  geom_line(linewidth = 1) +\n  labs(title = \"Ridge Regularization Paths\",\n       x = \"L2 Norm of Coefficients\", y = \"Coefficient\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\")\n\ng_lassonorms &lt;- paths_norm %&gt;%\n  filter(model == \"Lasso\") %&gt;%\n  ggplot(aes(x = l1_norm, y = estimate, color = term)) +\n  geom_line(linewidth = 1) +\n  labs(title = \"Lasso Regularization Paths\",\n       x = \"L1 Norm of Coefficients\", y = \"Coefficient\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"bottom\")\n\n\n# Combine all\n(g_ridge + g_lasso) / (g_ridgenorms + g_lassonorms)\n\n\n\n\n\n\n\n\nNote that we can use glmnet() to plot these regularizations plots directly, we do this in the next part. However, the x axis will then look slightly different as glmnet() uses norms as x axes, making the plot independent of the specific \\(\\lambda\\) scale.\nAlso note that in the above plot, each model is using its own \\(\\lambda\\) sequence generated by glmnet(), so:\n\nRidge might use \\(\\lambda\\) from \\(0.1 \\rightarrow 10^4\\)\nLasso might use \\(\\lambda\\) from \\(10^{-4} \\rightarrow 10^2\\)\n\nThis is because ridge and lasso get different grids, leading to different x-axis ranges. You can of course set the same grid for both which we do in the next part."
  },
  {
    "objectID": "teaching/stat-learn/material/07/07-model-selection-regularization.html#subset-selection-methods",
    "href": "teaching/stat-learn/material/07/07-model-selection-regularization.html#subset-selection-methods",
    "title": "Model Selection & Regularization",
    "section": "3.1 Subset Selection Methods",
    "text": "3.1 Subset Selection Methods\n\n3.1.1 Best Subset Selection\nHere we apply the best subset selection approach to the Hitters data from the ISLR2 package. We wish to predict a baseball player’s Salary on the basis of various statistics associated with performance in the previous year.\nFirst of all, we note that the Salary variable is missing for some of the players. The is.na() function can be used to identify the missing observations. It returns a vector of the same length as the input vector, with a TRUE for any elements that are missing, and a FALSE for non-missing elements. The sum() function can then be used to count all of the missing elements.\n\nnames(Hitters)\n\n [1] \"AtBat\"     \"Hits\"      \"HmRun\"     \"Runs\"      \"RBI\"       \"Walks\"    \n [7] \"Years\"     \"CAtBat\"    \"CHits\"     \"CHmRun\"    \"CRuns\"     \"CRBI\"     \n[13] \"CWalks\"    \"League\"    \"Division\"  \"PutOuts\"   \"Assists\"   \"Errors\"   \n[19] \"Salary\"    \"NewLeague\"\n\ndim(Hitters)\n\n[1] 322  20\n\nsum(is.na(Hitters$Salary))\n\n[1] 59\n\n\nHence we see that Salary is missing for \\(59\\) players. The na.omit() function removes all of the rows that have missing values in any variable.\n\nHitters &lt;- na.omit(Hitters)\ndim(Hitters)\n\n[1] 263  20\n\nsum(is.na(Hitters))\n\n[1] 0\n\n\nThe regsubsets() function (part of the leaps library) performs best subset selection by identifying the best model that contains a given number of predictors, where best is quantified using RSS. The syntax is the same as for lm(). The summary() command outputs the best set of variables for each model size.\n\nregfit.full &lt;- regsubsets(Salary ~ ., Hitters)\nsummary(regfit.full)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., Hitters)\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 8\nSelection Algorithm: exhaustive\n         AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 ) \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 ) \" \"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \"*\"    \" \"   \" \" \n8  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \"*\"    \"*\"   \" \" \n         CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 ) \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 ) \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 ) \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 ) \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n\n\nAn asterisk indicates that a given variable is included in the corresponding model. For instance, this output indicates that the best two-variable model contains only Hits and CRBI. By default, regsubsets() only reports results up to the best eight-variable model. But the nvmax option can be used in order to return as many variables as are desired. Here we fit up to a 19-variable model.\n\nregfit.full &lt;- regsubsets(Salary ~ ., data = Hitters,\n    nvmax = 19)\nreg.summary &lt;- summary(regfit.full)\n\nThe summary() function also returns \\(R^2\\), RSS, adjusted \\(R^2\\), \\(C_p\\), and BIC. We can examine these to try to select the best overall model.\n\nnames(reg.summary)\n\n[1] \"which\"  \"rsq\"    \"rss\"    \"adjr2\"  \"cp\"     \"bic\"    \"outmat\" \"obj\"   \n\n\nFor instance, we see that the \\(R^2\\) statistic increases from \\(32 \\%\\), when only one variable is included in the model, to almost \\(55 \\%\\), when all variables are included. As expected, the \\(R^2\\) statistic increases monotonically as more variables are included.\n\nreg.summary$rsq\n\n [1] 0.3214501 0.4252237 0.4514294 0.4754067 0.4908036 0.5087146 0.5141227\n [8] 0.5285569 0.5346124 0.5404950 0.5426153 0.5436302 0.5444570 0.5452164\n[15] 0.5454692 0.5457656 0.5459518 0.5460945 0.5461159\n\n\nPlotting RSS, adjusted \\(R^2\\), \\(C_p\\), and BIC for all of the models at once will help us decide which model to select.\n\ndf = data.frame(adjR2 = reg.summary$adjr2,\n                rss = reg.summary$rss,\n                Cp = reg.summary$cp,\n                BIC = reg.summary$bic,\n                Variables = seq(1:19))\ndf_long = df %&gt;%\n  pivot_longer(cols = c(adjR2, rss, Cp, BIC),\n               names_to = \"Metric\",\n               values_to = \"Value\")\n\ndf_long %&gt;%\n  ggplot(aes(x = Variables, y = Value)) +\n  geom_line() +\n  facet_wrap(~Metric, scales = \"free_y\") + \n  labs(x = \"Number of Variables\", y = \"Value\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe which.max() function can be used to identify the location of the maximum point of a vector. We will now plot a red dot to indicate the model with the largest adjusted \\(R^2\\) statistic.\n\nmax_index &lt;- which.max(df$adjR2)\n\ndf %&gt;%\n  ggplot(aes(x = Variables, y = adjR2)) +\n  geom_line() + \n  geom_point(aes(x = max_index, y = adjR2[max_index]),\n             color = \"red\", size = 4, shape = 20) + \n  labs(x = \"Number of Variables\", y = \"Adjusted R²\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn a similar fashion we can plot the \\(C_p\\) and BIC statistics, and indicate the models with the smallest statistic using which.min().\n\nmin_cp &lt;- which.min(df$Cp)\nmin_bic &lt;- which.min(df$BIC)\n\ndf %&gt;%\n  ggplot(aes(x = Variables, y = Cp)) +\n  geom_line() + \n  geom_point(aes(x = min_cp, y = Cp[min_cp]),\n             color = \"red\", size = 4, shape = 20) + \n  labs(x = \"Number of Variables\", y = \"Cp\") +\n  theme_minimal()\n\n\n\n\n\n\n\ndf %&gt;%\n  ggplot(aes(x = Variables, y = BIC)) +\n  geom_line() + \n  geom_point(aes(x = min_bic, y = BIC[min_bic]),\n             color = \"red\", size = 4, shape = 20) + \n  labs(x = \"Number of Variables\", y = \"BIC\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe regsubsets() function has a built-in plot() command which can be used to display the selected variables for the best model with a given number of predictors, ranked according to the BIC, \\(C_p\\), adjusted \\(R^2\\), or AIC. To find out more about this function, type ?plot.regsubsets.\n\nplot(regfit.full, scale = \"r2\")\n\n\n\n\n\n\n\nplot(regfit.full, scale = \"adjr2\")\n\n\n\n\n\n\n\nplot(regfit.full, scale = \"Cp\")\n\n\n\n\n\n\n\nplot(regfit.full, scale = \"bic\")\n\n\n\n\n\n\n\n\nThe top row of each plot contains a black square for each variable selected according to the optimal model associated with that statistic. For instance, we see that several models share a BIC close to \\(-150\\). However, the model with the lowest BIC is the six-variable model that contains only AtBat, Hits, Walks, CRBI, DivisionW, and PutOuts. We can use the coef() function to see the coefficient estimates associated with this model.\n\ncoef(regfit.full, 6)\n\n (Intercept)        AtBat         Hits        Walks         CRBI    DivisionW \n  91.5117981   -1.8685892    7.6043976    3.6976468    0.6430169 -122.9515338 \n     PutOuts \n   0.2643076 \n\n\n\n\n3.1.2 Forward and Backward Stepwise Selection\nWe can also use the regsubsets() function to perform forward stepwise or backward stepwise selection, using the argument method = \"forward\" or method = \"backward\".\n\nregfit.fwd &lt;- regsubsets(Salary ~ ., data = Hitters,\n                         nvmax = 19, method = \"forward\")\nsummary(regfit.fwd)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = \"forward\")\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: forward\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n4  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n7  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \" \"   \"*\" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \"*\" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\nregfit.bwd &lt;- regsubsets(Salary ~ ., data = Hitters,\n                         nvmax = 19, method = \"backward\")\nsummary(regfit.bwd)\n\nSubset selection object\nCall: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = \"backward\")\n19 Variables  (and intercept)\n           Forced in Forced out\nAtBat          FALSE      FALSE\nHits           FALSE      FALSE\nHmRun          FALSE      FALSE\nRuns           FALSE      FALSE\nRBI            FALSE      FALSE\nWalks          FALSE      FALSE\nYears          FALSE      FALSE\nCAtBat         FALSE      FALSE\nCHits          FALSE      FALSE\nCHmRun         FALSE      FALSE\nCRuns          FALSE      FALSE\nCRBI           FALSE      FALSE\nCWalks         FALSE      FALSE\nLeagueN        FALSE      FALSE\nDivisionW      FALSE      FALSE\nPutOuts        FALSE      FALSE\nAssists        FALSE      FALSE\nErrors         FALSE      FALSE\nNewLeagueN     FALSE      FALSE\n1 subsets of each size up to 19\nSelection Algorithm: backward\n          AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI\n1  ( 1 )  \" \"   \" \"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n2  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n3  ( 1 )  \" \"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n4  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \" \"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n5  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n6  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n7  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \" \" \n8  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \" \"    \" \"   \" \"    \"*\"   \"*\" \n9  ( 1 )  \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n10  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n11  ( 1 ) \"*\"   \"*\"  \" \"   \" \"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n12  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n13  ( 1 ) \"*\"   \"*\"  \" \"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n14  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \" \"   \" \"    \"*\"   \"*\" \n15  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \" \" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n16  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n17  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \" \"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n18  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \" \"    \"*\"   \"*\" \n19  ( 1 ) \"*\"   \"*\"  \"*\"   \"*\"  \"*\" \"*\"   \"*\"   \"*\"    \"*\"   \"*\"    \"*\"   \"*\" \n          CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN\n1  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n2  ( 1 )  \" \"    \" \"     \" \"       \" \"     \" \"     \" \"    \" \"       \n3  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n4  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n5  ( 1 )  \" \"    \" \"     \" \"       \"*\"     \" \"     \" \"    \" \"       \n6  ( 1 )  \" \"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n7  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n8  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n9  ( 1 )  \"*\"    \" \"     \"*\"       \"*\"     \" \"     \" \"    \" \"       \n10  ( 1 ) \"*\"    \" \"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n11  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n12  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \" \"    \" \"       \n13  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n14  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n15  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n16  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \" \"       \n17  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n18  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n19  ( 1 ) \"*\"    \"*\"     \"*\"       \"*\"     \"*\"     \"*\"    \"*\"       \n\n\nFor instance, we see that using forward stepwise selection, the best one-variable model contains only CRBI, and the best two-variable model additionally includes Hits. For this data, the best one-variable through six-variable models are each identical for best subset and forward selection. However, the best seven-variable models identified by forward stepwise selection, backward stepwise selection, and best subset selection are different.\n\ncoef(regfit.full, 7)\n\n (Intercept)         Hits        Walks       CAtBat        CHits       CHmRun \n  79.4509472    1.2833513    3.2274264   -0.3752350    1.4957073    1.4420538 \n   DivisionW      PutOuts \n-129.9866432    0.2366813 \n\ncoef(regfit.fwd, 7)\n\n (Intercept)        AtBat         Hits        Walks         CRBI       CWalks \n 109.7873062   -1.9588851    7.4498772    4.9131401    0.8537622   -0.3053070 \n   DivisionW      PutOuts \n-127.1223928    0.2533404 \n\ncoef(regfit.bwd, 7)\n\n (Intercept)        AtBat         Hits        Walks        CRuns       CWalks \n 105.6487488   -1.9762838    6.7574914    6.0558691    1.1293095   -0.7163346 \n   DivisionW      PutOuts \n-116.1692169    0.3028847 \n\n\n\n\n3.1.3 Choosing Among Models Using the Validation-Set Approach and Cross-Validation\nWe just saw that it is possible to choose among a set of models of different sizes using \\(C_p\\), BIC, and adjusted \\(R^2\\). We will now consider how to do this using the validation set and cross-validation approaches.\nIn order for these approaches to yield accurate estimates of the test error, we must use only the training observations to perform all aspects of model-fitting—including variable selection. Therefore, the determination of which model of a given size is best must be made using only the training observations. This point is subtle but important. If the full data set is used to perform the best subset selection step, the validation set errors and cross-validation errors that we obtain will not be accurate estimates of the test error.\n\nset.seed(1)\ntrain &lt;- sample(c(TRUE, FALSE),\n                nrow(Hitters),\n                replace = TRUE)\n\ntest &lt;- (!train)\n\nNow, we apply regsubsets() to the training set in order to perform best subset selection.\n\nregfit.best &lt;- regsubsets(Salary ~ .,\n                          data = Hitters[train, ],\n                          nvmax = 19)\n\nNotice that we subset the Hitters data frame directly in the call in order to access only the training subset of the data, using the expression Hitters[train, ]. We now compute the validation set error for the best model of each model size. We first make a model matrix from the test data.\n\ntest.mat &lt;- model.matrix(Salary ~ ., data = Hitters[test, ])\n\nThe model.matrix() function is used in many regression packages for building an “X” matrix from data. Now we run a loop, and for each size i, we extract the coefficients from regfit.best for the best model of that size, multiply them into the appropriate columns of the test model matrix to form the predictions, and compute the test MSE.\n\nval.errors &lt;- rep(NA, 19)\nfor (i in 1:19) {\n coefi &lt;- coef(regfit.best, id = i)\n pred &lt;- test.mat[, names(coefi)] %*% coefi\n val.errors[i] &lt;- mean((Hitters$Salary[test] - pred)^2)\n}\n\nWe find that the best model is the one that contains seven variables.\n\nval.errors\n\n [1] 164377.3 144405.5 152175.7 145198.4 137902.1 139175.7 126849.0 136191.4\n [9] 132889.6 135434.9 136963.3 140694.9 140690.9 141951.2 141508.2 142164.4\n[17] 141767.4 142339.6 142238.2\n\nwhich.min(val.errors)\n\n[1] 7\n\ncoef(regfit.best, 7)\n\n (Intercept)        AtBat         Hits        Walks        CRuns       CWalks \n  67.1085369   -2.1462987    7.0149547    8.0716640    1.2425113   -0.8337844 \n   DivisionW      PutOuts \n-118.4364998    0.2526925 \n\n\nThis was a little tedious, partly because there is no predict() method for regsubsets(). Since we will be using this function again, we can capture our steps above and write our own predict method.\n\n predict.regsubsets &lt;- function(object, newdata, id, ...) {\n  form &lt;- as.formula(object$call[[2]])\n  mat &lt;- model.matrix(form, newdata)\n  coefi &lt;- coef(object, id = id)\n  xvars &lt;- names(coefi)\n  mat[, xvars] %*% coefi\n }\n\nOur function pretty much mimics what we did above. The only complex part is how we extracted the formula used in the call to regsubsets(). We demonstrate how we use this function below, when we do cross-validation.\nFinally, we perform best subset selection on the full data set, and select the best seven-variable model. It is important that we make use of the full data set in order to obtain more accurate coefficient estimates.\nNote that we perform best subset selection on the full data set and select the best seven-variable model, rather than simply using the variables that were obtained from the training set, because the best seven-variable model on the full data set may differ from the corresponding model on the training set.\n\nregfit.best &lt;- regsubsets(Salary ~ ., data = Hitters,\n    nvmax = 19)\ncoef(regfit.best, 7)\n\n (Intercept)         Hits        Walks       CAtBat        CHits       CHmRun \n  79.4509472    1.2833513    3.2274264   -0.3752350    1.4957073    1.4420538 \n   DivisionW      PutOuts \n-129.9866432    0.2366813 \n\n\nIn fact, we see that the best seven-variable model on the full data set has a different set of variables than the best seven-variable model on the training set.\nWe now try to choose among the models of different sizes using cross-validation. This approach is somewhat involved, as we must perform best subset selection within each of the \\(k\\) training sets. Despite this, we see that with its clever subsetting syntax, R makes this job quite easy. First, we create a vector that allocates each observation to one of \\(k=10\\) folds, and we create a matrix in which we will store the results.\n\nk &lt;- 10\nn &lt;- nrow(Hitters)\nset.seed(1)\nfolds &lt;- sample(rep(1:k, length = n))\ncv.errors &lt;- matrix(NA, k, 19,\n    dimnames = list(NULL, paste(1:19)))\n\nNow we write a for loop that performs cross-validation. In the \\(j\\)th fold, the elements of folds that equal j are in the test set, and the remainder are in the training set. We make our predictions for each model size (using our new predict() method), compute the test errors on the appropriate subset, and store them in the appropriate slot in the matrix cv.errors. Note that in the following code R will automatically use our predict.regsubsets() function when we call predict() because the best.fit object has class regsubsets.\n\nfor (j in 1:k) {\n  best.fit &lt;- regsubsets(Salary ~ .,\n       data = Hitters[folds != j, ],\n       nvmax = 19)\n  for (i in 1:19) {\n    pred &lt;- predict(best.fit, Hitters[folds == j, ], id = i)\n    cv.errors[j, i] &lt;-\n         mean((Hitters$Salary[folds == j] - pred)^2)\n   }\n }\n\nThis has given us a \\(10 \\times 19\\) matrix, of which the \\((j,i)\\)th element corresponds to the test MSE for the \\(j\\)th cross-validation fold for the best \\(i\\)-variable model. We use the apply() function to average over the columns of this matrix in order to obtain a vector for which the \\(i\\)th element is the cross-validation error for the \\(i\\)-variable model.\n\nmean.cv.errors &lt;- apply(cv.errors, 2, mean)\nmean.cv.errors\n\n       1        2        3        4        5        6        7        8 \n143439.8 126817.0 134214.2 131782.9 130765.6 120382.9 121443.1 114363.7 \n       9       10       11       12       13       14       15       16 \n115163.1 109366.0 112738.5 113616.5 115557.6 115853.3 115630.6 116050.0 \n      17       18       19 \n116117.0 116419.3 116299.1 \n\npar(mfrow = c(1, 1))\nplot(mean.cv.errors, type = \"b\")\n\n\n\n\n\n\n\n\nWe see that cross-validation selects a 10-variable model. We now perform best subset selection on the full data set in order to obtain the 10-variable model.\n\nreg.best &lt;- regsubsets(Salary ~ ., data = Hitters,\n    nvmax = 19)\ncoef(reg.best, 10)\n\n (Intercept)        AtBat         Hits        Walks       CAtBat        CRuns \n 162.5354420   -2.1686501    6.9180175    5.7732246   -0.1300798    1.4082490 \n        CRBI       CWalks    DivisionW      PutOuts      Assists \n   0.7743122   -0.8308264 -112.3800575    0.2973726    0.2831680"
  },
  {
    "objectID": "teaching/stat-learn/material/07/07-model-selection-regularization.html#ridge-regression-and-the-lasso",
    "href": "teaching/stat-learn/material/07/07-model-selection-regularization.html#ridge-regression-and-the-lasso",
    "title": "Model Selection & Regularization",
    "section": "3.2 Ridge Regression and the Lasso",
    "text": "3.2 Ridge Regression and the Lasso\nWe will use the glmnet package in order to perform ridge regression and the lasso. The main function in this package is glmnet(), which can be used to fit ridge regression models, lasso models, and more. This function has slightly different syntax from other model-fitting functions that we have encountered thus far in this book. In particular, we must pass in an x matrix as well as a y vector, and we do not use the y ~ x syntax. We will now perform ridge regression and the lasso in order to predict Salary on the Hitters data. Before proceeding ensure that the missing values have been removed from the data, as described in Section 6.5.1.\n\nx &lt;- model.matrix(Salary ~ ., Hitters)[, -1]\ny &lt;- Hitters$Salary\n\nThe model.matrix() function is particularly useful for creating x; not only does it produce a matrix corresponding to the \\(19\\) predictors but it also automatically transforms any qualitative variables into dummy variables. The latter property is important because glmnet() can only take numerical, quantitative inputs.\n\n3.2.1 Ridge Regression\nThe glmnet() function has an alpha argument that determines what type of model is fit. If alpha=0 then a ridge regression model is fit, and if alpha=1 then a lasso model is fit. We first fit a ridge regression model.\n\ngrid &lt;- 10^seq(10, -2, length = 100)\nridge.mod &lt;- glmnet(x, y, alpha = 0, lambda = grid)\n\nBy default the glmnet() function performs ridge regression for an automatically selected range of \\(\\lambda\\) values. However, here we have chosen to implement the function over a grid of values ranging from \\(\\lambda=10^{10}\\) to \\(\\lambda=10^{-2}\\), essentially covering the full range of scenarios from the null model containing only the intercept, to the least squares fit. As we will see, we can also compute model fits for a particular value of \\(\\lambda\\) that is not one of the original grid values. Note that by default, the glmnet() function standardizes the variables so that they are on the same scale. To turn off this default setting, use the argument standardize = FALSE.\nAssociated with each value of \\(\\lambda\\) is a vector of ridge regression coefficients, stored in a matrix that can be accessed by coef(). In this case, it is a \\(20 \\times 100\\) matrix, with \\(20\\) rows (one for each predictor, plus an intercept) and \\(100\\) columns (one for each value of \\(\\lambda\\)).\n\ndim(coef(ridge.mod))\n\n[1]  20 100\n\n\nWe expect the coefficient estimates to be much smaller, in terms of \\(\\ell_2\\) norm, when a large value of \\(\\lambda\\) is used, as compared to when a small value of \\(\\lambda\\) is used. These are the coefficients when \\(\\lambda=11{,}498\\), along with their \\(\\ell_2\\) norm:\n\nridge.mod$lambda[50]\n\n[1] 11497.57\n\ncoef(ridge.mod)[, 50]\n\n  (Intercept)         AtBat          Hits         HmRun          Runs \n407.356050200   0.036957182   0.138180344   0.524629976   0.230701523 \n          RBI         Walks         Years        CAtBat         CHits \n  0.239841459   0.289618741   1.107702929   0.003131815   0.011653637 \n       CHmRun         CRuns          CRBI        CWalks       LeagueN \n  0.087545670   0.023379882   0.024138320   0.025015421   0.085028114 \n    DivisionW       PutOuts       Assists        Errors    NewLeagueN \n -6.215440973   0.016482577   0.002612988  -0.020502690   0.301433531 \n\nsqrt(sum(coef(ridge.mod)[-1, 50]^2))\n\n[1] 6.360612\n\n\nIn contrast, here are the coefficients when \\(\\lambda=705\\), along with their \\(\\ell_2\\) norm. Note the much larger \\(\\ell_2\\) norm of the coefficients associated with this smaller value of \\(\\lambda\\).\n\nridge.mod$lambda[60]\n\n[1] 705.4802\n\ncoef(ridge.mod)[, 60]\n\n (Intercept)        AtBat         Hits        HmRun         Runs          RBI \n 54.32519950   0.11211115   0.65622409   1.17980910   0.93769713   0.84718546 \n       Walks        Years       CAtBat        CHits       CHmRun        CRuns \n  1.31987948   2.59640425   0.01083413   0.04674557   0.33777318   0.09355528 \n        CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists \n  0.09780402   0.07189612  13.68370191 -54.65877750   0.11852289   0.01606037 \n      Errors   NewLeagueN \n -0.70358655   8.61181213 \n\nsqrt(sum(coef(ridge.mod)[-1, 60]^2))\n\n[1] 57.11001\n\n\nWe can use the predict() function for a number of purposes. For instance, we can obtain the ridge regression coefficients for a new value of \\(\\lambda\\), say \\(50\\):\n\npredict(ridge.mod, s = 50, type = \"coefficients\")[1:20, ]\n\n  (Intercept)         AtBat          Hits         HmRun          Runs \n 4.876610e+01 -3.580999e-01  1.969359e+00 -1.278248e+00  1.145892e+00 \n          RBI         Walks         Years        CAtBat         CHits \n 8.038292e-01  2.716186e+00 -6.218319e+00  5.447837e-03  1.064895e-01 \n       CHmRun         CRuns          CRBI        CWalks       LeagueN \n 6.244860e-01  2.214985e-01  2.186914e-01 -1.500245e-01  4.592589e+01 \n    DivisionW       PutOuts       Assists        Errors    NewLeagueN \n-1.182011e+02  2.502322e-01  1.215665e-01 -3.278600e+00 -9.496680e+00 \n\n\nWe now split the samples into a training set and a test set in order to estimate the test error of ridge regression and the lasso. There are two common ways to randomly split a data set. The first is to produce a random vector of TRUE, FALSE elements and select the observations corresponding to TRUE for the training data. The second is to randomly choose a subset of numbers between \\(1\\) and \\(n\\); these can then be used as the indices for the training observations. The two approaches work equally well. We used the former method in Section 6.5.1. Here we demonstrate the latter approach.\nWe first set a random seed so that the results obtained will be reproducible.\n\nset.seed(1)\ntrain &lt;- sample(1:nrow(x), nrow(x) / 2)\ntest &lt;- (-train)\ny.test &lt;- y[test]\n\nNext we fit a ridge regression model on the training set, and evaluate its MSE on the test set, using \\(\\lambda=4\\). Note the use of the predict() function again. This time we get predictions for a test set, by replacing type=\"coefficients\" with the newx argument.\n\nridge.mod &lt;- glmnet(x[train, ], y[train], alpha = 0,\n    lambda = grid, thresh = 1e-12)\n\nridge.pred &lt;- predict(ridge.mod, s = 4, newx = x[test, ])\nmean((ridge.pred - y.test)^2)\n\n[1] 142199.2\n\n\nThe test MSE is \\(142{,}199\\). Note that if we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations. In that case, we could compute the test set MSE like this:\n\nmean((mean(y[train]) - y.test)^2)\n\n[1] 224669.9\n\n\nWe could also get the same result by fitting a ridge regression model with a very large value of \\(\\lambda\\). Note that 1e10 means \\(10^{10}\\).\n\nridge.pred &lt;- predict(ridge.mod, s = 1e10, newx = x[test, ])\nmean((ridge.pred - y.test)^2)\n\n[1] 224669.8\n\n\nSo fitting a ridge regression model with \\(\\lambda=4\\) leads to a much lower test MSE than fitting a model with just an intercept. We now check whether there is any benefit to performing ridge regression with \\(\\lambda=4\\) instead of just performing least squares regression. Recall that least squares is simply ridge regression with \\(\\lambda=0\\). (In order for glmnet() to yield the exact least squares coefficients when \\(\\lambda=0\\), we use the argument exact = T when calling the predict() function. Otherwise, the predict() function will interpolate over the grid of \\(\\lambda\\) values used in fitting the glmnet() model, yielding approximate results. When we use exact = T, there remains a slight discrepancy in the third decimal place between the output of glmnet() when \\(\\lambda = 0\\) and the output of lm(); this is due to numerical approximation on the part of glmnet().)\n\nridge.pred &lt;- predict(ridge.mod, s = 0, newx = x[test, ],\n    exact = T, x = x[train, ], y = y[train])\nmean((ridge.pred - y.test)^2)\n\n[1] 168588.6\n\nlm(y ~ x, subset = train)\n\n\nCall:\nlm(formula = y ~ x, subset = train)\n\nCoefficients:\n(Intercept)       xAtBat        xHits       xHmRun        xRuns         xRBI  \n   274.0145      -0.3521      -1.6377       5.8145       1.5424       1.1243  \n     xWalks       xYears      xCAtBat       xCHits      xCHmRun       xCRuns  \n     3.7287     -16.3773      -0.6412       3.1632       3.4008      -0.9739  \n      xCRBI      xCWalks     xLeagueN   xDivisionW     xPutOuts     xAssists  \n    -0.6005       0.3379     119.1486    -144.0831       0.1976       0.6804  \n    xErrors  xNewLeagueN  \n    -4.7128     -71.0951  \n\npredict(ridge.mod, s = 0, exact = T, type = \"coefficients\",\n    x = x[train, ], y = y[train])[1:20, ]\n\n (Intercept)        AtBat         Hits        HmRun         Runs          RBI \n 274.0200994   -0.3521900   -1.6371383    5.8146692    1.5423361    1.1241837 \n       Walks        Years       CAtBat        CHits       CHmRun        CRuns \n   3.7288406  -16.3795195   -0.6411235    3.1629444    3.4005281   -0.9739405 \n        CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists \n  -0.6003976    0.3378422  119.1434637 -144.0853061    0.1976300    0.6804200 \n      Errors   NewLeagueN \n  -4.7127879  -71.0898914 \n\n\nIn general, if we want to fit a (unpenalized) least squares model, then we should use the lm() function, since that function provides more useful outputs, such as standard errors and p-values for the coefficients.\nIn general, instead of arbitrarily choosing \\(\\lambda=4\\), it would be better to use cross-validation to choose the tuning parameter \\(\\lambda\\). We can do this using the built-in cross-validation function, cv.glmnet(). By default, the function performs ten-fold cross-validation, though this can be changed using the argument nfolds. Note that we set a random seed first so our results will be reproducible, since the choice of the cross-validation folds is random.\n\nset.seed(1)\ncv.out &lt;- cv.glmnet(x[train, ], y[train], alpha = 0)\nplot(cv.out)\n\n\n\n\n\n\n\nbestlam &lt;- cv.out$lambda.min\nbestlam\n\n[1] 326.0828\n\n\nTherefore, we see that the value of \\(\\lambda\\) that results in the smallest cross-validation error is \\(326\\). What is the test MSE associated with this value of \\(\\lambda\\)?\n\nridge.pred &lt;- predict(ridge.mod, s = bestlam,\n    newx = x[test, ])\nmean((ridge.pred - y.test)^2)\n\n[1] 139856.6\n\n\nThis represents a further improvement over the test MSE that we got using \\(\\lambda=4\\). Finally, we refit our ridge regression model on the full data set, using the value of \\(\\lambda\\) chosen by cross-validation, and examine the coefficient estimates.\n\nout &lt;- glmnet(x, y, alpha = 0)\npredict(out, type = \"coefficients\", s = bestlam)[1:20, ]\n\n (Intercept)        AtBat         Hits        HmRun         Runs          RBI \n 15.44383120   0.07715547   0.85911582   0.60103106   1.06369007   0.87936105 \n       Walks        Years       CAtBat        CHits       CHmRun        CRuns \n  1.62444617   1.35254778   0.01134999   0.05746654   0.40680157   0.11456224 \n        CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists \n  0.12116504   0.05299202  22.09143197 -79.04032656   0.16619903   0.02941950 \n      Errors   NewLeagueN \n -1.36092945   9.12487765 \n\n\nAs expected, none of the coefficients are zero—ridge regression does not perform variable selection!\n\n\n3.2.2 The Lasso\nWe saw that ridge regression with a wise choice of \\(\\lambda\\) can outperform least squares as well as the null model on the Hitters data set. We now ask whether the lasso can yield either a more accurate or a more interpretable model than ridge regression. In order to fit a lasso model, we once again use the glmnet() function; however, this time we use the argument alpha=1. Other than that change, we proceed just as we did in fitting a ridge model.\n\nlasso.mod &lt;- glmnet(x[train, ], y[train], alpha = 1,\n    lambda = grid)\nplot(lasso.mod)\n\n\n\n\n\n\n\n\nWe can see from the coefficient plot that depending on the choice of tuning parameter, some of the coefficients will be exactly equal to zero. We now perform cross-validation and compute the associated test error.\n\nset.seed(1)\ncv.out &lt;- cv.glmnet(x[train, ], y[train], alpha = 1)\nplot(cv.out)\n\n\n\n\n\n\n\nbestlam &lt;- cv.out$lambda.min\nlasso.pred &lt;- predict(lasso.mod, s = bestlam,\n    newx = x[test, ])\nmean((lasso.pred - y.test)^2)\n\n[1] 143673.6\n\n\nThis is substantially lower than the test set MSE of the null model and of least squares, and very similar to the test MSE of ridge regression with \\(\\lambda\\) chosen by cross-validation.\nHowever, the lasso has a substantial advantage over ridge regression in that the resulting coefficient estimates are sparse. Here we see that 8 of the 19 coefficient estimates are exactly zero. So the lasso model with \\(\\lambda\\) chosen by cross-validation contains only eleven variables.\n\nout &lt;- glmnet(x, y, alpha = 1, lambda = grid)\nlasso.coef &lt;- predict(out, type = \"coefficients\",\n    s = bestlam)[1:20, ]\nlasso.coef\n\n  (Intercept)         AtBat          Hits         HmRun          Runs \n   1.27479059   -0.05497143    2.18034583    0.00000000    0.00000000 \n          RBI         Walks         Years        CAtBat         CHits \n   0.00000000    2.29192406   -0.33806109    0.00000000    0.00000000 \n       CHmRun         CRuns          CRBI        CWalks       LeagueN \n   0.02825013    0.21628385    0.41712537    0.00000000   20.28615023 \n    DivisionW       PutOuts       Assists        Errors    NewLeagueN \n-116.16755870    0.23752385    0.00000000   -0.85629148    0.00000000 \n\nlasso.coef[lasso.coef != 0]\n\n  (Intercept)         AtBat          Hits         Walks         Years \n   1.27479059   -0.05497143    2.18034583    2.29192406   -0.33806109 \n       CHmRun         CRuns          CRBI       LeagueN     DivisionW \n   0.02825013    0.21628385    0.41712537   20.28615023 -116.16755870 \n      PutOuts        Errors \n   0.23752385   -0.85629148"
  },
  {
    "objectID": "teaching/stat-learn/material/08/08-nonlinearity.html",
    "href": "teaching/stat-learn/material/08/08-nonlinearity.html",
    "title": "‘Non-Linear’ Linear Regression",
    "section": "",
    "text": "In order to make better models, sometimes we create new features in our data. This can be as simple as creating a column like BMI or bill_ratio like we’ve done in past classworks, or extracting the day of the week from a date string, but it can also be things like creating polynomial features, step functions, splines, or interactions.\nThese features can help our model perform better. For polynomial features, step functions, and splines, it also allows us to add non-linearity to our predictions even though we’re using linear regression.\nWe will use multiple approaches for modelling non-linearity and apply it to the Boston dataset included in the R library MASS. This dataset consists of 506 samples. The response variable is median value of owner-occupied homes in Boston (medv). The dataset has 13 associated predictor variables.\n\nlibrary(MASS)\nhelp(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\nhead(Boston)\n\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n  medv\n1 24.0\n2 21.6\n3 34.7\n4 33.4\n5 36.2\n6 28.7\n\n\nWe will analyse medv with respect to the predictor lstat (percentage of lower status population).\n\nhead(cbind(Boston$medv, Boston$lstat))\n\n     [,1] [,2]\n[1,] 24.0 4.98\n[2,] 21.6 9.14\n[3,] 34.7 4.03\n[4,] 33.4 2.94\n[5,] 36.2 5.33\n[6,] 28.7 5.21\n\n\nFor convenience we can name the response as y and the predictor x. We will also pre-define the labels for the x and y-axes that we will use repeatedly in figures throughout this practical.\n\ny = Boston$medv\nx = Boston$lstat\ny.lab = 'Median Property Value'\nx.lab = 'Lower Status (%)'\n\n\nplot( x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n      main = \"\", bty = 'l' )\n\n\n\n\n\n\n\n\n\n\nStart by fitting to the data a degree-2 polynomial using the command lm() and summarizing the results using summary().\n\npoly2 = lm(y ~ poly(x,  2,  raw = TRUE))\nsummary(poly2)\n\n\nCall:\nlm(formula = y ~ poly(x, 2, raw = TRUE))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             42.862007   0.872084   49.15   &lt;2e-16 ***\npoly(x, 2, raw = TRUE)1 -2.332821   0.123803  -18.84   &lt;2e-16 ***\npoly(x, 2, raw = TRUE)2  0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\nThe argument raw = TRUE In terms of fitting the curve poly(x, 2, raw = TRUE)) and poly(x, 2)) will give the same result! They are just based on different (orthogonal) basis but with polynomial regression we are almost never interested in the regression coefficients.\nFor plotting th results, we need to create an object, which we name sort.x, which has the sorted values of predictor x in a ascending order. Without sort.x we will not be able to produce the plots since in lecture. Then, we need to use predict() with sort.x as input in order to proceed to the next steps.\n\nsort.x = sort(x)\nsort.x[1:10]     # the first 10 sorted values of x \n\n [1] 1.73 1.92 1.98 2.47 2.87 2.88 2.94 2.96 2.97 2.98\n\npred2 = predict(poly2, newdata = list(x = sort.x), se = TRUE)\nnames(pred2)\n\n[1] \"fit\"            \"se.fit\"         \"df\"             \"residual.scale\"\n\n\nThe object pred2 contains fit, which are the fitted values, and se.fit, which are the standard errors of the mean prediction, that we need in order to construct the approximate 95% confidence intervals (of the mean prediction). With this information we can construct the confidence intervals using cbind(). Lets see how the first 10 fitted values and confidence intervals look like.\n\npred2$fit[1:10]  # the first 10 fitted values of the curve\n\n       1        2        3        4        5        6        7        8 \n38.95656 38.54352 38.41374 37.36561 36.52550 36.50468 36.37992 36.33840 \n       9       10 \n36.31765 36.29691 \n\nse.bands2 = cbind( pred2$fit - 2 * pred2$se.fit, \n                   pred2$fit + 2 * pred2$se.fit )\nse.bands2[1:10,] # the first 10 confidence intervals of the curve\n\n       [,1]     [,2]\n1  37.58243 40.33069\n2  37.20668 39.88036\n3  37.08853 39.73895\n4  36.13278 38.59845\n5  35.36453 37.68647\n6  35.34546 37.66390\n7  35.23118 37.52865\n8  35.19314 37.48365\n9  35.17413 37.46117\n10 35.15513 37.43870\n\n\nNow we can plot the results:\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"Degree-2 polynomial\", bty = 'l')\nlines(sort.x, pred2$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands2, lwd = 1.4, col = \"firebrick\", lty = 3)\n\n\n\n\n\n\n\n\nNote: We use lines() for pred2$fit because this is a vector, but for se.bands2, which is a matrix, we have to use matlines().\nThen we do similar steps to produce a plot of degree-3 up to degree-5 polynomial fits.\n\npoly3 = lm(y ~ poly(x,  3))\npoly4 = lm(y ~ poly(x,  4))\npoly5 = lm(y ~ poly(x, 5))\n\npred3 = predict(poly3, newdata = list(x = sort.x), se = TRUE)\npred4 = predict(poly4, newdata = list(x = sort.x), se = TRUE)\npred5 = predict(poly5, newdata = list(x = sort.x), se = TRUE)\n\nse.bands3 = cbind(pred3$fit + 2*pred3$se.fit, pred3$fit-2*pred3$se.fit)\nse.bands4 = cbind(pred4$fit + 2*pred4$se.fit, pred4$fit-2*pred4$se.fit)\nse.bands5 = cbind(pred5$fit + 2*pred5$se.fit, pred5$fit-2*pred5$se.fit)\n\n\npar(mfrow = c(2,2))\n# Degree-2\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"Degree-2 polynomial\", bty = 'l')\nlines(sort.x, pred2$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands2, lwd = 2, col = \"firebrick\", lty = 3)\n\n# Degree-3\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"Degree-3 polynomial\", bty = 'l')\nlines(sort.x, pred3$fit, lwd = 2, col = \"darkviolet\")\nmatlines(sort.x, se.bands3, lwd = 2, col = \"darkviolet\", lty = 3)\n\n# Degree-4\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"Degree-4 polynomial\", bty = 'l')\nlines(sort.x, pred4$fit, lwd = 2, col = \"royalblue\")\nmatlines(sort.x, se.bands4, lwd = 2, col = \"royalblue\", lty = 3)\n\n# Degree-5\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"Degree-5 polynomial\", bty = 'l')\nlines(sort.x, pred5$fit, lwd = 2, col = \"darkgreen\")\nmatlines(sort.x, se.bands5, lwd = 2, col = \"darkgreen\", lty = 3)\n\n\n\n\n\n\n\n\nAll four curves look reasonable given the data available. We may choose the degree-2 polynomial since it is simpler and seems to do about as well as the others. However, if we want to base our decision on a more formal procedure, we can use analysis-of-variance (ANOVA). Specifically, we will perform sequential comparisons based on the F-test, comparing first the linear model vs. the quadratic model (degree-2 polynomial), then the quadratic model vs. the cubic model (degree-3 polynomial) and so on. We therefore have to fit the simple linear model, and we also choose to fit the degree-6 polynomial to investigate the effects of an additional predictor as well. We can perform this analysis in RStudio using the command anova() as displayed below.\n\npoly1 = lm(y ~ x)\npoly6 = lm(y ~ poly(x, 6))\nanova(poly1, poly2, poly3, poly4, poly5, poly6)\n\nAnalysis of Variance Table\n\nModel 1: y ~ x\nModel 2: y ~ poly(x, 2, raw = TRUE)\nModel 3: y ~ poly(x, 3)\nModel 4: y ~ poly(x, 4)\nModel 5: y ~ poly(x, 5)\nModel 6: y ~ poly(x, 6)\n  Res.Df   RSS Df Sum of Sq        F    Pr(&gt;F)    \n1    504 19472                                    \n2    503 15347  1    4125.1 151.8623 &lt; 2.2e-16 ***\n3    502 14616  1     731.8  26.9390 3.061e-07 ***\n4    501 13968  1     647.8  23.8477 1.406e-06 ***\n5    500 13597  1     370.7  13.6453 0.0002452 ***\n6    499 13555  1      42.4   1.5596 0.2123125    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWhich model would you choose?\n\n\n\nFor step function regression we can make use of the command cut(), which automatically assigns samples to intervals given a specific number of intervals. We can check how this works by executing the following syntax:\n\ntable(cut(x, 2))\n\n\n(1.69,19.9]   (19.9,38] \n        430          76 \n\n\nWhat we see is that cut(x, 2) automatically created a factor with two levels, corresponding to the intervals \\((1.69,19.9]\\) and \\((19.9,38]\\) and assigned each entry in x to one of these factors depending on which interval it was in. The command table() tells us that 430 samples of x fall within the first interval and that 76 samples fall within the second interval. Note that cut(x, 2) generated 2 intervals, but this means there is only 1 cutpoint (at 19.9). The number of cutpoints is naturally one less than the number of intervals, but it is important to be aware that cut requires specification of the number of required intervals.\nSo, we can use cut() within lm() to easily fit regression models with step functions. Below we consider 4 models with 1, 2, 3 and 4 cutpoints (2, 3, 4 and 5 intervals) respectively.\n\nstep2 = lm(y ~ cut(x, 2))\nstep3 = lm(y ~ cut(x, 3))\nstep4 = lm(y ~ cut(x, 4))\nstep5 = lm(y ~ cut(x, 5))\n\nThe analysis then is essentially the same as previously. We plot the fitted lines of the four models, along with approximate 95% confidence intervals for the mean predictions.\n\n#| fig-width: 9\n#| fig-height: 8\npred2 = predict(step2, newdata = list(x = sort(x)), se = TRUE)\npred3 = predict(step3, newdata = list(x = sort(x)), se = TRUE)\npred4 = predict(step4, newdata = list(x = sort(x)), se = TRUE)\npred5 = predict(step5, newdata = list(x = sort(x)), se = TRUE)\n\nse.bands2 = cbind(pred2$fit + 2*pred2$se.fit, pred2$fit-2*pred2$se.fit)\nse.bands3 = cbind(pred3$fit + 2*pred3$se.fit, pred3$fit-2*pred3$se.fit)\nse.bands4 = cbind(pred4$fit + 2*pred4$se.fit, pred4$fit-2*pred4$se.fit)\nse.bands5 = cbind(pred5$fit + 2*pred5$se.fit, pred5$fit-2*pred5$se.fit)\n\npar(mfrow = c(2,2))\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"1 cutpoint\", bty = 'l')\nlines(sort(x), pred2$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort(x), se.bands2, lwd = 1.4, col = \"firebrick\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"2 cutpoints\", bty = 'l')\nlines(sort(x), pred3$fit, lwd = 2, col = \"darkviolet\")\nmatlines(sort(x), se.bands3, lwd = 1.4, col = \"darkviolet\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"3 cutpoints\", bty = 'l')\nlines(sort(x), pred4$fit, lwd = 2, col = \"royalblue\")\nmatlines(sort(x), se.bands4, lwd = 1.4, col = \"royalblue\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"4 cutpoints\", bty = 'l')\nlines(sort(x), pred5$fit, lwd = 2, col = \"darkgreen\")\nmatlines(sort(x), se.bands5, lwd = 1.4, col = \"darkgreen\", lty = 3)\n\n\n\n\n\n\n\n\nNote that we do not necessarily need to rely on the automatic selections of cutpoints used by cut(). We can define the intervals if we want to. For instance, if we want cutpoints at 10, 20 and 30 we can do the following\n\nbreaks4 = c(min(x), 10, 20, 30, max(x))\ntable(cut(x, breaks = breaks4))\n\n\n(1.73,10]   (10,20]   (20,30]   (30,38] \n      218       213        62        12 \n\n\nBy including min(x) and max(x) at the start and end, we ensure the intervals covered the entire range of x. Our model is then\n\nstep.new4 = lm(y ~ cut(x, breaks = breaks4))\nsummary(step.new4)\n\n\nCall:\nlm(formula = y ~ cut(x, breaks = breaks4))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.4803  -4.6239  -0.4239   2.8968  20.6197 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      29.3803     0.4415  66.540   &lt;2e-16 ***\ncut(x, breaks = breaks4)(10,20] -10.4563     0.6281 -16.648   &lt;2e-16 ***\ncut(x, breaks = breaks4)(20,30] -16.6770     0.9383 -17.773   &lt;2e-16 ***\ncut(x, breaks = breaks4)(30,38] -18.6886     1.9331  -9.668   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.519 on 501 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.4925,    Adjusted R-squared:  0.4895 \nF-statistic: 162.1 on 3 and 501 DF,  p-value: &lt; 2.2e-16\n\n\nWe can now make predictions at new data points using the constructed linear model as usual.\n\nnewx &lt;- c(10.56, 5.89)\npreds = predict(step.new4, newdata = list(x = newx), se = TRUE)\npreds\n\n$fit\n       1        2 \n18.92394 29.38028 \n\n$se.fit\n        1         2 \n0.4466955 0.4415432 \n\n$df\n[1] 501\n\n$residual.scale\n[1] 6.519307\n\n\n\n\n\nFor this analysis we will require package splines.\n\nlibrary(splines)\n\nInitially let’s fit regression splines by specifying knots. From the previous plot it is not clear where exactly we should place knots, so we will make use of the command summary in order to find the 25th, 50th and 75th percentiles ofx, which will be the positions where we will place the knots. We also sort the variable x before fitting the splines.\n\nsummary(x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.73    6.95   11.36   12.65   16.95   37.97 \n\ncuts = summary(x)[c(2, 3, 5)] \ncuts\n\n1st Qu.  Median 3rd Qu. \n  6.950  11.360  16.955 \n\nsort.x = sort(x)\n\nFor a start lets fit a linear spline using our selected placement of knots. For this we can use command lm() and inside it we use the command bs() in which we specify degree = 1 for a linear spline and knots = cuts for the placement of the knots at the three percentiles. We also calculate the corresponding fitted values and confidence intervals exactly in the same way we did in previous practical demonstrations.\n\nspline1 = lm(y ~  splines::bs(x, degree = 1, knots = cuts))\npred1 = predict(spline1, newdata = list(x = sort.x), se = TRUE)\nse.bands1 = cbind(pred1$fit + 2 * pred1$se.fit, \n                  pred1$fit - 2 * pred1$se.fit)\n\nLet’s plot the results:\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Linear Spline\", bty = 'l')\nlines(sort.x, pred1$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands1, lwd = 2, col = \"firebrick\", lty = 3)\n\n\n\n\n\n\n\n\nUsing ?bs we see that instead of using the argument knots we can use the argument df, which are the degrees of freedom. Splines have \\((d+1)+K\\) degrees of freedom, where \\(d\\) is the degree of the polynomial and \\(K\\) the number of knots. So in this case we have 1+1+3 = 5 degrees of freedom. Selecting df = 5 in bs() will automatically use 3 knots placed at the 25th, 50th and 75th percentiles. Below we check whether the plot based on df=5 is indeed the same as the previous plot and as we can see it is.\n\nspline1df = lm(y ~ splines::bs(x, degree = 1, df = 5))\npred1df = predict(spline1df, newdata = list(x = sort.x), se = TRUE)\nse.bands1df = cbind( pred1df$fit + 2 * pred1df$se.fit, \n                     pred1df$fit - 2 * pred1df$se.fit )\n\npar(mfrow = c(1, 2))\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Linear Spline (with knots)\", bty = 'l')\nlines(sort.x, pred1$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands1, lwd = 2, col = \"firebrick\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Linear Spline (with df)\", bty = 'l')\nlines(sort.x, pred1df$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands1df, lwd = 2, col = \"firebrick\", lty = 3)\nmatlines(sort.x, se.bands1, lwd = 2, col = \"firebrick\", lty = 3)\n\n\n\n\n\n\n\n\nHaving seen how this works we can also fit a degree-2 (quadratic) and degree-3 (cubic) spline to the data, all we have to do is change degree = 1 to degree = 2 and degree = 3 respectively. Also we increase the respective degrees of freedom from df = 5 to df = 6 and df = 7 in order to keep the same number (and position) of knots in the quadratic and cubic spline models.\n\nspline2 = lm(y ~ splines::bs(x, degree = 2, df = 6))\npred2 = predict(spline2, newdata = list(x = sort.x), se = TRUE)\nse.bands2 = cbind(pred2$fit + 2 * pred2$se.fit, pred2$fit - 2 * pred2$se.fit)\n\nspline3 = lm(y ~ splines::bs(x, degree = 3, df = 7))\npred3 = predict(spline3, newdata = list(x = sort.x), se = TRUE)\nse.bands3 = cbind(pred3$fit + 2 * pred3$se.fit, pred3$fit - 2 * pred3$se.fit)\n\npar(mfrow = c(1,3))\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Linear Spline\", bty = 'l')\nlines(sort.x, pred1$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands1, lwd = 2, col = \"firebrick\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Quadratic Spline\", bty = 'l')\nlines(sort.x, pred2$fit, lwd = 2, col = \"darkgreen\")\nmatlines(sort.x, se.bands2, lwd = 2, col = \"darkgreen\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Cubic Spline\", bty = 'l')\nlines(sort.x, pred3$fit, lwd = 2, col = \"royalblue\")\nmatlines(sort.x, se.bands3, lwd = 2, col = \"royalblue\", lty = 3)\n\n\n\n\n\n\n\n\n\n\n\nFor natural splines, we can use the command ns(). As with the command bs() previously, we again have the option to either specify the knots manually (via the argument knots) or to simply pre-define the degrees of freedom (via the argument df). Below we use the latter option to fit four natural splines with 1, 2, 3 and 4 degrees of freedom. As we see using 1 degree of freedom actually results in just a linear model.\n\nspline.ns1 = lm(y ~ splines::ns(x, df = 1))\npred.ns1 = predict(spline.ns1, newdata = list(x = sort.x), se = TRUE)\nse.bands.ns1 = cbind(pred.ns1$fit + 2 * pred.ns1$se.fit, \n                     pred.ns1$fit - 2 * pred.ns1$se.fit)\n\nspline.ns2 = lm(y ~ splines::ns(x, df = 2))\npred.ns2 = predict(spline.ns2, newdata = list(x = sort.x), se = TRUE)\nse.bands.ns2 = cbind(pred.ns2$fit + 2 * pred.ns2$se.fit, \n                     pred.ns2$fit - 2 * pred.ns2$se.fit)\n\nspline.ns3 = lm(y ~ splines::ns(x, df = 3))\npred.ns3 = predict(spline.ns3, newdata = list(x = sort.x), se = TRUE)\nse.bands.ns3 = cbind(pred.ns3$fit + 2 * pred.ns3$se.fit, \n                     pred.ns3$fit - 2 * pred.ns3$se.fit)\n\nspline.ns4 = lm(y ~ splines::ns(x, df = 4))\npred.ns4 = predict(spline.ns4, newdata = list(x = sort.x), se = TRUE)\nse.bands.ns4 = cbind(pred.ns4$fit + 2 * pred.ns4$se.fit, \n                     pred.ns4$fit - 2 * pred.ns4$se.fit)\n\npar(mfrow = c(2, 2))\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Natural Spline (1 df)\", bty = 'l')\nlines(sort.x, pred.ns1$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands.ns1, lwd = 2, col = \"firebrick\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Natural Spline (2 df)\", bty = 'l')\nlines(sort.x, pred.ns2$fit, lwd = 2, col = \"darkviolet\")\nmatlines(sort.x, se.bands.ns2, lwd = 2, col = \"darkviolet\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Natural Spline (3 df)\", bty = 'l')\nlines(sort.x, pred.ns3$fit, lwd = 2, col = \"royalblue\")\nmatlines(sort.x, se.bands.ns3, lwd = 2, col = \"royalblue\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Natural Spline (4 df)\", bty = 'l')\nlines(sort.x, pred.ns4$fit, lwd = 2, col = \"darkgreen\")\nmatlines(sort.x, se.bands.ns4, lwd = 2, col = \"darkgreen\", lty = 3)\n\n\n\n\n\n\n\n\nBelow we plot the cubic spline next to the natural cubic spline for comparison. As we can see, the natural cubic spline is generally smoother and closer to linear on the right boundary of the predictor space, where it has, additionally, narrower confidence intervals in comparison to the cubic spline.\n\npar(mfrow = c(1,2))\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Cubic Spline\", bty = 'l')\nlines(sort.x, pred3$fit, lwd = 2, col = \"darkblue\")\nmatlines(sort.x, se.bands3, lwd = 2, col = \"darkblue\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Natural Spline (3 df)\", bty = 'l')\nlines(sort.x, pred.ns3$fit, lwd = 2, col = \"royalblue\")\nmatlines(sort.x, se.bands.ns3, lwd = 2, col = \"royalblue\", lty = 3)\n\n\n\n\n\n\n\n\n\n\n\nFor fitting smoothing splines we use the command smooth.splines() instead of lm(). Under smoothing splines there are no knots to specify; the only parameter is \\(\\lambda\\). This can be specified via cross-validation by specifying cv = TRUE inside smooth.splines(). Alternatively, we can specify the effective degrees of freedom which correspond to some value of \\(\\lambda\\). Below we first fit a smoothing spline with 3 effective degrees of freedom (via the argument df = 3), and then also by tuning \\(\\lambda\\) via cross-validation. In this case we see that tuning \\(\\lambda\\) through cross-validation results in a curve which is slightly wiggly on the right boundary of the predictor space.\n\nsmooth1 = smooth.spline(x, y, df = 3)\nsmooth2 = smooth.spline(x, y, cv = TRUE)\n\nWarning in smooth.spline(x, y, cv = TRUE): cross-validation with non-unique 'x'\nvalues seems doubtful\n\npar(mfrow = c(1,2))\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Smoothing Spline (3 df)\", bty = 'l')\nlines(smooth1, lwd = 2, col = \"brown\")\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Smoothing Spline (CV)\", bty = 'l')\nlines(smooth2, lwd = 2, col = \"darkorange\")\n\n\n\n\n\n\n\n\nNote: the effective degrees of freedom of a smoothing spline are similar to the degrees of freedom in standard spline models and can be used as an alternative to cross-validation as a way to fix \\(\\lambda\\).\n\n\n\nIn this final part we will fit a generalized additive model (GAM) utilizing more than one predictor from the Boston dataset.We first use the command names() in order to check once again the available predictor variables.\n\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n\nLet’s say that we want to use predictors lstat, indus and chas for the analysis (use ?Boston again to check what these refer to).\nFor GAMs we will make use of the library gam, so the first thing that we have to do is to install this package by executing install.packages(\"gam\") once. Then we load the library.\n\nlibrary(gam)\n\nThe main function is gam(). Inside this function we can use any combination of non-linear and linear modelling of the various predictors. For example below we use a cubic spline with 5 degrees of freedom for lstat, a smoothing spline with 5 degrees of freedom for indus and a simple linear model for variable chas. We then plot the contributions of each predictor using the command plot(). As we can see, GAMs are very useful as they estimate the contribution of the effects of each predictor.\n\ngam = gam( medv ~ bs(lstat, degree = 3, df = 5) + s(indus, df = 5) + chas, \n           data = Boston )\npar( mfrow = c(1,3) )\nplot( gam,  se = TRUE, col = \"blue\" )\n\n\n\n\n\n\n\n\nNote that simply using chas inside gam() is just fitting a linear model for this variable. However, one thing that we observe is that chas is a binary variable as it only takes the values of 0 and 1. This we can see from the x-axis of the chas plot on the right above. So, it would be preferable to use a step function for this variable. In order to do this we have to change the variable chas to a factor. We first create a second object called Boston1 (in order not to change the initial dataset Boston) and then we use the command factor() to change variable chas. Then we fit again the same model. As we can see below now gam() fits a step function for variable chas which is more appropriate.\n\nBoston1 = Boston\nBoston1$chas = factor(Boston1$chas)\n\ngam1 = gam( medv ~ bs(lstat, degree = 3, df = 5) + s(indus, df = 5) + chas, \n            data = Boston1 )\npar(mfrow = c(1,3))\nplot(gam1,  se = TRUE, col = \"blue\")\n\n\n\n\n\n\n\n\nWe can make predictions from gam objects, just like lm objects, using the predict() method for the class gam. Here we make predictions on some new data. Note that when assigning the value 0 to chas, we enclose it in “” since we informed R to treat chas as a categorical factor with two levels: “0” and “1”.\n\npreds &lt;- predict( gam1, \n                  newdata = data.frame( chas = \"0\", indus = 3, lstat = 5 )  )\npreds\n\n       1 \n32.10065 \n\n\n\n\n\n\n\n\nLoad libraries MASS and faraway (contains dataset seatpos needed which includes daat on car seat positioning depending on driver size).\nWe will analyse the effects of predictor variable Ht on the response variable hipcenter. Assign the hipcenter values to a vector y, and the Ht variable values to a vector x.\nPlot variable Ht and hipcenter against each other. Remember to include suitable axis labels. From visual inspection of this plot, what sort of polynomial might be appropriate for this data?\nFit a first and second order polynomial to the data using the commands lm and poly. Look at the corresponding summary objects. Do these back up your answer to above?\nPlot the first and second polynomial fits to the data, along with ±2 standard deviation confidence intervals. What do you notice about the degree-2 polynomial plot?\nPerform an analysis of variance to confirm whether higher order degrees of polynomial are useful for modelling hipcenter based on Ht.\nUse step function regression with 5 cut-points to model hipcenter based on Ht. Plot the results.\n\n\n\n\nWe will here again analyze the effects of predictor variable Ht on the response variable hipcenter.\n\nLoad the necessary packages for the dataset and spline functions. Assign the hipcenter values to a vector y, and the Ht variable values to a vector x.\nFind the 25th, 50th and 75th percentiles of x, storing them in a vector cuts.\nUse a linear spline to model hipcenter as a function of Htv, putting knots at the 25th, 50th and 75th percentiles ofx`.\nPlot the fitted linear spline from part (c) over the data, along with ±2 standard deviation confidence intervals.\nUse a smoothing spline to model hipcenter as a function of Ht, selecting\n\\(\\lambda\\) with cross-validation, and generate a relevant plot. What do you notice?\n\n\n\n\n\nFit a GAM for hipcenter that consists of three terms:\n\n\na natural spline with 5 degrees of freedom for Age,\na smoothing spline with 3 degrees of freedom for Thigh\na simple linear model term for Ht.\n\n\nPlot the resulting contributions of each term to the GAM, and compare them with plots of hipcenter against each of the three variables Age, Thigh and Ht.\nDoes the contribution of each term of the GAM make sense in light of these pair-wise plots. Is the GAM fitting the data well?"
  },
  {
    "objectID": "teaching/stat-learn/material/08/08-nonlinearity.html#polynomial-regression",
    "href": "teaching/stat-learn/material/08/08-nonlinearity.html#polynomial-regression",
    "title": "‘Non-Linear’ Linear Regression",
    "section": "",
    "text": "Start by fitting to the data a degree-2 polynomial using the command lm() and summarizing the results using summary().\n\npoly2 = lm(y ~ poly(x,  2,  raw = TRUE))\nsummary(poly2)\n\n\nCall:\nlm(formula = y ~ poly(x, 2, raw = TRUE))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             42.862007   0.872084   49.15   &lt;2e-16 ***\npoly(x, 2, raw = TRUE)1 -2.332821   0.123803  -18.84   &lt;2e-16 ***\npoly(x, 2, raw = TRUE)2  0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\n\nThe argument raw = TRUE In terms of fitting the curve poly(x, 2, raw = TRUE)) and poly(x, 2)) will give the same result! They are just based on different (orthogonal) basis but with polynomial regression we are almost never interested in the regression coefficients.\nFor plotting th results, we need to create an object, which we name sort.x, which has the sorted values of predictor x in a ascending order. Without sort.x we will not be able to produce the plots since in lecture. Then, we need to use predict() with sort.x as input in order to proceed to the next steps.\n\nsort.x = sort(x)\nsort.x[1:10]     # the first 10 sorted values of x \n\n [1] 1.73 1.92 1.98 2.47 2.87 2.88 2.94 2.96 2.97 2.98\n\npred2 = predict(poly2, newdata = list(x = sort.x), se = TRUE)\nnames(pred2)\n\n[1] \"fit\"            \"se.fit\"         \"df\"             \"residual.scale\"\n\n\nThe object pred2 contains fit, which are the fitted values, and se.fit, which are the standard errors of the mean prediction, that we need in order to construct the approximate 95% confidence intervals (of the mean prediction). With this information we can construct the confidence intervals using cbind(). Lets see how the first 10 fitted values and confidence intervals look like.\n\npred2$fit[1:10]  # the first 10 fitted values of the curve\n\n       1        2        3        4        5        6        7        8 \n38.95656 38.54352 38.41374 37.36561 36.52550 36.50468 36.37992 36.33840 \n       9       10 \n36.31765 36.29691 \n\nse.bands2 = cbind( pred2$fit - 2 * pred2$se.fit, \n                   pred2$fit + 2 * pred2$se.fit )\nse.bands2[1:10,] # the first 10 confidence intervals of the curve\n\n       [,1]     [,2]\n1  37.58243 40.33069\n2  37.20668 39.88036\n3  37.08853 39.73895\n4  36.13278 38.59845\n5  35.36453 37.68647\n6  35.34546 37.66390\n7  35.23118 37.52865\n8  35.19314 37.48365\n9  35.17413 37.46117\n10 35.15513 37.43870\n\n\nNow we can plot the results:\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"Degree-2 polynomial\", bty = 'l')\nlines(sort.x, pred2$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands2, lwd = 1.4, col = \"firebrick\", lty = 3)\n\n\n\n\n\n\n\n\nNote: We use lines() for pred2$fit because this is a vector, but for se.bands2, which is a matrix, we have to use matlines().\nThen we do similar steps to produce a plot of degree-3 up to degree-5 polynomial fits.\n\npoly3 = lm(y ~ poly(x,  3))\npoly4 = lm(y ~ poly(x,  4))\npoly5 = lm(y ~ poly(x, 5))\n\npred3 = predict(poly3, newdata = list(x = sort.x), se = TRUE)\npred4 = predict(poly4, newdata = list(x = sort.x), se = TRUE)\npred5 = predict(poly5, newdata = list(x = sort.x), se = TRUE)\n\nse.bands3 = cbind(pred3$fit + 2*pred3$se.fit, pred3$fit-2*pred3$se.fit)\nse.bands4 = cbind(pred4$fit + 2*pred4$se.fit, pred4$fit-2*pred4$se.fit)\nse.bands5 = cbind(pred5$fit + 2*pred5$se.fit, pred5$fit-2*pred5$se.fit)\n\n\npar(mfrow = c(2,2))\n# Degree-2\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"Degree-2 polynomial\", bty = 'l')\nlines(sort.x, pred2$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands2, lwd = 2, col = \"firebrick\", lty = 3)\n\n# Degree-3\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"Degree-3 polynomial\", bty = 'l')\nlines(sort.x, pred3$fit, lwd = 2, col = \"darkviolet\")\nmatlines(sort.x, se.bands3, lwd = 2, col = \"darkviolet\", lty = 3)\n\n# Degree-4\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"Degree-4 polynomial\", bty = 'l')\nlines(sort.x, pred4$fit, lwd = 2, col = \"royalblue\")\nmatlines(sort.x, se.bands4, lwd = 2, col = \"royalblue\", lty = 3)\n\n# Degree-5\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"Degree-5 polynomial\", bty = 'l')\nlines(sort.x, pred5$fit, lwd = 2, col = \"darkgreen\")\nmatlines(sort.x, se.bands5, lwd = 2, col = \"darkgreen\", lty = 3)\n\n\n\n\n\n\n\n\nAll four curves look reasonable given the data available. We may choose the degree-2 polynomial since it is simpler and seems to do about as well as the others. However, if we want to base our decision on a more formal procedure, we can use analysis-of-variance (ANOVA). Specifically, we will perform sequential comparisons based on the F-test, comparing first the linear model vs. the quadratic model (degree-2 polynomial), then the quadratic model vs. the cubic model (degree-3 polynomial) and so on. We therefore have to fit the simple linear model, and we also choose to fit the degree-6 polynomial to investigate the effects of an additional predictor as well. We can perform this analysis in RStudio using the command anova() as displayed below.\n\npoly1 = lm(y ~ x)\npoly6 = lm(y ~ poly(x, 6))\nanova(poly1, poly2, poly3, poly4, poly5, poly6)\n\nAnalysis of Variance Table\n\nModel 1: y ~ x\nModel 2: y ~ poly(x, 2, raw = TRUE)\nModel 3: y ~ poly(x, 3)\nModel 4: y ~ poly(x, 4)\nModel 5: y ~ poly(x, 5)\nModel 6: y ~ poly(x, 6)\n  Res.Df   RSS Df Sum of Sq        F    Pr(&gt;F)    \n1    504 19472                                    \n2    503 15347  1    4125.1 151.8623 &lt; 2.2e-16 ***\n3    502 14616  1     731.8  26.9390 3.061e-07 ***\n4    501 13968  1     647.8  23.8477 1.406e-06 ***\n5    500 13597  1     370.7  13.6453 0.0002452 ***\n6    499 13555  1      42.4   1.5596 0.2123125    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWhich model would you choose?"
  },
  {
    "objectID": "teaching/stat-learn/material/08/08-nonlinearity.html#step-functions",
    "href": "teaching/stat-learn/material/08/08-nonlinearity.html#step-functions",
    "title": "‘Non-Linear’ Linear Regression",
    "section": "",
    "text": "For step function regression we can make use of the command cut(), which automatically assigns samples to intervals given a specific number of intervals. We can check how this works by executing the following syntax:\n\ntable(cut(x, 2))\n\n\n(1.69,19.9]   (19.9,38] \n        430          76 \n\n\nWhat we see is that cut(x, 2) automatically created a factor with two levels, corresponding to the intervals \\((1.69,19.9]\\) and \\((19.9,38]\\) and assigned each entry in x to one of these factors depending on which interval it was in. The command table() tells us that 430 samples of x fall within the first interval and that 76 samples fall within the second interval. Note that cut(x, 2) generated 2 intervals, but this means there is only 1 cutpoint (at 19.9). The number of cutpoints is naturally one less than the number of intervals, but it is important to be aware that cut requires specification of the number of required intervals.\nSo, we can use cut() within lm() to easily fit regression models with step functions. Below we consider 4 models with 1, 2, 3 and 4 cutpoints (2, 3, 4 and 5 intervals) respectively.\n\nstep2 = lm(y ~ cut(x, 2))\nstep3 = lm(y ~ cut(x, 3))\nstep4 = lm(y ~ cut(x, 4))\nstep5 = lm(y ~ cut(x, 5))\n\nThe analysis then is essentially the same as previously. We plot the fitted lines of the four models, along with approximate 95% confidence intervals for the mean predictions.\n\n#| fig-width: 9\n#| fig-height: 8\npred2 = predict(step2, newdata = list(x = sort(x)), se = TRUE)\npred3 = predict(step3, newdata = list(x = sort(x)), se = TRUE)\npred4 = predict(step4, newdata = list(x = sort(x)), se = TRUE)\npred5 = predict(step5, newdata = list(x = sort(x)), se = TRUE)\n\nse.bands2 = cbind(pred2$fit + 2*pred2$se.fit, pred2$fit-2*pred2$se.fit)\nse.bands3 = cbind(pred3$fit + 2*pred3$se.fit, pred3$fit-2*pred3$se.fit)\nse.bands4 = cbind(pred4$fit + 2*pred4$se.fit, pred4$fit-2*pred4$se.fit)\nse.bands5 = cbind(pred5$fit + 2*pred5$se.fit, pred5$fit-2*pred5$se.fit)\n\npar(mfrow = c(2,2))\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"1 cutpoint\", bty = 'l')\nlines(sort(x), pred2$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort(x), se.bands2, lwd = 1.4, col = \"firebrick\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"2 cutpoints\", bty = 'l')\nlines(sort(x), pred3$fit, lwd = 2, col = \"darkviolet\")\nmatlines(sort(x), se.bands3, lwd = 1.4, col = \"darkviolet\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"3 cutpoints\", bty = 'l')\nlines(sort(x), pred4$fit, lwd = 2, col = \"royalblue\")\nmatlines(sort(x), se.bands4, lwd = 1.4, col = \"royalblue\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"4 cutpoints\", bty = 'l')\nlines(sort(x), pred5$fit, lwd = 2, col = \"darkgreen\")\nmatlines(sort(x), se.bands5, lwd = 1.4, col = \"darkgreen\", lty = 3)\n\n\n\n\n\n\n\n\nNote that we do not necessarily need to rely on the automatic selections of cutpoints used by cut(). We can define the intervals if we want to. For instance, if we want cutpoints at 10, 20 and 30 we can do the following\n\nbreaks4 = c(min(x), 10, 20, 30, max(x))\ntable(cut(x, breaks = breaks4))\n\n\n(1.73,10]   (10,20]   (20,30]   (30,38] \n      218       213        62        12 \n\n\nBy including min(x) and max(x) at the start and end, we ensure the intervals covered the entire range of x. Our model is then\n\nstep.new4 = lm(y ~ cut(x, breaks = breaks4))\nsummary(step.new4)\n\n\nCall:\nlm(formula = y ~ cut(x, breaks = breaks4))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.4803  -4.6239  -0.4239   2.8968  20.6197 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      29.3803     0.4415  66.540   &lt;2e-16 ***\ncut(x, breaks = breaks4)(10,20] -10.4563     0.6281 -16.648   &lt;2e-16 ***\ncut(x, breaks = breaks4)(20,30] -16.6770     0.9383 -17.773   &lt;2e-16 ***\ncut(x, breaks = breaks4)(30,38] -18.6886     1.9331  -9.668   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.519 on 501 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.4925,    Adjusted R-squared:  0.4895 \nF-statistic: 162.1 on 3 and 501 DF,  p-value: &lt; 2.2e-16\n\n\nWe can now make predictions at new data points using the constructed linear model as usual.\n\nnewx &lt;- c(10.56, 5.89)\npreds = predict(step.new4, newdata = list(x = newx), se = TRUE)\npreds\n\n$fit\n       1        2 \n18.92394 29.38028 \n\n$se.fit\n        1         2 \n0.4466955 0.4415432 \n\n$df\n[1] 501\n\n$residual.scale\n[1] 6.519307"
  },
  {
    "objectID": "teaching/stat-learn/material/08/08-nonlinearity.html#regression-splines",
    "href": "teaching/stat-learn/material/08/08-nonlinearity.html#regression-splines",
    "title": "‘Non-Linear’ Linear Regression",
    "section": "",
    "text": "For this analysis we will require package splines.\n\nlibrary(splines)\n\nInitially let’s fit regression splines by specifying knots. From the previous plot it is not clear where exactly we should place knots, so we will make use of the command summary in order to find the 25th, 50th and 75th percentiles ofx, which will be the positions where we will place the knots. We also sort the variable x before fitting the splines.\n\nsummary(x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.73    6.95   11.36   12.65   16.95   37.97 \n\ncuts = summary(x)[c(2, 3, 5)] \ncuts\n\n1st Qu.  Median 3rd Qu. \n  6.950  11.360  16.955 \n\nsort.x = sort(x)\n\nFor a start lets fit a linear spline using our selected placement of knots. For this we can use command lm() and inside it we use the command bs() in which we specify degree = 1 for a linear spline and knots = cuts for the placement of the knots at the three percentiles. We also calculate the corresponding fitted values and confidence intervals exactly in the same way we did in previous practical demonstrations.\n\nspline1 = lm(y ~  splines::bs(x, degree = 1, knots = cuts))\npred1 = predict(spline1, newdata = list(x = sort.x), se = TRUE)\nse.bands1 = cbind(pred1$fit + 2 * pred1$se.fit, \n                  pred1$fit - 2 * pred1$se.fit)\n\nLet’s plot the results:\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Linear Spline\", bty = 'l')\nlines(sort.x, pred1$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands1, lwd = 2, col = \"firebrick\", lty = 3)\n\n\n\n\n\n\n\n\nUsing ?bs we see that instead of using the argument knots we can use the argument df, which are the degrees of freedom. Splines have \\((d+1)+K\\) degrees of freedom, where \\(d\\) is the degree of the polynomial and \\(K\\) the number of knots. So in this case we have 1+1+3 = 5 degrees of freedom. Selecting df = 5 in bs() will automatically use 3 knots placed at the 25th, 50th and 75th percentiles. Below we check whether the plot based on df=5 is indeed the same as the previous plot and as we can see it is.\n\nspline1df = lm(y ~ splines::bs(x, degree = 1, df = 5))\npred1df = predict(spline1df, newdata = list(x = sort.x), se = TRUE)\nse.bands1df = cbind( pred1df$fit + 2 * pred1df$se.fit, \n                     pred1df$fit - 2 * pred1df$se.fit )\n\npar(mfrow = c(1, 2))\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Linear Spline (with knots)\", bty = 'l')\nlines(sort.x, pred1$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands1, lwd = 2, col = \"firebrick\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Linear Spline (with df)\", bty = 'l')\nlines(sort.x, pred1df$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands1df, lwd = 2, col = \"firebrick\", lty = 3)\nmatlines(sort.x, se.bands1, lwd = 2, col = \"firebrick\", lty = 3)\n\n\n\n\n\n\n\n\nHaving seen how this works we can also fit a degree-2 (quadratic) and degree-3 (cubic) spline to the data, all we have to do is change degree = 1 to degree = 2 and degree = 3 respectively. Also we increase the respective degrees of freedom from df = 5 to df = 6 and df = 7 in order to keep the same number (and position) of knots in the quadratic and cubic spline models.\n\nspline2 = lm(y ~ splines::bs(x, degree = 2, df = 6))\npred2 = predict(spline2, newdata = list(x = sort.x), se = TRUE)\nse.bands2 = cbind(pred2$fit + 2 * pred2$se.fit, pred2$fit - 2 * pred2$se.fit)\n\nspline3 = lm(y ~ splines::bs(x, degree = 3, df = 7))\npred3 = predict(spline3, newdata = list(x = sort.x), se = TRUE)\nse.bands3 = cbind(pred3$fit + 2 * pred3$se.fit, pred3$fit - 2 * pred3$se.fit)\n\npar(mfrow = c(1,3))\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Linear Spline\", bty = 'l')\nlines(sort.x, pred1$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands1, lwd = 2, col = \"firebrick\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Quadratic Spline\", bty = 'l')\nlines(sort.x, pred2$fit, lwd = 2, col = \"darkgreen\")\nmatlines(sort.x, se.bands2, lwd = 2, col = \"darkgreen\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Cubic Spline\", bty = 'l')\nlines(sort.x, pred3$fit, lwd = 2, col = \"royalblue\")\nmatlines(sort.x, se.bands3, lwd = 2, col = \"royalblue\", lty = 3)"
  },
  {
    "objectID": "teaching/stat-learn/material/08/08-nonlinearity.html#natural-splines",
    "href": "teaching/stat-learn/material/08/08-nonlinearity.html#natural-splines",
    "title": "‘Non-Linear’ Linear Regression",
    "section": "",
    "text": "For natural splines, we can use the command ns(). As with the command bs() previously, we again have the option to either specify the knots manually (via the argument knots) or to simply pre-define the degrees of freedom (via the argument df). Below we use the latter option to fit four natural splines with 1, 2, 3 and 4 degrees of freedom. As we see using 1 degree of freedom actually results in just a linear model.\n\nspline.ns1 = lm(y ~ splines::ns(x, df = 1))\npred.ns1 = predict(spline.ns1, newdata = list(x = sort.x), se = TRUE)\nse.bands.ns1 = cbind(pred.ns1$fit + 2 * pred.ns1$se.fit, \n                     pred.ns1$fit - 2 * pred.ns1$se.fit)\n\nspline.ns2 = lm(y ~ splines::ns(x, df = 2))\npred.ns2 = predict(spline.ns2, newdata = list(x = sort.x), se = TRUE)\nse.bands.ns2 = cbind(pred.ns2$fit + 2 * pred.ns2$se.fit, \n                     pred.ns2$fit - 2 * pred.ns2$se.fit)\n\nspline.ns3 = lm(y ~ splines::ns(x, df = 3))\npred.ns3 = predict(spline.ns3, newdata = list(x = sort.x), se = TRUE)\nse.bands.ns3 = cbind(pred.ns3$fit + 2 * pred.ns3$se.fit, \n                     pred.ns3$fit - 2 * pred.ns3$se.fit)\n\nspline.ns4 = lm(y ~ splines::ns(x, df = 4))\npred.ns4 = predict(spline.ns4, newdata = list(x = sort.x), se = TRUE)\nse.bands.ns4 = cbind(pred.ns4$fit + 2 * pred.ns4$se.fit, \n                     pred.ns4$fit - 2 * pred.ns4$se.fit)\n\npar(mfrow = c(2, 2))\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Natural Spline (1 df)\", bty = 'l')\nlines(sort.x, pred.ns1$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands.ns1, lwd = 2, col = \"firebrick\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Natural Spline (2 df)\", bty = 'l')\nlines(sort.x, pred.ns2$fit, lwd = 2, col = \"darkviolet\")\nmatlines(sort.x, se.bands.ns2, lwd = 2, col = \"darkviolet\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Natural Spline (3 df)\", bty = 'l')\nlines(sort.x, pred.ns3$fit, lwd = 2, col = \"royalblue\")\nmatlines(sort.x, se.bands.ns3, lwd = 2, col = \"royalblue\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Natural Spline (4 df)\", bty = 'l')\nlines(sort.x, pred.ns4$fit, lwd = 2, col = \"darkgreen\")\nmatlines(sort.x, se.bands.ns4, lwd = 2, col = \"darkgreen\", lty = 3)\n\n\n\n\n\n\n\n\nBelow we plot the cubic spline next to the natural cubic spline for comparison. As we can see, the natural cubic spline is generally smoother and closer to linear on the right boundary of the predictor space, where it has, additionally, narrower confidence intervals in comparison to the cubic spline.\n\npar(mfrow = c(1,2))\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Cubic Spline\", bty = 'l')\nlines(sort.x, pred3$fit, lwd = 2, col = \"darkblue\")\nmatlines(sort.x, se.bands3, lwd = 2, col = \"darkblue\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Natural Spline (3 df)\", bty = 'l')\nlines(sort.x, pred.ns3$fit, lwd = 2, col = \"royalblue\")\nmatlines(sort.x, se.bands.ns3, lwd = 2, col = \"royalblue\", lty = 3)"
  },
  {
    "objectID": "teaching/stat-learn/material/08/08-nonlinearity.html#smoothing-splines",
    "href": "teaching/stat-learn/material/08/08-nonlinearity.html#smoothing-splines",
    "title": "‘Non-Linear’ Linear Regression",
    "section": "",
    "text": "For fitting smoothing splines we use the command smooth.splines() instead of lm(). Under smoothing splines there are no knots to specify; the only parameter is \\(\\lambda\\). This can be specified via cross-validation by specifying cv = TRUE inside smooth.splines(). Alternatively, we can specify the effective degrees of freedom which correspond to some value of \\(\\lambda\\). Below we first fit a smoothing spline with 3 effective degrees of freedom (via the argument df = 3), and then also by tuning \\(\\lambda\\) via cross-validation. In this case we see that tuning \\(\\lambda\\) through cross-validation results in a curve which is slightly wiggly on the right boundary of the predictor space.\n\nsmooth1 = smooth.spline(x, y, df = 3)\nsmooth2 = smooth.spline(x, y, cv = TRUE)\n\nWarning in smooth.spline(x, y, cv = TRUE): cross-validation with non-unique 'x'\nvalues seems doubtful\n\npar(mfrow = c(1,2))\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Smoothing Spline (3 df)\", bty = 'l')\nlines(smooth1, lwd = 2, col = \"brown\")\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Smoothing Spline (CV)\", bty = 'l')\nlines(smooth2, lwd = 2, col = \"darkorange\")\n\n\n\n\n\n\n\n\nNote: the effective degrees of freedom of a smoothing spline are similar to the degrees of freedom in standard spline models and can be used as an alternative to cross-validation as a way to fix \\(\\lambda\\)."
  },
  {
    "objectID": "teaching/stat-learn/material/08/08-nonlinearity.html#gams",
    "href": "teaching/stat-learn/material/08/08-nonlinearity.html#gams",
    "title": "‘Non-Linear’ Linear Regression",
    "section": "",
    "text": "In this final part we will fit a generalized additive model (GAM) utilizing more than one predictor from the Boston dataset.We first use the command names() in order to check once again the available predictor variables.\n\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n\nLet’s say that we want to use predictors lstat, indus and chas for the analysis (use ?Boston again to check what these refer to).\nFor GAMs we will make use of the library gam, so the first thing that we have to do is to install this package by executing install.packages(\"gam\") once. Then we load the library.\n\nlibrary(gam)\n\nThe main function is gam(). Inside this function we can use any combination of non-linear and linear modelling of the various predictors. For example below we use a cubic spline with 5 degrees of freedom for lstat, a smoothing spline with 5 degrees of freedom for indus and a simple linear model for variable chas. We then plot the contributions of each predictor using the command plot(). As we can see, GAMs are very useful as they estimate the contribution of the effects of each predictor.\n\ngam = gam( medv ~ bs(lstat, degree = 3, df = 5) + s(indus, df = 5) + chas, \n           data = Boston )\npar( mfrow = c(1,3) )\nplot( gam,  se = TRUE, col = \"blue\" )\n\n\n\n\n\n\n\n\nNote that simply using chas inside gam() is just fitting a linear model for this variable. However, one thing that we observe is that chas is a binary variable as it only takes the values of 0 and 1. This we can see from the x-axis of the chas plot on the right above. So, it would be preferable to use a step function for this variable. In order to do this we have to change the variable chas to a factor. We first create a second object called Boston1 (in order not to change the initial dataset Boston) and then we use the command factor() to change variable chas. Then we fit again the same model. As we can see below now gam() fits a step function for variable chas which is more appropriate.\n\nBoston1 = Boston\nBoston1$chas = factor(Boston1$chas)\n\ngam1 = gam( medv ~ bs(lstat, degree = 3, df = 5) + s(indus, df = 5) + chas, \n            data = Boston1 )\npar(mfrow = c(1,3))\nplot(gam1,  se = TRUE, col = \"blue\")\n\n\n\n\n\n\n\n\nWe can make predictions from gam objects, just like lm objects, using the predict() method for the class gam. Here we make predictions on some new data. Note that when assigning the value 0 to chas, we enclose it in “” since we informed R to treat chas as a categorical factor with two levels: “0” and “1”.\n\npreds &lt;- predict( gam1, \n                  newdata = data.frame( chas = \"0\", indus = 3, lstat = 5 )  )\npreds\n\n       1 \n32.10065"
  },
  {
    "objectID": "teaching/stat-learn/material/08/08-nonlinearity.html#classwork-exercises",
    "href": "teaching/stat-learn/material/08/08-nonlinearity.html#classwork-exercises",
    "title": "‘Non-Linear’ Linear Regression",
    "section": "",
    "text": "Load libraries MASS and faraway (contains dataset seatpos needed which includes daat on car seat positioning depending on driver size).\nWe will analyse the effects of predictor variable Ht on the response variable hipcenter. Assign the hipcenter values to a vector y, and the Ht variable values to a vector x.\nPlot variable Ht and hipcenter against each other. Remember to include suitable axis labels. From visual inspection of this plot, what sort of polynomial might be appropriate for this data?\nFit a first and second order polynomial to the data using the commands lm and poly. Look at the corresponding summary objects. Do these back up your answer to above?\nPlot the first and second polynomial fits to the data, along with ±2 standard deviation confidence intervals. What do you notice about the degree-2 polynomial plot?\nPerform an analysis of variance to confirm whether higher order degrees of polynomial are useful for modelling hipcenter based on Ht.\nUse step function regression with 5 cut-points to model hipcenter based on Ht. Plot the results.\n\n\n\n\nWe will here again analyze the effects of predictor variable Ht on the response variable hipcenter.\n\nLoad the necessary packages for the dataset and spline functions. Assign the hipcenter values to a vector y, and the Ht variable values to a vector x.\nFind the 25th, 50th and 75th percentiles of x, storing them in a vector cuts.\nUse a linear spline to model hipcenter as a function of Htv, putting knots at the 25th, 50th and 75th percentiles ofx`.\nPlot the fitted linear spline from part (c) over the data, along with ±2 standard deviation confidence intervals.\nUse a smoothing spline to model hipcenter as a function of Ht, selecting\n\\(\\lambda\\) with cross-validation, and generate a relevant plot. What do you notice?\n\n\n\n\n\nFit a GAM for hipcenter that consists of three terms:\n\n\na natural spline with 5 degrees of freedom for Age,\na smoothing spline with 3 degrees of freedom for Thigh\na simple linear model term for Ht.\n\n\nPlot the resulting contributions of each term to the GAM, and compare them with plots of hipcenter against each of the three variables Age, Thigh and Ht.\nDoes the contribution of each term of the GAM make sense in light of these pair-wise plots. Is the GAM fitting the data well?"
  },
  {
    "objectID": "teaching/stat-learn/material/06/06-model-validation.html",
    "href": "teaching/stat-learn/material/06/06-model-validation.html",
    "title": "Model Validation",
    "section": "",
    "text": "We learned about 3 types of model validation that help us estimate how well our model might do on data it has never seen before.\n\nTrain Test Split (TTS) (aka validation vet approach): We take our data and break it up into two groups, training (used to fit model) and testing (used to see how the model does on data it has never seen before)\nK-Fold Cross Validation (KF): We take our data and break it up into K groups. We train K different models using a different group as the test set each time. The other K-1 groups are used to train the model.\nLeave One Out Cross Validation (LOOCV): Like K-Fold but each data point is it’s own fold. This means we fit N models (where N is the number of data points) using N-1 data points to train, and 1 data point to test.\n\nRemember the purpose of a test set is to be UNSEEN data. We should NEVER fit ANYTHING on the test set. In fact we should not even TOUCH the test set until our model is completely done training."
  },
  {
    "objectID": "teaching/stat-learn/material/06/06-model-validation.html#test-train-split",
    "href": "teaching/stat-learn/material/06/06-model-validation.html#test-train-split",
    "title": "Model Validation",
    "section": "1.1 Test-Train-split",
    "text": "1.1 Test-Train-split\nIn this approach, the available data is divided into two subsets: a training set and a validation set. The training set is used to train the model, and the validation set is used to evaluate its performance. Predictions done by this method could be largely affected by the subset of observations used in testing set. If the test set is not representative of the entire data, this method may lead to overfitting.\n\n### Data splitting\n\n# set seed to generate a reproducible random sample\nset.seed(123)\n\n# create training and testing data set using index, training data contains 80% of the data set\n# 'list = FALSE' allows us to create a matrix data structure with the indices of the observations in the subsets along the rows.\ntrain.index.vsa &lt;- createDataPartition(iris$Species, p= 0.8, list = FALSE)\ntrain.vsa &lt;- iris[train.index.vsa,]\ntest.vsa &lt;- iris[-train.index.vsa,]\n\n# see how the the subsets are randomized\nrole = rep('train',nrow(iris))\nrole[-train.index.vsa] = 'test'\nggplot(data = cbind(iris,role)) + \n  geom_point(aes(x = Sepal.Length,\n                 y = Petal.Width,\n                 color = role)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n### Training: linear model is fit using all availbale predictors\nmodel.vsa &lt;- lm(Petal.Width ~., data = train.vsa)\n\n\n### Testing\npredictions.vsa &lt;- model.vsa %&gt;% predict(test.vsa)\n\n\n### Evaluating\ndata.frame(RMSE = RMSE(predictions.vsa, test.vsa$Petal.Width),\n           R2 = R2(predictions.vsa, test.vsa$Petal.Width),\n           MAE = MAE(predictions.vsa, test.vsa$Petal.Width))\n\n       RMSE        R2      MAE\n1 0.1675093 0.9497864 0.128837"
  },
  {
    "objectID": "teaching/stat-learn/material/06/06-model-validation.html#leave-one-out-cross-validation",
    "href": "teaching/stat-learn/material/06/06-model-validation.html#leave-one-out-cross-validation",
    "title": "Model Validation",
    "section": "1.2 Leave-One-Out Cross Validation",
    "text": "1.2 Leave-One-Out Cross Validation\n\n# Data splitting: leave one out\ntrain.loocv &lt;- trainControl(method = \"LOOCV\")\n\n# Training\nmodel.loocv &lt;- train(Petal.Width ~.,\n                     data = iris,\n                     method = \"lm\",\n                     trControl = train.loocv)\n\n#  Present results\nprint(model.loocv)\n\nLinear Regression \n\n150 samples\n  4 predictor\n\nNo pre-processing\nResampling: Leave-One-Out Cross-Validation \nSummary of sample sizes: 149, 149, 149, 149, 149, 149, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.1705606  0.9496003  0.1268164\n\nTuning parameter 'intercept' was held constant at a value of TRUE"
  },
  {
    "objectID": "teaching/stat-learn/material/06/06-model-validation.html#k-fold-cross-validation",
    "href": "teaching/stat-learn/material/06/06-model-validation.html#k-fold-cross-validation",
    "title": "Model Validation",
    "section": "1.3 K-Fold Cross Validation",
    "text": "1.3 K-Fold Cross Validation\n\n# Data splitting\n\n# set seed to generate a reproducible random sample\nset.seed(123)\n# the number of K is set to be 5\ntrain.kfold &lt;- trainControl(method = \"cv\", number = 5)\n\n# Training\nmodel.kfold &lt;- train(Petal.Width ~.,\n                     data = iris,\n                     method = \"lm\",\n                     trControl = train.kfold)\n\n# Present results\nprint(model.kfold)\n\nLinear Regression \n\n150 samples\n  4 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 122, 120, 118, 121, 119 \nResampling results:\n\n  RMSE       Rsquared   MAE    \n  0.1704321  0.9514251  0.12891\n\nTuning parameter 'intercept' was held constant at a value of TRUE"
  },
  {
    "objectID": "teaching/stat-learn/material/06/06-model-validation.html#repeated-k-fold-cross-validation",
    "href": "teaching/stat-learn/material/06/06-model-validation.html#repeated-k-fold-cross-validation",
    "title": "Model Validation",
    "section": "1.4 Repeated K-Fold Cross Validation",
    "text": "1.4 Repeated K-Fold Cross Validation\n\n1.4.1 Data splitting\n\n# set seed to generate a reproducible random sample\nset.seed(123)\n# the number of K is set to be 5\ntrain.rkfold &lt;- trainControl(method = \"repeatedcv\", number = 5, repeats = 3)\n\n### Training\nmodel.rkfold &lt;- train(Petal.Width ~.,\n                     data = iris,\n                     method = \"lm\",\n                     trControl = train.rkfold)\n\n# Present results\nmodel.rkfold\n\nLinear Regression \n\n150 samples\n  4 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold, repeated 3 times) \nSummary of sample sizes: 122, 120, 118, 121, 119, 119, ... \nResampling results:\n\n  RMSE      Rsquared   MAE      \n  0.168445  0.9525634  0.1266377\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\nmodel.kfold\n\nLinear Regression \n\n150 samples\n  4 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 122, 120, 118, 121, 119 \nResampling results:\n\n  RMSE       Rsquared   MAE    \n  0.1704321  0.9514251  0.12891\n\nTuning parameter 'intercept' was held constant at a value of TRUE"
  },
  {
    "objectID": "teaching/stat-learn/material/06/06-model-validation.html#lets-summarize-the-results",
    "href": "teaching/stat-learn/material/06/06-model-validation.html#lets-summarize-the-results",
    "title": "Model Validation",
    "section": "1.5 Let’s summarize the results",
    "text": "1.5 Let’s summarize the results\n\n\n\nCV method\nRMSE\nR2\nMAE\n\n\n\n\nValidation Set\n0.1675\n0.9498\n0.1288\n\n\nLOOCV\n0.1706\n0.9496\n0.1268\n\n\nK-Fold\n0.1704\n0.9514\n0.1289\n\n\nK-Fold repeat\n0.1704\n0.9514\n0.1289\n\n\n\n\n1.5.1 Question\n\nWhat do you note?"
  },
  {
    "objectID": "teaching/stat-learn/material/06/06-model-validation.html#knn-classifier-algorithm-for-univariate-xs-and-binary-ys",
    "href": "teaching/stat-learn/material/06/06-model-validation.html#knn-classifier-algorithm-for-univariate-xs-and-binary-ys",
    "title": "Model Validation",
    "section": "2.1 KNN classifier algorithm (for univariate \\(x\\)’s and binary \\(y\\)’s)",
    "text": "2.1 KNN classifier algorithm (for univariate \\(x\\)’s and binary \\(y\\)’s)\nWe create a function called KNN⁠ for performing KNN regression using the following arguments:\n\n\\(x_0\\) as the new point at which we wish to predict \\(y\\)\n\\({\\bf x} = (x_1,x_2, \\dots, x_n)\\) as the vector of training \\(x\\)’s\n\\({\\bf y} = (y_1,y_2, \\dots, y_n)\\) as the vector of training \\(y\\)’s\n\\(K\\) as number of neighbors to use\n\\(\\hat{y}_0\\) as the predicted value of \\(y\\) at \\(x_0\\)\n\nThe function calculates the Euclidean distance between \\(x_0\\) and each of the \\(x_i\\)’s in the training set \\((x_1, x_2, \\dots, x_n)\\). Then we order them from nearest to furthest away and takes the mean of the \\(y\\) values of the \\(K\\) nearest points yielding the predicted value of \\(y\\):\n\n#   x0 = new point at which to predict y\n#   x = (x_1,...,x_n) = vector of training x's\n#   y = (y_1,...,y_n) = vector of training y's\n#   K = number of neighbors to use\n#   y0_hat = predicted value of y at x0\n\nKNN = function(x0, x, y, K) {\n  distances = abs(x - x0) \n  o = order(distances) \n  y0_hat = mean(y[o[1:K]]) \n  return(y0_hat)  \n}"
  },
  {
    "objectID": "teaching/stat-learn/material/06/06-model-validation.html#simulate-data",
    "href": "teaching/stat-learn/material/06/06-model-validation.html#simulate-data",
    "title": "Model Validation",
    "section": "2.2 Simulate data",
    "text": "2.2 Simulate data\nWe simulate training vector \\(\\bf{x}\\) from a uniform distribution on the interval \\([0,5]\\) and simulate training vector \\(\\bf{y}\\) by assuming \\[y = f(x) + \\varepsilon\\] where \\(f(x) = \\cos(x)\\) and \\(\\varepsilon \\sim N(0, \\sigma^2)\\) and \\(\\sigma = 0.3\\).\n\nset.seed(1)  # set random number generator\nn = 20  # number of samples\nx = 5*runif(n)  \nsigma = 0.3  \nf = function(x) { cos(x) }  \ny = f(x) + sigma*rnorm(n)  \n\nLet’s plot of the training data\n\nplot(x,y,col=2,pch=20,cex=2)  # plot training data\nx_grid = seq(from=0, to=5, by=0.01)  # grid of x values for plotting f(x) values\nlines(x_grid,f(x_grid))  # plot true f(x) values for the grid\n\n\n\n\n\n\n\n\nNow we run the KNN function to predict \\(y\\) at each point on the grid of \\(x\\) values. For that we need to define \\(K\\), that is number of nearest neighbors to use. We start with setting it equal to 1 but this can be changed later as an exercise.\n\nK = 5 \ny_grid_hat = sapply(x_grid, function(x0) { KNN(x0, x, y, K) })\n\nNext we add the predicted values to our plot:\n\nplot(x,y,col=2,pch=20,cex=2)  # plot training data\ntitle(paste(\"K =\",K))\nlines(x_grid,f(x_grid))  # plot true f(x) values\nlines(x_grid,y_grid_hat,col=4)  # plot predicted y values \n\n\n\n\n\n\n\n\n\n2.2.1 Question\n\nWhat happens to predicted curve when you change the value of \\(K\\)?"
  },
  {
    "objectID": "teaching/stat-learn/material/06/06-model-validation.html#k-fold-cross-validation-1",
    "href": "teaching/stat-learn/material/06/06-model-validation.html#k-fold-cross-validation-1",
    "title": "Model Validation",
    "section": "2.3 K-Fold Cross Validation",
    "text": "2.3 K-Fold Cross Validation\nNow we are going to use cross validation to estimate test performance of the KNN classifier. We set number of neighbors as \\(K=1\\) and use the 10-fold cross validation. We do a random ordering of all the available data, and initialize a vector to hold MSE for each fold. For each fold, we then create a training and test (hold one out/validation) set, run KNN at each \\(x\\) in this test set (the one left out), and compute MSE on this test set. Then we average the MSE over all folds to obtain the CV estimate of test MSE:\n\nK = 1  \nnfolds = 10 \npermutation = sample(1:n)  \nMSE_fold = rep(0,nfolds)  \nfor (j in 1:nfolds) {\n    test = permutation[floor((j-1)*n/nfolds+1) : floor(j*n/nfolds)]  \n    train = setdiff(1:n, test)  \n    y_hat = sapply(x[test], function(x0) { KNN(x0, x[train], y[train], K) }) \n    MSE_fold[j] = mean((y[test] - y_hat)^2) \n}\nMSE_cv = mean(MSE_fold)  \nMSE_cv\n\n[1] 0.1241132\n\n\nNext we compare with the ground truth estimate of test performance, given this training set. Because this is a simulation example, we can generate lots of test data. We simulate \\(x\\)’s and \\(y\\)’s from the true data generating process. Then we run the KNN classifier at each \\(x\\) in the test set and compute the MSE on the test set:\n\nn_test = 100000\nx_test = 5*runif(n_test)  \ny_test = f(x_test) + sigma*rnorm(n_test)  \ny_test_hat = sapply(x_test, function(x0) { KNN(x0, x, y, K) })  \nMSE_test = mean((y_test - y_test_hat)^2)  \n\nLet’s compare the two values:\n\nMSE_test\n\n[1] 0.1656885\n\nMSE_cv\n\n[1] 0.1241132\n\n\nBe careful when calculating the root MSE (RMSE) since it corresponds to root mean squared error or square root of MSE: Let’s try with\n\nsqrt(MSE_test)  # test RMSE\n\n[1] 0.4070485\n\nsqrt(mean(MSE_fold))  # sqrt of MSE_cv\n\n[1] 0.352297\n\nmean(sqrt(MSE_fold))  # can we use this?\n\n[1] 0.3066843"
  },
  {
    "objectID": "teaching/stat-learn/material/06/06-model-validation.html#leave-one-out-cross-validation-1",
    "href": "teaching/stat-learn/material/06/06-model-validation.html#leave-one-out-cross-validation-1",
    "title": "Model Validation",
    "section": "2.4 Leave-One Out Cross Validation",
    "text": "2.4 Leave-One Out Cross Validation\nUse the leave-one out cross validation (LOOCV) in the above example and report the CV estimate of test MSE and the MSE given ground truth.\n\nK = 1  \nnfolds = n\npermutation = sample(1:n)  \nMSE_fold = rep(0,nfolds)  \nfor (j in 1:nfolds) {\n    test = permutation[floor((j-1)*n/nfolds+1) : floor(j*n/nfolds)]  \n    train = setdiff(1:n, test)  \n    y_hat = sapply(x[test], function(x0) { KNN(x0, x[train], y[train], K) }) \n    MSE_fold[j] = mean((y[test] - y_hat)^2) \n}\nMSE_loocv = mean(MSE_fold)  \nMSE_test = mean((y_test - y_test_hat)^2)  \nMSE_loocv\n\n[1] 0.1241132\n\nMSE_test\n\n[1] 0.1656885"
  },
  {
    "objectID": "teaching/stat-learn/material/06/06-model-validation.html#hyperparameter-tuning-choosing-model-settings",
    "href": "teaching/stat-learn/material/06/06-model-validation.html#hyperparameter-tuning-choosing-model-settings",
    "title": "Model Validation",
    "section": "2.5 Hyperparameter Tuning: Choosing Model Settings",
    "text": "2.5 Hyperparameter Tuning: Choosing Model Settings\nWith the following example, we will illustrate how to use cross validation to choose the optimal number of neighbors \\(K\\) in KNN. We start with a rather high number of \\(K\\) to try for KNN (\\(K=30\\)) and use 10 folds for each of these cases in the cross validation. Then we do a random ordering of data and initialize vector for holding MSE’s. For each number of folds in the range, we compute the training and test set as before (this is again the validation set). For each \\(K\\) up to 30, we then run KNN at each \\(x\\) in the test set (the one left out), and compute MSE on the this test set. We average across folds to obtain CV estimate of test MSE for each \\(K\\) and plot the results:\n\nK_max = 30 \nnfolds = 10  \npermutation = sample(1:n)  \nMSE_fold = matrix(0,nfolds,K_max)  \nfor (j in 1:nfolds) {\n    test = permutation[floor((j-1)*n/nfolds+1) : floor(j*n/nfolds)]  \n    train = setdiff(1:n, test) \n    for (K in 1:K_max) {\n        y_hat = sapply(x[test], function(x0) { KNN(x0, x[train], y[train], K) })\n        MSE_fold[j,K] = mean((y[test] - y_hat)^2)  \n    }\n}\nMSE_cv = colMeans(MSE_fold)  \n\nWe plot CV estimate of test MSE against number of neighbors \\(K=1,2,\\dots,30\\), and choose the value of \\(K\\) that minimizes estimated test MSE. Compare with a ground truth estimate of test performance by using the chosen number of \\(K\\) and running KNN on each \\(x\\) in the test set (denoted x_test above).\n\nplot(1:K_max, MSE_cv, pch=19)  # plot CV estimate of test MSE for each K\n\n\n\n\n\n\n\n# Choose the value of K that minimizes estimated test MSE\nK_cv = which.min(MSE_cv)\nK_cv\n\n[1] 3\n\n\n\n2.5.1 Question?\n\nWhy do you think the test performance estimate for the chosen \\(K\\) tend to be smaller than the ground truth estimate of test performance in this example?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\\(MSE_{cv}(K_{cv})\\) may systematically underestimate or overestimate test MSE! There are two sources of bias: \\(K_{cv}\\) is the minimum, and the pseudo-training set is smaller than \\(n\\)."
  },
  {
    "objectID": "teaching/stat-learn/material/06/06-model-validation.html#choosing-the-number-of-folds",
    "href": "teaching/stat-learn/material/06/06-model-validation.html#choosing-the-number-of-folds",
    "title": "Model Validation",
    "section": "2.6 Choosing the number of folds",
    "text": "2.6 Choosing the number of folds\nWe start by simulating training data as before:\n\nset.seed(1) \nn = 20\nx = 5*runif(n)  \nsigma = 0.3 \ny = f(x) + sigma*rnorm(n)  \n\nWe then compute “ground truth” estimate of test performance, given this training set. We set \\(K=10\\), and run KNN at each \\(x\\) in the test set and compute MSE on the test set:\n\nK = 10\ny_test_hat = sapply(x_test, function(x0) { KNN(x0, x, y, K) })  \nMSE_test = mean((y_test - y_test_hat)^2)  \n\nNext, we repeatedly run CV for a range of number of folds nfolds up to maximum \\(n=20\\) (same as \\(n\\) above in our simulated data). We repeat the simulation 200 times, and for each repetition and number of folds, we split the training data into training and test (hold one out/validation set). We run KNN at each \\(x\\) in this test set and compute MSE. We then average the MSE’s for each case with a different number of folds:\n\nnfolds_max = n  # maximum value of nfolds to use for CV\nnreps = 200  # number of times to repeat the simulation\nMSE_cv = matrix(0,nreps,nfolds_max)  \nfor (r in 1:nreps) {  \n    for (nfolds in 1:nfolds_max) {\n        permutation = sample(1:n) \n        MSE_fold = rep(0,nfolds)  \n        for (j in 1:nfolds) {\n            test = permutation[floor((j-1)*n/nfolds+1) : floor(j*n/nfolds)]  \n            train = setdiff(1:n, test)  \n            y_hat = sapply(x[test], function(x0) { KNN(x0, x[train], y[train], K) }) \n            MSE_fold[j] = mean((y[test] - y_hat)^2) \n        }\n        MSE_cv[r,nfolds] = mean(MSE_fold)\n    }\n}\n\nWe compute the MSE, bias, and variance of the CV estimate of test MSE, for each value of nfolds and plot MSE, bias^2, and variance of the CV estimate, for each value of nfolds.\n\nmse = colMeans((MSE_cv - MSE_test)^2)\nbias = colMeans(MSE_cv) - MSE_test\nvariance = apply(MSE_cv,2,var)\n\n# plot of MSE, bias^2 and variance against number of folds\nplot(1:nfolds_max, type=\"n\", ylim=c(0,max(mse[2:nfolds_max])*1.1), xlab=\"nfolds\", ylab=\"mse\", main=\"MSE of the CV estimates\")\nlines(1:nfolds_max, mse, col=1, lty=2, lwd=2, ylim=c(0,0.2))\nlines(1:nfolds_max, bias^2, col=2, lwd=2)\nlines(1:nfolds_max, variance, col=4, lwd=2)\nlegend(\"topright\", legend=c(\"mse\",\"bias^2\",\"variance\"), col=c(1,2,4), lwd=2)\n\n\n\n\n\n\n\n# plot bias against number of folds\nplot(1:nfolds_max, bias)\nlines(1:nfolds_max, bias, col=2, lwd=2)\n\n\n\n\n\n\n\n\n\n2.6.1 Question\nIn the above plot below, why do you think the bias of the CV estimate of test MSE is always positive?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBecause the “pseudo”-training set (each fold) is smaller than the training set."
  },
  {
    "objectID": "teaching/stat-learn/material/06/06-model-validation.html#k-fold-cross-validation-2",
    "href": "teaching/stat-learn/material/06/06-model-validation.html#k-fold-cross-validation-2",
    "title": "Model Validation",
    "section": "3.1 K-Fold Cross Validation",
    "text": "3.1 K-Fold Cross Validation\nNow we cross validate as follows: - 5-fold CV - z-score predictors inside each fold - Train/test split per fold - Fit linear regression - Compute train & test MSE/MAE - Print per-fold metrics + averages - returns clean summary tables\nNote also that we the package called Metrics here (which you loaded earlier):\n\nset.seed(123)\n\n# Set up 5-fold cross validation indices\nset.seed(123)\nfolds &lt;- createFolds(y, k = 5, returnTrain = TRUE)\n\nmse_train &lt;- c()\nmse_test  &lt;- c()\nmae_train &lt;- c()\nmae_test  &lt;- c()\n\n\n# Perform 5-fold CV\nfor (i in 1:5) {\n\n  train_idx &lt;- folds[[i]]\n  test_idx  &lt;- setdiff(seq_len(nrow(X)), train_idx)\n\n  X_train &lt;- X[train_idx, ]\n  X_test  &lt;- X[test_idx, ]\n  y_train &lt;- y[train_idx]\n  y_test  &lt;- y[test_idx]\n\n  # z-score within this fold\n  preproc &lt;- preProcess(X_train, method = c(\"center\", \"scale\"))\n  X_train_scaled &lt;- predict(preproc, X_train)\n  X_test_scaled  &lt;- predict(preproc, X_test)\n\n  # fit\n  model &lt;- lm(y_train ~ ., data = X_train_scaled)\n\n  # predict\n  y_pred_train &lt;- predict(model, newdata = X_train_scaled)\n  y_pred_test  &lt;- predict(model, newdata = X_test_scaled)\n\n  # metrics\n  mse_train[i] &lt;- mse(y_train, y_pred_train)\n  mse_test[i]  &lt;- mse(y_test, y_pred_test)\n\n  mae_train[i] &lt;- mae(y_train, y_pred_train)\n  mae_test[i]  &lt;- mae(y_test, y_pred_test)\n}\n\n# results using tidyverse tibble\ncv_results &lt;- tibble(\n  Fold      = 1:5,\n  Train_MSE = mse_train,\n  Test_MSE  = mse_test,\n  Train_MAE = mae_train,\n  Test_MAE  = mae_test\n)\n\ncv_results\n\n# A tibble: 5 × 5\n   Fold Train_MSE Test_MSE Train_MAE Test_MAE\n  &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1     1      8.84    22.2       2.02     2.35\n2     2     11.1      9.21      2.17     2.24\n3     3     10.9     11.0       2.21     2.37\n4     4      9.84    14.2       2.16     2.05\n5     5     11.2      8.88      2.23     2.27\n\n# averaged over all folds\nkfold_summary &lt;- tibble(\n  Mean_Train_MSE = mean(mse_train),\n  Mean_Test_MSE  = mean(mse_test),\n  Mean_Train_MAE = mean(mae_train),\n  Mean_Test_MAE  = mean(mae_test)\n)\n\nkfold_summary\n\n# A tibble: 1 × 4\n  Mean_Train_MSE Mean_Test_MSE Mean_Train_MAE Mean_Test_MAE\n           &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;\n1           10.4          13.1           2.16          2.26"
  },
  {
    "objectID": "teaching/stat-learn/material/06/06-model-validation.html#leave-one-out-cross-validation-2",
    "href": "teaching/stat-learn/material/06/06-model-validation.html#leave-one-out-cross-validation-2",
    "title": "Model Validation",
    "section": "3.2 Leave-One-Out Cross Validation",
    "text": "3.2 Leave-One-Out Cross Validation\nThe below code does the following: loads & cleans the data - extracts predictors and response - performs LOOCV manually - z-scores inside each fold - computes train/test MSE & MAE - returns clean summary tables\n\nset.seed(123)\n\n\n# Set up predictors & response as before\n\npredictors &lt;- c(\"List.Price\", \"NumPages\", \"Weight.oz\", \"Thick\", \"Height\", \"Width\")\n\nX &lt;- ama[, predictors]\ny &lt;- ama$Amazon.Price\nn &lt;- nrow(X)\n\n# LOOCV: Leave-One-Out Cross Validation\n\nmse_train &lt;- numeric(n)\nmse_test  &lt;- numeric(n)\nmae_train &lt;- numeric(n)\nmae_test  &lt;- numeric(n)\n\nfor (i in 1:n) {\n\n  # train/test split\n  test_idx  &lt;- i\n  train_idx &lt;- setdiff(1:n, test_idx)\n\n  X_train &lt;- X[train_idx, ]\n  X_test  &lt;- X[test_idx, , drop = FALSE]\n  y_train &lt;- y[train_idx]\n  y_test  &lt;- y[test_idx]\n\n  # Z-score within fold \n  train_means &lt;- apply(X_train, 2, mean)\n  train_sds   &lt;- apply(X_train, 2, sd)\n\n  X_train_scaled &lt;- scale(X_train, center = train_means, scale = train_sds)\n  X_test_scaled  &lt;- sweep(sweep(X_test, 2, train_means), 2, train_sds, \"/\")\n\n  # Fit linear model\n\n  model &lt;- lm(y_train ~ ., data = as.data.frame(X_train_scaled))\n\n  # Predictions\n  y_pred_train &lt;- predict(model, newdata = as.data.frame(X_train_scaled))\n  y_pred_test  &lt;- predict(model, newdata = as.data.frame(X_test_scaled))\n\n  # Metrics\n  mse_train[i] &lt;- mse(y_train, y_pred_train)\n  mse_test[i]  &lt;- mse(y_test, y_pred_test)\n\n  mae_train[i] &lt;- mae(y_train, y_pred_train)\n  mae_test[i]  &lt;- mae(y_test, y_pred_test)\n}\n\n# Results\nresults &lt;- tibble(\n  Train_MSE = mse_train,\n  Test_MSE  = mse_test,\n  Train_MAE = mae_train,\n  Test_MAE  = mae_test\n)\n\n\n# averaged over all folds\nloo_summary &lt;- tibble(\n  Mean_Train_MSE = mean(mse_train),\n  Mean_Test_MSE  = mean(mse_test),\n  Mean_Train_MAE = mean(mae_train),\n  Mean_Test_MAE  = mean(mae_test)\n)\n\nloo_summary\n\n# A tibble: 1 × 4\n  Mean_Train_MSE Mean_Test_MSE Mean_Train_MAE Mean_Test_MAE\n           &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;\n1           10.6          13.8           2.16          2.27\n\n\nNow visualize the results:\n\n# Put into a single tibble\nresults &lt;- bind_rows(\n  kfold_summary %&gt;%  mutate(Method = \"K-Fold (K=5)\"),\n  loo_summary   %&gt;%  mutate(Method = \"LOO\")\n)\n\n# Convert to long format\nresults_long &lt;- results |&gt; \n  pivot_longer(cols = starts_with(\"Mean\"),\n               names_to = \"Metric\",\n               values_to = \"Value\")\n\n# Nice readable metric labels\nresults_long$Metric &lt;- recode(results_long$Metric,\n  \"Mean_Train_MSE\" = \"Train MSE\",\n  \"Mean_Test_MSE\"  = \"Test MSE\",\n  \"Mean_Train_MAE\" = \"Train MAE\",\n  \"Mean_Test_MAE\"  = \"Test MAE\"\n)\n\n# Plot\nggplot(results_long, aes(x = Metric, y = Value, fill = Method)) +\n  geom_col(position = \"dodge\") +\n  labs(title = \"K-Fold vs LOOCV: Average Performance Metrics\",\n       x = \"\",\n       y = \"Error\") +\n  theme_minimal(base_size = 14) +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\n\n3.2.1 Questions\nUsing the above plot, what can you conclude about:\n\nHow well the linear regression model generalizes to new data, and\nWhich validation method you would use to estimate the linear regression model’s test error, and why?\nUsing these results, explain how the bias–variance tradeoff helps us understand why K-Fold CV and LOOCV give slightly different test errors, even though both methods fit the same model on nearly the same data. What do your results suggest about how bias and variance differ between K-Fold CV and LOOCV for this problem?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nBoth K-Fold CV and LOOCV are applied to the same linear regression model; the only thing that changes is how we estimate its out-of-sample error, not the model itself.\nFrom the plot:\n\nTrain MSE/MAE are almost identical for K-Fold and LOOCV, which indicates that the fitted lm model itself is behaving the same in both setups (as expected, since it’s the same model and same data).\nTest MSE is slightly lower for K-Fold than for LOOCV, and Test MAE is essentially the same. This suggests that, for this lm model, K-Fold gives an estimate of test error that is at least as good as (and in this case a bit better than) LOOCV.\n\n\nThe lm model’s generalization performance appears very similar under both methods, with K-Fold showing marginally better test MSE, and\n\nLOOCV is far more computationally expensive (it refits lm once per observation, versus only 5 times for 5-fold CV),\n\nThe reasonable choice is to use K-Fold CV to evaluate this linear regression model. It provides a practically equivalent (or slightly better) estimate of the lm model’s test error at a much lower computational cost.\n\nThe bias–variance tradeoff explains the small differences we observe between K-Fold CV and LOOCV in the plot.\n\n\nLOOCV uses almost the full dataset for every training fold, so its estimates have\n\nlower bias (its training sets closely mimic the full dataset)\n\nhigher variance (each fold differs by only one observation, making the fitted model extremely sensitive to individual points)\n\nK-Fold CV, in contrast, uses smaller training sets and reshuffles data more substantially between folds, leading to\n\nslightly higher bias\n\nlower variance\n\n\nIn our results, the test MSE for LOOCV is only slightly lower than K-Fold’s, which is consistent with LOOCV’s lower bias.\nHowever, the difference is small, and K-Fold’s lower variance means it typically provides a more stable estimate of true generalization error.\nThus, the bias–variance tradeoff helps explain why LOOCV and K-Fold CV produce similar but not identical estimates: LOOCV trades stability (higher variance) for slightly lower bias, whereas K-Fold provides a more reliable, lower-variance estimate of the model’s test error."
  },
  {
    "objectID": "teaching/stat-learn/material/06/06-model-validation.html#assumption-checks-with-tts-kfold-loo",
    "href": "teaching/stat-learn/material/06/06-model-validation.html#assumption-checks-with-tts-kfold-loo",
    "title": "Model Validation",
    "section": "3.3 Assumption Checks with TTS, KFold, LOO",
    "text": "3.3 Assumption Checks with TTS, KFold, LOO\nWhen checking assumptions AND using model validation, when do we check assumptions?\nCreating a residual plot requires we have residuals, which requires a model. Check assumptions after fitting the model by making residual plot(s).\n\n3.3.1 TTS\nWe need a model to check residuals, so do it AFTER the model is built. This is done using an “assumption” plot (residuals vs fitted):\n\n# Train/Test Split (80/20)\nset.seed(123)\ntrain_index &lt;- createDataPartition(y, p = 0.8, list = FALSE)\n\nX_train &lt;- X[train_index, ]\nX_test  &lt;- X[-train_index, ]\n\ny_train &lt;- y[train_index]\ny_test  &lt;- y[-train_index]\n\n\n# Z-scores\npreproc &lt;- preProcess(X_train, method = c(\"center\", \"scale\"))\n\nX_train_scaled &lt;- predict(preproc, X_train)\nX_test_scaled  &lt;- predict(preproc, X_test)\n\n# Fit linear regression\nmodel &lt;- lm(y_train ~ ., data = X_train_scaled)\n\n\n# Predict\ny_pred_train &lt;- predict(model, newdata = X_train_scaled)\ny_pred_test  &lt;- predict(model, newdata = X_test_scaled)\n\n\n# Assumption plot (residuals vs predictions)\nassump_train &lt;- tibble(\n  predicted = y_pred_train,\n  errors = y_train - y_pred_train\n)\n\nggplot(assump_train, aes(x = predicted, y = errors)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Residuals vs Fitted (Train Data)\")\n\n\n\n\n\n\n\n\n\n\n3.3.2 K-Fold\nSame check now but for K-fold CV used earlier with K=5.\n\nlibrary(patchwork) # to have several panes next to each other\n\nset.seed(123)\n\n# K-fold setup\nfolds &lt;- createFolds(y, k = 5, returnTrain = TRUE)\n\nmse_train &lt;- c()\nmse_test  &lt;- c()\n\nall_assump &lt;- list()  # store residuals per fold\n\nfor (i in seq_along(folds)) {\n  train_idx &lt;- folds[[i]]\n  test_idx  &lt;- setdiff(seq_len(nrow(X)), train_idx)\n\n  X_train &lt;- X[train_idx, ]\n  X_test  &lt;- X[test_idx, ]\n  y_train &lt;- y[train_idx]\n  y_test  &lt;- y[test_idx]\n\n  # z-score within fold\n  preproc &lt;- preProcess(X_train, method = c(\"center\", \"scale\"))\n  X_train_scaled &lt;- predict(preproc, X_train)\n  X_test_scaled  &lt;- predict(preproc, X_test)\n\n  # fit model\n  model &lt;- lm(y_train ~ ., data = X_train_scaled)\n\n  # predict\n  y_pred_train &lt;- predict(model, newdata = X_train_scaled)\n  y_pred_test  &lt;- predict(model, newdata = X_test_scaled)\n\n  # store residuals with fold id\n  all_assump[[i]] &lt;- tibble(\n    Fold = paste0(\"Fold \", i),\n    predicted = y_pred_train,\n    errors = y_train - y_pred_train\n  )\n\n  # metrics\n  mse_train[i] &lt;- mse(y_train, y_pred_train)\n  mse_test[i]  &lt;- mse(y_test,  y_pred_test)\n}\n\n# Bind all residuals into one data frame\nassump_df &lt;- bind_rows(all_assump)\n\n# 1) Residual plots faceted by fold \n\np_resid &lt;- ggplot(assump_df, aes(x = predicted, y = errors)) +\n  geom_point(alpha = 0.6) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  facet_wrap(~ Fold) +\n  theme_minimal() +\n  labs(\n    title = \"Residuals vs Fitted by Fold\",\n    x = \"Predicted (Train)\",\n    y = \"Residuals\"\n  )\n\n# 2) MSE barplot per fold\n\nmse_df &lt;- tibble(\n  Fold = paste0(\"Fold \", seq_along(mse_train)),\n  Train_MSE = mse_train,\n  Test_MSE  = mse_test\n) %&gt;% \n  pivot_longer(cols = c(Train_MSE, Test_MSE),\n               names_to = \"Set\",\n               values_to = \"MSE\")\n\np_mse &lt;- ggplot(mse_df, aes(x = Fold, y = MSE, fill = Set)) +\n  geom_col(position = \"dodge\") +\n  theme_minimal() +\n  labs(\n    title = \"Train/Test MSE by Fold\",\n    x = \"\",\n    y = \"MSE\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# 3) Combine with patchwork \n\np_resid / p_mse\n\n\n\n\n\n\n\n\nThe residual plots across the five folds show that the linear regression model behaves consistently regardless of how the data is split. Residuals remain centered around zero with a similar spread in each fold, suggesting that the model does not exhibit major bias and performs stably across training subsets. There is some mild heteroscedasticity, as errors tend to increase for higher predicted values, and a few outliers appear in each fold, which is expected in real-world price data.\nThe train and test MSE values show the expected pattern: training error is slightly lower than test error in every fold. Most folds have test MSE values in a similar range, indicating that the model generalizes reliably. Fold 1 exhibits a noticeably higher test MSE than the others, which likely reflects sampling variability rather than a structural issue with the model.\nOverall, the K-Fold results suggest that the linear regression model fits reasonably well, generalizes consistently across folds, and does not appear to be severely overfitting, although prediction accuracy decreases somewhat for higher-priced books."
  },
  {
    "objectID": "teaching/tidyverse-I/material/bechdel.html",
    "href": "teaching/tidyverse-I/material/bechdel.html",
    "title": "Bechdel",
    "section": "",
    "text": "In this mini analysis we work with the data used in the FiveThirtyEight story titled “The Dollar-And-Cents Case Against Hollywood’s Exclusion of Women”. We will together fill in the blanks denoted by ___."
  },
  {
    "objectID": "teaching/tidyverse-I/material/bechdel.html#data-and-packages",
    "href": "teaching/tidyverse-I/material/bechdel.html#data-and-packages",
    "title": "Bechdel",
    "section": "Data and packages",
    "text": "Data and packages\nWe start with loading the packages we’ll use.\n\nlibrary(fivethirtyeight)\nlibrary(tidyverse)\n\nThe dataset contains information on 1794 movies released between 1970 and 2013. However we’ll focus our analysis on movies released between 1990 and 2013.\n\nbechdel90_13 &lt;- bechdel %&gt;% \n  filter(between(year, 1990, 2013))\n\nThere are ___ such movies.\nThe financial variables we’ll focus on are the following:\n\nbudget_2013: Budget in 2013 inflation adjusted dollars\ndomgross_2013: Domestic gross (US) in 2013 inflation adjusted dollars\nintgross_2013: Total International (i.e., worldwide) gross in 2013 inflation adjusted dollars\n\nAnd we’ll also use the binary and clean_test variables for grouping."
  },
  {
    "objectID": "teaching/tidyverse-I/material/bechdel.html#analysis",
    "href": "teaching/tidyverse-I/material/bechdel.html#analysis",
    "title": "Bechdel",
    "section": "Analysis",
    "text": "Analysis\nLet’s take a look at how median budget and gross vary by whether the movie passed the Bechdel test, which is stored in the binary variable.\n\nbechdel90_13 %&gt;%\n  group_by(binary) %&gt;%\n  summarise(\n    med_budget = median(budget_2013),\n    med_domgross = median(domgross_2013, na.rm = TRUE),\n    med_intgross = median(intgross_2013, na.rm = TRUE)\n    )\n\n# A tibble: 2 × 4\n  binary med_budget med_domgross med_intgross\n  &lt;chr&gt;       &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 FAIL    48385984.    57318606.    104475669\n2 PASS    31070724     45330446.     80124349\n\n\nNext, let’s take a look at how median budget and gross vary by a more detailed indicator of the Bechdel test result. This information is stored in the clean_test variable, which takes on the following values:\n\nok = passes test\ndubious\nmen = women only talk about men\nnotalk = women don’t talk to each other\nnowomen = fewer than two women\n\n\nbechdel90_13 %&gt;%\n  #group_by(___) %&gt;%\n  summarise(\n    med_budget = median(budget_2013),\n    med_domgross = median(domgross_2013, na.rm = TRUE),\n    med_intgross = median(intgross_2013, na.rm = TRUE)\n    )\n\n# A tibble: 1 × 3\n  med_budget med_domgross med_intgross\n       &lt;int&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1   37878971     52270207     93523336\n\n\nIn order to evaluate how return on investment varies among movies that pass and fail the Bechdel test, we’ll first create a new variable called roi as the ratio of the gross to budget.\n\nbechdel90_13 &lt;- bechdel90_13 %&gt;%\n  mutate(roi = (intgross_2013 + domgross_2013) / budget_2013)\n\nLet’s see which movies have the highest return on investment.\n\nbechdel90_13 %&gt;%\n  arrange(desc(roi)) %&gt;% \n  select(title, roi, year)\n\n# A tibble: 1,615 × 3\n   title                     roi  year\n   &lt;chr&gt;                   &lt;dbl&gt; &lt;int&gt;\n 1 Paranormal Activity      671.  2007\n 2 The Blair Witch Project  648.  1999\n 3 El Mariachi              583.  1992\n 4 Clerks.                  258.  1994\n 5 In the Company of Men    231.  1997\n 6 Napoleon Dynamite        227.  2004\n 7 Once                     190.  2006\n 8 The Devil Inside         155.  2012\n 9 Primer                   142.  2004\n10 Fireproof                134.  2008\n# ℹ 1,605 more rows\n\n\nBelow is a visualization of the return on investment by test result, however it’s difficult to see the distributions due to a few extreme observations.\n\nggplot(data = bechdel90_13, \n       mapping = aes(x = clean_test, y = roi, color = binary)) +\n  geom_boxplot() +\n  labs(\n    title = \"Return on investment vs. Bechdel test result\",\n    x = \"Detailed Bechdel result\",\n    y = \"___\",\n    color = \"Binary Bechdel result\"\n    )\n\n\n\n\n\n\n\n\nWhat are those movies with very high returns on investment?\n\nbechdel90_13 %&gt;%\n  filter(roi &gt; 400) %&gt;%\n  select(title, budget_2013, domgross_2013, year)\n\n# A tibble: 3 × 4\n  title                   budget_2013 domgross_2013  year\n  &lt;chr&gt;                         &lt;int&gt;         &lt;dbl&gt; &lt;int&gt;\n1 Paranormal Activity          505595     121251476  2007\n2 The Blair Witch Project      839077     196538593  1999\n3 El Mariachi                   11622       3388636  1992\n\n\nZooming in on the movies with roi &lt; ___ provides a better view of how the medians across the categories compare:\n\nggplot(data = bechdel90_13, mapping = aes(x = clean_test, y = roi, color = binary)) +\n  geom_boxplot() +\n  labs(\n    title = \"Return on investment vs. Bechdel test result\",\n    subtitle = \"___\", # Something about zooming in to a certain level\n    x = \"Detailed Bechdel result\",\n    y = \"Return on investment\",\n    color = \"Binary Bechdel result\"\n    ) +\n  coord_cartesian(ylim = c(0, 15))"
  },
  {
    "objectID": "teaching/tidyverse-I/material/sales-excel.html",
    "href": "teaching/tidyverse-I/material/sales-excel.html",
    "title": "Sales",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl)\n\n\nRead in the Excel file called sales.xlsx from the data-raw/ folder such that it looks like the following.\n\n\n\n\n\n\n\n\n\n\n\nStretch goal: Manipulate the sales data such such that it looks like the following."
  },
  {
    "objectID": "teaching/tidyverse-I/material/starwars-ws.html",
    "href": "teaching/tidyverse-I/material/starwars-ws.html",
    "title": "Visualizing Starwars characters",
    "section": "",
    "text": "Glimpse at the starwars data frame.\n\n\nglimpse(starwars)\n\nRows: 87\nColumns: 14\n$ name       &lt;chr&gt; \"Luke Skywalker\", \"C-3PO\", \"R2-D2\", \"Darth Vader\", \"Leia Or…\n$ height     &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 2…\n$ mass       &lt;dbl&gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77.…\n$ hair_color &lt;chr&gt; \"blond\", NA, NA, \"none\", \"brown\", \"brown, grey\", \"brown\", N…\n$ skin_color &lt;chr&gt; \"fair\", \"gold\", \"white, blue\", \"white\", \"light\", \"light\", \"…\n$ eye_color  &lt;chr&gt; \"blue\", \"yellow\", \"red\", \"yellow\", \"brown\", \"blue\", \"blue\",…\n$ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57.0, …\n$ sex        &lt;chr&gt; \"male\", \"none\", \"none\", \"male\", \"female\", \"male\", \"female\",…\n$ gender     &lt;chr&gt; \"masculine\", \"masculine\", \"masculine\", \"masculine\", \"femini…\n$ homeworld  &lt;chr&gt; \"Tatooine\", \"Tatooine\", \"Naboo\", \"Tatooine\", \"Alderaan\", \"T…\n$ species    &lt;chr&gt; \"Human\", \"Droid\", \"Droid\", \"Human\", \"Human\", \"Human\", \"Huma…\n$ films      &lt;list&gt; &lt;\"A New Hope\", \"The Empire Strikes Back\", \"Return of the J…\n$ vehicles   &lt;list&gt; &lt;\"Snowspeeder\", \"Imperial Speeder Bike\"&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, \"Imp…\n$ starships  &lt;list&gt; &lt;\"X-wing\", \"Imperial shuttle\"&gt;, &lt;&gt;, &lt;&gt;, \"TIE Advanced x1\",…\n\n\n\nModify the following plot to change the color of all points to \"pink\".\n\n\nggplot(starwars, \n       aes(x = height, y = mass, color = gender, size = birth_year)) +\n  geom_point(color = \"pink\")\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nAdd labels for title, x and y axes, and size of points. Uncomment to see the effect.\n\n\nggplot(starwars, \n       aes(x = height, y = mass, color = gender, size = birth_year)) +\n  geom_point(color = \"#30509C\") +\n  labs(\n    #title = \"___\",\n    #x = \"___\", \n    #y = \"___\",\n    #___\n    )\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nPick a single numerical variable and make a histogram of it. Select a reasonable binwidth for it.\n\n(A little bit of starter code is provided below, and the code chunk is set to not be evaluated with eval: false because the current code in there is not valid code and hence the document wouldn’t knit. Once you replace the code with valid code, set the chunk option to eval: true, or remove the eval option altogether since it’s set to true by default.)\n\nggplot(starwars, aes(___)) +\n  geom___\n\n\nPick a numerical variable and a categorical variable and make a visualization (you pick the type!) to visualization the relationship between the two variables. Along with your code and output, provide an interpretation of the visualization.\n\nInterpretation goes here…\n\nPick a single categorical variable from the data set and make a bar plot of its distribution.\n\n\nPick two categorical variables and make a visualization to visualize the relationship between the two variables. Along with your code and output, provide an interpretation of the visualization.\n\nInterpretation goes here…\n\nPick two numerical variables and two categorical variables and make a visualization that incorporates all of them and provide an interpretation with your answer.\n\n(This time no starter code is provided, you’re on your own!)\nInterpretation goes here…"
  },
  {
    "objectID": "teaching/tidyverse-I/material/countries-of-the-world.html",
    "href": "teaching/tidyverse-I/material/countries-of-the-world.html",
    "title": "Countries of the world",
    "section": "",
    "text": "In order to complete this assignment you will need a Chrome browser with the Selector Gadget extension installed.\nThis website lists the names of 250 countries, as well as their flag, capital, population and size in square kilometres. Our goal could be to read this information into R for each country so that we can potentially analyse it further.\nBefore we start, we should load the required packages (we will also need the tidyverse package this time) and read the website with the function read_html() and assign it to an R object.\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(DT)\n\npage &lt;- read_html(\"https://scrapethissite.com/pages/simple/\")"
  },
  {
    "objectID": "teaching/tidyverse-I/material/countries-of-the-world.html#country-names",
    "href": "teaching/tidyverse-I/material/countries-of-the-world.html#country-names",
    "title": "Countries of the world",
    "section": "Country names",
    "text": "Country names\nUse the Selector Gadget to identify the CSS selectors needed to extract country names.\n\ncountry &lt;- page %&gt;%\n  html_elements(\".country-name\") %&gt;%\n  html_text(trim = TRUE) \n\nhead(country)\n\n[1] \"Andorra\"              \"United Arab Emirates\" \"Afghanistan\"         \n[4] \"Antigua and Barbuda\"  \"Anguilla\"             \"Albania\""
  },
  {
    "objectID": "teaching/tidyverse-I/material/countries-of-the-world.html#capitals-population-and-area",
    "href": "teaching/tidyverse-I/material/countries-of-the-world.html#capitals-population-and-area",
    "title": "Countries of the world",
    "section": "Capitals, population and area",
    "text": "Capitals, population and area\nLet us now turn to the further information for each country. Again use the selector gadget to identify the CSS selector needed which in this case is .country-info:\n\npage %&gt;%\n  html_elements(\".country-info\") %&gt;%\n  html_text(trim = TRUE) %&gt;% \n  head(n = 10)\n\n [1] \"Capital: Andorra la VellaPopulation: 84000Area (km2): 468.0\"   \n [2] \"Capital: Abu DhabiPopulation: 4975593Area (km2): 82880.0\"      \n [3] \"Capital: KabulPopulation: 29121286Area (km2): 647500.0\"        \n [4] \"Capital: St. John'sPopulation: 86754Area (km2): 443.0\"         \n [5] \"Capital: The ValleyPopulation: 13254Area (km2): 102.0\"         \n [6] \"Capital: TiranaPopulation: 2986952Area (km2): 28748.0\"         \n [7] \"Capital: YerevanPopulation: 2968000Area (km2): 29800.0\"        \n [8] \"Capital: LuandaPopulation: 13068161Area (km2): 1246700.0\"      \n [9] \"Capital: NonePopulation: 0Area (km2): 1.4E7\"                   \n[10] \"Capital: Buenos AiresPopulation: 41343201Area (km2): 2766890.0\"\n\n\nSo we get the names of the capitals, but also the population and the size of the country. The selector was not specific enough and we have to tell html_elements() more precisely which of these we are interested in. These CSS selectors differ between the three countries’ information:\n\nThe selector country-capital gives us the capital of the countries:\n\n\ncapital &lt;- page %&gt;%\n  html_elements(\".country-capital\") %&gt;%\n  html_text(trim = TRUE) \n\nhead(capital)\n\n[1] \"Andorra la Vella\" \"Abu Dhabi\"        \"Kabul\"            \"St. John's\"      \n[5] \"The Valley\"       \"Tirana\"          \n\n\n\nThe selector country-population gives us the population of the countries:\n\n\npopulation &lt;-  page %&gt;%\n  html_elements(\".country-population\") %&gt;%\n  html_text() %&gt;% \n  as.numeric()\nhead(population)\n\n[1]    84000  4975593 29121286    86754    13254  2986952\n\n\n\nThe selector country-area gives us the area of the countries:\n\n\narea &lt;-  page %&gt;%\n  html_elements(\".country-area\") %&gt;%\n  html_text() %&gt;% \n  as.numeric()\nhead(area)\n\n[1]    468  82880 647500    443    102  28748\n\n\nNote that we need to tell R to interpret the “text” read from the HTML code as numbers using the function as.numeric()."
  },
  {
    "objectID": "teaching/tidyverse-I/material/countries-of-the-world.html#merge-into-one-tibble",
    "href": "teaching/tidyverse-I/material/countries-of-the-world.html#merge-into-one-tibble",
    "title": "Countries of the world",
    "section": "Merge into one tibble",
    "text": "Merge into one tibble\nWe could already continue working with this, but for many applications it is more practical if we combine the data in a vertical form:\n\ncountries &lt;- tibble(\n  country = country,\n  capital = capital,\n  population = population,\n  area = area\n)\ncountries\n\n# A tibble: 250 × 4\n   country              capital          population     area\n   &lt;chr&gt;                &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;\n 1 Andorra              Andorra la Vella      84000      468\n 2 United Arab Emirates Abu Dhabi           4975593    82880\n 3 Afghanistan          Kabul              29121286   647500\n 4 Antigua and Barbuda  St. John's            86754      443\n 5 Anguilla             The Valley            13254      102\n 6 Albania              Tirana              2986952    28748\n 7 Armenia              Yerevan             2968000    29800\n 8 Angola               Luanda             13068161  1246700\n 9 Antarctica           None                      0 14000000\n10 Argentina            Buenos Aires       41343201  2766890\n# ℹ 240 more rows"
  },
  {
    "objectID": "teaching/tidyverse-I/material/countries-of-the-world.html#all-in-one-step",
    "href": "teaching/tidyverse-I/material/countries-of-the-world.html#all-in-one-step",
    "title": "Countries of the world",
    "section": "All in one step",
    "text": "All in one step\nIf we are sure that we do not need the individual vectors, we can also perform the reading of the data and the creation of the tibble in a single step. Below you can see how the complete scraping process can be completed in relatively few lines.\n\npage &lt;- \"https://scrapethissite.com/pages/simple/\" %&gt;%\n  read_html()\n\ncountries_2 &lt;- tibble(\n  Land = page %&gt;%\n    html_elements(css = \".country-name\") %&gt;% \n    html_text(trim = TRUE),\n  capital = page %&gt;% \n    html_elements(css = \".country-capital\") %&gt;% \n    html_text(),\n  population = page %&gt;% \n    html_elements(css = \".country-population\") %&gt;% \n    html_text() %&gt;% \n    as.numeric(),\n  area = page %&gt;% \n    html_elements(css = \".country-area\") %&gt;% \n    html_text() %&gt;% \n    as.numeric()\n)\n\ncountries_2\n\n# A tibble: 250 × 4\n   Land                 capital          population     area\n   &lt;chr&gt;                &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;\n 1 Andorra              Andorra la Vella      84000      468\n 2 United Arab Emirates Abu Dhabi           4975593    82880\n 3 Afghanistan          Kabul              29121286   647500\n 4 Antigua and Barbuda  St. John's            86754      443\n 5 Anguilla             The Valley            13254      102\n 6 Albania              Tirana              2986952    28748\n 7 Armenia              Yerevan             2968000    29800\n 8 Angola               Luanda             13068161  1246700\n 9 Antarctica           None                      0 14000000\n10 Argentina            Buenos Aires       41343201  2766890\n# ℹ 240 more rows"
  },
  {
    "objectID": "teaching/tidyverse-I/material/la-quinta-ws.html",
    "href": "teaching/tidyverse-I/material/la-quinta-ws.html",
    "title": "La Quinta is Spanish for next to Denny’s",
    "section": "",
    "text": "Have you ever taken a road trip in the US and thought to yourself “I wonder what La Quinta means”. Well, the late comedian Mitch Hedberg thinks it’s Spanish for next to Denny’s.\nIf you’re not familiar with these two establishments, Denny’s is a casual diner chain that is open 24 hours and La Quinta Inn and Suites is a hotel chain.\nThese two establishments tend to be clustered together, or at least this observation is a joke made famous by Mitch Hedberg. In this lab we explore the validity of this joke and along the way learn some more data wrangling and tips for visualizing spatial data.\nThe inspiration for this comes from a blog post by John Reiser on his new jersey geographer blog. You can read that analysis here. Reiser’s blog post focuses on scraping data from Denny’s and La Quinta Inn and Suites websites using Python. Here, we focus on visualization and analysis of these data."
  },
  {
    "objectID": "teaching/tidyverse-I/material/la-quinta-ws.html#packages",
    "href": "teaching/tidyverse-I/material/la-quinta-ws.html#packages",
    "title": "La Quinta is Spanish for next to Denny’s",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for much of the data wrangling and visualisation and the data lives in the dsbox package. These packages are already installed for you. You can load them by running the following in your Console:\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "teaching/tidyverse-I/material/la-quinta-ws.html#data",
    "href": "teaching/tidyverse-I/material/la-quinta-ws.html#data",
    "title": "La Quinta is Spanish for next to Denny’s",
    "section": "Data",
    "text": "Data\nThe data sets we’ll use are called dennys and laquinta and are available for download. Note that these data were scraped from here and here, respectively. You can find information about the data sets here and here. To help with our analysis we will also use a data set on US states.\n\nlaquinta &lt;- read_csv(\"data-4-wrangling-II/laquinta/laquinta.csv\")\ndennys &lt;- read_csv(\"data-4-wrangling-II/laquinta//dennys.csv\")\nstates &lt;- read_csv(\"data-4-wrangling-II/laquinta//states.csv\")\n\nEach observation in the states dataset represents a state, including DC. Along with the name of the state we have the two-letter abbreviation and we have the geographic area of the state (in square miles)."
  },
  {
    "objectID": "teaching/tidyverse-I/material/brexit.html",
    "href": "teaching/tidyverse-I/material/brexit.html",
    "title": "Brexit",
    "section": "",
    "text": "library(tidyverse)\n\nIn September 2019, YouGov survey asked 1,639 GB adults the following question:\n\nIn hindsight, do you think Britain was right/wrong to vote to leave EU?\n\nRight to leave\n\nWrong to leave\n\nDon’t know\n\n\nThe data from the survey is in data/brexit.csv.\n\nbrexit &lt;- read_csv(\"data-brexit/brexit.csv\")\n\nIn the course video we made the following visualisation.\n\nbrexit &lt;- brexit %&gt;%\n  mutate(\n    region = fct_relevel(region, \"london\", \"rest_of_south\", \"midlands_wales\", \"north\", \"scot\"),\n    region = fct_recode(region, London = \"london\", `Rest of South` = \"rest_of_south\", `Midlands / Wales` = \"midlands_wales\", North = \"north\", Scotland = \"scot\")\n  )\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  ) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\",\n    \"Right\" = \"#67a9cf\",\n    \"Don't know\" = \"gray\"\n  )) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn this application exercise we tell different stories with the same data.\n\nExercise 1 - Free scales\nAdd scales = \"free_x\" as an argument to the facet_wrap() function. How does the visualisation change? How is the story this visualisation telling different than the story the original plot tells?\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region,\n    nrow = 1, labeller = label_wrap_gen(width = 12),\n    # ___\n  ) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  ) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\",\n    \"Right\" = \"#67a9cf\",\n    \"Don't know\" = \"gray\"\n  )) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nExercise 2 - Comparing proportions across facets\nFirst, calculate the proportion of wrong, right, and don’t know answers in each category and then plot these proportions (rather than the counts) and then improve axis labeling. How is the story this visualisation telling different than the story the original plot tells? Hint: You’ll need the scales package to improve axis labeling, which means you’ll need to load it on top of the document as well.\n\n\nExercise 3 - Comparing proportions across bars\nRecreate the same visualisation from the previous exercise, this time dodging the bars for opinion proportions for each region, rather than faceting by region and then improve the legend. How is the story this visualisation telling different than the story the previous plot tells?"
  },
  {
    "objectID": "teaching/tidyverse-I/material/legos.html",
    "href": "teaching/tidyverse-I/material/legos.html",
    "title": "Legos",
    "section": "",
    "text": "Here, we work with (simulated) data from Lego sales in 2018 for a sample of customers who bought Legos in the US."
  },
  {
    "objectID": "teaching/tidyverse-I/material/legos.html#data-and-packages",
    "href": "teaching/tidyverse-I/material/legos.html#data-and-packages",
    "title": "Legos",
    "section": "Data and Packages",
    "text": "Data and Packages\nWe’ll use the tidyverse package for much of the data wrangling and visualisation and the data is given to import.\n\nlibrary(tidyverse)\n\nThe following variables are available in the data set:\n\nfirst_name: First name of customer\nlast_name: Last name of customer\nage: Age of customer\nphone_number: Phone number of customer\nset_id: Set ID of lego set purchased\nnumber: Item number of lego set purchased\ntheme: Theme of lego set purchased\nsubtheme: Sub theme of lego set purchased\nyear: Year of purchase\nname: Name of lego set purchased\npieces: Number of pieces of legos in set purchased\nus_price: Price of set purchase in US Dollars\nimage_url: Image URL of lego set purchased\nquantity: Quantity of lego set(s) purchased"
  },
  {
    "objectID": "teaching/tidyverse-I/material/starwars.html",
    "href": "teaching/tidyverse-I/material/starwars.html",
    "title": "Visualizing Starwars characters",
    "section": "",
    "text": "Glimpse at the starwars data frame.\n\n\nglimpse(starwars)\n\nRows: 87\nColumns: 14\n$ name       &lt;chr&gt; \"Luke Skywalker\", \"C-3PO\", \"R2-D2\", \"Darth Vader\", \"Leia Or…\n$ height     &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 2…\n$ mass       &lt;dbl&gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77.…\n$ hair_color &lt;chr&gt; \"blond\", NA, NA, \"none\", \"brown\", \"brown, grey\", \"brown\", N…\n$ skin_color &lt;chr&gt; \"fair\", \"gold\", \"white, blue\", \"white\", \"light\", \"light\", \"…\n$ eye_color  &lt;chr&gt; \"blue\", \"yellow\", \"red\", \"yellow\", \"brown\", \"blue\", \"blue\",…\n$ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57.0, …\n$ sex        &lt;chr&gt; \"male\", \"none\", \"none\", \"male\", \"female\", \"male\", \"female\",…\n$ gender     &lt;chr&gt; \"masculine\", \"masculine\", \"masculine\", \"masculine\", \"femini…\n$ homeworld  &lt;chr&gt; \"Tatooine\", \"Tatooine\", \"Naboo\", \"Tatooine\", \"Alderaan\", \"T…\n$ species    &lt;chr&gt; \"Human\", \"Droid\", \"Droid\", \"Human\", \"Human\", \"Human\", \"Huma…\n$ films      &lt;list&gt; &lt;\"A New Hope\", \"The Empire Strikes Back\", \"Return of the J…\n$ vehicles   &lt;list&gt; &lt;\"Snowspeeder\", \"Imperial Speeder Bike\"&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, \"Imp…\n$ starships  &lt;list&gt; &lt;\"X-wing\", \"Imperial shuttle\"&gt;, &lt;&gt;, &lt;&gt;, \"TIE Advanced x1\",…\n\n\n\nModify the following plot to change the color of all points to \"pink\".\n\n\nggplot(starwars, \n       aes(x = height, y = mass, color = gender, size = birth_year)) +\n  geom_point(color = \"pink\")\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nAdd labels for title, x and y axes, and size of points. Uncomment to see the effect.\n\n\nggplot(starwars, \n       aes(x = height, y = mass, color = gender, size = birth_year)) +\n  geom_point(color = \"#30509C\") +\n  labs(\n    #title = \"___\",\n    #x = \"___\", \n    #y = \"___\",\n    #___\n    )\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nPick a single numerical variable and make a histogram of it. Select a reasonable binwidth for it.\n\n(A little bit of starter code is provided below, and the code chunk is set to not be evaluated with eval: false because the current code in there is not valid code and hence the document wouldn’t knit. Once you replace the code with valid code, set the chunk option to eval: true, or remove the eval option altogether since it’s set to true by default.)\n\nggplot(starwars, aes(___)) +\n  geom___\n\n\nPick a numerical variable and a categorical variable and make a visualization (you pick the type!) to visualization the relationship between the two variables. Along with your code and output, provide an interpretation of the visualization.\n\nInterpretation goes here…\n\nPick a single categorical variable from the data set and make a bar plot of its distribution.\n\n\nPick two categorical variables and make a visualization to visualize the relationship between the two variables. Along with your code and output, provide an interpretation of the visualization.\n\nInterpretation goes here…\n\nPick two numerical variables and two categorical variables and make a visualization that incorporates all of them and provide an interpretation with your answer.\n\n(This time no starter code is provided, you’re on your own!)\nInterpretation goes here…"
  },
  {
    "objectID": "teaching/tidyverse-I/material/plastic-waste-ws.html",
    "href": "teaching/tidyverse-I/material/plastic-waste-ws.html",
    "title": "Global plastic waste",
    "section": "",
    "text": "Plastic pollution is a major and growing problem, negatively affecting oceans and wildlife health. Our World in Data has a lot of great data at various levels including globally, per country, and over time. For this lab we focus on data from 2010.\nAdditionally, National Geographic ran a data visualization communication contest on plastic waste as seen here."
  },
  {
    "objectID": "teaching/tidyverse-I/material/plastic-waste-ws.html#packages",
    "href": "teaching/tidyverse-I/material/plastic-waste-ws.html#packages",
    "title": "Global plastic waste",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for this analysis.\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "teaching/tidyverse-I/material/plastic-waste-ws.html#data",
    "href": "teaching/tidyverse-I/material/plastic-waste-ws.html#data",
    "title": "Global plastic waste",
    "section": "Data",
    "text": "Data\nThe dataset for this assignment can be found as a csv file. You can read it in using the following (make sure you save the data in your working directory).\n\nplastic_waste &lt;- read_csv(\"data-pw/plastic-waste.csv\")\n\nThe variable descriptions are as follows:\n\ncode: 3 Letter country code\nentity: Country name\ncontinent: Continent name\nyear: Year\ngdp_per_cap: GDP per capita constant 2011 international $, rate\nplastic_waste_per_cap: Amount of plastic waste per capita in kg/day\nmismanaged_plastic_waste_per_cap: Amount of mismanaged plastic waste per capita in kg/day\nmismanaged_plastic_waste: Tonnes of mismanaged plastic waste\ncoastal_pop: Number of individuals living on/near coast\ntotal_pop: Total population according to Gapminder"
  },
  {
    "objectID": "teaching/tidyverse-I/material/uoe-art.html",
    "href": "teaching/tidyverse-I/material/uoe-art.html",
    "title": "University of Edinburgh Art Collection",
    "section": "",
    "text": "The University of Edinburgh Art Collection “supports the world-leading research and teaching that happens within the University. Comprised of an astonishing range of objects and ideas spanning two millennia and a multitude of artistic forms, the collection reflects not only the long and rich trajectory of the University, but also major national and international shifts in art history.”\nIn this practical we’ll scrape data on all art pieces in the Edinburgh College of Art collection."
  },
  {
    "objectID": "teaching/tidyverse-I/material/uoe-art.html#r-scripts-vs.-quarto-documents",
    "href": "teaching/tidyverse-I/material/uoe-art.html#r-scripts-vs.-quarto-documents",
    "title": "University of Edinburgh Art Collection",
    "section": "R scripts vs. Quarto documents",
    "text": "R scripts vs. Quarto documents\nToday you’ll be using both R scripts and R Markdown documents:\n\nuse R scripts in the web scraping stage and ultimately save the scraped data as a csv.\nuse an Quarto document in the web analysis stage, where we start off by reading in the csv file we wrote out in the scraping stage."
  },
  {
    "objectID": "teaching/tidyverse-I/material/uoe-art.html#packages",
    "href": "teaching/tidyverse-I/material/uoe-art.html#packages",
    "title": "University of Edinburgh Art Collection",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for much of the data wrangling and visualisation, the robotstxt package to check if we’re allowed to scrape the data, the rvest package for data scraping.\n\nlibrary(tidyverse) \nlibrary(robotstxt)\nlibrary(rvest)"
  },
  {
    "objectID": "teaching/tidyverse-I/material/uoe-art.html#data",
    "href": "teaching/tidyverse-I/material/uoe-art.html#data",
    "title": "University of Edinburgh Art Collection",
    "section": "Data",
    "text": "Data\nThis assignment does not come with any prepared datasets. Instead you’ll be scraping the data! But before doing so, let’s check that a bot has permissions to access pages on this domain.\n\npaths_allowed(\"https://collections.ed.ac.uk/art)\")\n\n\n collections.ed.ac.uk                      \n\n\n[1] TRUE"
  },
  {
    "objectID": "teaching/tidyverse-I/material/uoe-art.html#scraping-a-single-page",
    "href": "teaching/tidyverse-I/material/uoe-art.html#scraping-a-single-page",
    "title": "University of Edinburgh Art Collection",
    "section": "Scraping a single page",
    "text": "Scraping a single page\nWe will start off by scraping data on the first 10 pieces in the collection from here.\nFirst, we define a new object called first_url, which is the link above. Then, we read the page at this url with the read_html() function from the rvest package. The code for this is already provided in 01-scrape-page-one.R.\n\n# set url\nfirst_url &lt;- \"https://collections.ed.ac.uk/art/search/*:*/Collection:%22edinburgh+college+of+art%7C%7C%7CEdinburgh+College+of+Art%22?offset=0\"\n\n# read html page\npage &lt;- read_html(first_url)\n\nFor the ten pieces on this page we will extract title, artist, and link information, and put these three variables in a data frame.\n\nTitles\nLet’s start with titles. We make use of the SelectorGadget to identify the tags for the relevant nodes:\n\n\n\n\n\n\n\n\n\n\npage %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%\n  html_node(\"h3 a\")\n\n{xml_nodeset (10)}\n [1] &lt;a href=\"./record/20696?highlight=*:*\"&gt;South Frieze of the Parthenon Fri ...\n [2] &lt;a href=\"./record/53701?highlight=*:*\"&gt;Espresso Cup                      ...\n [3] &lt;a href=\"./record/99347?highlight=*:*\"&gt;Untitled - Two Apes Sun Bathing   ...\n [4] &lt;a href=\"./record/21212?highlight=*:*\"&gt;Portrait of a Seated Woman        ...\n [5] &lt;a href=\"./record/21289?highlight=*:*\"&gt;Seated Male Nude                  ...\n [6] &lt;a href=\"./record/99370?highlight=*:*\"&gt;Nighttime Scene of the City and R ...\n [7] &lt;a href=\"./record/21178?highlight=*:*\"&gt;Portrait of Man in Red Jacket     ...\n [8] &lt;a href=\"./record/20743?highlight=*:*\"&gt;Harbour Scene 'KY16'              ...\n [9] &lt;a href=\"./record/21568?highlight=*:*\"&gt;Untitled                          ...\n[10] &lt;a href=\"./record/102688?highlight=*:*\"&gt;Machine stitched net             ...\n\n\nThen we extract the text with html_text():\n\npage %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%\n  html_node(\"h3 a\") %&gt;%\n  html_text()\n\n [1] \"South Frieze of the Parthenon Frieze                                                                            (1836-1837)\"\n [2] \"Espresso Cup                                    \"                                                                           \n [3] \"Untitled - Two Apes Sun Bathing                                                                            (1963)\"          \n [4] \"Portrait of a Seated Woman                                                                            (1954)\"               \n [5] \"Seated Male Nude                                                                            (1961)\"                         \n [6] \"Nighttime Scene of the City and River                                                                            (1962)\"    \n [7] \"Portrait of Man in Red Jacket                                                                            (1968)\"            \n [8] \"Harbour Scene 'KY16'                                                                            (1964)\"                     \n [9] \"Untitled                                                                            (May 1987)\"                             \n[10] \"Machine stitched net                                                                            (1946)\"                     \n\n\nAnd get rid of all the spurious white space in the text with str_squish(), which reduces repeated whitespace inside a string.\nTake a look at the help for str_squish() to find out more about how it works and how it’s different from str_trim().\n\npage %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%\n  html_node(\"h3 a\") %&gt;%\n  html_text() %&gt;% \n  str_squish()\n\n [1] \"South Frieze of the Parthenon Frieze (1836-1837)\"\n [2] \"Espresso Cup\"                                    \n [3] \"Untitled - Two Apes Sun Bathing (1963)\"          \n [4] \"Portrait of a Seated Woman (1954)\"               \n [5] \"Seated Male Nude (1961)\"                         \n [6] \"Nighttime Scene of the City and River (1962)\"    \n [7] \"Portrait of Man in Red Jacket (1968)\"            \n [8] \"Harbour Scene 'KY16' (1964)\"                     \n [9] \"Untitled (May 1987)\"                             \n[10] \"Machine stitched net (1946)\"                     \n\n\nAnd finally save the resulting data as a vector of length 10:\n\ntitles &lt;- page %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%\n  html_node(\"h3 a\") %&gt;%\n  html_text() %&gt;%\n  str_squish()\n\n\n\nLinks\nThe same nodes that contain the text for the titles also contains information on the links to individual art piece pages for each title. We can extract this information using a new function from the rvest package, html_attr(), which extracts attributes.\nA mini HTML lesson! The following is how we define hyperlinked text in HTML:\n&lt;a href=\"https://www.google.com\"&gt;Search on Google&lt;/a&gt;\nAnd this is how the text would look like on a webpage: Search on Google.\nHere the text is Search on Google and the href attribute contains the url of the website you’d go to if you click on the hyperlinked text: https://www.google.com.\nThe moral of the story is: the link is stored in the href attribute.\n\npage %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%   # same nodes\n  html_node(\"h3 a\") %&gt;%         # as before\n  html_attr(\"href\")             # but get href attribute instead of text\n\n [1] \"./record/20696?highlight=*:*\"  \"./record/53701?highlight=*:*\" \n [3] \"./record/99347?highlight=*:*\"  \"./record/21212?highlight=*:*\" \n [5] \"./record/21289?highlight=*:*\"  \"./record/99370?highlight=*:*\" \n [7] \"./record/21178?highlight=*:*\"  \"./record/20743?highlight=*:*\" \n [9] \"./record/21568?highlight=*:*\"  \"./record/102688?highlight=*:*\"\n\n\nThese don’t really look like URLs as we know then though. They’re relative links.\nSee the help for str_replace() to find out how it works. Remember that the first argument is passed in from the pipeline, so you just need to define the pattern and replacement arguments.\n\nClick on one of art piece titles in your browser and take note of the url of the webpage it takes you to. Think about how that url compares to what we scraped above? How is it different? Using str_replace(), fix the URLs. You’ll note something special happening in the pattern to replace. We want to replace the ., but we have it as \\\\.. This is because the period . is a special character and so we need to escape it first with backslashes, \\\\s.\n\n\n\nArtists\n\nFill in the blanks to scrape artist names.\n\n\n\nPut it altogether\n\nFill in the blanks to organize everything in a tibble.\n\n\n\nScrape the next page\n\nClick on the next page, and grab its url. Fill in the blank in to define a new object: second_url. Copy-paste code from top of the R script to scrape the new set of art pieces, and save the resulting data frame as second_ten."
  },
  {
    "objectID": "teaching/tidyverse-I/material/uoe-art.html#functions",
    "href": "teaching/tidyverse-I/material/uoe-art.html#functions",
    "title": "University of Edinburgh Art Collection",
    "section": "Functions",
    "text": "Functions\nYou’ve been using R functions, now it’s time to write your own!\nLet’s start simple. Here is a function that takes in an argument x, and adds 2 to it.\n\nadd_two &lt;- function(x){\n  x + 2\n}\n\nLet’s test it:\n\nadd_two(3)\n\n[1] 5\n\nadd_two(10)\n\n[1] 12\n\n\nThe skeleton for defining functions in R is as follows:\n\nfunction_name &lt;- function(input){\n  # do something with the input(s)\n  # return something\n}\n\nThen, a function for scraping a page should look something like:\nReminder: Function names should be short but evocative verbs.\n\nfunction_name &lt;- function(url){\n  # read page at url\n  # extract title, link, artist info for n pieces on page\n  # return a n x 3 tibble\n}\n\n\nFill in the blanks using code you already developed in the previous exercises. Name the function scrape_page.\n\nTest out your new function by running the following in the console. Does the output look right? Discuss with teammates whether you’re getting the same results as before.\n\nscrape_page(first_url)\nscrape_page(second_url)"
  },
  {
    "objectID": "teaching/tidyverse-I/material/uoe-art.html#iteration",
    "href": "teaching/tidyverse-I/material/uoe-art.html#iteration",
    "title": "University of Edinburgh Art Collection",
    "section": "Iteration",
    "text": "Iteration\nWe went from manually scraping individual pages to writing a function to do the same. Next, we will work on making our workflow a little more efficient by using R to iterate over all pages that contain information on the art collection.\nThat means we give develop a list of URLs (of pages that each have 10 art pieces), and write some code that applies the scrape_page() function to each page, and combines the resulting data frames from each page into a single data frame with 3289 rows and 3 columns.\n\nList of URLs\nClick through the first few of the pages in the art collection and observe their URLs to confirm the following pattern:\n[sometext]offset=0     # Pieces 1-10\n[sometext]offset=10    # Pieces 11-20\n[sometext]offset=20    # Pieces 21-30\n[sometext]offset=30    # Pieces 31-40\n...\n[sometext]offset=3280  # Pieces 3281-3289\nWe can construct these URLs in R by pasting together two pieces: (1) a common (root) text for the beginning of the URL, and (2) numbers starting at 0, increasing by 10, all the way up to 3289. Two new functions are helpful for accomplishing this: glue() for pasting two pieces of text and seq() for generating a sequence of numbers.\n\nFill in the blanks to construct the list of URLs.\n\n\n\nMapping\nFinally, we’re ready to iterate over the list of URLs we constructed. We will do this by mapping the function we developed over the list of URLs. There are a series of mapping functions in R and they each take the following form:\nmap([x], [function to apply to each element of x])\nIn our case x is the list of URLs we constructed and the function to apply to each element of x is the function we developed earlier, scrape_page. And as a result we want a data frame, so we use map_dfr function:\n\nmap_dfr(urls, scrape_page)\n\n\nFill in the blanks to scrape all pages, and to create a new data frame called uoe_art.\n\n\n\nWrite out data\n\nFinally write out the data frame you constructed into the data folder so that you can use it in the analysis section."
  },
  {
    "objectID": "teaching/tidyverse-I/material/uoe-art.html#analysis",
    "href": "teaching/tidyverse-I/material/uoe-art.html#analysis",
    "title": "University of Edinburgh Art Collection",
    "section": "Analysis",
    "text": "Analysis\nFor the rest of the exercises you can work in Quarto/R Markdown.\nNow that we have a tidy dataset that we can analyze, let’s do that!\nWe’ll start with some data cleaning, to clean up the dates that appear at the end of some title text in parentheses. Some of these are years, others are more specific dates, some art pieces have no date information whatsoever, and others have some non-date information in parentheses. This should be interesting to clean up!\nFirst thing we’ll try is to separate the title column into two: one for the actual title and the other for the date if it exists. In human speak, we need to\n“separate the title column at the first occurrence of ( and put the contents on one side of the ( into a column called title and the contents on the other side into a column called date”\nLuckily, there’s a function that does just this: separate()!\nAnd once we have completed separating the single title column into title and date, we need to do further clean-up in the date column to get rid of extraneous )s with str_remove(), capture year information, and save the data as a numeric variable.\n\nFill in the blanks in to implement the data wrangling we described above. Note that this will result in some warnings when you run the code, and that’s OK! Read the warnings, and explain what they mean, and why we are ok with leaving them in given that our objective is to just capture year where it’s convenient to do so.\nPrint out a summary of the data frame using the skim() function. How many pieces have artist info missing? How many have year info missing?\nMake a histogram of years. Use a reasonable binwidth. Do you see anything out of the ordinary?\nFind which piece has the out of the ordinary year and go to its page on the art collection website to find the correct year for it. Can you tell why our code didn’t capture the correct year information? Correct the error in the data frame and visualize the data again.\n\nHint: You’ll want to use mutate() and if_else() or case_when() to implement the correction.\n\nWho is the most commonly featured artist in the collection? Do you know them? Any guess as to why the university has so many pieces from them?\nFinal question! How many art pieces have the word “child” in their title? Try to figure it out, and ask for help if you’re stuck.\n\nHint: str_subset() can be helful here. You should consider how you might capture titles where the word appears as “child” and “Child”.\nSource: https://collections.ed.ac.uk/art/about"
  },
  {
    "objectID": "teaching/tidyverse-I/material/nobel-laureates.html",
    "href": "teaching/tidyverse-I/material/nobel-laureates.html",
    "title": "Nobel Laureates",
    "section": "",
    "text": "In January 2017, Buzzfeed published an article on why Nobel laureates show immigration is so important for American science. You can read the article here. In the article they show that while most living Nobel laureates in the sciences are based in the US, many of them were born in other countries. This is one reason why scientific leaders say that immigration is vital for progress. In this lab we will work with the data from this article to recreate some of their visualizations as well as explore new questions."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nobel-laureates.html#packages",
    "href": "teaching/tidyverse-I/material/nobel-laureates.html#packages",
    "title": "Nobel Laureates",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for much of the data wrangling. This package is already installed for you. You can load them by running the following in your Console:\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "teaching/tidyverse-I/material/nobel-laureates.html#data",
    "href": "teaching/tidyverse-I/material/nobel-laureates.html#data",
    "title": "Nobel Laureates",
    "section": "Data",
    "text": "Data\nThe dataset for this assignment can be found as a CSV (comma separated values) file. You can read it in using the following.\n\nnobel &lt;- read_csv(\"data-4-wrangling-I/nobel.csv\")\n\nThe variable descriptions are as follows:\n\nid: ID number\nfirstname: First name of laureate\nsurname: Surname\nyear: Year prize won\ncategory: Category of prize\naffiliation: Affiliation of laureate\ncity: City of laureate in prize year\ncountry: Country of laureate in prize year\nborn_date: Birth date of laureate\ndied_date: Death date of laureate\ngender: Gender of laureate\nborn_city: City where laureate was born\nborn_country: Country where laureate was born\nborn_country_code: Code of country where laureate was born\ndied_city: City where laureate died\ndied_country: Country where laureate died\ndied_country_code: Code of country where laureate died\noverall_motivation: Overall motivation for recognition\nshare: Number of other winners award is shared with\nmotivation: Motivation for recognition\n\nIn a few cases the name of the city/country changed after laureate was given (e.g. in 1975 Bosnia and Herzegovina was called the Socialist Federative Republic of Yugoslavia). In these cases the variables below reflect a different name than their counterparts without the suffix `_original`.\n\nborn_country_original: Original country where laureate was born\nborn_city_original: Original city where laureate was born\ndied_country_original: Original country where laureate died\ndied_city_original: Original city where laureate died\ncity_original: Original city where laureate lived at the time of winning the award\ncountry_original: Original country where laureate lived at the time of winning the award"
  },
  {
    "objectID": "teaching/tidyverse-I/material/nobel-laureates.html#get-to-know-your-data",
    "href": "teaching/tidyverse-I/material/nobel-laureates.html#get-to-know-your-data",
    "title": "Nobel Laureates",
    "section": "Get to know your data",
    "text": "Get to know your data\n\nHow many observations and how many variables are in the dataset? Use inline code to answer this question. What does each row represent?\n\nThere are some observations in this dataset that we will exclude from our analysis to match the Buzzfeed results.\n\nCreate a new data frame called nobel_living that filters for\n\n\nlaureates for whom country is available\nlaureates who are people as opposed to organizations (organizations are denoted with \"org\" as their gender)\nlaureates who are still alive (their died_date is NA)\n\nConfirm that once you have filtered for these characteristics you are left with a data frame with 228 observations, once again using inline code."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nobel-laureates.html#most-living-nobel-laureates-were-based-in-the-us-when-they-won-their-prizes",
    "href": "teaching/tidyverse-I/material/nobel-laureates.html#most-living-nobel-laureates-were-based-in-the-us-when-they-won-their-prizes",
    "title": "Nobel Laureates",
    "section": "Most living Nobel laureates were based in the US when they won their prizes",
    "text": "Most living Nobel laureates were based in the US when they won their prizes\n… says the Buzzfeed article. Let’s see if that’s true.\nFirst, we’ll create a new variable to identify whether the laureate was in the US when they won their prize. We’ll use the mutate() function for this. The following pipeline mutates the nobel_living data frame by adding a new variable called country_us. We use an if statement to create this variable. The first argument in the if_else() function we’re using to write this if statement is the condition we’re testing for. If country is equal to \"USA\", we set country_us to \"USA\". If not, we set the country_us to \"Other\".\nNote: we can achieve the same result using the fct_other() function we’ve seen before (i.e. with country_us = fct_other(country, \"USA\")). We decided to use the if_else() here to show you one example of an if statement in R.\n\nnobel_living &lt;- nobel_living %&gt;%\n  mutate(\n    country_us = if_else(country == \"USA\", \"USA\", \"Other\")\n  )\n\nNext, we will limit our analysis to only the following categories: Physics, Medicine, Chemistry, and Economics.\n\nnobel_living_science &lt;- nobel_living %&gt;%\n  filter(category %in% c(\"Physics\", \"Medicine\", \"Chemistry\", \"Economics\"))\n\nFor the next exercise work with the nobel_living_science data frame you created above.\n\nCreate a faceted bar plot visualizing the relationship between the category of prize and whether the laureate was in the US when they won the nobel prize. Interpret your visualization, and say a few words about whether the Buzzfeed headline is supported by the data.\n\nYour visualization should be faceted by category.\nFor each facet you should have two bars, one for winners in the US and one for Other.\nFlip the coordinates so the bars are horizontal, not vertical."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nobel-laureates.html#but-of-those-us-based-nobel-laureates-many-were-born-in-other-countries",
    "href": "teaching/tidyverse-I/material/nobel-laureates.html#but-of-those-us-based-nobel-laureates-many-were-born-in-other-countries",
    "title": "Nobel Laureates",
    "section": "But of those US-based Nobel laureates, many were born in other countries",
    "text": "But of those US-based Nobel laureates, many were born in other countries\nHint: You should be able to cheat borrow from code you used earlier to create the country_us variable.\n\nCreate a new variable called born_country_us that has the value \"USA\" if the laureate is born in the US, and \"Other\" otherwise. How many of the winners are born in the US?\nAdd a second variable to your visualization from Exercise 3 based on whether the laureate was born in the US or not. Based on your visualization, do the data appear to support Buzzfeed’s claim? Explain your reasoning in 1-2 sentences.\n\nYour final visualization should contain a facet for each category.\nWithin each facet, there should be a bar for whether the laureate won the award in the US or not.\nEach bar should have segments for whether the laureate was born in the US or not."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nobel-laureates.html#heres-where-those-immigrant-nobelists-were-born",
    "href": "teaching/tidyverse-I/material/nobel-laureates.html#heres-where-those-immigrant-nobelists-were-born",
    "title": "Nobel Laureates",
    "section": "Here’s where those immigrant Nobelists were born",
    "text": "Here’s where those immigrant Nobelists were born\nNote: your bar plot won’t exactly match the one from the Buzzfeed article. This is likely because the data has been updated since the article was published.\n\nIn a single pipeline, filter for laureates who won their prize in the US, but were born outside of the US, and then create a frequency table (with the count() function) for their birth country (born_country) and arrange the resulting data frame in descending order of number of observations for each country. Which country is the most common?"
  },
  {
    "objectID": "teaching/tidyverse-I/material/college-majors.html",
    "href": "teaching/tidyverse-I/material/college-majors.html",
    "title": "What should I major in?",
    "section": "",
    "text": "The first step in the process of turning information into knowledge process is to summarize and describe the raw information - the data. In this assignment we explore data on college majors and earnings, specifically the data begin the FiveThirtyEight story “The Economic Guide To Picking A College Major”.\nThese data originally come from the American Community Survey (ACS) 2010-2012 Public Use Microdata Series. While this is outside the scope of this assignment, if you are curious about how raw data from the ACS were cleaned and prepared, see the code FiveThirtyEight authors used.\nWe should also note that there are many considerations that go into picking a major. Earnings potential and employment prospects are two of them, and they are important, but they don’t tell the whole story. Keep this in mind as you analyze the data."
  },
  {
    "objectID": "teaching/tidyverse-I/material/college-majors.html#packages",
    "href": "teaching/tidyverse-I/material/college-majors.html#packages",
    "title": "What should I major in?",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for much of the data wrangling and visualisation, the scales package for better formatting of labels on visualisations, and the data lives in the fivethirtyeight package. These packages are already installed for you. You can load them by running the following in your Console:\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(fivethirtyeight)"
  },
  {
    "objectID": "teaching/tidyverse-I/material/college-majors.html#data",
    "href": "teaching/tidyverse-I/material/college-majors.html#data",
    "title": "What should I major in?",
    "section": "Data",
    "text": "Data\nThe data can be found in the fivethirtyeight package, and it’s called college_recent_grads. Since the dataset is distributed with the package, we don’t need to load it separately; it becomes available to us when we load the package. You can find out more about the dataset by inspecting its documentation, which you can access by running ?college_recent_grads in the Console or using the Help menu in RStudio to search for college_recent_grads. You can also find this information here.\nYou can also take a quick peek at your data frame and view its dimensions with the glimpse function.\n\nglimpse(college_recent_grads)\n\nRows: 173\nColumns: 21\n$ rank                        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,…\n$ major_code                  &lt;int&gt; 2419, 2416, 2415, 2417, 2405, 2418, 6202, …\n$ major                       &lt;chr&gt; \"Petroleum Engineering\", \"Mining And Miner…\n$ major_category              &lt;chr&gt; \"Engineering\", \"Engineering\", \"Engineering…\n$ total                       &lt;int&gt; 2339, 756, 856, 1258, 32260, 2573, 3777, 1…\n$ sample_size                 &lt;int&gt; 36, 7, 3, 16, 289, 17, 51, 10, 1029, 631, …\n$ men                         &lt;int&gt; 2057, 679, 725, 1123, 21239, 2200, 2110, 8…\n$ women                       &lt;int&gt; 282, 77, 131, 135, 11021, 373, 1667, 960, …\n$ sharewomen                  &lt;dbl&gt; 0.1205643, 0.1018519, 0.1530374, 0.1073132…\n$ employed                    &lt;int&gt; 1976, 640, 648, 758, 25694, 1857, 2912, 15…\n$ employed_fulltime           &lt;int&gt; 1849, 556, 558, 1069, 23170, 2038, 2924, 1…\n$ employed_parttime           &lt;int&gt; 270, 170, 133, 150, 5180, 264, 296, 553, 1…\n$ employed_fulltime_yearround &lt;int&gt; 1207, 388, 340, 692, 16697, 1449, 2482, 82…\n$ unemployed                  &lt;int&gt; 37, 85, 16, 40, 1672, 400, 308, 33, 4650, …\n$ unemployment_rate           &lt;dbl&gt; 0.018380527, 0.117241379, 0.024096386, 0.0…\n$ p25th                       &lt;dbl&gt; 95000, 55000, 50000, 43000, 50000, 50000, …\n$ median                      &lt;dbl&gt; 110000, 75000, 73000, 70000, 65000, 65000,…\n$ p75th                       &lt;dbl&gt; 125000, 90000, 105000, 80000, 75000, 10200…\n$ college_jobs                &lt;int&gt; 1534, 350, 456, 529, 18314, 1142, 1768, 97…\n$ non_college_jobs            &lt;int&gt; 364, 257, 176, 102, 4440, 657, 314, 500, 1…\n$ low_wage_jobs               &lt;int&gt; 193, 50, 0, 0, 972, 244, 259, 220, 3253, 3…\n\n\nThe college_recent_grads data frame is a trove of information. Let’s think about some questions we might want to answer with these data:\n\nWhich major has the lowest unemployment rate?\nWhich major has the highest percentage of women?\nHow do the distributions of median income compare across major categories?\nDo women tend to choose majors with lower or higher earnings?\n\nIn the next section we aim to answer these questions."
  },
  {
    "objectID": "teaching/tidyverse-I/material/college-majors.html#which-major-has-the-lowest-unemployment-rate",
    "href": "teaching/tidyverse-I/material/college-majors.html#which-major-has-the-lowest-unemployment-rate",
    "title": "What should I major in?",
    "section": "Which major has the lowest unemployment rate?",
    "text": "Which major has the lowest unemployment rate?\nIn order to answer this question all we need to do is sort the data. We use the arrange function to do this, and sort it by the unemployment_rate variable. By default arrange sorts in ascending order, which is what we want here – we’re interested in the major with the lowest unemployment rate.\n\ncollege_recent_grads %&gt;%\n  arrange(unemployment_rate)\n\n# A tibble: 173 × 21\n    rank major_code major           major_category total sample_size   men women\n   &lt;int&gt;      &lt;int&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;int&gt;       &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1    53       4005 Mathematics An… Computers & M…   609           7   500   109\n 2    74       3801 Military Techn… Industrial Ar…   124           4   124     0\n 3    84       3602 Botany          Biology & Lif…  1329           9   626   703\n 4   113       1106 Soil Science    Agriculture &…   685           4   476   209\n 5   121       2301 Educational Ad… Education        804           5   280   524\n 6    15       2409 Engineering Me… Engineering     4321          30  3526   795\n 7    20       3201 Court Reporting Law & Public …  1148          14   877   271\n 8   120       2305 Mathematics Te… Education      14237         123  3872 10365\n 9     1       2419 Petroleum Engi… Engineering     2339          36  2057   282\n10    65       1100 General Agricu… Agriculture &… 10399         158  6053  4346\n# ℹ 163 more rows\n# ℹ 13 more variables: sharewomen &lt;dbl&gt;, employed &lt;int&gt;,\n#   employed_fulltime &lt;int&gt;, employed_parttime &lt;int&gt;,\n#   employed_fulltime_yearround &lt;int&gt;, unemployed &lt;int&gt;,\n#   unemployment_rate &lt;dbl&gt;, p25th &lt;dbl&gt;, median &lt;dbl&gt;, p75th &lt;dbl&gt;,\n#   college_jobs &lt;int&gt;, non_college_jobs &lt;int&gt;, low_wage_jobs &lt;int&gt;\n\n\nThis gives us what we wanted, but not in an ideal form. First, the name of the major barely fits on the page. Second, some of the variables are not that useful (e.g. major_code, major_category) and some we might want front and center are not easily viewed (e.g. unemployment_rate).\nWe can use the select function to choose which variables to display, and in which order:\n\ncollege_recent_grads %&gt;%\n  arrange(unemployment_rate) %&gt;%\n  select(rank, major, unemployment_rate)\n\n# A tibble: 173 × 3\n    rank major                                      unemployment_rate\n   &lt;int&gt; &lt;chr&gt;                                                  &lt;dbl&gt;\n 1    53 Mathematics And Computer Science                     0      \n 2    74 Military Technologies                                0      \n 3    84 Botany                                               0      \n 4   113 Soil Science                                         0      \n 5   121 Educational Administration And Supervision           0      \n 6    15 Engineering Mechanics Physics And Science            0.00633\n 7    20 Court Reporting                                      0.0117 \n 8   120 Mathematics Teacher Education                        0.0162 \n 9     1 Petroleum Engineering                                0.0184 \n10    65 General Agriculture                                  0.0196 \n# ℹ 163 more rows\n\n\nOk, this is looking better, but do we really need to display all those decimal places in the unemployment variable? Not really!\nWe can use the percent() function to clean up the display a bit.\n\ncollege_recent_grads %&gt;%\n  arrange(unemployment_rate) %&gt;%\n  select(rank, major, unemployment_rate) %&gt;%\n  mutate(unemployment_rate = percent(unemployment_rate))\n\n# A tibble: 173 × 3\n    rank major                                      unemployment_rate\n   &lt;int&gt; &lt;chr&gt;                                      &lt;chr&gt;            \n 1    53 Mathematics And Computer Science           0.00000%         \n 2    74 Military Technologies                      0.00000%         \n 3    84 Botany                                     0.00000%         \n 4   113 Soil Science                               0.00000%         \n 5   121 Educational Administration And Supervision 0.00000%         \n 6    15 Engineering Mechanics Physics And Science  0.63343%         \n 7    20 Court Reporting                            1.16897%         \n 8   120 Mathematics Teacher Education              1.62028%         \n 9     1 Petroleum Engineering                      1.83805%         \n10    65 General Agriculture                        1.96425%         \n# ℹ 163 more rows"
  },
  {
    "objectID": "teaching/tidyverse-I/material/college-majors.html#which-major-has-the-highest-percentage-of-women",
    "href": "teaching/tidyverse-I/material/college-majors.html#which-major-has-the-highest-percentage-of-women",
    "title": "What should I major in?",
    "section": "Which major has the highest percentage of women?",
    "text": "Which major has the highest percentage of women?\nTo answer such a question we need to arrange the data in descending order. For example, if earlier we were interested in the major with the highest unemployment rate, we would use the following:\n\nThe `desc` function specifies that we want `unemployment_rate` in descending order.\n\n\ncollege_recent_grads %&gt;%\n  arrange(desc(unemployment_rate)) %&gt;%\n  select(rank, major, unemployment_rate)\n\n# A tibble: 173 × 3\n    rank major                                      unemployment_rate\n   &lt;int&gt; &lt;chr&gt;                                                  &lt;dbl&gt;\n 1     6 Nuclear Engineering                                    0.177\n 2    90 Public Administration                                  0.159\n 3    85 Computer Networking And Telecommunications             0.152\n 4   171 Clinical Psychology                                    0.149\n 5    30 Public Policy                                          0.128\n 6   106 Communication Technologies                             0.120\n 7     2 Mining And Mineral Engineering                         0.117\n 8    54 Computer Programming And Data Processing               0.114\n 9    80 Geography                                              0.113\n10    59 Architecture                                           0.113\n# ℹ 163 more rows\n\n\n\nUsing what you’ve learned so far, arrange the data in descending order with respect to proportion of women in a major, and display only the major, the total number of people with major, and proportion of women. Show only the top 3 majors by adding top_n(3) at the end of the pipeline."
  },
  {
    "objectID": "teaching/tidyverse-I/material/college-majors.html#how-do-the-distributions-of-median-income-compare-across-major-categories",
    "href": "teaching/tidyverse-I/material/college-majors.html#how-do-the-distributions-of-median-income-compare-across-major-categories",
    "title": "What should I major in?",
    "section": "How do the distributions of median income compare across major categories?",
    "text": "How do the distributions of median income compare across major categories?\nNote: A percentile is a measure used in statistics indicating the value below which a given percentage of observations in a group of observations fall. For example, the 20th percentile is the value below which 20% of the observations may be found. (Source: Wikipedia\nThere are three types of incomes reported in this data frame: p25th, median, and p75th. These correspond to the 25th, 50th, and 75th percentiles of the income distribution of sampled individuals for a given major.\n\nWhy do we often choose the median, rather than the mean, to describe the typical income of a group of people?\n\nThe question we want to answer “How do the distributions of median income compare across major categories?”. We need to do a few things to answer this question: First, we need to group the data by major_category. Then, we need a way to summarize the distributions of median income within these groups. This decision will depend on the shapes of these distributions. So first, we need to visualize the data.\nWe use the ggplot() function to do this. The first argument is the data frame, and the next argument gives the mapping of the variables of the data to the aesthetic elements of the plot.\nLet’s start simple and take a look at the distribution of all median incomes, without considering the major categories.\n\nggplot(data = college_recent_grads, mapping = aes(x = median)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\nAlong with the plot, we get a message:\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nThis is telling us that we might want to reconsider the binwidth we chose for our histogram – or more accurately, the binwidth we didn’t specify. It’s good practice to always think in the context of the data and try out a few binwidths before settling on a binwidth. You might ask yourself: “What would be a meaningful difference in median incomes?” $1 is obviously too little, $10000 might be too high.\n\nTry binwidths of $1000 and $5000 and choose one. Explain your reasoning for your choice. Note that the binwidth is an argument for the geom_histogram function. So to specify a binwidth of $1000, you would use geom_histogram(binwidth = 1000).\n\nWe can also calculate summary statistics for this distribution using the summarise function:\n\ncollege_recent_grads %&gt;%\n  summarise(min = min(median), max = max(median),\n            mean = mean(median), med = median(median),\n            sd = sd(median), \n            q1 = quantile(median, probs = 0.25),\n            q3 = quantile(median, probs = 0.75))\n\n# A tibble: 1 × 7\n    min    max   mean   med     sd    q1    q3\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 22000 110000 40151. 36000 11470. 33000 45000\n\n\n\nBased on the shape of the histogram you created in the previous exercise, determine which of these summary statistics is useful for describing the distribution. Write up your description (remember shape, center, spread, any unusual observations) and include the summary statistic output as well.\nPlot the distribution of median income using a histogram, faceted by major_category. Use the binwidth you chose in the earlier exercise.\n\nNow that we’ve seen the shapes of the distributions of median incomes for each major category, we should have a better idea for which summary statistic to use to quantify the typical median income.\n\nWhich major category has the highest typical (you’ll need to decide what this means) median income? Use the partial code below, filling it in with the appropriate statistic and function. Also note that we are looking for the highest statistic, so make sure to arrange in the correct direction.\n\n\ncollege_recent_grads %&gt;%\n  group_by(major_category) %&gt;%\n  summarise(___ = ___(median)) %&gt;%\n  arrange(___)\n\n\nWhich major category is the least popular in this sample? To answer this question we use a new function called count, which first groups the data and then counts the number of observations in each category (see below). Add to the pipeline appropriately to arrange the results so that the major with the lowest observations is on top.\n\n\ncollege_recent_grads %&gt;%\n  count(major_category)\n\n# A tibble: 16 × 2\n   major_category                          n\n   &lt;chr&gt;                               &lt;int&gt;\n 1 Agriculture & Natural Resources        10\n 2 Arts                                    8\n 3 Biology & Life Science                 14\n 4 Business                               13\n 5 Communications & Journalism             4\n 6 Computers & Mathematics                11\n 7 Education                              16\n 8 Engineering                            29\n 9 Health                                 12\n10 Humanities & Liberal Arts              15\n11 Industrial Arts & Consumer Services     7\n12 Interdisciplinary                       1\n13 Law & Public Policy                     5\n14 Physical Sciences                      10\n15 Psychology & Social Work                9\n16 Social Science                          9"
  },
  {
    "objectID": "teaching/tidyverse-I/material/college-majors.html#all-stem-fields-arent-the-same",
    "href": "teaching/tidyverse-I/material/college-majors.html#all-stem-fields-arent-the-same",
    "title": "What should I major in?",
    "section": "All STEM fields aren’t the same",
    "text": "All STEM fields aren’t the same\nOne of the sections of the FiveThirtyEight story is “All STEM fields aren’t the same”. Let’s see if this is true.\nFirst, let’s create a new vector called stem_categories that lists the major categories that are considered STEM fields.\n\nstem_categories &lt;- c(\"Biology & Life Science\",\n                     \"Computers & Mathematics\",\n                     \"Engineering\",\n                     \"Physical Sciences\")\n\nThen, we can use this to create a new variable in our data frame indicating whether a major is STEM or not.\n\ncollege_recent_grads &lt;- college_recent_grads %&gt;%\n  mutate(major_type = ifelse(major_category %in% stem_categories, \"stem\", \"not stem\"))\n\nLet’s unpack this: with mutate we create a new variable called major_type, which is defined as \"stem\" if the major_category is in the vector called stem_categories we created earlier, and as \"not stem\" otherwise.\n%in% is a logical operator. Other logical operators that are commonly used are\n\n\n\nOperator\nOperation\n\n\n\n\nx &lt; y\nless than\n\n\nx &gt; y\ngreater than\n\n\nx &lt;= y\nless than or equal to\n\n\nx &gt;= y\ngreater than or equal to\n\n\nx != y\nnot equal to\n\n\nx == y\nequal to\n\n\nx %in% y\ncontains\n\n\nx | y\nor\n\n\nx & y\nand\n\n\n!x\nnot\n\n\n\nWe can use the logical operators to also filter our data for STEM majors whose median earnings is less than median for all majors’ median earnings, which we found to be $36,000 earlier.\n\ncollege_recent_grads %&gt;%\n  filter(\n    major_type == \"stem\",\n    median &lt; 36000\n  )\n\n# A tibble: 10 × 22\n    rank major_code major        major_category  total sample_size    men  women\n   &lt;int&gt;      &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;           &lt;int&gt;       &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n 1    93       1301 Environment… Biology & Lif…  25965         225  10787  15178\n 2    98       5098 Multi-Disci… Physical Scie…  62052         427  27015  35037\n 3   102       3608 Physiology   Biology & Lif…  22060          99   8422  13638\n 4   106       2001 Communicati… Computers & M…  18035         208  11431   6604\n 5   109       3611 Neuroscience Biology & Lif…  13663          53   4944   8719\n 6   111       5002 Atmospheric… Physical Scie…   4043          32   2744   1299\n 7   123       3699 Miscellaneo… Biology & Lif…  10706          63   4747   5959\n 8   124       3600 Biology      Biology & Lif… 280709        1370 111762 168947\n 9   133       3604 Ecology      Biology & Lif…   9154          86   3878   5276\n10   169       3609 Zoology      Biology & Lif…   8409          47   3050   5359\n# ℹ 14 more variables: sharewomen &lt;dbl&gt;, employed &lt;int&gt;,\n#   employed_fulltime &lt;int&gt;, employed_parttime &lt;int&gt;,\n#   employed_fulltime_yearround &lt;int&gt;, unemployed &lt;int&gt;,\n#   unemployment_rate &lt;dbl&gt;, p25th &lt;dbl&gt;, median &lt;dbl&gt;, p75th &lt;dbl&gt;,\n#   college_jobs &lt;int&gt;, non_college_jobs &lt;int&gt;, low_wage_jobs &lt;int&gt;,\n#   major_type &lt;chr&gt;\n\n\n\nWhich STEM majors have median salaries equal to or less than the median for all majors’ median earnings? Your output should only show the major name and median, 25th percentile, and 75th percentile earning for that major as and should be sorted such that the major with the highest median earning is on top."
  },
  {
    "objectID": "teaching/tidyverse-I/material/college-majors.html#what-types-of-majors-do-women-tend-to-major-in",
    "href": "teaching/tidyverse-I/material/college-majors.html#what-types-of-majors-do-women-tend-to-major-in",
    "title": "What should I major in?",
    "section": "What types of majors do women tend to major in?",
    "text": "What types of majors do women tend to major in?\n\nCreate a scatterplot of median income vs. proportion of women in that major, coloured by whether the major is in a STEM field or not. Describe the association between these three variables."
  },
  {
    "objectID": "teaching/tidyverse-I/material/college-majors.html#further-exploration",
    "href": "teaching/tidyverse-I/material/college-majors.html#further-exploration",
    "title": "What should I major in?",
    "section": "Further exploration",
    "text": "Further exploration\n\nAsk a question of interest to you, and answer it using summary statistic(s) and/or visualization(s)."
  },
  {
    "objectID": "rpackage/netropy/index.html",
    "href": "rpackage/netropy/index.html",
    "title": "netropy",
    "section": "",
    "text": "Termeh Shafie"
  }
]