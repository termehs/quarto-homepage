[
  {
    "objectID": "rpackage/multigraphr/index.html",
    "href": "rpackage/multigraphr/index.html",
    "title": "multigraphr",
    "section": "",
    "text": "Termeh Shafie"
  },
  {
    "objectID": "rpackage/index.html",
    "href": "rpackage/index.html",
    "title": "R packages",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights-ws.html",
    "href": "teaching/tidyverse-I/material/nyc-flights-ws.html",
    "title": "NYC flights",
    "section": "",
    "text": "For the below exercies we use the tidyverse package for much of the data wrangling and visualisation and the data lives in the nycflights13 package. This is data a dataset of flights departing from New York City (NYC) airports in the year 2013.\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\nThe data sets available in the package can be viewed using the following syntax:\nYou will need one or combinations of these to solve the following exercises. These tables are organized as the figure below shows."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights-ws.html#packages-and-data",
    "href": "teaching/tidyverse-I/material/nyc-flights-ws.html#packages-and-data",
    "title": "NYC flights",
    "section": "",
    "text": "For the below exercies we use the tidyverse package for much of the data wrangling and visualisation and the data lives in the nycflights13 package. This is data a dataset of flights departing from New York City (NYC) airports in the year 2013.\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\nThe data sets available in the package can be viewed using the following syntax:\nYou will need one or combinations of these to solve the following exercises. These tables are organized as the figure below shows."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights-ws.html#familiarizing-ourselves-with-the-dataset",
    "href": "teaching/tidyverse-I/material/nyc-flights-ws.html#familiarizing-ourselves-with-the-dataset",
    "title": "NYC flights",
    "section": "Familiarizing ourselves with the dataset",
    "text": "Familiarizing ourselves with the dataset\n\nWhat variables are included in the flights dataset? How many rows are there?\nWhat variables are included in the airports dataset? How many rows are there?\nWhich variables are included in the airlines dataset? How many rows are there?"
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights-ws.html#focusing-on-atlanta",
    "href": "teaching/tidyverse-I/material/nyc-flights-ws.html#focusing-on-atlanta",
    "title": "NYC flights",
    "section": "Focusing on Atlanta",
    "text": "Focusing on Atlanta\n\nLet’s focus on flights from NYC area airports to Atlanta GA (FAA code ATL). Create a new object atlanta that includes only these flights. Hint: use filter()). How many flights to Atlanta were there in 2013?"
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights-ws.html#seasonality",
    "href": "teaching/tidyverse-I/material/nyc-flights-ws.html#seasonality",
    "title": "NYC flights",
    "section": "Seasonality",
    "text": "Seasonality\n\nIs there a difference in the number of flights per month?\nSummarize the number of flights for each month and provide a sorted list with the months with the most flights first. Hint: use group_by() in combination with summarize())."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights-ws.html#use-filter",
    "href": "teaching/tidyverse-I/material/nyc-flights-ws.html#use-filter",
    "title": "NYC flights",
    "section": "Use filter()",
    "text": "Use filter()\n\nFind all flights that\n\n\nHad an arrival delay of two or more hours.\nFlew to Houston (IAH or HOU)\nWere operated by United, American, or Delta. Hint: In the flights dataset, the column carrier indicates the airline, but it uses two-character carrier codes. You can find the carrier codes for the airlines in the airlines dataset. Since the carrier code dataset only has 16 rows, and the names of the airlines in that dataset are not exactly “United”, “American”, or “Delta”, it is easiest to manually look up their carrier codes in that data.\nDeparted in summer (July, August, and September). Hint: the summer flights are those that departed in months 7 (July), 8 (August), and 9 (September).\nArrived more than two hours late, but didn’t leave late. Hint: Flights that arrived more than two hours late, but didn’t leave late will have an arrival delay of more than 120 minutes (arr_delay &gt; 120) and a non-positive departure delay (dep_delay &lt;=0)\nWere delayed by at least an hour, but made up over 30 minutes in flight. Hint: If a flight was delayed by at least an hour, then dep_delay &gt;= 60. If the flight didn’t make up any time in the air, then its arrival would be delayed by the same amount as its departure, meaning dep_delay == arr_delay, or alternatively, dep_delay - arr_delay == 0. If it makes up over 30 minutes in the air, then the arrival delay must be at least 30 minutes less than the departure delay, which is stated as dep_delay - arr_delay &gt; 30.\nDeparted between midnight and 6 am (inclusive). Hint: In dep_time, midnight is represented by 2400, not 0. You can verify this by checking the minimum and maximum of dep_time."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights-ws.html#arrange-rows-with-arrange",
    "href": "teaching/tidyverse-I/material/nyc-flights-ws.html#arrange-rows-with-arrange",
    "title": "NYC flights",
    "section": "Arrange rows with arrange()",
    "text": "Arrange rows with arrange()\n\nHow could you use arrange() to sort all missing values to the start? Hint: use is.na()) and add an indicator of whether the column has a missing value, the flights will first be sorted by desc(is.na(dep_time)). Since desc(is.na(dep_time)) is either TRUE when dep_time is missing, or FALSE, when it is not, the rows with missing values of dep_time will come first, since TRUE &gt; FALSE.\nSort flights to find the most delayed flights. Find the flights that left earliest.\nSort flights to find the fastest flights."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights-ws.html#seelct-variables-with-select",
    "href": "teaching/tidyverse-I/material/nyc-flights-ws.html#seelct-variables-with-select",
    "title": "NYC flights",
    "section": "Seelct variables with select()",
    "text": "Seelct variables with select()\n\nWhat does the one_of() function do? Why might it be helpful in conjunction with this vector?"
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights-ws.html#add-new-variables-with-mutate",
    "href": "teaching/tidyverse-I/material/nyc-flights-ws.html#add-new-variables-with-mutate",
    "title": "NYC flights",
    "section": "Add new variables with mutate()",
    "text": "Add new variables with mutate()\n\nCompare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it?\nCome up with another approach that will give you the same output as not_cancelled %&gt;% count(dest) and not_cancelled %&gt;% count(tailnum, wt = distance) (without using count()).\nLook at the number of cancelled flights per day. Is there a pattern? Create a plot to visualize your answers.\nFor each destination, compute the total minutes of delay. For each flight, compute the proportion of the total delay for its destination.\nDelays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag() to explore how the delay of a flight is related to the delay of the immediately preceding flight. Use a plot to visualize this."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights-ws.html#more-viz",
    "href": "teaching/tidyverse-I/material/nyc-flights-ws.html#more-viz",
    "title": "NYC flights",
    "section": "More Viz",
    "text": "More Viz\n\nVisualize the distribution of on time departure rate across the three airports using a segmented bar plot. Hint: Remove NA’s and suppose that a flight that is delayed for less than 5 minutes is basically “on time”."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights-ws.html#advanced-exercises",
    "href": "teaching/tidyverse-I/material/nyc-flights-ws.html#advanced-exercises",
    "title": "NYC flights",
    "section": "Advanced Exercises:",
    "text": "Advanced Exercises:\n\nImagine you wanted to draw (approximately) the route each plane flies from its origin to its destination. What variables would you need? What tables from the package you loaded would you need to combine?\nThis plots the approximate flight paths of the first 100 flights in the flights dataset. Try reproducing it. Hint: you can create a layer of map borders using borders(state).\nWe know that some days of the year are “special”, and fewer people than usual fly on them. Since it is US data for 2013 we will consider: New Years Day, Independence Day, Thanksgiving Day, Christmas Day.\n\nHow might you represent that data as a data frame? What would be the primary keys of that table? How would it connect to the existing tables?\nWe can add a table of special dates, similar to the following table.\n\nspecial_days &lt;- tribble(\n  ~year, ~month, ~day, ~holiday,\n  2013, 01, 01, \"New Years Day\",\n  2013, 07, 04, \"Independence Day\",\n  2013, 11, 29, \"Thanksgiving Day\",\n  2013, 12, 25, \"Christmas Day\"\n)\n\nThe primary key of the table would be the (year, month, day) columns. The (year, month, day) columns could be used to join special_days with other tables.\n\nCreate a visualization fo your own to illustrate if indeed fewer people than usual fly on the above special days.\nCompute the average delay by destination, then join on the airports data frame so you can show the spatial distribution of delays. Here’s an easy way to draw a map of the United States (can you understand why we choose semi-join?):\n\n\nairports %&gt;%\n  semi_join(flights, c(\"faa\" = \"dest\")) %&gt;%\n  ggplot(aes(lon, lat)) +\n  borders(\"state\") +\n  geom_point() +\n  coord_quickmap() + \n  theme_void()\n\n\n\n\n\n\n\n\nHint: You might want to use the size or color of the points to display the average delay for each airport.\n\nWhat weather conditions make it more likely to see a delay? Use the variable precip (precipitation) from the weather dataset to answer this.\nWhat happened on June 13, 2013? Reproduce the following plot which displays the spatial pattern of delays, and then use Google to cross-reference with the weather. Hint: use library(viridis) to get the same colors."
  },
  {
    "objectID": "teaching/tidyverse-I/material/la-quinta.html",
    "href": "teaching/tidyverse-I/material/la-quinta.html",
    "title": "La Quinta is Spanish for next to Denny’s",
    "section": "",
    "text": "Have you ever taken a road trip in the US and thought to yourself “I wonder what La Quinta means”. Well, the late comedian Mitch Hedberg thinks it’s Spanish for next to Denny’s.\nIf you’re not familiar with these two establishments, Denny’s is a casual diner chain that is open 24 hours and La Quinta Inn and Suites is a hotel chain.\nThese two establishments tend to be clustered together, or at least this observation is a joke made famous by Mitch Hedberg. In this lab we explore the validity of this joke and along the way learn some more data wrangling and tips for visualizing spatial data.\nThe inspiration for this comes from a blog post by John Reiser on his new jersey geographer blog. You can read that analysis here. Reiser’s blog post focuses on scraping data from Denny’s and La Quinta Inn and Suites websites using Python. Here, we focus on visualization and analysis of these data."
  },
  {
    "objectID": "teaching/tidyverse-I/material/la-quinta.html#packages",
    "href": "teaching/tidyverse-I/material/la-quinta.html#packages",
    "title": "La Quinta is Spanish for next to Denny’s",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for much of the data wrangling and visualisation and the data lives in the dsbox package. These packages are already installed for you. You can load them by running the following in your Console:\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "teaching/tidyverse-I/material/la-quinta.html#data",
    "href": "teaching/tidyverse-I/material/la-quinta.html#data",
    "title": "La Quinta is Spanish for next to Denny’s",
    "section": "Data",
    "text": "Data\nThe data sets we’ll use are called dennys and laquinta and are available for download. Note that these data were scraped from here and here, respectively. You can find information about the data sets here and here. To help with our analysis we will also use a data set on US states.\n\nlaquinta &lt;- read_csv(\"data/laquinta.csv\")\ndennys &lt;- read_csv(\"data/dennys.csv\")\nstates &lt;- read_csv(\"data/states.csv\")\n\nEach observation in the states dataset represents a state, including DC. Along with the name of the state we have the two-letter abbreviation and we have the geographic area of the state (in square miles)."
  },
  {
    "objectID": "teaching/tidyverse-I/material/type-coercion.html",
    "href": "teaching/tidyverse-I/material/type-coercion.html",
    "title": "Type coercion",
    "section": "",
    "text": "c(1, 1L, \"C\")\n\n\nc(1, 1L, \"C\")\n\n[1] \"1\" \"1\" \"C\"\n\n\n\n1\n\n[1] 1\n\n1L\n\n[1] 1\n\n\"C\"\n\n[1] \"C\"\n\n\n\n#typeof(c(1, 1L, \"C\"))\n\n\nc(1L / 0, \"A\")\n\n\nc(1L / 0, \"A\")\n\n[1] \"Inf\" \"A\"  \n\n\n\ntypeof(1L)\n\n[1] \"integer\"\n\ntypeof(0)\n\n[1] \"double\"\n\ntypeof(1L/0)\n\n[1] \"double\"\n\ntypeof(\"A\")\n\n[1] \"character\"\n\n\n\n#typeof(c(1L / 0, \"A\"))\n\n\nc(1:3, 5)\n\n\nc(1:3, 5)\n\n[1] 1 2 3 5\n\n\n\ntypeof(1:3)\n\n[1] \"integer\"\n\ntypeof(5)\n\n[1] \"double\"\n\n\n\n#typeof(c(1:3, 5))\n\n\nc(3, \"3+\")\n\n\nc(3, \"3+\")\n\n[1] \"3\"  \"3+\"\n\n\n\ntypeof(3)\n\n[1] \"double\"\n\ntypeof(\"3+\")\n\n[1] \"character\"\n\n\n\n#typeof(c(3, \"3+\"))\n\n\nc(NA, TRUE)\n\n\nc(NA, TRUE)\n\n[1]   NA TRUE\n\n\n\ntypeof(NA)\n\n[1] \"logical\"\n\ntypeof(TRUE)\n\n[1] \"logical\"\n\n\n\n#typeof(c(NA, TRUE))"
  },
  {
    "objectID": "teaching/tidyverse-I/material/plastic-waste.html",
    "href": "teaching/tidyverse-I/material/plastic-waste.html",
    "title": "Global plastic waste",
    "section": "",
    "text": "Plastic pollution is a major and growing problem, negatively affecting oceans and wildlife health. Our World in Data has a lot of great data at various levels including globally, per country, and over time. For this lab we focus on data from 2010.\nAdditionally, National Geographic ran a data visualization communication contest on plastic waste as seen here."
  },
  {
    "objectID": "teaching/tidyverse-I/material/plastic-waste.html#packages",
    "href": "teaching/tidyverse-I/material/plastic-waste.html#packages",
    "title": "Global plastic waste",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for this analysis.\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "teaching/tidyverse-I/material/plastic-waste.html#data",
    "href": "teaching/tidyverse-I/material/plastic-waste.html#data",
    "title": "Global plastic waste",
    "section": "Data",
    "text": "Data\nThe dataset for this assignment can be found as a csv file. You can read it in using the following (make sure you save the data in your working directory).\n\nplastic_waste &lt;- read_csv(\"data-pw/plastic-waste.csv\")\n\nThe variable descriptions are as follows:\n\ncode: 3 Letter country code\nentity: Country name\ncontinent: Continent name\nyear: Year\ngdp_per_cap: GDP per capita constant 2011 international $, rate\nplastic_waste_per_cap: Amount of plastic waste per capita in kg/day\nmismanaged_plastic_waste_per_cap: Amount of mismanaged plastic waste per capita in kg/day\nmismanaged_plastic_waste: Tonnes of mismanaged plastic waste\ncoastal_pop: Number of individuals living on/near coast\ntotal_pop: Total population according to Gapminder"
  },
  {
    "objectID": "teaching/tidyverse-I/material/nobels-csv.html",
    "href": "teaching/tidyverse-I/material/nobels-csv.html",
    "title": "Nobel winners",
    "section": "",
    "text": "library(tidyverse)\n\nLet’s first load the data:\n\nnobel &lt;- ___(___)\n\nThen let’s split the data into two:\n\n# stem laureates\n___ &lt;- nobel %&gt;%\n  filter(___)\n\n# non-steam laureates\n___ &lt;- nobel %&gt;%\n  filter(___)\n\nAnd finally write out the data:\n\n# add code for writing out the two data frames here"
  },
  {
    "objectID": "teaching/tidyverse-I/material/brexit-ws.html",
    "href": "teaching/tidyverse-I/material/brexit-ws.html",
    "title": "Brexit",
    "section": "",
    "text": "library(tidyverse)\n\nIn September 2019, YouGov survey asked 1,639 GB adults the following question:\n\nIn hindsight, do you think Britain was right/wrong to vote to leave EU?\n\nRight to leave\n\nWrong to leave\n\nDon’t know\n\n\nThe data from the survey is in data/brexit.csv.\n\nbrexit &lt;- read_csv(\"data-brexit/brexit.csv\")\n\nIn the course video we made the following visualisation.\n\nbrexit &lt;- brexit %&gt;%\n  mutate(\n    region = fct_relevel(region, \"london\", \"rest_of_south\", \"midlands_wales\", \"north\", \"scot\"),\n    region = fct_recode(region, London = \"london\", `Rest of South` = \"rest_of_south\", `Midlands / Wales` = \"midlands_wales\", North = \"north\", Scotland = \"scot\")\n  )\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  ) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\",\n    \"Right\" = \"#67a9cf\",\n    \"Don't know\" = \"gray\"\n  )) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn this application exercise we tell different stories with the same data.\n\nExercise 1 - Free scales\nAdd scales = \"free_x\" as an argument to the facet_wrap() function. How does the visualisation change? How is the story this visualisation telling different than the story the original plot tells?\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region,\n    nrow = 1, labeller = label_wrap_gen(width = 12),\n    # ___\n  ) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  ) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\",\n    \"Right\" = \"#67a9cf\",\n    \"Don't know\" = \"gray\"\n  )) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nExercise 2 - Comparing proportions across facets\nFirst, calculate the proportion of wrong, right, and don’t know answers in each category and then plot these proportions (rather than the counts) and then improve axis labeling. How is the story this visualisation telling different than the story the original plot tells? Hint: You’ll need the scales package to improve axis labeling, which means you’ll need to load it on top of the document as well.\n\n# code goes here\n\n\n\nExercise 3 - Comparing proportions across bars\nRecreate the same visualisation from the previous exercise, this time dodging the bars for opinion proportions for each region, rather than faceting by region and then improve the legend. How is the story this visualisation telling different than the story the previous plot tells?\n\n# code goes here"
  },
  {
    "objectID": "teaching/tidyverse-I/material/hotels-forcats.html",
    "href": "teaching/tidyverse-I/material/hotels-forcats.html",
    "title": "Hotel bookings - factors",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)\n\nLoad the hotels data set we used in a previous practical. Render and view the following visualisation. How are the months ordered? What would be a better order? Then, reorder the months on the x-axis (levels of arrival_date_month) in a way that makes more sense. You will want to use a function from the forcats package, see https://forcats.tidyverse.org/reference/index.html for inspiration and help.\nStretch goal: If you finish the above task before time is up, change the y-axis label so the values are shown with dollar signs, e.g. $80 instead of 80. You will want to use a function from the scales package, see https://scales.r-lib.org/reference/index.html for inspiration and help.\n\nhotels %&gt;%\n  group_by(hotel, arrival_date_month) %&gt;%   # group by hotel type and arrival month\n  summarise(mean_adr = mean(adr)) %&gt;%       # calculate mean adr for each group\n  ggplot(aes(\n    x = arrival_date_month,                 # x-axis = arrival_date_month\n    y = mean_adr,                           # y-axis = mean_adr calculated above\n    group = hotel,                          # group lines by hotel type\n    color = hotel)                          # and color by hotel type\n    ) +\n  geom_line() +                             # use lines to represent data\n  theme_minimal() +                         # use a minimal theme\n  labs(\n    x = \"Arrival month\",                 # customize labels\n    y = \"Mean ADR (average daily rate)\",\n    title = \"Comparison of resort and city hotel prices across months\",\n    subtitle = \"Resort hotel prices soar in the summer while ciry hotel prices remain relatively constant throughout the year\",\n    color = \"Hotel type\"\n    )"
  },
  {
    "objectID": "teaching/tidyverse-I/material/bechdel-ws.html",
    "href": "teaching/tidyverse-I/material/bechdel-ws.html",
    "title": "Bechdel",
    "section": "",
    "text": "In this mini analysis we work with the data used in the FiveThirtyEight story titled “The Dollar-And-Cents Case Against Hollywood’s Exclusion of Women”. We will together fill in the blanks denoted by ___."
  },
  {
    "objectID": "teaching/tidyverse-I/material/bechdel-ws.html#data-and-packages",
    "href": "teaching/tidyverse-I/material/bechdel-ws.html#data-and-packages",
    "title": "Bechdel",
    "section": "Data and packages",
    "text": "Data and packages\nWe start with loading the packages we’ll use.\n\nlibrary(fivethirtyeight)\nlibrary(tidyverse)\n\nThe dataset contains information on 1794 movies released between 1970 and 2013. However we’ll focus our analysis on movies released between 1990 and 2013.\n\nbechdel90_13 &lt;- bechdel %&gt;% \n  filter(between(year, 1990, 2013))\n\nThere are ___ such movies.\nThe financial variables we’ll focus on are the following:\n\nbudget_2013: Budget in 2013 inflation adjusted dollars\ndomgross_2013: Domestic gross (US) in 2013 inflation adjusted dollars\nintgross_2013: Total International (i.e., worldwide) gross in 2013 inflation adjusted dollars\n\nAnd we’ll also use the binary and clean_test variables for grouping."
  },
  {
    "objectID": "teaching/tidyverse-I/material/bechdel-ws.html#analysis",
    "href": "teaching/tidyverse-I/material/bechdel-ws.html#analysis",
    "title": "Bechdel",
    "section": "Analysis",
    "text": "Analysis\nLet’s take a look at how median budget and gross vary by whether the movie passed the Bechdel test, which is stored in the binary variable.\n\nbechdel90_13 %&gt;%\n  group_by(binary) %&gt;%\n  summarise(\n    med_budget = median(budget_2013),\n    med_domgross = median(domgross_2013, na.rm = TRUE),\n    med_intgross = median(intgross_2013, na.rm = TRUE)\n    )\n\n# A tibble: 2 × 4\n  binary med_budget med_domgross med_intgross\n  &lt;chr&gt;       &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 FAIL    48385984.    57318606.    104475669\n2 PASS    31070724     45330446.     80124349\n\n\nNext, let’s take a look at how median budget and gross vary by a more detailed indicator of the Bechdel test result. This information is stored in the clean_test variable, which takes on the following values:\n\nok = passes test\ndubious\nmen = women only talk about men\nnotalk = women don’t talk to each other\nnowomen = fewer than two women\n\n\nbechdel90_13 %&gt;%\n  #group_by(___) %&gt;%\n  summarise(\n    med_budget = median(budget_2013),\n    med_domgross = median(domgross_2013, na.rm = TRUE),\n    med_intgross = median(intgross_2013, na.rm = TRUE)\n    )\n\n# A tibble: 1 × 3\n  med_budget med_domgross med_intgross\n       &lt;int&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1   37878971     52270207     93523336\n\n\nIn order to evaluate how return on investment varies among movies that pass and fail the Bechdel test, we’ll first create a new variable called roi as the ratio of the gross to budget.\n\nbechdel90_13 &lt;- bechdel90_13 %&gt;%\n  mutate(roi = (intgross_2013 + domgross_2013) / budget_2013)\n\nLet’s see which movies have the highest return on investment.\n\nbechdel90_13 %&gt;%\n  arrange(desc(roi)) %&gt;% \n  select(title, roi, year)\n\n# A tibble: 1,615 × 3\n   title                     roi  year\n   &lt;chr&gt;                   &lt;dbl&gt; &lt;int&gt;\n 1 Paranormal Activity      671.  2007\n 2 The Blair Witch Project  648.  1999\n 3 El Mariachi              583.  1992\n 4 Clerks.                  258.  1994\n 5 In the Company of Men    231.  1997\n 6 Napoleon Dynamite        227.  2004\n 7 Once                     190.  2006\n 8 The Devil Inside         155.  2012\n 9 Primer                   142.  2004\n10 Fireproof                134.  2008\n# ℹ 1,605 more rows\n\n\nBelow is a visualization of the return on investment by test result, however it’s difficult to see the distributions due to a few extreme observations.\n\nggplot(data = bechdel90_13, \n       mapping = aes(x = clean_test, y = roi, color = binary)) +\n  geom_boxplot() +\n  labs(\n    title = \"Return on investment vs. Bechdel test result\",\n    x = \"Detailed Bechdel result\",\n    y = \"___\",\n    color = \"Binary Bechdel result\"\n    )\n\n\n\n\n\n\n\n\nWhat are those movies with very high returns on investment?\n\nbechdel90_13 %&gt;%\n  filter(roi &gt; 400) %&gt;%\n  select(title, budget_2013, domgross_2013, year)\n\n# A tibble: 3 × 4\n  title                   budget_2013 domgross_2013  year\n  &lt;chr&gt;                         &lt;int&gt;         &lt;dbl&gt; &lt;int&gt;\n1 Paranormal Activity          505595     121251476  2007\n2 The Blair Witch Project      839077     196538593  1999\n3 El Mariachi                   11622       3388636  1992\n\n\nZooming in on the movies with roi &lt; ___ provides a better view of how the medians across the categories compare:\n\nggplot(data = bechdel90_13, mapping = aes(x = clean_test, y = roi, color = binary)) +\n  geom_boxplot() +\n  labs(\n    title = \"Return on investment vs. Bechdel test result\",\n    subtitle = \"___\", # Something about zooming in to a certain level\n    x = \"Detailed Bechdel result\",\n    y = \"Return on investment\",\n    color = \"Binary Bechdel result\"\n    ) +\n  coord_cartesian(ylim = c(0, 15))"
  },
  {
    "objectID": "teaching/tidyverse-I/material/data-type-class-exercises.html",
    "href": "teaching/tidyverse-I/material/data-type-class-exercises.html",
    "title": "Data Type and Data Classes: Exercises",
    "section": "",
    "text": "Double check that you do not have stored objects in your current session with the following command. This will list all objects that you have in your current R session.\n\nls()\n\ncharacter(0)\n\n\nIn case you have objects that you want to remove from the current session you can do so with the rm() function.  This command will remove all objects available in your current environment.\n\nrm(list = ls())\n\nThis command uses commands that we have not talked about yet. If you do not understand how it works now, you will do so after tomorrows lectures and exercises.\n\nCreate variables var1 and var2 and initialize them with two integers of choice.\nAdd the two variables and save them as a new variable named var3 and print the result.\nCheck the class, mode, and type for var1, var2, var3 and π (is found under the variable name pi in R)\nCreate two character variables containing a text of choice. Check the mode, class, and type of the first one.\n\nAdd var1 to it. What is the result and why?\n\n\n\n\nConvert var3 to an integer, cast an integer variable to double, cast a string to a double.\nReport floor and ceiling of π and round π to 3 decimal places.\nIs floor of π an integer?\nTreat \"3.56437\" string as number.\nDivide ∞ by - ∞\nPrint a truth table for OR (for three distinct logical values). Read about truth tables here.\nMultiply a logical TRUE by a logical FALSE. Rise the logical true to the 7-th power.\nCreate two character variables containing two verses of a chosen song.\n\n\nConcatenate the two variables,\n\nPaste the variables with ‘*’ as separator.\n\nFind if ‘and’ occurs in the second line,\n\nSubstitute a word for another,\n\nExtract substring starting at the 5th character and 5 characters long."
  },
  {
    "objectID": "teaching/tidyverse-I/material/data-type-class-exercises.html#exercise",
    "href": "teaching/tidyverse-I/material/data-type-class-exercises.html#exercise",
    "title": "Data Type and Data Classes: Exercises",
    "section": "",
    "text": "Double check that you do not have stored objects in your current session with the following command. This will list all objects that you have in your current R session.\n\nls()\n\ncharacter(0)\n\n\nIn case you have objects that you want to remove from the current session you can do so with the rm() function.  This command will remove all objects available in your current environment.\n\nrm(list = ls())\n\nThis command uses commands that we have not talked about yet. If you do not understand how it works now, you will do so after tomorrows lectures and exercises.\n\nCreate variables var1 and var2 and initialize them with two integers of choice.\nAdd the two variables and save them as a new variable named var3 and print the result.\nCheck the class, mode, and type for var1, var2, var3 and π (is found under the variable name pi in R)\nCreate two character variables containing a text of choice. Check the mode, class, and type of the first one.\n\nAdd var1 to it. What is the result and why?\n\n\n\n\nConvert var3 to an integer, cast an integer variable to double, cast a string to a double.\nReport floor and ceiling of π and round π to 3 decimal places.\nIs floor of π an integer?\nTreat \"3.56437\" string as number.\nDivide ∞ by - ∞\nPrint a truth table for OR (for three distinct logical values). Read about truth tables here.\nMultiply a logical TRUE by a logical FALSE. Rise the logical true to the 7-th power.\nCreate two character variables containing two verses of a chosen song.\n\n\nConcatenate the two variables,\n\nPaste the variables with ‘*’ as separator.\n\nFind if ‘and’ occurs in the second line,\n\nSubstitute a word for another,\n\nExtract substring starting at the 5th character and 5 characters long."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights.html",
    "href": "teaching/tidyverse-I/material/nyc-flights.html",
    "title": "NYC flights",
    "section": "",
    "text": "For the below exercies we use the tidyverse package for much of the data wrangling and visualisation and the data lives in the nycflights13 package. This is data a dataset of flights departing from New York City (NYC) airports in the year 2013.\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\nThe data sets available in the package can be viewed using the following syntax:\nYou will need one or combinations of these to solve the following exercises. These tables are organized as the figure below shows."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights.html#packages-and-data",
    "href": "teaching/tidyverse-I/material/nyc-flights.html#packages-and-data",
    "title": "NYC flights",
    "section": "",
    "text": "For the below exercies we use the tidyverse package for much of the data wrangling and visualisation and the data lives in the nycflights13 package. This is data a dataset of flights departing from New York City (NYC) airports in the year 2013.\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\nThe data sets available in the package can be viewed using the following syntax:\nYou will need one or combinations of these to solve the following exercises. These tables are organized as the figure below shows."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights.html#familiarizing-ourselves-with-the-dataset",
    "href": "teaching/tidyverse-I/material/nyc-flights.html#familiarizing-ourselves-with-the-dataset",
    "title": "NYC flights",
    "section": "Familiarizing ourselves with the dataset",
    "text": "Familiarizing ourselves with the dataset\n\nWhat variables are included in the flights dataset? How many rows are there?\nWhat variables are included in the airports dataset? How many rows are there?\nWhich variables are included in the airlines dataset? How many rows are there?"
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights.html#focusing-on-atlanta",
    "href": "teaching/tidyverse-I/material/nyc-flights.html#focusing-on-atlanta",
    "title": "NYC flights",
    "section": "Focusing on Atlanta",
    "text": "Focusing on Atlanta\n\nLet’s focus on flights from NYC area airports to Atlanta GA (FAA code ATL). Create a new object atlanta that includes only these flights. Hint: use filter()). How many flights to Atlanta were there in 2013?"
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights.html#seasonality",
    "href": "teaching/tidyverse-I/material/nyc-flights.html#seasonality",
    "title": "NYC flights",
    "section": "Seasonality",
    "text": "Seasonality\n\nIs there a difference in the number of flights per month?\nSummarize the number of flights for each month and provide a sorted list with the months with the most flights first. Hint: use group_by() in combination with summarize())."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights.html#use-filter",
    "href": "teaching/tidyverse-I/material/nyc-flights.html#use-filter",
    "title": "NYC flights",
    "section": "Use filter()",
    "text": "Use filter()\n\nFind all flights that\n\n\nHad an arrival delay of two or more hours.\nFlew to Houston (IAH or HOU)\nWere operated by United, American, or Delta. Hint: In the flights dataset, the column carrier indicates the airline, but it uses two-character carrier codes. You can find the carrier codes for the airlines in the airlines dataset. Since the carrier code dataset only has 16 rows, and the names of the airlines in that dataset are not exactly “United”, “American”, or “Delta”, it is easiest to manually look up their carrier codes in that data.\nDeparted in summer (July, August, and September). Hint: the summer flights are those that departed in months 7 (July), 8 (August), and 9 (September).\nArrived more than two hours late, but didn’t leave late. Hint: Flights that arrived more than two hours late, but didn’t leave late will have an arrival delay of more than 120 minutes (arr_delay &gt; 120) and a non-positive departure delay (dep_delay &lt;=0)\nWere delayed by at least an hour, but made up over 30 minutes in flight. Hint: If a flight was delayed by at least an hour, then dep_delay &gt;= 60. If the flight didn’t make up any time in the air, then its arrival would be delayed by the same amount as its departure, meaning dep_delay == arr_delay, or alternatively, dep_delay - arr_delay == 0. If it makes up over 30 minutes in the air, then the arrival delay must be at least 30 minutes less than the departure delay, which is stated as dep_delay - arr_delay &gt; 30.\nDeparted between midnight and 6 am (inclusive). Hint: In dep_time, midnight is represented by 2400, not 0. You can verify this by checking the minimum and maximum of dep_time."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights.html#arrange-rows-with-arrange",
    "href": "teaching/tidyverse-I/material/nyc-flights.html#arrange-rows-with-arrange",
    "title": "NYC flights",
    "section": "Arrange rows with arrange()",
    "text": "Arrange rows with arrange()\n\nHow could you use arrange() to sort all missing values to the start? Hint: use is.na()) and add an indicator of whether the column has a missing value, the flights will first be sorted by desc(is.na(dep_time)). Since desc(is.na(dep_time)) is either TRUE when dep_time is missing, or FALSE, when it is not, the rows with missing values of dep_time will come first, since TRUE &gt; FALSE.\nSort flights to find the most delayed flights. Find the flights that left earliest.\nSort flights to find the fastest flights."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights.html#seelct-variables-with-select",
    "href": "teaching/tidyverse-I/material/nyc-flights.html#seelct-variables-with-select",
    "title": "NYC flights",
    "section": "Seelct variables with select()",
    "text": "Seelct variables with select()\n\nWhat does the one_of() function do? Why might it be helpful in conjunction with this vector?"
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights.html#add-new-variables-with-mutate",
    "href": "teaching/tidyverse-I/material/nyc-flights.html#add-new-variables-with-mutate",
    "title": "NYC flights",
    "section": "Add new variables with mutate()",
    "text": "Add new variables with mutate()\n\nCompare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it?\nCome up with another approach that will give you the same output as not_cancelled %&gt;% count(dest) and not_cancelled %&gt;% count(tailnum, wt = distance) (without using count()).\nLook at the number of cancelled flights per day. Is there a pattern? Create a plot to visualize your answers.\nFor each destination, compute the total minutes of delay. For each flight, compute the proportion of the total delay for its destination.\nDelays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag() to explore how the delay of a flight is related to the delay of the immediately preceding flight. Use a plot to visualize this."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights.html#more-viz",
    "href": "teaching/tidyverse-I/material/nyc-flights.html#more-viz",
    "title": "NYC flights",
    "section": "More Viz",
    "text": "More Viz\n\nVisualize the distribution of on time departure rate across the three airports using a segmented bar plot. Hint: Remove NA’s and suppose that a flight that is delayed for less than 5 minutes is basically “on time”."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nyc-flights.html#advanced-exercises",
    "href": "teaching/tidyverse-I/material/nyc-flights.html#advanced-exercises",
    "title": "NYC flights",
    "section": "Advanced Exercises:",
    "text": "Advanced Exercises:\n\nImagine you wanted to draw (approximately) the route each plane flies from its origin to its destination. What variables would you need? What tables from the package you loaded would you need to combine?\nThis plots the approximate flight paths of the first 100 flights in the flights dataset. Try reproducing it. Hint: you can create a layer of map borders using borders(state).\n\n\n\n\n\n\n\n\n\n\n\nWe know that some days of the year are “special”, and fewer people than usual fly on them. Since it is US data for 2013 we will consider: New Years Day, Independence Day, Thanksgiving Day, Christmas Day.\n\nHow might you represent that data as a data frame? What would be the primary keys of that table? How would it connect to the existing tables?\nWe can add a table of special dates, similar to the following table.\n\nspecial_days &lt;- tribble(\n  ~year, ~month, ~day, ~holiday,\n  2013, 01, 01, \"New Years Day\",\n  2013, 07, 04, \"Independence Day\",\n  2013, 11, 29, \"Thanksgiving Day\",\n  2013, 12, 25, \"Christmas Day\"\n)\n\nThe primary key of the table would be the (year, month, day) columns. The (year, month, day) columns could be used to join special_days with other tables.\n\nCreate a visualization fo your own to illustrate if indeed fewer people than usual fly on the above special days.\nCompute the average delay by destination, then join on the airports data frame so you can show the spatial distribution of delays. Here’s an easy way to draw a map of the United States (can you understand why we choose semi-join?):\n\n\nairports %&gt;%\n  semi_join(flights, c(\"faa\" = \"dest\")) %&gt;%\n  ggplot(aes(lon, lat)) +\n  borders(\"state\") +\n  geom_point() +\n  coord_quickmap() + \n  theme_void()\n\n\n\n\n\n\n\n\nHint: You might want to use the size or color of the points to display the average delay for each airport.\n\nWhat weather conditions make it more likely to see a delay? Use the variable precip (precipitation) from the weather dataset to answer this.\nWhat happened on June 13, 2013? Reproduce the following plot which displays the spatial pattern of delays, and then use Google to cross-reference with the weather. Hint: use library(viridis) to get the same colors."
  },
  {
    "objectID": "teaching/tidyverse-I/material/hotels-datawrangling.html",
    "href": "teaching/tidyverse-I/material/hotels-datawrangling.html",
    "title": "Hotel bookings - data wrangling",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)\n# From TidyTuesday: https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-02-11/readme.md\nhotels &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\nThe data is also available as a csv file which you can import directly."
  },
  {
    "objectID": "teaching/tidyverse-I/material/hotels-datawrangling.html#exercises",
    "href": "teaching/tidyverse-I/material/hotels-datawrangling.html#exercises",
    "title": "Hotel bookings - data wrangling",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\nWarm up! Take a look at an overview of the data with the skim() function.\nNote: I already gave you the answer to this exercise. You just need to knit the document and view the output. A definition of all variables is given in the Data dictionary section at the end, though you don’t need to familiarize yourself with all variables in order to work through these exercises.\n\nskim(hotels)\n\n\nData summary\n\n\nName\nhotels\n\n\nNumber of rows\n119390\n\n\nNumber of columns\n32\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n13\n\n\nDate\n1\n\n\nnumeric\n18\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nhotel\n0\n1\n10\n12\n0\n2\n0\n\n\narrival_date_month\n0\n1\n3\n9\n0\n12\n0\n\n\nmeal\n0\n1\n2\n9\n0\n5\n0\n\n\ncountry\n0\n1\n2\n4\n0\n178\n0\n\n\nmarket_segment\n0\n1\n6\n13\n0\n8\n0\n\n\ndistribution_channel\n0\n1\n3\n9\n0\n5\n0\n\n\nreserved_room_type\n0\n1\n1\n1\n0\n10\n0\n\n\nassigned_room_type\n0\n1\n1\n1\n0\n12\n0\n\n\ndeposit_type\n0\n1\n10\n10\n0\n3\n0\n\n\nagent\n0\n1\n1\n4\n0\n334\n0\n\n\ncompany\n0\n1\n1\n4\n0\n353\n0\n\n\ncustomer_type\n0\n1\n5\n15\n0\n4\n0\n\n\nreservation_status\n0\n1\n7\n9\n0\n3\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nreservation_status_date\n0\n1\n2014-10-17\n2017-09-14\n2016-08-07\n926\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nis_canceled\n0\n1\n0.37\n0.48\n0.00\n0.00\n0.00\n1\n1\n▇▁▁▁▅\n\n\nlead_time\n0\n1\n104.01\n106.86\n0.00\n18.00\n69.00\n160\n737\n▇▂▁▁▁\n\n\narrival_date_year\n0\n1\n2016.16\n0.71\n2015.00\n2016.00\n2016.00\n2017\n2017\n▃▁▇▁▆\n\n\narrival_date_week_number\n0\n1\n27.17\n13.61\n1.00\n16.00\n28.00\n38\n53\n▅▇▇▇▅\n\n\narrival_date_day_of_month\n0\n1\n15.80\n8.78\n1.00\n8.00\n16.00\n23\n31\n▇▇▇▇▆\n\n\nstays_in_weekend_nights\n0\n1\n0.93\n1.00\n0.00\n0.00\n1.00\n2\n19\n▇▁▁▁▁\n\n\nstays_in_week_nights\n0\n1\n2.50\n1.91\n0.00\n1.00\n2.00\n3\n50\n▇▁▁▁▁\n\n\nadults\n0\n1\n1.86\n0.58\n0.00\n2.00\n2.00\n2\n55\n▇▁▁▁▁\n\n\nchildren\n4\n1\n0.10\n0.40\n0.00\n0.00\n0.00\n0\n10\n▇▁▁▁▁\n\n\nbabies\n0\n1\n0.01\n0.10\n0.00\n0.00\n0.00\n0\n10\n▇▁▁▁▁\n\n\nis_repeated_guest\n0\n1\n0.03\n0.18\n0.00\n0.00\n0.00\n0\n1\n▇▁▁▁▁\n\n\nprevious_cancellations\n0\n1\n0.09\n0.84\n0.00\n0.00\n0.00\n0\n26\n▇▁▁▁▁\n\n\nprevious_bookings_not_canceled\n0\n1\n0.14\n1.50\n0.00\n0.00\n0.00\n0\n72\n▇▁▁▁▁\n\n\nbooking_changes\n0\n1\n0.22\n0.65\n0.00\n0.00\n0.00\n0\n21\n▇▁▁▁▁\n\n\ndays_in_waiting_list\n0\n1\n2.32\n17.59\n0.00\n0.00\n0.00\n0\n391\n▇▁▁▁▁\n\n\nadr\n0\n1\n101.83\n50.54\n-6.38\n69.29\n94.58\n126\n5400\n▇▁▁▁▁\n\n\nrequired_car_parking_spaces\n0\n1\n0.06\n0.25\n0.00\n0.00\n0.00\n0\n8\n▇▁▁▁▁\n\n\ntotal_of_special_requests\n0\n1\n0.57\n0.79\n0.00\n0.00\n0.00\n1\n5\n▇▁▁▁▁\n\n\n\n\n\n\n\nExercise 2.\nAre people traveling on a whim? Let’s see…\nFill in the blanks for filtering for hotel bookings where the guest is not from the US (country code \"USA\") and the lead_time is less than 1 day.\n\nhotels %&gt;%\n  filter(\n    country ____ \"USA\", \n    lead_time ____ ____\n    )\n\n\n\nExercise 3.\nHow many bookings involve at least 1 child or baby?\nIn the following chunk, replace\n\n[AT LEAST] with the logical operator for “at least” (in two places)\n[OR] with the logical operator for “or”\n\nNote: You will need to set eval=TRUE when you have an answer you want to try out in the qmd file.\n\nhotels %&gt;%\n  filter(\n    children [AT LEAST] 1 [OR] babies [AT LEAST] 1\n    )\n\n\n\nExercise 4.\nDo you think it’s more likely to find bookings with children or babies in city hotels or resort hotels? Test your intuition. Using filter() determine the number of bookings in resort hotels that have more than 1 child or baby in the room? Then, do the same for city hotels, and compare the numbers of rows in the resulting filtered data frames.\n\n# add code here\n# pay attention to correctness and code style\n\n\n# add code here\n# pay attention to correctness and code style\n\n\n\nExercise 5.\nCreate a frequency table of the number of adults in a booking. Display the results in descending order so the most common observation is on top. What is the most common number of adults in bookings in this dataset? Are there any surprising results?\n\n# add code here\n# pay attention to correctness and code style\n\n\n\nExercise 6.\nRepeat Exercise 5, once for canceled bookings (is_canceled coded as 1) and once for not canceled bookings (is_canceled coded as 0). What does this reveal about the surprising results you spotted in the previous exercise?\n\n# add code here\n# pay attention to correctness and code style\n\n\n\nExercise 7.\nCalculate minimum, mean, median, and maximum average daily rate (adr) grouped by hotel type so that you can get these statistics separately for resort and city hotels. Which type of hotel is higher, on average?\n\n# add code here\n# pay attention to correctness and code style\n\n\n\nExercise 8.\nWe observe two unusual values in the summary statistics above – a negative minimum, and a very high maximum). What types of hotels are these? Locate these observations in the dataset and find out the arrival date (year and month) as well as how many people (adults, children, and babies) stayed in the room. You can investigate the data in the viewer to locate these values, but preferably you should identify them in a reproducible way with some code.\nHint: For example, you can filter for the given adr amounts and select the relevant columns.\n\n# add code here\n# pay attention to correctness and code style"
  },
  {
    "objectID": "teaching/tidyverse-I/material/hotels-datawrangling.html#data-dictionary",
    "href": "teaching/tidyverse-I/material/hotels-datawrangling.html#data-dictionary",
    "title": "Hotel bookings - data wrangling",
    "section": "Data dictionary",
    "text": "Data dictionary\nBelow is the full data dictionary. Note that it is long (there are lots of variables in the data), but we will be using a limited set of the variables for our analysis.\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nhotel\ncharacter\nHotel (H1 = Resort Hotel or H2 = City Hotel)\n\n\nis_canceled\ndouble\nValue indicating if the booking was canceled (1) or not (0)\n\n\nlead_time\ndouble\nNumber of days that elapsed between the entering date of the booking into the PMS and the arrival date\n\n\narrival_date_year\ndouble\nYear of arrival date\n\n\narrival_date_month\ncharacter\nMonth of arrival date\n\n\narrival_date_week_number\ndouble\nWeek number of year for arrival date\n\n\narrival_date_day_of_month\ndouble\nDay of arrival date\n\n\nstays_in_weekend_nights\ndouble\nNumber of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel\n\n\nstays_in_week_nights\ndouble\nNumber of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel\n\n\nadults\ndouble\nNumber of adults\n\n\nchildren\ndouble\nNumber of children\n\n\nbabies\ndouble\nNumber of babies\n\n\nmeal\ncharacter\nType of meal booked. Categories are presented in standard hospitality meal packages:  Undefined/SC – no meal package;BB – Bed & Breakfast;  HB – Half board (breakfast and one other meal – usually dinner);  FB – Full board (breakfast, lunch and dinner)\n\n\ncountry\ncharacter\nCountry of origin. Categories are represented in the ISO 3155–3:2013 format\n\n\nmarket_segment\ncharacter\nMarket segment designation. In categories, the term “TA” means “Travel Agents” and “TO” means “Tour Operators”\n\n\ndistribution_channel\ncharacter\nBooking distribution channel. The term “TA” means “Travel Agents” and “TO” means “Tour Operators”\n\n\nis_repeated_guest\ndouble\nValue indicating if the booking name was from a repeated guest (1) or not (0)\n\n\nprevious_cancellations\ndouble\nNumber of previous bookings that were cancelled by the customer prior to the current booking\n\n\nprevious_bookings_not_canceled\ndouble\nNumber of previous bookings not cancelled by the customer prior to the current booking\n\n\nreserved_room_type\ncharacter\nCode of room type reserved. Code is presented instead of designation for anonymity reasons\n\n\nassigned_room_type\ncharacter\nCode for the type of room assigned to the booking. Sometimes the assigned room type differs from the reserved room type due to hotel operation reasons (e.g. overbooking) or by customer request. Code is presented instead of designation for anonymity reasons\n\n\nbooking_changes\ndouble\nNumber of changes/amendments made to the booking from the moment the booking was entered on the PMS until the moment of check-in or cancellation\n\n\ndeposit_type\ncharacter\nIndication on if the customer made a deposit to guarantee the booking. This variable can assume three categories:No Deposit – no deposit was made;Non Refund – a deposit was made in the value of the total stay cost;Refundable – a deposit was made with a value under the total cost of stay.\n\n\nagent\ncharacter\nID of the travel agency that made the booking\n\n\ncompany\ncharacter\nID of the company/entity that made the booking or responsible for paying the booking. ID is presented instead of designation for anonymity reasons\n\n\ndays_in_waiting_list\ndouble\nNumber of days the booking was in the waiting list before it was confirmed to the customer\n\n\ncustomer_type\ncharacter\nType of booking, assuming one of four categories:Contract - when the booking has an allotment or other type of contract associated to it;Group – when the booking is associated to a group;Transient – when the booking is not part of a group or contract, and is not associated to other transient booking;Transient-party – when the booking is transient, but is associated to at least other transient booking\n\n\nadr\ndouble\nAverage Daily Rate as defined by dividing the sum of all lodging transactions by the total number of staying nights\n\n\nrequired_car_parking_spaces\ndouble\nNumber of car parking spaces required by the customer\n\n\ntotal_of_special_requests\ndouble\nNumber of special requests made by the customer (e.g. twin bed or high floor)\n\n\nreservation_status\ncharacter\nReservation last status, assuming one of three categories:Canceled – booking was canceled by the customer;Check-Out – customer has checked in but already departed;No-Show – customer did not check-in and did inform the hotel of the reason why\n\n\nreservation_status_date\ndouble\nDate at which the last status was set. This variable can be used in conjunction with the ReservationStatus to understand when was the booking canceled or when did the customer checked-out of the hotel"
  },
  {
    "objectID": "teaching/tidyverse-I/index.html",
    "href": "teaching/tidyverse-I/index.html",
    "title": "Data Science with Tidyverse I",
    "section": "",
    "text": "Make sure to install and load Tidyverse:\n\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n\nSchedule\n\n\n\n\nslides\npractical\ndata\nworksheet\n\n\n\n\n1: Meet the toolkit\n\n\n\n.qmd\n\n\n2: Data visualization and ggplot\n\n\n\n.qmd\n\n\n3: Visualizing numerical and categorical data\n\n\n.zip\n.qmd\n\n\n4: Effective Visualization\n\n\n.zip\n.qmd\n\n\n5: Grammar of data wrangling I\n\n \n.zip\n.qmd\n\n\n6: Grammar of data wrangling II\n\n\n.zip\n.qmd\n\n\n7: Tidying Data\n\n\n.zip\n\n\n\n     More Practicals\n\n \n.zip\n.qmd\n\n\n8: Data Types and Data Classes\n\n  \n.zip\n.qmd\n\n\n9: Importing and Recoding Data\n\n  \n.zip\n.qmd\n\n\n10: Functions and Iteration"
  },
  {
    "objectID": "teaching/math-ds/index.html",
    "href": "teaching/math-ds/index.html",
    "title": "Mathematics for Social Scientists",
    "section": "",
    "text": "Schedule\n\n\n\n\n\n\n\n\nslides\n\n\n\n\n1: Preliminaries\n\n\n\n2: Algebra Review, Modular Arithmetic, Boolean Algebra\n\n\n\n3: Functions & Relations, Sequences & Series, Limits & Continuity\n\n\n\n4: Calculus Fundamentals: Differentiation\n\n\n\n5: Calculus Fundamentals: The Integral\n\n\n\n6: Extrema in One Dimension\n\n\n\n7: Introduction to Probability\n\n\n\n8: Discrete Distributions\n\n\n\n9: Continuous Distributions\n\n\n\n10: Introduction Linear Algebra\n\n\n\n11: Vector Spaces and Systems of Equations\n\n\n\n12: Eigenvalues and Eigenvectors\n\n\n\n13: Multivariate Calculus & Optimization\n\n\n\n14: Multivariate Calculus & Constrained Optimization"
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nSubtitle\n\n\n\n\n\n\n\n\nData Science with Tidyverse I\n\n\nData Wrangling & Visualization\n\n\n\n\n\n\nData Science with Tidyverse II\n\n\nTidymodeling\n\n\n\n\n\n\nIntroductory Statistics\n\n\nBSc course\n\n\n\n\n\n\nMathematics for Social Scientists\n\n\nMSc course\n\n\n\n\n\n\nSocial Network Analysis\n\n\nMSc course\n\n\n\n\n\n\nStatistical Learning\n\n\nMSc course\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/coefficients-ws.html",
    "href": "teaching/tidyverse-II/material/case-studies/coefficients-ws.html",
    "title": "Working with model coefficients",
    "section": "",
    "text": "There are many types of statistical models with diverse kinds of structure. Some models have coefficients (a.k.a. weights) for each term in the model. Familiar examples of such models are linear or logistic regression, but more complex models (e.g. neural networks, MARS) can also have model coefficients. When we work with models that use weights or coefficients, we often want to examine the estimated coefficients."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/coefficients-ws.html#introduction",
    "href": "teaching/tidyverse-II/material/case-studies/coefficients-ws.html#introduction",
    "title": "Working with model coefficients",
    "section": "",
    "text": "There are many types of statistical models with diverse kinds of structure. Some models have coefficients (a.k.a. weights) for each term in the model. Familiar examples of such models are linear or logistic regression, but more complex models (e.g. neural networks, MARS) can also have model coefficients. When we work with models that use weights or coefficients, we often want to examine the estimated coefficients."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/coefficients-ws.html#linear-regression",
    "href": "teaching/tidyverse-II/material/case-studies/coefficients-ws.html#linear-regression",
    "title": "Working with model coefficients",
    "section": "Linear regression",
    "text": "Linear regression\nLet’s start with a linear regression model:\n\\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\ldots + \\hat{\\beta}_px_p\\]\nThe \\(\\beta\\) values are the coefficients and the \\(x_j\\) are model predictors, or features.\nLet’s use the Chicago train data where we predict the ridership at the Clark and Lake station (column name: ridership) with the previous ridership data 14 days prior at three of the stations.\nThe data are in the modeldata package:\n\nlibrary(tidymodels)\ntidymodels_prefer()\ntheme_set(theme_bw())\n\ndata(Chicago)\n\nChicago &lt;- Chicago %&gt;% select(ridership, Clark_Lake, Austin, Harlem)\n\n\nA single model\nLet’s start by fitting only a single parsnip model object. We’ll create a model specification using linear_reg().\n\n\n\n\n\n\nNote\n\n\n\nThe default engine is \"lm\" so no call to set_engine() is required.\n\n\nThe fit() function estimates the model coefficients, given a formula and data set.\n\nlm_spec &lt;- linear_reg()\nlm_fit &lt;- fit(lm_spec, ridership ~ ., data = Chicago)\nlm_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = ridership ~ ., data = data)\n\nCoefficients:\n(Intercept)   Clark_Lake       Austin       Harlem  \n     1.6778       0.9035       0.6123      -0.5550  \n\n\nThe best way to retrieve the fitted parameters is to use the tidy() method. This function, in the broom package, returns the coefficients and their associated statistics in a data frame with standardized column names:\n\ntidy(lm_fit)\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    1.68     0.156      10.7  1.11e- 26\n2 Clark_Lake     0.904    0.0280     32.3  5.14e-210\n3 Austin         0.612    0.320       1.91 5.59e-  2\n4 Harlem        -0.555    0.165      -3.36 7.85e-  4\n\n\nWe’ll use this function in subsequent sections.\n\n\nResampled or tuned models\nThe tidymodels framework emphasizes the use of resampling methods to evaluate and characterize how well a model works. While time series resampling methods are appropriate for these data, we can also use the bootstrap to resample the data. This is a standard resampling approach when evaluating the uncertainty in statistical estimates.\nWe’ll use five bootstrap resamples of the data to simplify the plots and output (normally, we would use a larger number of resamples for more reliable estimates).\n\nset.seed(123)\nbt &lt;- bootstraps(Chicago, times = 5)\n\nWith resampling, we fit the same model to the different simulated versions of the data set produced by resampling. The tidymodels function fit_resamples() is the recommended approach for doing so.\n\n\n\n\n\n\nWarning\n\n\n\nThe fit_resamples() function does not automatically save the model objects for each resample since these can be quite large and its main purpose is estimating performance. However, we can pass a function to fit_resamples() that can save the model object or any other aspect of the fit.\n\n\nThis function takes a single argument that represents the fitted workflow object (even if you don’t give fit_resamples() a workflow).\nFrom this, we can extract the model fit. There are two “levels” of model objects that are available:\n\nThe parsnip model object, which wraps the underlying model object. We retrieve this using the extract_fit_parsnip() function.\nThe underlying model object (a.k.a. the engine fit) via the extract_fit_engine().\n\nWe’ll use the latter option and then tidy this model object as we did in the previous section. Let’s add this to the control function so that we can re-use it.\n\nget_lm_coefs &lt;- function(x) {\n  x %&gt;% \n    # get the lm model object\n    extract_fit_engine() %&gt;% \n    # transform its format\n    tidy()\n}\ntidy_ctrl &lt;- control_grid(extract = get_lm_coefs)\n\nThis argument is then passed to fit_resamples():\n\nlm_res &lt;- \n  lm_spec %&gt;% \n  fit_resamples(ridership ~ ., resamples = bt, control = tidy_ctrl)\nlm_res\n\n# Resampling results\n# Bootstrap sampling \n# A tibble: 5 × 5\n  splits              id         .metrics         .notes           .extracts\n  &lt;list&gt;              &lt;chr&gt;      &lt;list&gt;           &lt;list&gt;           &lt;list&gt;   \n1 &lt;split [5698/2076]&gt; Bootstrap1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n2 &lt;split [5698/2098]&gt; Bootstrap2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n3 &lt;split [5698/2064]&gt; Bootstrap3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n4 &lt;split [5698/2082]&gt; Bootstrap4 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n5 &lt;split [5698/2088]&gt; Bootstrap5 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n\n\nNote that there is a .extracts column in our resampling results. This object contains the output of our get_lm_coefs() function for each resample. The structure of the elements of this column is a little complex. Let’s start by looking at the first element (which corresponds to the first resample):\n\nlm_res$.extracts[[1]]\n\n# A tibble: 1 × 2\n  .extracts        .config             \n  &lt;list&gt;           &lt;chr&gt;               \n1 &lt;tibble [4 × 5]&gt; Preprocessor1_Model1\n\n\nThere is another column in this element called .extracts that has the results of the tidy() function call:\n\nlm_res$.extracts[[1]]$.extracts[[1]]\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    1.40     0.157       8.90 7.23e- 19\n2 Clark_Lake     0.842    0.0280     30.1  2.39e-184\n3 Austin         1.46     0.320       4.54 5.70e-  6\n4 Harlem        -0.637    0.163      -3.92 9.01e-  5\n\n\nThese nested columns can be flattened via the purrr unnest() function:\n\nlm_res %&gt;% \n  select(id, .extracts) %&gt;% \n  unnest(.extracts) \n\n# A tibble: 5 × 3\n  id         .extracts        .config             \n  &lt;chr&gt;      &lt;list&gt;           &lt;chr&gt;               \n1 Bootstrap1 &lt;tibble [4 × 5]&gt; Preprocessor1_Model1\n2 Bootstrap2 &lt;tibble [4 × 5]&gt; Preprocessor1_Model1\n3 Bootstrap3 &lt;tibble [4 × 5]&gt; Preprocessor1_Model1\n4 Bootstrap4 &lt;tibble [4 × 5]&gt; Preprocessor1_Model1\n5 Bootstrap5 &lt;tibble [4 × 5]&gt; Preprocessor1_Model1\n\n\nWe still have a column of nested tibbles, so we can run the same command again to get the data into a more useful format:\n\nlm_coefs &lt;- \n  lm_res %&gt;% \n  select(id, .extracts) %&gt;% \n  unnest(.extracts) %&gt;% \n  unnest(.extracts)\n\nlm_coefs %&gt;% select(id, term, estimate, p.value)\n\n# A tibble: 20 × 4\n   id         term        estimate   p.value\n   &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n 1 Bootstrap1 (Intercept)    1.40  7.23e- 19\n 2 Bootstrap1 Clark_Lake     0.842 2.39e-184\n 3 Bootstrap1 Austin         1.46  5.70e-  6\n 4 Bootstrap1 Harlem        -0.637 9.01e-  5\n 5 Bootstrap2 (Intercept)    1.69  2.87e- 28\n 6 Bootstrap2 Clark_Lake     0.911 1.06e-219\n 7 Bootstrap2 Austin         0.595 5.93e-  2\n 8 Bootstrap2 Harlem        -0.580 3.88e-  4\n 9 Bootstrap3 (Intercept)    1.27  3.43e- 16\n10 Bootstrap3 Clark_Lake     0.859 5.03e-194\n11 Bootstrap3 Austin         1.09  6.77e-  4\n12 Bootstrap3 Harlem        -0.470 4.34e-  3\n13 Bootstrap4 (Intercept)    1.95  2.91e- 34\n14 Bootstrap4 Clark_Lake     0.974 1.47e-233\n15 Bootstrap4 Austin        -0.116 7.21e-  1\n16 Bootstrap4 Harlem        -0.620 2.11e-  4\n17 Bootstrap5 (Intercept)    1.87  1.98e- 33\n18 Bootstrap5 Clark_Lake     0.901 1.16e-210\n19 Bootstrap5 Austin         0.494 1.15e-  1\n20 Bootstrap5 Harlem        -0.512 1.73e-  3\n\n\nThat’s better! Now, let’s plot the model coefficients for each resample:\n\nlm_coefs %&gt;%\n  filter(term != \"(Intercept)\") %&gt;% \n  ggplot(aes(x = term, y = estimate, group = id, col = id)) +  \n  geom_hline(yintercept = 0, lty = 3) + \n  geom_line(alpha = 0.3, lwd = 1.2) + \n  labs(y = \"Coefficient\", x = NULL) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nThere seems to be a lot of uncertainty in the coefficient for the Austin station data, but less for the other two.\nLooking at the code for unnesting the results, you may find the double-nesting structure excessive or cumbersome. However, the extraction functionality is flexible, and a simpler structure would prevent many use cases."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/coefficients-ws.html#more-complex-a-glmnet-model",
    "href": "teaching/tidyverse-II/material/case-studies/coefficients-ws.html#more-complex-a-glmnet-model",
    "title": "Working with model coefficients",
    "section": "More complex: a glmnet model",
    "text": "More complex: a glmnet model\nThe glmnet model can fit the same linear regression model structure shown above. It uses regularization (a.k.a penalization) to estimate the model parameters. This has the benefit of shrinking the coefficients towards zero, important in situations where there are strong correlations between predictors or if some feature selection is required. Both of these cases are true for our Chicago train data set.\nThere are two types of penalization that this model uses:\n\nLasso (a.k.a. \\(L_1\\)) penalties can shrink the model terms so much that they are absolute zero (i.e. their effect is entirely removed from the model).\nWeight decay (a.k.a ridge regression or \\(L_2\\)) uses a different type of penalty that is most useful for highly correlated predictors.\n\nThe glmnet model has two primary tuning parameters, the total amount of penalization and the mixture of the two penalty types. For example, this specification:\n\nglmnet_spec &lt;- \n  linear_reg(penalty = 0.1, mixture = 0.95) %&gt;% \n  set_engine(\"glmnet\")\n\nhas a penalty that is 95% lasso and 5% weight decay. The total amount of these two penalties is 0.1 (which is fairly high).\n\n\n\n\n\n\nNote\n\n\n\nModels with regularization require that predictors are all on the same scale. The ridership at our three stations are very different, but glmnet automatically centers and scales the data. You can use recipes to center and scale your data yourself.\n\n\nLet’s combine the model specification with a formula in a model workflow() and then fit the model to the data:\n\nglmnet_wflow &lt;- \n  workflow() %&gt;% \n  add_model(glmnet_spec) %&gt;% \n  add_formula(ridership ~ .)\n\nglmnet_fit &lt;- fit(glmnet_wflow, Chicago)\nglmnet_fit\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nridership ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\n\nCall:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~0.95) \n\n   Df  %Dev Lambda\n1   0  0.00 6.1040\n2   1 12.75 5.5620\n3   1 23.45 5.0680\n4   1 32.43 4.6180\n5   1 39.95 4.2070\n6   1 46.25 3.8340\n7   1 51.53 3.4930\n8   1 55.94 3.1830\n9   1 59.62 2.9000\n10  1 62.70 2.6420\n11  2 65.28 2.4080\n12  2 67.44 2.1940\n13  2 69.23 1.9990\n14  2 70.72 1.8210\n15  2 71.96 1.6600\n16  2 73.00 1.5120\n17  2 73.86 1.3780\n18  2 74.57 1.2550\n19  2 75.17 1.1440\n20  2 75.66 1.0420\n21  2 76.07 0.9496\n22  2 76.42 0.8653\n23  2 76.70 0.7884\n24  2 76.94 0.7184\n25  2 77.13 0.6545\n26  2 77.30 0.5964\n27  2 77.43 0.5434\n28  2 77.55 0.4951\n29  2 77.64 0.4512\n30  2 77.72 0.4111\n31  2 77.78 0.3746\n32  2 77.84 0.3413\n33  2 77.88 0.3110\n34  2 77.92 0.2833\n35  2 77.95 0.2582\n36  2 77.98 0.2352\n37  2 78.00 0.2143\n38  2 78.01 0.1953\n39  2 78.03 0.1779\n40  2 78.04 0.1621\n41  2 78.05 0.1477\n42  2 78.06 0.1346\n43  2 78.07 0.1226\n44  2 78.07 0.1118\n45  2 78.08 0.1018\n46  2 78.08 0.0928\n\n...\nand 9 more lines.\n\n\nIn this output, the term lambda is used to represent the penalty.\nNote that the output shows many values of the penalty despite our specification of penalty = 0.1. It turns out that this model fits a “path” of penalty values. Even though we are interested in a value of 0.1, we can get the model coefficients for many associated values of the penalty from the same model object.\nLet’s look at two different approaches to obtaining the coefficients. Both will use the tidy() method. One will tidy a glmnet object and the other will tidy a tidymodels object.\n\nUsing glmnet penalty values\nThis glmnet fit contains multiple penalty values which depend on the data set; changing the data (or the mixture amount) often produces a different set of values. For this data set, there are 55 penalties available. To get the set of penalties produced for this data set, we can extract the engine fit and tidy:\n\nglmnet_fit %&gt;% \n  extract_fit_engine() %&gt;% \n  tidy() %&gt;% \n  rename(penalty = lambda) %&gt;%   # &lt;- for consistent naming\n  filter(term != \"(Intercept)\")\n\n# A tibble: 99 × 5\n   term        step estimate penalty dev.ratio\n   &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 Clark_Lake     2   0.0753    5.56     0.127\n 2 Clark_Lake     3   0.145     5.07     0.234\n 3 Clark_Lake     4   0.208     4.62     0.324\n 4 Clark_Lake     5   0.266     4.21     0.400\n 5 Clark_Lake     6   0.319     3.83     0.463\n 6 Clark_Lake     7   0.368     3.49     0.515\n 7 Clark_Lake     8   0.413     3.18     0.559\n 8 Clark_Lake     9   0.454     2.90     0.596\n 9 Clark_Lake    10   0.491     2.64     0.627\n10 Clark_Lake    11   0.526     2.41     0.653\n# ℹ 89 more rows\n\n\nThis works well but, it turns out that our penalty value (0.1) is not in the list produced by the model! The underlying package has functions that use interpolation to produce coefficients for this specific value, but the tidy() method for glmnet objects does not use it.\n\n\nUsing specific penalty values\nIf we run the tidy() method on the workflow or parsnip object, a different function is used that returns the coefficients for the penalty value that we specified:\n\ntidy(glmnet_fit)\n\n# A tibble: 4 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)    1.69      0.1\n2 Clark_Lake     0.846     0.1\n3 Austin         0.271     0.1\n4 Harlem         0         0.1\n\n\nFor any another (single) penalty, we can use an additional argument:\n\ntidy(glmnet_fit, penalty = 5.5620)  # A value from above\n\n# A tibble: 4 × 3\n  term        estimate penalty\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  12.6       5.56\n2 Clark_Lake    0.0753    5.56\n3 Austin        0         5.56\n4 Harlem        0         5.56\n\n\nThe reason for having two tidy() methods is that, with tidymodels, the focus is on using a specific penalty value.\n\n\nTuning a glmnet model\nIf we know a priori acceptable values for penalty and mixture, we can use the fit_resamples() function as we did before with linear regression. Otherwise, we can tune those parameters with the tidymodels tune_*() functions.\nLet’s tune our glmnet model over both parameters with this grid:\n\npen_vals &lt;- 10^seq(-3, 0, length.out = 10)\ngrid &lt;- crossing(penalty = pen_vals, mixture = c(0.1, 1.0))\n\nHere is where more glmnet-related complexity comes in: we know that each resample and each value of mixture will probably produce a different set of penalty values contained in the model object. How can we look at the coefficients at the specific penalty values that we are using to tune?\nThe approach that we suggest is to use the special path_values option for glmnet. Details are described in the technical documentation about glmnet and tidymodels but in short, this parameter will assign the collection of penalty values used by each glmnet fit (regardless of the data or value of mixture).\nWe can pass these as an engine argument and then update our previous workflow object:\n\nglmnet_tune_spec &lt;- \n  linear_reg(penalty = tune(), mixture = tune()) %&gt;% \n  set_engine(\"glmnet\", path_values = pen_vals)\n\nglmnet_wflow &lt;- \n  glmnet_wflow %&gt;% \n  update_model(glmnet_tune_spec)\n\nNow we will use an extraction function similar to when we used ordinary least squares. We add an additional argument to retain coefficients that are shrunk to zero by the lasso penalty:\n\nget_glmnet_coefs &lt;- function(x) {\n  x %&gt;% \n    extract_fit_engine() %&gt;% \n    tidy(return_zeros = TRUE) %&gt;% \n    rename(penalty = lambda)\n}\nparsnip_ctrl &lt;- control_grid(extract = get_glmnet_coefs)\n\nglmnet_res &lt;- \n  glmnet_wflow %&gt;% \n  tune_grid(\n    resamples = bt,\n    grid = grid,\n    control = parsnip_ctrl\n  )\nglmnet_res\n\n# Tuning results\n# Bootstrap sampling \n# A tibble: 5 × 5\n  splits              id         .metrics          .notes           .extracts\n  &lt;list&gt;              &lt;chr&gt;      &lt;list&gt;            &lt;list&gt;           &lt;list&gt;   \n1 &lt;split [5698/2076]&gt; Bootstrap1 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n2 &lt;split [5698/2098]&gt; Bootstrap2 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n3 &lt;split [5698/2064]&gt; Bootstrap3 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n4 &lt;split [5698/2082]&gt; Bootstrap4 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n5 &lt;split [5698/2088]&gt; Bootstrap5 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n\n\nAs noted before, the elements of the main .extracts column have an embedded list column with the results of get_glmnet_coefs():\n\nglmnet_res$.extracts[[1]] %&gt;% head()\n\n# A tibble: 6 × 4\n  penalty mixture .extracts         .config              \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;list&gt;            &lt;chr&gt;                \n1       1     0.1 &lt;tibble [40 × 5]&gt; Preprocessor1_Model01\n2       1     0.1 &lt;tibble [40 × 5]&gt; Preprocessor1_Model02\n3       1     0.1 &lt;tibble [40 × 5]&gt; Preprocessor1_Model03\n4       1     0.1 &lt;tibble [40 × 5]&gt; Preprocessor1_Model04\n5       1     0.1 &lt;tibble [40 × 5]&gt; Preprocessor1_Model05\n6       1     0.1 &lt;tibble [40 × 5]&gt; Preprocessor1_Model06\n\nglmnet_res$.extracts[[1]]$.extracts[[1]] %&gt;% head()\n\n# A tibble: 6 × 5\n  term         step estimate penalty dev.ratio\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     1    0.568  1          0.769\n2 (Intercept)     2    0.432  0.464      0.775\n3 (Intercept)     3    0.607  0.215      0.779\n4 (Intercept)     4    0.846  0.1        0.781\n5 (Intercept)     5    1.06   0.0464     0.782\n6 (Intercept)     6    1.22   0.0215     0.783\n\n\nAs before, we’ll have to use a double unnest(). Since the penalty value is in both the top-level and lower-level .extracts, we’ll use select() to get rid of the first version (but keep mixture):\n\nglmnet_res %&gt;% \n  select(id, .extracts) %&gt;% \n  unnest(.extracts) %&gt;% \n  select(id, mixture, .extracts) %&gt;%  # &lt;- removes the first penalty column\n  unnest(.extracts)\n\nBut wait! We know that each glmnet fit contains all of the coefficients. This means, for a specific resample and value of mixture, the results are the same:\n\nall.equal(\n  # First bootstrap, first `mixture`, first `penalty`\n  glmnet_res$.extracts[[1]]$.extracts[[1]],\n  # First bootstrap, first `mixture`, second `penalty`\n  glmnet_res$.extracts[[1]]$.extracts[[2]]\n)\n\n[1] TRUE\n\n\nFor this reason, we’ll add a slice(1) when grouping by id and mixture. This will get rid of the replicated results.\n\nglmnet_coefs &lt;- \n  glmnet_res %&gt;% \n  select(id, .extracts) %&gt;% \n  unnest(.extracts) %&gt;% \n  select(id, mixture, .extracts) %&gt;% \n  group_by(id, mixture) %&gt;%          # ┐\n  slice(1) %&gt;%                       # │ Remove the redundant results\n  ungroup() %&gt;%                      # ┘\n  unnest(.extracts)\n\nglmnet_coefs %&gt;% \n  select(id, penalty, mixture, term, estimate) %&gt;% \n  filter(term != \"(Intercept)\")\n\n# A tibble: 300 × 5\n   id         penalty mixture term       estimate\n   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1 Bootstrap1 1           0.1 Clark_Lake    0.391\n 2 Bootstrap1 0.464       0.1 Clark_Lake    0.485\n 3 Bootstrap1 0.215       0.1 Clark_Lake    0.590\n 4 Bootstrap1 0.1         0.1 Clark_Lake    0.680\n 5 Bootstrap1 0.0464      0.1 Clark_Lake    0.746\n 6 Bootstrap1 0.0215      0.1 Clark_Lake    0.793\n 7 Bootstrap1 0.01        0.1 Clark_Lake    0.817\n 8 Bootstrap1 0.00464     0.1 Clark_Lake    0.828\n 9 Bootstrap1 0.00215     0.1 Clark_Lake    0.834\n10 Bootstrap1 0.001       0.1 Clark_Lake    0.837\n# ℹ 290 more rows\n\n\nNow we have the coefficients. Let’s look at how they behave as more regularization is used:\n\nglmnet_coefs %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(mixture = format(mixture)) %&gt;% \n  ggplot(aes(x = penalty, y = estimate, col = mixture, groups = id)) + \n  geom_hline(yintercept = 0, lty = 3) +\n  geom_line(alpha = 0.5, lwd = 1.2) + \n  facet_wrap(~ term) + \n  scale_x_log10() +\n  scale_color_brewer(palette = \"Accent\") +\n  labs(y = \"coefficient\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nNotice a couple of things:\n\nWith a pure lasso model (i.e., mixture = 1), the Austin station predictor is selected out in each resample. With a mixture of both penalties, its influence increases. Also, as the penalty increases, the uncertainty in this coefficient decreases.\nThe Harlem predictor is either quickly selected out of the model or goes from negative to positive."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/xtabs-ws.html",
    "href": "teaching/tidyverse-II/material/case-studies/xtabs-ws.html",
    "title": "Statistical analysis of contingency tables",
    "section": "",
    "text": "This article only requires that you have the tidymodels package installed.\nHere, we’ll walk through conducting a \\(\\chi^2\\) (chi-squared) test of independence and a chi-squared goodness of fit test using infer. We’ll start out with a chi-squared test of independence, which can be used to test the association between two categorical variables. Then, we’ll move on to a chi-squared goodness of fit test, which tests how well the distribution of one categorical variable can be approximated by some theoretical distribution.\nThroughout this vignette, we’ll make use of the ad_data data set (available in the modeldata package, which is part of tidymodels). This data set is related to cognitive impairment in 333 patients from Craig-Schapiro et al (2011). See ?ad_data for more information on the variables included and their source. One of the main research questions in these data were how a person’s genetics related to the Apolipoprotein E gene affect their cognitive skills. The data shows:\n\nlibrary(tidymodels) # Includes the infer package\nset.seed(1234)\n\ndata(ad_data, package = \"modeldata\")\nad_data %&gt;%\n  select(Genotype, Class)\n\n# A tibble: 333 × 2\n   Genotype Class   \n   &lt;fct&gt;    &lt;fct&gt;   \n 1 E3E3     Control \n 2 E3E4     Control \n 3 E3E4     Control \n 4 E3E4     Control \n 5 E3E3     Control \n 6 E4E4     Impaired\n 7 E2E3     Control \n 8 E2E3     Control \n 9 E3E3     Control \n10 E2E3     Impaired\n# ℹ 323 more rows\n\n\nThe three main genetic variants are called E2, E3, and E4. The values in Genotype represent the genetic makeup of patients based on what they inherited from their parents (i.e, a value of “E2E4” means E2 from one parent and E4 from the other)."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/xtabs-ws.html#introduction",
    "href": "teaching/tidyverse-II/material/case-studies/xtabs-ws.html#introduction",
    "title": "Statistical analysis of contingency tables",
    "section": "",
    "text": "This article only requires that you have the tidymodels package installed.\nHere, we’ll walk through conducting a \\(\\chi^2\\) (chi-squared) test of independence and a chi-squared goodness of fit test using infer. We’ll start out with a chi-squared test of independence, which can be used to test the association between two categorical variables. Then, we’ll move on to a chi-squared goodness of fit test, which tests how well the distribution of one categorical variable can be approximated by some theoretical distribution.\nThroughout this vignette, we’ll make use of the ad_data data set (available in the modeldata package, which is part of tidymodels). This data set is related to cognitive impairment in 333 patients from Craig-Schapiro et al (2011). See ?ad_data for more information on the variables included and their source. One of the main research questions in these data were how a person’s genetics related to the Apolipoprotein E gene affect their cognitive skills. The data shows:\n\nlibrary(tidymodels) # Includes the infer package\nset.seed(1234)\n\ndata(ad_data, package = \"modeldata\")\nad_data %&gt;%\n  select(Genotype, Class)\n\n# A tibble: 333 × 2\n   Genotype Class   \n   &lt;fct&gt;    &lt;fct&gt;   \n 1 E3E3     Control \n 2 E3E4     Control \n 3 E3E4     Control \n 4 E3E4     Control \n 5 E3E3     Control \n 6 E4E4     Impaired\n 7 E2E3     Control \n 8 E2E3     Control \n 9 E3E3     Control \n10 E2E3     Impaired\n# ℹ 323 more rows\n\n\nThe three main genetic variants are called E2, E3, and E4. The values in Genotype represent the genetic makeup of patients based on what they inherited from their parents (i.e, a value of “E2E4” means E2 from one parent and E4 from the other)."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/xtabs-ws.html#test-of-independence",
    "href": "teaching/tidyverse-II/material/case-studies/xtabs-ws.html#test-of-independence",
    "title": "Statistical analysis of contingency tables",
    "section": "Test of independence",
    "text": "Test of independence\nTo carry out a chi-squared test of independence, we’ll examine the association between their cognitive ability (impaired and healthy) and the genetic makeup. This is what the relationship looks like in the sample data:\n\n\n\n\n\n\n\n\n\nIf there were no relationship, we would expect to see the purple bars reaching to the same length, regardless of cognitive ability. Are the differences we see here, though, just due to random noise?\nFirst, to calculate the observed statistic, we can use specify() and calculate().\n\n# calculate the observed statistic\nobserved_indep_statistic &lt;- ad_data %&gt;%\n  specify(Genotype ~ Class) %&gt;%\n  calculate(stat = \"Chisq\")\n\nThe observed \\(\\chi^2\\) statistic is 21.5774809. Now, we want to compare this statistic to a null distribution, generated under the assumption that these variables are not actually related, to get a sense of how likely it would be for us to see this observed statistic if there were actually no association between cognitive ability and genetics.\nWe can generate() the null distribution in one of two ways: using randomization or theory-based methods. The randomization approach permutes the response and explanatory variables, so that each person’s genetics is matched up with a random cognitive rating from the sample in order to break up any association between the two.\n\n# generate the null distribution using randomization\nnull_distribution_simulated &lt;- ad_data %&gt;%\n  specify(Genotype ~ Class) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 5000, type = \"permute\") %&gt;%\n  calculate(stat = \"Chisq\")\n\nNote that, in the line specify(Genotype ~ Class) above, we could use the equivalent syntax specify(response = Genotype, explanatory = Class). The same goes in the code below, which generates the null distribution using theory-based methods instead of randomization.\n\n# generate the null distribution by theoretical approximation\nnull_distribution_theoretical &lt;- ad_data %&gt;%\n  specify(Genotype ~ Class) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  # note that we skip the generation step here!\n  calculate(stat = \"Chisq\")\n\nTo get a sense for what these distributions look like, and where our observed statistic falls, we can use visualize():\n\n# visualize the null distribution and test statistic!\nnull_distribution_simulated %&gt;%\n  visualize() + \n  shade_p_value(observed_indep_statistic,\n                direction = \"greater\")\n\n\n\n\n\n\n\n\nWe could also visualize the observed statistic against the theoretical null distribution. Note that we skip the generate() and calculate() steps when using the theoretical approach, and that we now need to provide method = \"theoretical\" to visualize().\n\n# visualize the theoretical null distribution and test statistic!\nad_data %&gt;%\n  specify(Genotype ~ Class) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  visualize(method = \"theoretical\") + \n  shade_p_value(observed_indep_statistic,\n                direction = \"greater\")\n\n\n\n\n\n\n\n\nTo visualize both the randomization-based and theoretical null distributions to get a sense of how the two relate, we can pipe the randomization-based null distribution into visualize(), and further provide method = \"both\".\n\n# visualize both null distributions and the test statistic!\nnull_distribution_simulated %&gt;%\n  visualize(method = \"both\") + \n  shade_p_value(observed_indep_statistic,\n                direction = \"greater\")\n\n\n\n\n\n\n\n\nEither way, it looks like our observed test statistic would be fairly unlikely if there were actually no association between cognition and genotype. More exactly, we can calculate the p-value:\n\n# calculate the p value from the observed statistic and null distribution\np_value_independence &lt;- null_distribution_simulated %&gt;%\n  get_p_value(obs_stat = observed_indep_statistic,\n              direction = \"greater\")\n\np_value_independence\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1  0.0006\n\n\nThus, if there were really no relationship between cognition and genotype, the probability that we would see a statistic as or more extreme than 21.5774809 is approximately 6^{-4}.\nNote that, equivalently to the steps shown above, the package supplies a wrapper function, chisq_test, to carry out Chi-Squared tests of independence on tidy data. The syntax goes like this:\n\nchisq_test(ad_data, Genotype ~ Class)\n\n# A tibble: 1 × 3\n  statistic chisq_df  p_value\n      &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;\n1      21.6        5 0.000630"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/xtabs-ws.html#goodness-of-fit",
    "href": "teaching/tidyverse-II/material/case-studies/xtabs-ws.html#goodness-of-fit",
    "title": "Statistical analysis of contingency tables",
    "section": "Goodness of fit",
    "text": "Goodness of fit\nNow, moving on to a chi-squared goodness of fit test, we’ll take a look at just the genotype data. Many papers have investigated the relationship of Apolipoprotein E to diseases. For example, Song et al (2004) conducted a meta-analysis of numerous studies that looked at this gene and heart disease. In their paper, they describe the frequency of the different genotypes across many samples. For the cognition study, it might be interesting to see if our sample of genotypes was consistent with this literature (treating the rates, for this analysis, as known).\nThe rates of the meta-analysis and our observed data are:\n\n# Song, Y., Stampfer, M. J., & Liu, S. (2004). Meta-Analysis: Apolipoprotein E \n# Genotypes and Risk for Coronary Heart Disease. Annals of Internal Medicine, \n# 141(2), 137.\nmeta_rates &lt;- c(\"E2E2\" = 0.71, \"E2E3\" = 11.4, \"E2E4\" = 2.32,\n                \"E3E3\" = 61.0, \"E3E4\" = 22.6, \"E4E4\" = 2.22)\nmeta_rates &lt;- meta_rates/sum(meta_rates) # these add up to slightly &gt; 100%\n\nobs_rates &lt;- table(ad_data$Genotype)/nrow(ad_data)\nround(cbind(obs_rates, meta_rates) * 100, 2)\n\n     obs_rates meta_rates\nE2E2      0.60       0.71\nE2E3     11.11      11.37\nE2E4      2.40       2.31\nE3E3     50.15      60.85\nE3E4     31.83      22.54\nE4E4      3.90       2.21\n\n\nSuppose our null hypothesis is that Genotype follows the same frequency distribution as the meta-analysis. Lets now test whether this difference in distributions is statistically significant.\nFirst, to carry out this hypothesis test, we would calculate our observed statistic.\n\n# calculating the null distribution\nobserved_gof_statistic &lt;- ad_data %&gt;%\n  specify(response = Genotype) %&gt;%\n  hypothesize(null = \"point\", p = meta_rates) %&gt;%\n  calculate(stat = \"Chisq\")\n\nThe observed statistic is 23.3838483. Now, generating a null distribution, by just dropping in a call to generate():\n\n# generating a null distribution\nnull_distribution_gof &lt;- ad_data %&gt;%\n  specify(response = Genotype) %&gt;%\n  hypothesize(null = \"point\", p = meta_rates) %&gt;%\n  generate(reps = 5000, type = \"simulate\") %&gt;%\n  calculate(stat = \"Chisq\")\n\nAgain, to get a sense for what these distributions look like, and where our observed statistic falls, we can use visualize():\n\n# visualize the null distribution and test statistic!\nnull_distribution_gof %&gt;%\n  visualize() + \n  shade_p_value(observed_gof_statistic,\n                direction = \"greater\")\n\n\n\n\n\n\n\n\nThis statistic seems like it would be unlikely if our rates were the same as the rates from the meta-analysis! How unlikely, though? Calculating the p-value:\n\n# calculate the p-value\np_value_gof &lt;- null_distribution_gof %&gt;%\n  get_p_value(observed_gof_statistic,\n              direction = \"greater\")\n\np_value_gof\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.001\n\n\nThus, if each genotype occurred at the same rate as the Song paper, the probability that we would see a distribution like the one we did is approximately 0.001.\nAgain, equivalently to the steps shown above, the package supplies a wrapper function, chisq_test, to carry out chi-squared goodness of fit tests on tidy data. The syntax goes like this:\n\nchisq_test(ad_data, response = Genotype, p = meta_rates)\n\n# A tibble: 1 × 3\n  statistic chisq_df  p_value\n      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1      23.4        5 0.000285"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/xtabs.html",
    "href": "teaching/tidyverse-II/material/case-studies/xtabs.html",
    "title": "Statistical analysis of contingency tables",
    "section": "",
    "text": "This article only requires that you have the tidymodels package installed.\nHere, we’ll walk through conducting a \\(\\chi^2\\) (chi-squared) test of independence and a chi-squared goodness of fit test using infer. We’ll start out with a chi-squared test of independence, which can be used to test the association between two categorical variables. Then, we’ll move on to a chi-squared goodness of fit test, which tests how well the distribution of one categorical variable can be approximated by some theoretical distribution.\nThroughout this vignette, we’ll make use of the ad_data data set (available in the modeldata package, which is part of tidymodels). This data set is related to cognitive impairment in 333 patients from Craig-Schapiro et al (2011). See ?ad_data for more information on the variables included and their source. One of the main research questions in these data were how a person’s genetics related to the Apolipoprotein E gene affect their cognitive skills. The data shows:\n\nlibrary(tidymodels) # Includes the infer package\nset.seed(1234)\n\ndata(ad_data, package = \"modeldata\")\nad_data %&gt;%\n  select(Genotype, Class)\n#&gt; # A tibble: 333 × 2\n#&gt;    Genotype Class   \n#&gt;    &lt;fct&gt;    &lt;fct&gt;   \n#&gt;  1 E3E3     Control \n#&gt;  2 E3E4     Control \n#&gt;  3 E3E4     Control \n#&gt;  4 E3E4     Control \n#&gt;  5 E3E3     Control \n#&gt;  6 E4E4     Impaired\n#&gt;  7 E2E3     Control \n#&gt;  8 E2E3     Control \n#&gt;  9 E3E3     Control \n#&gt; 10 E2E3     Impaired\n#&gt; # ℹ 323 more rows\n\nThe three main genetic variants are called E2, E3, and E4. The values in Genotype represent the genetic makeup of patients based on what they inherited from their parents (i.e, a value of “E2E4” means E2 from one parent and E4 from the other)."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/xtabs.html#introduction",
    "href": "teaching/tidyverse-II/material/case-studies/xtabs.html#introduction",
    "title": "Statistical analysis of contingency tables",
    "section": "",
    "text": "This article only requires that you have the tidymodels package installed.\nHere, we’ll walk through conducting a \\(\\chi^2\\) (chi-squared) test of independence and a chi-squared goodness of fit test using infer. We’ll start out with a chi-squared test of independence, which can be used to test the association between two categorical variables. Then, we’ll move on to a chi-squared goodness of fit test, which tests how well the distribution of one categorical variable can be approximated by some theoretical distribution.\nThroughout this vignette, we’ll make use of the ad_data data set (available in the modeldata package, which is part of tidymodels). This data set is related to cognitive impairment in 333 patients from Craig-Schapiro et al (2011). See ?ad_data for more information on the variables included and their source. One of the main research questions in these data were how a person’s genetics related to the Apolipoprotein E gene affect their cognitive skills. The data shows:\n\nlibrary(tidymodels) # Includes the infer package\nset.seed(1234)\n\ndata(ad_data, package = \"modeldata\")\nad_data %&gt;%\n  select(Genotype, Class)\n#&gt; # A tibble: 333 × 2\n#&gt;    Genotype Class   \n#&gt;    &lt;fct&gt;    &lt;fct&gt;   \n#&gt;  1 E3E3     Control \n#&gt;  2 E3E4     Control \n#&gt;  3 E3E4     Control \n#&gt;  4 E3E4     Control \n#&gt;  5 E3E3     Control \n#&gt;  6 E4E4     Impaired\n#&gt;  7 E2E3     Control \n#&gt;  8 E2E3     Control \n#&gt;  9 E3E3     Control \n#&gt; 10 E2E3     Impaired\n#&gt; # ℹ 323 more rows\n\nThe three main genetic variants are called E2, E3, and E4. The values in Genotype represent the genetic makeup of patients based on what they inherited from their parents (i.e, a value of “E2E4” means E2 from one parent and E4 from the other)."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/xtabs.html#test-of-independence",
    "href": "teaching/tidyverse-II/material/case-studies/xtabs.html#test-of-independence",
    "title": "Statistical analysis of contingency tables",
    "section": "Test of independence",
    "text": "Test of independence\nTo carry out a chi-squared test of independence, we’ll examine the association between their cognitive ability (impaired and healthy) and the genetic makeup. This is what the relationship looks like in the sample data:\n\n\n\n\n\n\n\n\n\nIf there were no relationship, we would expect to see the purple bars reaching to the same length, regardless of cognitive ability. Are the differences we see here, though, just due to random noise?\nFirst, to calculate the observed statistic, we can use specify() and calculate().\n\n# calculate the observed statistic\nobserved_indep_statistic &lt;- ad_data %&gt;%\n  specify(Genotype ~ Class) %&gt;%\n  calculate(stat = \"Chisq\")\n\nThe observed \\(\\chi^2\\) statistic is 21.5774809. Now, we want to compare this statistic to a null distribution, generated under the assumption that these variables are not actually related, to get a sense of how likely it would be for us to see this observed statistic if there were actually no association between cognitive ability and genetics.\nWe can generate() the null distribution in one of two ways: using randomization or theory-based methods. The randomization approach permutes the response and explanatory variables, so that each person’s genetics is matched up with a random cognitive rating from the sample in order to break up any association between the two.\n\n# generate the null distribution using randomization\nnull_distribution_simulated &lt;- ad_data %&gt;%\n  specify(Genotype ~ Class) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 5000, type = \"permute\") %&gt;%\n  calculate(stat = \"Chisq\")\n\nNote that, in the line specify(Genotype ~ Class) above, we could use the equivalent syntax specify(response = Genotype, explanatory = Class). The same goes in the code below, which generates the null distribution using theory-based methods instead of randomization.\n\n# generate the null distribution by theoretical approximation\nnull_distribution_theoretical &lt;- ad_data %&gt;%\n  specify(Genotype ~ Class) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  # note that we skip the generation step here!\n  calculate(stat = \"Chisq\")\n\nTo get a sense for what these distributions look like, and where our observed statistic falls, we can use visualize():\n\n# visualize the null distribution and test statistic!\nnull_distribution_simulated %&gt;%\n  visualize() + \n  shade_p_value(observed_indep_statistic,\n                direction = \"greater\")\n\n\n\n\n\n\n\n\nWe could also visualize the observed statistic against the theoretical null distribution. Note that we skip the generate() and calculate() steps when using the theoretical approach, and that we now need to provide method = \"theoretical\" to visualize().\n\n# visualize the theoretical null distribution and test statistic!\nad_data %&gt;%\n  specify(Genotype ~ Class) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  visualize(method = \"theoretical\") + \n  shade_p_value(observed_indep_statistic,\n                direction = \"greater\")\n\n\n\n\n\n\n\n\nTo visualize both the randomization-based and theoretical null distributions to get a sense of how the two relate, we can pipe the randomization-based null distribution into visualize(), and further provide method = \"both\".\n\n# visualize both null distributions and the test statistic!\nnull_distribution_simulated %&gt;%\n  visualize(method = \"both\") + \n  shade_p_value(observed_indep_statistic,\n                direction = \"greater\")\n\n\n\n\n\n\n\n\nEither way, it looks like our observed test statistic would be fairly unlikely if there were actually no association between cognition and genotype. More exactly, we can calculate the p-value:\n\n# calculate the p value from the observed statistic and null distribution\np_value_independence &lt;- null_distribution_simulated %&gt;%\n  get_p_value(obs_stat = observed_indep_statistic,\n              direction = \"greater\")\n\np_value_independence\n#&gt; # A tibble: 1 × 1\n#&gt;   p_value\n#&gt;     &lt;dbl&gt;\n#&gt; 1  0.0006\n\nThus, if there were really no relationship between cognition and genotype, the probability that we would see a statistic as or more extreme than 21.5774809 is approximately 6^{-4}.\nNote that, equivalently to the steps shown above, the package supplies a wrapper function, chisq_test, to carry out Chi-Squared tests of independence on tidy data. The syntax goes like this:\n\nchisq_test(ad_data, Genotype ~ Class)\n#&gt; # A tibble: 1 × 3\n#&gt;   statistic chisq_df  p_value\n#&gt;       &lt;dbl&gt;    &lt;int&gt;    &lt;dbl&gt;\n#&gt; 1      21.6        5 0.000630"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/xtabs.html#goodness-of-fit",
    "href": "teaching/tidyverse-II/material/case-studies/xtabs.html#goodness-of-fit",
    "title": "Statistical analysis of contingency tables",
    "section": "Goodness of fit",
    "text": "Goodness of fit\nNow, moving on to a chi-squared goodness of fit test, we’ll take a look at just the genotype data. Many papers have investigated the relationship of Apolipoprotein E to diseases. For example, Song et al (2004) conducted a meta-analysis of numerous studies that looked at this gene and heart disease. In their paper, they describe the frequency of the different genotypes across many samples. For the cognition study, it might be interesting to see if our sample of genotypes was consistent with this literature (treating the rates, for this analysis, as known).\nThe rates of the meta-analysis and our observed data are:\n\n# Song, Y., Stampfer, M. J., & Liu, S. (2004). Meta-Analysis: Apolipoprotein E \n# Genotypes and Risk for Coronary Heart Disease. Annals of Internal Medicine, \n# 141(2), 137.\nmeta_rates &lt;- c(\"E2E2\" = 0.71, \"E2E3\" = 11.4, \"E2E4\" = 2.32,\n                \"E3E3\" = 61.0, \"E3E4\" = 22.6, \"E4E4\" = 2.22)\nmeta_rates &lt;- meta_rates/sum(meta_rates) # these add up to slightly &gt; 100%\n\nobs_rates &lt;- table(ad_data$Genotype)/nrow(ad_data)\nround(cbind(obs_rates, meta_rates) * 100, 2)\n#&gt;      obs_rates meta_rates\n#&gt; E2E2      0.60       0.71\n#&gt; E2E3     11.11      11.37\n#&gt; E2E4      2.40       2.31\n#&gt; E3E3     50.15      60.85\n#&gt; E3E4     31.83      22.54\n#&gt; E4E4      3.90       2.21\n\nSuppose our null hypothesis is that Genotype follows the same frequency distribution as the meta-analysis. Lets now test whether this difference in distributions is statistically significant.\nFirst, to carry out this hypothesis test, we would calculate our observed statistic.\n\n# calculating the null distribution\nobserved_gof_statistic &lt;- ad_data %&gt;%\n  specify(response = Genotype) %&gt;%\n  hypothesize(null = \"point\", p = meta_rates) %&gt;%\n  calculate(stat = \"Chisq\")\n\nThe observed statistic is 23.3838483. Now, generating a null distribution, by just dropping in a call to generate():\n\n# generating a null distribution\nnull_distribution_gof &lt;- ad_data %&gt;%\n  specify(response = Genotype) %&gt;%\n  hypothesize(null = \"point\", p = meta_rates) %&gt;%\n  generate(reps = 5000, type = \"simulate\") %&gt;%\n  calculate(stat = \"Chisq\")\n\nAgain, to get a sense for what these distributions look like, and where our observed statistic falls, we can use visualize():\n\n# visualize the null distribution and test statistic!\nnull_distribution_gof %&gt;%\n  visualize() + \n  shade_p_value(observed_gof_statistic,\n                direction = \"greater\")\n\n\n\n\n\n\n\n\nThis statistic seems like it would be unlikely if our rates were the same as the rates from the meta-analysis! How unlikely, though? Calculating the p-value:\n\n# calculate the p-value\np_value_gof &lt;- null_distribution_gof %&gt;%\n  get_p_value(observed_gof_statistic,\n              direction = \"greater\")\n\np_value_gof\n#&gt; # A tibble: 1 × 1\n#&gt;   p_value\n#&gt;     &lt;dbl&gt;\n#&gt; 1   0.001\n\nThus, if each genotype occurred at the same rate as the Song paper, the probability that we would see a distribution like the one we did is approximately 0.001.\nAgain, equivalently to the steps shown above, the package supplies a wrapper function, chisq_test, to carry out chi-squared goodness of fit tests on tidy data. The syntax goes like this:\n\nchisq_test(ad_data, response = Genotype, p = meta_rates)\n#&gt; # A tibble: 1 × 3\n#&gt;   statistic chisq_df  p_value\n#&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1      23.4        5 0.000285"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/xtabs.html#session-info",
    "href": "teaching/tidyverse-II/material/case-studies/xtabs.html#session-info",
    "title": "Statistical analysis of contingency tables",
    "section": "Session information",
    "text": "Session information\n\n#&gt; ─ Session info ─────────────────────────────────────────────────────\n#&gt;  version  R version 4.5.0 (2025-04-11)\n#&gt;  language (EN)\n#&gt;  date     2025-07-07\n#&gt;  pandoc   3.2\n#&gt;  quarto   1.7.29\n#&gt; \n#&gt; ─ Packages ─────────────────────────────────────────────────────────\n#&gt;  package      version date (UTC) source\n#&gt;  broom        1.0.8   2025-03-28 CRAN (R 4.5.0)\n#&gt;  dials        1.4.0   2025-02-13 CRAN (R 4.5.0)\n#&gt;  dplyr        1.1.4   2023-11-17 CRAN (R 4.5.0)\n#&gt;  ggplot2      3.5.2   2025-04-09 CRAN (R 4.5.0)\n#&gt;  infer        1.0.8   2025-04-14 CRAN (R 4.5.0)\n#&gt;  parsnip      1.3.1   2025-03-12 CRAN (R 4.5.0)\n#&gt;  purrr        1.0.4   2025-02-05 CRAN (R 4.5.0)\n#&gt;  recipes      1.3.0   2025-04-17 CRAN (R 4.5.0)\n#&gt;  rlang        1.1.6   2025-04-11 CRAN (R 4.5.0)\n#&gt;  rsample      1.3.0   2025-04-02 CRAN (R 4.5.0)\n#&gt;  tibble       3.2.1   2023-03-20 CRAN (R 4.5.0)\n#&gt;  tidymodels   1.3.0   2025-02-21 CRAN (R 4.5.0)\n#&gt;  tune         1.3.0   2025-02-21 CRAN (R 4.5.0)\n#&gt;  workflows    1.2.0   2025-02-19 CRAN (R 4.5.0)\n#&gt;  yardstick    1.3.2   2025-01-22 CRAN (R 4.5.0)\n#&gt; \n#&gt; ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "teaching/tidyverse-II/index.html",
    "href": "teaching/tidyverse-II/index.html",
    "title": "Data Science with Tidyverse II",
    "section": "",
    "text": "Make sure to install and load Tidymodels:\n\ninstall.packages(\"tidymodels\")\nlibrary(tidymodels)\n\n\nCourse Material\n\n\n\n\n\nSlides\nWorksheet\n\n\n\n\nIntroduction\n\n\n\n\nPart I\n\n1.Your Data Budget \n\n\n\n\n2. Build a Model \n\n\n\n\n3. Evaluate the Model \n\n\n\n\n4. Model Tuning \n\n\nPart II\n\n1. Feature Engineering \n\n\n\n\n2. Tuning Hyperparameters \n\n\n\n\n3. Grid Search via Racing \n\n\n\n\n4. Iterative Search \n\n\nCase Study: Chicago L-Train\n\n\n\n\n\n\nExtensions & Hands-On Exercises\n\n\n\n\nPractical\nWorksheet\n\n\n\n\nK-means\n\n\n\n\nBootstrapping\n\n\n\n\nCoefficients\n\n\n\n\nInference"
  },
  {
    "objectID": "teaching/sna/material/06/06-netviz-1.html",
    "href": "teaching/sna/material/06/06-netviz-1.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "library(igraph)\nlibrary(graphlayouts)\nlibrary(ggraph)\nlibrary(networkdata)"
  },
  {
    "objectID": "teaching/sna/material/06/06-netviz-1.html#load-packages",
    "href": "teaching/sna/material/06/06-netviz-1.html#load-packages",
    "title": "Social Network Analysis",
    "section": "",
    "text": "library(igraph)\nlibrary(graphlayouts)\nlibrary(ggraph)\nlibrary(networkdata)"
  },
  {
    "objectID": "teaching/sna/material/06/06-netviz-1.html#prepare-data",
    "href": "teaching/sna/material/06/06-netviz-1.html#prepare-data",
    "title": "Social Network Analysis",
    "section": "Prepare data",
    "text": "Prepare data\n\n# load the game of thrones dataset\ndata(got)\ngotS1 &lt;- got[[1]]"
  },
  {
    "objectID": "teaching/sna/material/06/06-netviz-1.html#basic-plotting",
    "href": "teaching/sna/material/06/06-netviz-1.html#basic-plotting",
    "title": "Social Network Analysis",
    "section": "Basic plotting",
    "text": "Basic plotting\n\nplot(gotS1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis plot is not representative for the capabilities of igraph. Check out this tutorial\n\n\n\n# quick plot function of ggraph\nautograph(gotS1)\n\n\n\n\n\n\n\n\nautograph() allows you to specify node/edge colours too but it really is only meant to give you a quick overview without writing a massive amount of code. Think of it as the plot() function for ggraph.\nBefore we continue, we add some more node attributes to the GoT network that can be used during visualization.\n\n# define a custom color palette\ngot_palette &lt;- c(\n  \"#1A5878\", \"#C44237\", \"#AD8941\", \"#E99093\",\n  \"#50594B\", \"#8968CD\", \"#9ACD32\"\n)\n\n# compute a clustering for node colors\nV(gotS1)$clu &lt;- as.character(membership(cluster_louvain(gotS1)))\n\n# compute degree as node size\nV(gotS1)$size &lt;- degree(gotS1)\n\n\nAesthetics and scales examples\n\nggraph(gotS1, layout = \"stress\") +\n  geom_edge_link0(aes(edge_width = weight), edge_colour = \"grey66\") +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  geom_node_text(aes(filter = (size &gt;= 25), label = name), size = 6) +\n  scale_fill_manual(values = got_palette) +\n  scale_edge_width(range = c(0.2, 3)) +\n  scale_size(range = c(3, 12)) +\n  theme_graph() +\n  theme(legend.position = \"none\")+\n  coord_fixed()\n\n\n\n\n\n\n\n\nUsing geom_edge_link instead of geom_edge_link0 for gradients along edges\n\nggraph(gotS1, layout = \"stress\") +\n  geom_edge_link(aes(alpha = after_stat(index)), edge_colour = \"black\") +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  scale_fill_manual(values = got_palette) +\n  scale_edge_width_continuous(range = c(0.2, 3)) +\n  scale_size_continuous(range = c(1, 6)) +\n  theme_graph() +\n  theme(legend.position = \"none\")+\n  coord_fixed()\n\n\n\n\n\n\n\n\nUsing geom_edge_link2 instead of geom_edge_link0 for more advanced gradients along edges\n\nggraph(gotS1, layout = \"stress\") +\n  geom_edge_link2(aes(edge_colour = node.clu),edge_width = 0.5)+\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  scale_fill_manual(values = got_palette) +\n  scale_edge_color_manual(values = got_palette) +\n  scale_edge_width_continuous(range = c(0.2, 3)) +\n  scale_size_continuous(range = c(1, 6)) +\n  theme_graph() +\n  theme(legend.position = \"none\")+\n  coord_fixed()\n\n\n\n\n\n\n\n\nUsing a different geom\n\nggraph(gotS1, layout = \"stress\") +\n  geom_edge_arc0(aes(edge_width = weight), edge_colour = \"grey66\",strength = 0.1) +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  scale_fill_manual(values = got_palette) +\n  scale_edge_width(range = c(0.2, 3)) +\n  scale_size(range = c(1, 6)) +\n  theme_graph() +\n  theme(legend.position = \"none\")+\n  coord_fixed()\n\n\n\n\n\n\n\n\nNot specifying any scales:\n\nggraph(gotS1, layout = \"stress\") +\n  geom_edge_link0(aes(edge_width = weight), edge_colour = \"grey66\") +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  theme_graph() +\n  theme(legend.position = \"none\")+\n  coord_fixed()\n\n\n\n\n\n\n\n\n\nggraph(gotS1, layout = \"stress\") +\n  geom_edge_link0(aes(edge_width = weight), edge_colour = \"grey66\") +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  scale_edge_width_continuous(range = c(0.2, 3)) +\n  scale_size_continuous(range = c(1, 6)) +\n  theme_graph() +\n  theme(legend.position = \"none\")+\n  coord_fixed()\n\n\n\n\n\n\n\n\n\nggraph(gotS1, layout = \"stress\") +\n  geom_edge_link0(aes(edge_width = weight), edge_colour = \"grey66\") +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  scale_fill_manual(values = got_palette) +\n  scale_edge_width_continuous(range = c(0.2, 3), guide = \"none\") +\n  scale_size_continuous(range = c(1, 6), guide = \"none\") +\n  theme_graph() +\n  theme(legend.position = \"bottom\") +\n  coord_fixed()"
  },
  {
    "objectID": "teaching/sna/material/06/06-netviz-1.html#contentric-layouts",
    "href": "teaching/sna/material/06/06-netviz-1.html#contentric-layouts",
    "title": "Social Network Analysis",
    "section": "Contentric layouts",
    "text": "Contentric layouts\nPlot that focuses on Ned Stark:\n\nggraph(gotS1, layout = \"focus\", focus = 1) +\n  geom_edge_link0(aes(edge_width = weight), edge_colour = \"grey66\") +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  geom_node_text(aes(filter = (name == \"Ned\"), size = size, label = name),\n    family = \"serif\"\n  ) +\n  scale_edge_width_continuous(range = c(0.2, 1.2)) +\n  scale_size_continuous(range = c(1, 5)) +\n  scale_fill_manual(values = got_palette) +\n  theme_graph() +\n  theme(legend.position = \"none\") + \n  coord_fixed()\n\n\n\n\n\n\n\n\nAdding draw_circle:\n\nggraph(gotS1, layout = \"focus\", focus = 2) +\n  draw_circle(col = \"#00BFFF\", use = \"focus\", max.circle = 3) +\n  geom_edge_link0(aes(width = weight), edge_colour = \"grey66\") +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  geom_node_text(aes(filter = (name == \"Daenerys\"), size = size, label = name),\n    family = \"serif\"\n  ) +\n  scale_edge_width_continuous(range = c(0.2, 1.2)) +\n  scale_size_continuous(range = c(1, 5)) +\n  scale_fill_manual(values = got_palette) +\n  theme_graph() +\n  theme(legend.position = \"none\") + \n  coord_fixed()\n\n\n\n\n\n\n\n\nConcentric layout based on a centrality index:\n\nggraph(gotS1, layout = \"centrality\", cent = strength(gotS1)) +\n  geom_edge_link0(aes(edge_width = weight), edge_colour = \"grey66\") +\n  geom_node_point(aes(fill = clu, size = size), shape = 21) +\n  geom_node_text(aes(size = size, label = name), family = \"serif\") +\n  scale_edge_width_continuous(range = c(0.2, 0.9)) +\n  scale_size_continuous(range = c(1, 8)) +\n  scale_fill_manual(values = got_palette) +\n  theme_graph() +\n  theme(legend.position = \"none\") + \n  coord_fixed()"
  },
  {
    "objectID": "teaching/sna/material/06/06-netviz-1.html#exercise-1",
    "href": "teaching/sna/material/06/06-netviz-1.html#exercise-1",
    "title": "Social Network Analysis",
    "section": "Exercise 1",
    "text": "Exercise 1\nUse the network of a different season to produce a plot by yourself\nOR choose a dataset from the networkdata package data(package = \"networkdata\")\nThere are no constrains, just try out and play around"
  },
  {
    "objectID": "teaching/sna/material/06/06-netviz-1.html#exercise-2",
    "href": "teaching/sna/material/06/06-netviz-1.html#exercise-2",
    "title": "Social Network Analysis",
    "section": "Exercise 2",
    "text": "Exercise 2\nRecreate the iconic “polblogs” network visualization \nThe network shows the linking between political blogs during the 2004 election in the US. Red nodes are conservative leaning blogs and blue ones liberal.\n\ndata(\"polblogs\")"
  },
  {
    "objectID": "teaching/sna/material/08/08-cugd.html",
    "href": "teaching/sna/material/08/08-cugd.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "In this session, we will be using conditional uniform graph distributions to simulate random networks. These random networks correspond to the null model and generate the null distribution to which we can compare our observed features to. Thus, we can conclude whether or not an observed feature of interested is significantly different than those from the null model. Most of the examples here are those presented in the lecture.\n\n\n\nlibrary(statnet)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(intergraph)\nlibrary(patchwork)\nlibrary(networkdata)\n\n\n\n\nWe will be primarily be working with matrix, network and graph objects. It is important that you can understand and pay attention to these since some functions only work with graph objects, and others with network/matrix objects. We try to keep it clear here by using suffix g, net and mat to clarify object assignment."
  },
  {
    "objectID": "teaching/sna/material/08/08-cugd.html#packages-needed",
    "href": "teaching/sna/material/08/08-cugd.html#packages-needed",
    "title": "Social Network Analysis",
    "section": "",
    "text": "library(statnet)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(intergraph)\nlibrary(patchwork)\nlibrary(networkdata)"
  },
  {
    "objectID": "teaching/sna/material/08/08-cugd.html#object-types",
    "href": "teaching/sna/material/08/08-cugd.html#object-types",
    "title": "Social Network Analysis",
    "section": "",
    "text": "We will be primarily be working with matrix, network and graph objects. It is important that you can understand and pay attention to these since some functions only work with graph objects, and others with network/matrix objects. We try to keep it clear here by using suffix g, net and mat to clarify object assignment."
  },
  {
    "objectID": "teaching/sna/material/08/08-cugd.html#load-a-dataset-and-extract-adjacency-matrix",
    "href": "teaching/sna/material/08/08-cugd.html#load-a-dataset-and-extract-adjacency-matrix",
    "title": "Social Network Analysis",
    "section": "Load a dataset and extract adjacency matrix",
    "text": "Load a dataset and extract adjacency matrix\nWe are going to use a data set, coleman, which is automatically loaded with the package statnet. To get information about it type ?coleman and select Colemans High School Friendship Data. This should open a help file with information about the data set. Read the description of the data in the help file in order to know what you are working with. To load the data in your session:\n\ndata(coleman, package = \"sna\")\n\nAs described in the help file, the data set is an array with 2 observations on the friendship nominations of 73 students (one for fall and one for spring). We will start by focusing on the fall network here, and create the adjacency matrix for the network:\n\nfall_mat &lt;- coleman[1,,] \n\nQ1: How can you check whether the network is directed or undirected?\nQ2: How can you calculate the number of ties you have in the fall network?"
  },
  {
    "objectID": "teaching/sna/material/08/08-cugd.html#visualize-the-network",
    "href": "teaching/sna/material/08/08-cugd.html#visualize-the-network",
    "title": "Social Network Analysis",
    "section": "Visualize the network",
    "text": "Visualize the network\nCreate a graph object from the adjacency matrix and visualize the network:\n\nfall_g &lt;- graph_from_adjacency_matrix(fall_mat, \"directed\")\nfall_p &lt;- ggraph(fall_g , layout = \"nicely\") + \n          geom_edge_link(edge_colour = \"#666060\", end_cap = circle(9,\"pt\"), \n                         n = 2, edge_width = 0.4, edge_alpha = 1, \n                         arrow = arrow(angle = 15, \n                         length = unit(0.1, \"inches\"), \n                         ends = \"last\", type = \"closed\"))  +\n            geom_node_point(fill = \"#525252\",colour = \"#FFFFFF\", \n                           size = 5, stroke = 1.1, shape = 21) + \n            theme_graph() + \n          ggtitle(\"fall friendship network\") +\n            theme(legend.position = \"none\")\nfall_p"
  },
  {
    "objectID": "teaching/sna/material/08/08-cugd.html#dyad-census-and-triad-census",
    "href": "teaching/sna/material/08/08-cugd.html#dyad-census-and-triad-census",
    "title": "Social Network Analysis",
    "section": "Dyad census and triad census",
    "text": "Dyad census and triad census\nTabulate the number of dyads that do not have any ties, have exactly one tie, and that have two ties (i.e. are reciprocated or mutual dyads). We calculate these numbers for both the observed and the random network using igraph:\n\ndyad_census(fall_g) \ndyad_census(sim2_g)\n# using sna package with matrix objects instead\nsna::dyad.census(fall_mat) \nsna::dyad.census(sim2_mat)\n\nQ4: Where do you note the strongest difference between the observed and simulated network?\nNow we do the same to compare the number of transitive triads. Using the sna function for calculating triad census might be easier since it also includes the triad labels:\n\ntriad_census(fall_g) \ntriad_census(sim2_g)\n# using sna package with matrix objects instead\nsna::triad.census(fall_mat) \nsna::triad.census(sim2_mat)\n\nQ4: Where do you note the strongest difference between the observed and simulated network in terms of transitivity?\nNote that the number of complete triads (MAN: 300) is 22 in the observed data and 0 for the random network. However, this might be an unfair comparison as the complete 300 triangle contains three reciprocated ties and we already saw that the Coleman data had a much higher number mutual dyads than the random network. But is this just a coincidence? To answer this we need to generate a world of hypothetical networks by generating many many random networks.\nTo see just how unusual mutual ties are in the alternative world, we can generate 1000 random networks while conditioning on the observed number of ties (the exact density):\n\nsim2.1000_mat &lt;- rgnm(n = 1000, nv = dim(fall_mat)[1], m = sum(fall_mat), mode = \"digraph\")\nsim2.1000_dc &lt;- as.data.frame(sna::dyad.census(sim2.1000_mat))\n\nNow we can draw the histogram for the distribution of mutual dyads through:\n\np_sim2.1000_dc &lt;- ggplot(sim2.1000_dc, aes(x= Mut))  +\n  geom_histogram(binwidth = 1, color=\"darkgrey\", fill=\"lightgrey\") +\n  coord_cartesian(ylim=c(0,200)) +\n  labs(title = \"\", x = \"number of mutual ties\") \np_sim2.1000_dc\n\n\n\n\n\n\n\n\nQ5: Do any of the 1000 random networks have as large a number of mutual dyads as in the observed fall network?"
  },
  {
    "objectID": "teaching/sna/material/08/08-cugd.html#data",
    "href": "teaching/sna/material/08/08-cugd.html#data",
    "title": "Social Network Analysis",
    "section": "Data",
    "text": "Data\nThis data set comes from a network study of corporate law partnership that was carried out in a Northeastern US corporate law firm. You can read about this data set here. We will go through two examples for testing homophily using this data set. Thus, the null and alternative hypotheses for both tests are: \\(H_0\\): observed homophily effect is from \\(\\mathcal{U}|L\\) model\n\\(H_1\\): observed homophily effect is not from \\(\\mathcal{U}|L\\) model\nbut we will use different relations and check for social selection based on different attributes."
  },
  {
    "objectID": "teaching/sna/material/08/08-cugd.html#analysing-homophily-using-non-parametric-null-distribution",
    "href": "teaching/sna/material/08/08-cugd.html#analysing-homophily-using-non-parametric-null-distribution",
    "title": "Social Network Analysis",
    "section": "Analysing homophily using non-parametric null distribution",
    "text": "Analysing homophily using non-parametric null distribution\n\nTest 1: Friendship based on gender\nWe start with a simple homophily test: do lawyers befriend those with the same gender? To load the friendship network:\n\ndata(\"law_friends\")\n\nWe then create an adjacency matrix from the directed graph, calculate number of ties and nodes:\n\nlaw_mat.frn &lt;- as_adjacency_matrix(law_friends, sparse = FALSE)\nlaw_nodes &lt;- dim(law_mat.frn)[1]\nlaw_ties.frn &lt;- sum(law_mat.frn)\n\nNext we save the binary attribute ‘gender’ from the loaded graph object as a vector, which we then convert into a data frame:\n\nlaw_attr.gend &lt;- as.data.frame(vertex_attr(law_cowork)$gender)\n\nTo calculate the number of observed homophilous ties:\n\nhomoph_obs.frn &lt;- sum(law_mat.frn[law_attr.gend == 1, law_attr.gend == 1]) + \n              sum(law_mat.frn[law_attr.gend == 2, law_attr.gend == 2])\n\nNext, generate 1000 random graphs with the same number of ties as the observed one, i.e. the null model \\({\\cal{U}}|L\\):\n\nlaw_sim1000.frn &lt;- rgnm(1000, law_nodes, law_ties.frn, mode='digraph')\n\nFor each random network generated, calculate the number of homophilous ties in the same way:\n\nhomoph_sim.frn &lt;- apply(law_sim1000.frn, 1, function(x) {\n                sum(x[law_attr.gend == 1,law_attr.gend == 1]) + \n                sum(x[law_attr.gend == 2, law_attr.gend == 2])})\n\nTo plot the distribution of homophilous ties under the null hypothesis \\(H_0\\):\n\nhomoph_sim.frn &lt;- as.data.frame(homoph_sim.frn)\np_lawsim1000.frn &lt;- ggplot(homoph_sim.frn, aes(x= homoph_sim.frn))  +\n  geom_histogram(binwidth = 5, color=\"darkgrey\", fill=\"lightgrey\") +\n  coord_cartesian(ylim=c(0,200)) +\n  labs(title = \"\", x = \"number of homophilious ties\")\np_lawsim1000.frn \n\n\n\n\n\n\n\n\nQ8: Can you reject the null? Why or why not?\n\n\nTest 1: Cowork based on law practice\nWe want to check whether or not the partners of the firm more frequently work together with other partners having the same practice. We import the data as a graph object from the networkdata package:\n\ndata(\"law_cowork\")\n\nWe then create an adjacency matrix from the directed graph for the first 36 lawyers in the network corresponding to the partners of the firm (see attribute ‘status’). To test homophily now, we only consider the reciprocal ties so we need to symmetrize the matrix to create and undirected graph:\n\nlaw_mat_cwdir &lt;- as_adjacency_matrix(law_cowork, sparse = FALSE)\nlaw_mat_cwdir &lt;- law_mat_cwdir[1:36,1:36]\nlaw_mat_cw &lt;- (law_mat_cwdir == t(law_mat_cwdir) & law_mat_cwdir ==1) + 0\nlaw_nodes_cw &lt;- dim(law_mat_cw)[1]\nlaw_ties_cw &lt;- sum(law_mat_cw)/2\n\nNext we save the binary attribute ‘practice’ (1 = litigation, 2 = corporate) from the graph object as a vector, which is then in turn converted into a data frame (again only for the first 36 lawyers who are partners):\n\nlaw_attr.pract &lt;- as.data.frame(vertex_attr(law_cowork)$pract[1:36])\n\nTo calculate the number of observed homophilous ties:\n\nhomoph_obs.cw &lt;- sum(\n  law_mat_cw[law_attr.pract == 1, law_attr.pract == 1])/2 + \n  sum(law_mat_cw[law_attr.pract == 2, law_attr.pract == 2])/2\nhomoph_obs.cw\n\n[1] 72\n\n\nNext, generate 1000 random graphs with the same number of ties as the observed one, i.e. the null model \\({\\cal{U}}|L\\):\n\nset.seed(7722) # so that we all get the same results\nlaw_sim1000.cw &lt;- rgnm(1000, law_nodes_cw, law_ties_cw, mode='graph')\n\nFor each random network generated, calculate the number of homophilous ties in the same way:\n\nhomoph_sim.cw &lt;- apply(law_sim1000.cw, 1, function(x) {\n                sum(x[law_attr.pract == 1,law_attr.pract == 1])/2 + \n                sum(x[law_attr.pract == 2, law_attr.pract == 2])/2})\n\nThe distribution of homophilous ties under the null hypothesis \\(H_0\\) is plotted below with a red line indicating where the observed value falls in this distribution:\n\nhomoph_sim.cw &lt;- as.data.frame(homoph_sim.cw)\np_lawsim1000.cw &lt;- ggplot(homoph_sim.cw , aes(x= homoph_sim.cw))  +\n  geom_histogram(binwidth = 1, color=\"darkgrey\", fill=\"lightgrey\") +\n  coord_cartesian(ylim=c(0,100)) +\n  geom_vline(xintercept = homoph_obs.cw, lwd=0.5, colour=\"red\") +\n  labs(title = \"\", x = \"number of homophilious ties\")\np_lawsim1000.cw\n\n\n\n\n\n\n\n\nQ9: Can you reject the null? Why or why not? We can also calculate the \\(p\\)-value for this test. Thus, you can use this distribution to calculate probability \\(P(\\text{test statistic} &gt; \\textrm{ observed value } |H_0 \\textrm{ true })\\):\n\nsum(homoph_sim.cw  &gt; homoph_obs.cw)/1000\n\n[1] 0.002\n\n\nQ10: How do you interpret this value?"
  },
  {
    "objectID": "teaching/sna/material/07/07-netviz-2.html",
    "href": "teaching/sna/material/07/07-netviz-2.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "library(igraph)\nlibrary(graphlayouts)\nlibrary(ggraph)\nlibrary(networkdata)"
  },
  {
    "objectID": "teaching/sna/material/07/07-netviz-2.html#load-packages",
    "href": "teaching/sna/material/07/07-netviz-2.html#load-packages",
    "title": "Social Network Analysis",
    "section": "",
    "text": "library(igraph)\nlibrary(graphlayouts)\nlibrary(ggraph)\nlibrary(networkdata)"
  },
  {
    "objectID": "teaching/sna/material/07/07-netviz-2.html#dynamic-layouts",
    "href": "teaching/sna/material/07/07-netviz-2.html#dynamic-layouts",
    "title": "Social Network Analysis",
    "section": "Dynamic layouts",
    "text": "Dynamic layouts\n\nlibrary(gganimate)\nlibrary(ggplot2)\nlibrary(patchwork)\n# also install package 'gifski' to create gifs\n\n\n# downloaded from https://www.stats.ox.ac.uk/~snijders/siena/siena_datasets.htm\ndata(\"s50\")\ns50\n\n[[1]]\nIGRAPH dd2aeb6 UN-- 50 74 -- \n+ attr: name (v/c), smoke (v/n)\n+ edges from dd2aeb6 (vertex names):\n [1] V1 --V11 V1 --V14 V2 --V7  V2 --V11 V3 --V4  V3 --V9  V4 --V9  V5 --V32\n [9] V6 --V8  V7 --V12 V7 --V26 V7 --V42 V7 --V44 V10--V11 V10--V14 V10--V15\n[17] V10--V33 V11--V14 V11--V15 V11--V16 V11--V19 V11--V30 V12--V42 V12--V44\n[25] V15--V16 V17--V18 V17--V19 V17--V21 V17--V22 V17--V24 V18--V19 V18--V35\n[33] V19--V24 V19--V26 V19--V30 V21--V22 V21--V24 V21--V31 V21--V32 V22--V24\n[41] V22--V25 V22--V31 V22--V34 V22--V43 V23--V24 V25--V31 V25--V32 V26--V29\n[49] V26--V30 V26--V44 V27--V28 V27--V29 V27--V30 V29--V30 V29--V33 V30--V33\n[57] V31--V32 V31--V34 V31--V37 V32--V37 V34--V37 V36--V38 V36--V41 V38--V41\n+ ... omitted several edges\n\n[[2]]\nIGRAPH 6e28c77 UN-- 50 81 -- \n+ attr: name (v/c), smoke (v/n)\n+ edges from 6e28c77 (vertex names):\n [1] V1 --V10 V1 --V11 V1 --V14 V1 --V33 V2 --V26 V3 --V4  V3 --V9  V4 --V5 \n [9] V4 --V17 V4 --V34 V5 --V17 V6 --V8  V6 --V35 V7 --V26 V7 --V44 V10--V11\n[17] V10--V14 V10--V33 V11--V14 V11--V19 V11--V26 V11--V30 V12--V15 V12--V26\n[25] V12--V42 V12--V44 V15--V16 V15--V36 V15--V42 V16--V26 V16--V42 V16--V44\n[33] V17--V22 V17--V24 V17--V27 V17--V32 V18--V35 V19--V21 V19--V23 V19--V30\n[41] V19--V36 V19--V41 V21--V31 V21--V37 V21--V40 V22--V24 V23--V50 V24--V25\n[49] V24--V28 V25--V27 V25--V28 V25--V32 V26--V42 V27--V28 V28--V35 V29--V30\n[57] V29--V33 V29--V42 V30--V33 V30--V36 V30--V41 V31--V32 V31--V37 V32--V37\n+ ... omitted several edges\n\n[[3]]\nIGRAPH 9285b7f UN-- 50 77 -- \n+ attr: name (v/c), smoke (v/n)\n+ edges from 9285b7f (vertex names):\n [1] V1 --V10 V1 --V11 V1 --V14 V1 --V41 V2 --V7  V2 --V23 V2 --V26 V3 --V4 \n [9] V3 --V9  V3 --V34 V4 --V32 V4 --V34 V5 --V17 V5 --V32 V6 --V24 V6 --V27\n[17] V6 --V28 V7 --V16 V7 --V26 V7 --V42 V7 --V44 V8 --V25 V10--V11 V10--V12\n[25] V10--V14 V10--V33 V11--V14 V11--V15 V11--V33 V12--V15 V12--V33 V14--V33\n[33] V15--V29 V15--V33 V15--V36 V16--V26 V16--V42 V16--V44 V17--V22 V17--V27\n[41] V19--V29 V19--V30 V19--V36 V21--V31 V21--V37 V21--V40 V21--V45 V24--V27\n[49] V24--V28 V25--V50 V26--V44 V27--V28 V29--V30 V29--V33 V30--V33 V30--V36\n[57] V31--V37 V35--V37 V35--V50 V36--V38 V36--V41 V37--V47 V38--V41 V39--V43\n+ ... omitted several edges\n\n\nThe dataset consists of three networks with 50 actors each and a vertex attribute for the smoking behaviour of students. As a first step, we need to create a layout for all three networks. You can basically use any type of layout for each network, but I’d recommend layout_as_dynamic() from the package {{graphlayouts}}. The algorithm calculates a reference layout which is a layout of the union of all networks and individual layouts based on stress minimization and combines those in a linear combination which is controlled by the alpha parameter. For alpha=1, only the reference layout is used and all graphs have the same layout. For alpha=0, the stress layout of each individual graph is used. Values in-between interpolate between the two layouts.\n\n# Try other values for alpha\nxy &lt;- layout_as_dynamic(s50, alpha = 0.2)\npList &lt;- vector(\"list\", length(s50))\n\n\n#static plots\nfor (i in 1:length(s50)) {\n  pList[[i]] &lt;- ggraph(s50[[i]], layout = \"manual\", x = xy[[i]][, 1], y = xy[[i]][, 2]) +\n    geom_edge_link0(edge_width = 0.6, edge_colour = \"grey66\") +\n    geom_node_point(shape = 21, aes(fill = as.factor(smoke)), size = 6) +\n    geom_node_text(label = 1:50, repel = FALSE, color = \"white\", size = 4) +\n    scale_fill_manual(\n      values = c(\"forestgreen\", \"grey25\", \"firebrick\"),\n      guide = ifelse(i != 2, \"none\", \"legend\"),\n      name = \"smoking\",\n      labels = c(\"never\", \"occasionally\", \"regularly\")\n    ) +\n    theme_graph() +\n    theme(legend.position = \"bottom\") +\n    labs(title = paste0(\"Wave \", i))\n}\n# Reduce(\"+\", pList)\npList[[1]] + pList[[2]] + pList[[3]]\n\n\n\n\n\n\n\n\nThis is nice but of course we want to animate the changes. This is where we say goodbye to ggraph and hello to good-old ggplot2. First, we create a list of data frames for all nodes and add the layout to it.\n\n# create a list which contains all nodes and layout\nnodes_lst &lt;- lapply(1:length(s50), function(i) {\n  cbind(igraph::as_data_frame(s50[[i]], \"vertices\"),\n    x = xy[[i]][, 1], y = xy[[i]][, 2], frame = i\n  )\n})\n\n\nedges_lst &lt;- lapply(1:length(s50), function(i) {\n  cbind(igraph::as_data_frame(s50[[i]], \"edges\"), frame = i)\n})\n\nedges_lst &lt;- lapply(1:length(s50), function(i) {\n  edges_lst[[i]]$x &lt;- nodes_lst[[i]]$x[match(edges_lst[[i]]$from, nodes_lst[[i]]$name)]\n  edges_lst[[i]]$y &lt;- nodes_lst[[i]]$y[match(edges_lst[[i]]$from, nodes_lst[[i]]$name)]\n  edges_lst[[i]]$xend &lt;- nodes_lst[[i]]$x[match(edges_lst[[i]]$to, nodes_lst[[i]]$name)]\n  edges_lst[[i]]$yend &lt;- nodes_lst[[i]]$y[match(edges_lst[[i]]$to, nodes_lst[[i]]$name)]\n  edges_lst[[i]]$id &lt;- paste0(edges_lst[[i]]$from, \"-\", edges_lst[[i]]$to)\n  edges_lst[[i]]$status &lt;- TRUE\n  edges_lst[[i]]\n})\n\nhead(edges_lst[[1]])\n\n  from  to frame         x         y      xend       yend     id status\n1   V1 V11     1  1.803046  0.247353  2.123787 -0.7457415 V1-V11   TRUE\n2   V1 V14     1  1.803046  0.247353  2.391336  0.2295657 V1-V14   TRUE\n3   V2  V7     1  3.623264 -1.313577  3.870197 -1.9488863  V2-V7   TRUE\n4   V2 V11     1  3.623264 -1.313577  2.123787 -0.7457415 V2-V11   TRUE\n5   V3  V4     1 -4.877254 -2.719843 -3.854924 -2.8766208  V3-V4   TRUE\n6   V3  V9     1 -4.877254 -2.719843 -5.416373 -3.4092101  V3-V9   TRUE\n\n\nWe have expanded the edge data frame in a way that also includes the coordinates of the endpoints from the layout that we calculated earlier.\nNow we create a helper matrix which includes all edges that are present in any of the networks.\n\nall_edges &lt;- do.call(\"rbind\", lapply(s50, get.edgelist))\nall_edges &lt;- all_edges[!duplicated(all_edges), ]\nall_edges &lt;- cbind(all_edges, paste0(all_edges[, 1], \"-\", all_edges[, 2]))\n\nThis is used to impute the edges into all networks. So any edge that is not present in time frame two and three gets added to time frame one. But to keep track of these, we set there status to FALSE.\n\nedges_lst &lt;- lapply(1:length(s50), function(i) {\n  idx &lt;- which(!all_edges[, 3] %in% edges_lst[[i]]$id)\n  if (length(idx != 0)) {\n    tmp &lt;- data.frame(from = all_edges[idx, 1], to = all_edges[idx, 2], id = all_edges[idx, 3])\n    tmp$x &lt;- nodes_lst[[i]]$x[match(tmp$from, nodes_lst[[i]]$name)]\n    tmp$y &lt;- nodes_lst[[i]]$y[match(tmp$from, nodes_lst[[i]]$name)]\n    tmp$xend &lt;- nodes_lst[[i]]$x[match(tmp$to, nodes_lst[[i]]$name)]\n    tmp$yend &lt;- nodes_lst[[i]]$y[match(tmp$to, nodes_lst[[i]]$name)]\n    tmp$frame &lt;- i\n    tmp$status &lt;- FALSE\n    edges_lst[[i]] &lt;- rbind(edges_lst[[i]], tmp)\n  }\n  edges_lst[[i]]\n})\n\nWhy are we doing this? After a lot of experimenting, I came to the conclusion that it is always best to draw all edges, but use zero opacity if status = FALSE. In that way, one gets a smoother transition for edges that (dis)appear. There are probably other workarounds though.\nIn the last step, we create a data frame out of the lists.\n\nedges_df &lt;- do.call(\"rbind\", edges_lst)\nnodes_df &lt;- do.call(\"rbind\", nodes_lst)\n\nhead(edges_df)\n\n  from  to frame         x         y      xend       yend     id status\n1   V1 V11     1  1.803046  0.247353  2.123787 -0.7457415 V1-V11   TRUE\n2   V1 V14     1  1.803046  0.247353  2.391336  0.2295657 V1-V14   TRUE\n3   V2  V7     1  3.623264 -1.313577  3.870197 -1.9488863  V2-V7   TRUE\n4   V2 V11     1  3.623264 -1.313577  2.123787 -0.7457415 V2-V11   TRUE\n5   V3  V4     1 -4.877254 -2.719843 -3.854924 -2.8766208  V3-V4   TRUE\n6   V3  V9     1 -4.877254 -2.719843 -5.416373 -3.4092101  V3-V9   TRUE\n\nhead(nodes_df)\n\n   name smoke         x         y frame\nV1   V1     2  1.803046  0.247353     1\nV2   V2     3  3.623264 -1.313577     1\nV3   V3     1 -4.877254 -2.719843     1\nV4   V4     1 -3.854924 -2.876621     1\nV5   V5     1 -2.824439 -3.368968     1\nV6   V6     1 -1.529341 -5.292054     1\n\n\nAnd that’s it in terms of data wrangling. All that is left is to plot/animate the data.\n\nggplot() +\n  geom_segment(\n    data = edges_df,\n    aes(x = x, xend = xend, y = y, yend = yend, group = id, alpha = status),\n    show.legend = FALSE\n  ) +\n  geom_point(\n    data = nodes_df, aes(x, y, group = name, fill = as.factor(smoke)),\n    shape = 21, size = 4, show.legend = FALSE\n  ) +\n  scale_fill_manual(values = c(\"forestgreen\", \"grey25\", \"firebrick\")) +\n  scale_alpha_manual(values = c(0, 1)) +\n  ease_aes(\"quadratic-in-out\") +\n  transition_states(frame, state_length = 0.5, wrap = FALSE) +\n  labs(title = \"Wave {closest_state}\") +\n  theme_void()"
  },
  {
    "objectID": "teaching/sna/material/07/07-netviz-2.html#interactive-plots-with-visnetwork",
    "href": "teaching/sna/material/07/07-netviz-2.html#interactive-plots-with-visnetwork",
    "title": "Social Network Analysis",
    "section": "Interactive plots with visNetwork",
    "text": "Interactive plots with visNetwork\n\nlibrary(visNetwork)\ndata(\"karate\")\n\n\nvisIgraph(karate)\n\n\n\n\n\n\nkarate_df &lt;- toVisNetworkData(karate)\nvisNetwork(nodes = karate_df$nodes, \n           edges = karate_df$edges, height = \"300px\")"
  },
  {
    "objectID": "teaching/sna/material/07/07-netviz-2.html#gimmicks",
    "href": "teaching/sna/material/07/07-netviz-2.html#gimmicks",
    "title": "Social Network Analysis",
    "section": "Gimmicks",
    "text": "Gimmicks\nThe ggforce package works pretty nicely with ggraph. You can, for instance, use the geom_mark_*() functions to highlight clusters.\n\nlibrary(ggforce)\n\n\nset.seed(665)\n\n#create network with a group structure\ng &lt;- sample_islands(9, 40, 0.4, 15)\ng &lt;- simplify(g)\nV(g)$grp &lt;- as.character(rep(1:9, each = 40))\n\n\nggraph(g, layout = \"backbone\", keep = 0.4) +\n  geom_edge_link0(edge_color = \"grey66\", edge_width = 0.2) +\n  geom_node_point(aes(fill = grp), shape = 21, size = 3) +\n  geom_mark_hull(\n    aes(x, y, group = grp, fill = grp),\n    concavity = 4,\n    expand = unit(2, \"mm\"),\n    alpha = 0.25\n  ) +\n  scale_color_brewer(palette = \"Set1\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme_graph()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nOf course you can also add a label to your clusters.\n\nggraph(g, layout = \"backbone\", keep = 0.4) +\n  geom_edge_link0(edge_color = \"grey66\", edge_width = 0.2) +\n  geom_node_point(aes(fill = grp), shape = 21, size = 3) +\n  geom_mark_hull(\n    aes(x, y, group = grp, fill = grp, label=grp),\n    concavity = 4,\n    expand = unit(2, \"mm\"),\n    alpha = 0.25\n  ) +\n  scale_color_brewer(palette = \"Set1\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme_graph()+\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n“How can I achieve that my directed edges stop at the node border, independent from the node size?”\n\nOut of the box you will probably end up with something like this\n\n# create a random network\nset.seed(1071)\ng &lt;- sample_pa(30, 1)\nV(g)$degree &lt;- degree(g, mode = \"in\")\n\nggraph(g, \"stress\") +\n  geom_edge_link(\n    aes(end_cap = circle(node2.degree + 2, \"pt\")),\n    edge_colour = \"black\",\n    arrow = arrow(\n      angle = 10,\n      length = unit(0.15, \"inches\"),\n      ends = \"last\",\n      type = \"closed\"\n    )\n  ) +\n  geom_node_point(aes(size = degree), col = \"grey66\", show.legend = FALSE) +\n  scale_size(range = c(3, 11)) +\n  theme_graph()\n\n\n\n\n\n\n\n\nThe overlap can be avoided by using the I() function from base R, which treats the entries of a vector “as is”. So we know that if a node has degree 5, it will be mapped to a circle with radius (or diameter?) “5pt”. Since this means, that you have no control over the scaling, you need to do that beforehand.\n\nnormalise &lt;- function(x, from = range(x), to = c(0, 1)) {\n  x &lt;- (x - from[1]) / (from[2] - from[1])\n  if (!identical(to, c(0, 1))) {\n    x &lt;- x * (to[2] - to[1]) + to[1]\n  }\n  x\n}\n\n# map to the range you want\nV(g)$degree &lt;- normalise(V(g)$degree, to = c(3, 11))\n\nggraph(g, \"stress\") +\n  geom_edge_link(\n    aes(end_cap = circle(node2.degree + 2, \"pt\")),\n    edge_colour = \"grey25\",\n    arrow = arrow(\n      angle = 10,\n      length = unit(0.15, \"inches\"),\n      ends = \"last\",\n      type = \"closed\"\n    )\n  ) +\n  geom_node_point(aes(size = I(degree)), col = \"grey66\") +\n  theme_graph()\n\n\n\n\n\n\n\n\n\n“How can I lower the opacity of nodes without making edges visible underneath?”\n\nOne of the rules you should try to follow is that edges should not be visible on top of nodes. Usually that is easy to achieve by drawing the edges before the nodes. But if you want to lower the opacity of nodes, they do become visible again.\n\ng &lt;- sample_gnp(20, 0.5)\nV(g)$degree &lt;- degree(g)\n\nggraph(g, \"stress\") +\n  geom_edge_link(edge_colour = \"grey66\") +\n  geom_node_point(\n    size = 8,\n    aes(alpha = degree),\n    col = \"red\",\n    show.legend = FALSE\n  ) +\n  theme_graph()\n\n\n\n\n\n\n\n\nThe solution is rather simple. Just add a node layer with the same aesthetics below with alpha=1 (default) and color=\"white\" (or the background color of the plot).\n\nggraph(g, \"stress\") +\n  geom_edge_link(edge_colour = \"grey66\") +\n  geom_node_point(size = 8, col = \"white\") +\n  geom_node_point(\n    aes(alpha = degree),\n    size = 8,\n    col = \"red\",\n    show.legend = FALSE\n  ) +\n  theme_graph()\n\n\n\n\n\n\n\n\nOf course you could also use start_cap and end_cap here, but you may have to fiddle again as in the last example.\n\n“How can I enhance readability of node labels in hairball graphs?”\n\nSometimes it is really hard to make labels readable when the network is very cluttered\n\ng &lt;- sample_gnp(50, 0.7)\nV(g)$name &lt;- sapply(1:50, function(x) paste0(sample(LETTERS, 4), collapse = \"\"))\nE(g)$weight &lt;- runif(ecount(g))\n\nggraph(g) +\n  geom_edge_link0(aes(edge_color = weight, edge_width = weight), show.legend = FALSE) +\n  geom_node_point(size = 8, color = \"#44a6c6\") +\n  geom_node_text(aes(label = name), fontface = \"bold\") +\n  scale_edge_color_continuous(low = \"grey66\", high = \"black\") +\n  scale_edge_width(range = c(0.1, 0.5)) +\n  theme_graph() +\n  coord_fixed()\n\nUsing \"stress\" as default layout\n\n\n\n\n\n\n\n\n\nHere you can make use of the fact that the layout of the nodes are stored in a “hidden” data frame when a ggraph object is constructed (this is what we made use of with geom_mark_hull() too). That means you can use other geoms from other packages. In this case, the shadowtext package as shown below.\n\nggraph(g,\"stress\") +\n  geom_edge_link0(aes(edge_color = weight, edge_width = weight), show.legend = FALSE) +\n  geom_node_point(size = 8, color = \"#44a6c6\") +\n  shadowtext::geom_shadowtext(aes(x, y, label = name), color = \"black\", size = 4, bg.colour = \"white\") +\n  scale_edge_color_continuous(low = \"grey66\", high = \"black\") +\n  scale_edge_width(range = c(0.1, 0.5)) +\n  theme_graph() +\n  coord_fixed()"
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms2.html",
    "href": "teaching/sna/material/10/10-ergms2.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "Following the same modeling outline as before, we will here fit ERGMs on two-mode networks. Here, the model terms are slightly different than for the case of one-mode data. We will use the data used here, but I have already cleaned the data so you can import it as an .RDS file.\nThe data is on students joining clubs in a high school and we have basic information about the students, the clubs, and which students are members of which clubs. Familiarize yourself with this data set.\nRecall that this page is useful for looking up different model terms: ERGM terms.\n\n\n\nlibrary(statnet) # also loads the ERGM package\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(intergraph)\nlibrary(patchwork)\nlibrary(networkdata)\n\n# download data (network object):\nbnet96 &lt;- readRDS(\"bnet96.rds\")\n\n\n\n\nWe start with a simple model and just include a term for edges. The edges term is directly analogous to the edges term for one mode networks, except here we must remember that edges can only exist between actors of different modes, i.e. edges are only possible between women and dates for social events.\n\nmod1 &lt;- ergm(bnet96 ~ edges)\nsummary(mod1)\n\nCall:\nergm(formula = bnet96 ~ edges)\n\nMaximum Likelihood Results:\n\n      Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges -3.44427    0.01977      0  -174.3   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 118331  on 85358  degrees of freedom\n Residual Deviance:  23558  on 85357  degrees of freedom\n \nAIC: 23560  BIC: 23569  (Smaller is better. MC Std. Err. = 0)\n\n\nQ1 How do you interpret the coefficient here? Recall that the probability of a tie is then given by exp(-3.44427) / (1 + exp(-3.44427)) = .0309402.\n\n\n\nWe have some structural conditions to consider: some student-club combinations cannot occur, or occur at extremely low rates. We need to adjust our model for these cases, or run the risk of biasing the estimates. For example, girls are very unlikely to join boys sports teams (e.g., wrestling or football), while boys are unlikely to join girls sports teams (volleyball). There are similar structural issues with grade, as some clubs are restricted to certain grades. For example, students in grade 9, 10, 11 or 12 do not join eighth grade sports teams (e.g., 8th grade football).\nWe will include nodemix terms in the model for gender and grade, capturing the student-club combinations that are unlikely to exist. The two attributes of interest are studentgender_clubgender and studentgrade96_clubgrad.\nWe will start with gender. Remember that student gender is measured as male or female, while club gender is measured as boys, boys_girls, or girls. We want to print out the combinations of student-club genders (e.g., “male.boys”). By default, nodemix will print the terms sorted alphabetically, first by the second level (clubs) and then by the first level (students). Let’s create our vector of names (student-club combinations) with that ordering in mind.\n\nstudent_gender &lt;- sort(as.character(c(\"female\", \"male\")))\nclub_gender &lt;- sort(c(\"boys\", \"boys_girls\", \"girls\"))\n\ngender_levels &lt;- paste(rep(student_gender, times = length(club_gender)), \n                      rep(club_gender, each = length(student_gender)),\n                      sep = \".\" )\ndata.frame(gender_levels)\n\n      gender_levels\n1       female.boys\n2         male.boys\n3 female.boys_girls\n4   male.boys_girls\n5      female.girls\n6        male.girls\n\n\nThe terms of interest are female.boys and male.girls; as these are the rare events we want to adjust for, girls joining boys clubs and boys joining girls clubs. This corresponds to spots 1 and 6 from the summary statistics, so let’s create a vector holding that information.\n\ngender_terms &lt;- c(1, 6)\n\nAnd now we look at the grade attribute, studentgrade96_clubgrade. Student grade is measured as 8, 9, 10, 11 or 12. Club grade is measured as: eighth (just eighth graders), ninth (just ninth graders), ninth_tenth_eleventh (just ninth, tenth or eleventh graders), and ninth+ (no eighth graders). The value is all_grades if there are no restrictions on membership, in terms of grade. Let’s get all student-club combinations for grade and put them together to be consistent with the nodemix term (sorted by the second level and then by the first level):\n\nstudent_grades &lt;- sort(as.character(c(8:12)))\nclub_grades &lt;- sort(c(\"all_grades\", \"eight\", \"ninth\", \"ninth_tenth_eleventh\", \"ninth+\"))\n\ngrade_levels &lt;- paste(rep(student_grades, times = length(club_grades)), \n                      rep(club_grades, each = length(student_grades)),\n                      sep = \".\" )\ndata.frame(grade_levels)\n\n              grade_levels\n1            10.all_grades\n2            11.all_grades\n3            12.all_grades\n4             8.all_grades\n5             9.all_grades\n6                 10.eight\n7                 11.eight\n8                 12.eight\n9                  8.eight\n10                 9.eight\n11                10.ninth\n12                11.ninth\n13                12.ninth\n14                 8.ninth\n15                 9.ninth\n16 10.ninth_tenth_eleventh\n17 11.ninth_tenth_eleventh\n18 12.ninth_tenth_eleventh\n19  8.ninth_tenth_eleventh\n20  9.ninth_tenth_eleventh\n21               10.ninth+\n22               11.ninth+\n23               12.ninth+\n24                8.ninth+\n25                9.ninth+\n\n\nWe want to include terms for any student-club combination that should not exist; like 12th graders in a ninth grade club, 12.ninth. Based on the order from the summary statistics, this corresponds to: 6 (10.eighth), 7 (11.eighth), 8 (12.eighth), 10 (9.eighth), 11 (10.ninth), 12 (11.ninth), 13 (12.ninth), 14 (8.ninth), 18 (12.ninth_tenth_eleventh), 19 (8.ninth_tenth_eleventh) and 24 (8.ninth+).\n\ngrade_terms &lt;- c(6, 7, 8, 10, 11, 12, 13, 14, 18, 19, 24)\n\nWe an now estimate our model, including nodemix terms for studentgender_clubgender and studentgrade96_clubgrade. We will specify which terms to include using the levels2 argument, setting it to the vectors defined above. To simplify the estimation of the model, we will specify these terms using an Offset() function (although we could have estimated the coefficients within the model). When using Offset(), the coefficients are not estimated and are instead set using the values supplied in the coef argument. This is appropriate in our case as the coefficients are based on logical conditions (e.g., 12th graders do not join 9th grade clubs) and can be set a priori by the researcher. Here, we set the coefficients to -10 for every term. We set the coefficients to -10 as we want to estimate the models conditioned on the fact that these student-club combinations are very rare.\n\noffset_coefs_gender &lt;- rep(-10, length(gender_terms))\noffset_coefs_grade &lt;- rep(-10, length(grade_terms))\n\nmod2 &lt;- ergm(bnet96 ~ edges + \n               Offset(~ nodemix(\"studentgender_clubgender\", \n                              levels2 = gender_terms), \n                      coef = offset_coefs_gender) + \n               Offset(~ nodemix(\"studentgrade96_clubgrade\", \n                              levels2 = grade_terms), \n                      coef = offset_coefs_grade))\nsummary(mod2)\n\nCall:\nergm(formula = bnet96 ~ edges + Offset(~nodemix(\"studentgender_clubgender\", \n    levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(\"studentgrade96_clubgrade\", \n    levels2 = grade_terms), coef = offset_coefs_grade))\n\nMaximum Likelihood Results:\n\n      Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges -2.99179    0.01994      0    -150   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 118331  on 85358  degrees of freedom\n Residual Deviance:  21515  on 85357  degrees of freedom\n \nAIC: 21517  BIC: 21526  (Smaller is better. MC Std. Err. = 0)\n\n\nAll of the offset terms are set to -10, although they are not printed out here. We see that the edge coefficient is different than with mod1, suggesting the importance of adjusting our model for structural/logical constraints. Note also that the model fit should only be compared to other models with the same set of offset terms.\nWe used nodemix terms to capture structural conditions in the data, but we could imagine using nodemix terms to answer more substantive questions. We could test if certain types of students (e.g., girls) are more likely to join certain types of clubs (academic, leadership, etc.), and this is left as an exercise.\n\n\n\nWe now add nodefactor and homophily terms to our model. With two-mode networks, these terms can take two forms, with different terms for the first and second mode. Here we focus on the second mode, the clubs.\n\n\nnodefactor terms capture differences in degree across nodes, here clubs, with different attributes. We are interested in the main effect of club type (sports, academic, etc.) and club profile (low, moderate, high, very high) on membership. For example, do high profile clubs have more members than low profile clubs?\nThe term of interest is b2factor (b2 indicating the second mode of a bipartite network). We will include b2factor terms for each club attribute of interest, club_type_detailed and club_profile. We include a levels argument for club_profile to set the order that the results are printed. By default, the results are printed in alphabetical order. In this case, that would correspond to high (1), low (2), moderate (3), very high (4), with high excluded as the reference. But we want the results to run from moderate (3) to high (1) to very high (4), with low (2) excluded (as this is easier to interpret). For club_type_detailed, we use the levels argument to set the second category, academic interest, as the reference. We set levels to -2 to exclude only the second category.\n\nmod3 &lt;- ergm(bnet96 ~ edges + \n               Offset(~nodemix(\"studentgender_clubgender\", \n                              levels2 = gender_terms), \n                      coef = offset_coefs_gender) + \n               Offset(~nodemix(\"studentgrade96_clubgrade\", \n                              levels2 = grade_terms), \n                      coef = offset_coefs_grade) + \n                b2factor(\"club_type_detailed\", levels = -2) + \n                b2factor(\"club_profile\", levels = c(3, 1, 4)))\nsummary(mod3)\n\nCall:\nergm(formula = bnet96 ~ edges + Offset(~nodemix(\"studentgender_clubgender\", \n    levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(\"studentgrade96_clubgrade\", \n    levels2 = grade_terms), coef = offset_coefs_grade) + b2factor(\"club_type_detailed\", \n    levels = -2) + b2factor(\"club_profile\", levels = c(3, 1, \n    4)))\n\nMaximum Likelihood Results:\n\n                                                 Estimate Std. Error MCMC %\nedges                                            -2.82059    0.04035      0\nb2factor.club_type_detailed.Academic Competition -0.44051    0.08242      0\nb2factor.club_type_detailed.Ethnic Interest      -1.03174    0.16677      0\nb2factor.club_type_detailed.Individual Sports    -0.41835    0.08391      0\nb2factor.club_type_detailed.Leadership           -0.36924    0.18887      0\nb2factor.club_type_detailed.Media                -0.16730    0.14109      0\nb2factor.club_type_detailed.Performance Art       0.10725    0.06436      0\nb2factor.club_type_detailed.Service               0.31118    0.06262      0\nb2factor.club_type_detailed.Team Sports           0.45881    0.07798      0\nb2factor.club_profile.moderate                   -0.29070    0.05741      0\nb2factor.club_profile.high                       -0.94788    0.09396      0\nb2factor.club_profile.very_high                  -0.48185    0.11402      0\n                                                 z value Pr(&gt;|z|)    \nedges                                            -69.902   &lt;1e-04 ***\nb2factor.club_type_detailed.Academic Competition  -5.345   &lt;1e-04 ***\nb2factor.club_type_detailed.Ethnic Interest       -6.186   &lt;1e-04 ***\nb2factor.club_type_detailed.Individual Sports     -4.986   &lt;1e-04 ***\nb2factor.club_type_detailed.Leadership            -1.955   0.0506 .  \nb2factor.club_type_detailed.Media                 -1.186   0.2357    \nb2factor.club_type_detailed.Performance Art        1.666   0.0956 .  \nb2factor.club_type_detailed.Service                4.970   &lt;1e-04 ***\nb2factor.club_type_detailed.Team Sports            5.883   &lt;1e-04 ***\nb2factor.club_profile.moderate                    -5.064   &lt;1e-04 ***\nb2factor.club_profile.high                       -10.088   &lt;1e-04 ***\nb2factor.club_profile.very_high                   -4.226   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 118331  on 85358  degrees of freedom\n Residual Deviance:  21116  on 85346  degrees of freedom\n \nAIC: 21140  BIC: 21252  (Smaller is better. MC Std. Err. = 0)\n\n\nWe see, for example, that competitive academic clubs and individual sports tend to have fewer members than academic interest clubs (the reference), while service clubs (like National Honors Society) and team sports tend to be large. We also see that high profile clubs have, if anything, fewer members than low profile clubs.\nMore formally, we can interpret the coefficient on individual sports (for example) as follows: the odds of a student being part of an individual sports team is exp(-0.41835) times lower than the odds of being part of an academic interest club. It is worth emphasizing that b2factor is based only on club attributes, and is thus different from nodemix (see above), which incorporates attributes from both modes.\n\n\n\nHomophily is more complicated with two-mode networks than with one-mode networks. This is the case as there are no direct ties from nodes of the same type; in our case, there are no ties from students to students or from clubs to clubs. So, if we are interested in homophily on a club attribute, say club type, we cannot ask if team sports are tied to other team sports, as there are no ties between clubs. Instead, we must ask if similar clubs are linked together through students; e.g., do students in team sports tend to be members of other team sports?\nWe must counts two-paths, based on homophily on the attribute of interest. The basic idea is to sum up the number of times that we see A-\\(i\\)-B, where A and B are clubs with the same attribute (e.g., both team sports) and \\(i\\) is a student in both A and B. The count is technically over half the number of two-paths, to avoid double counting (as A-\\(i\\)-B is substantively the same as B-\\(i\\)-A). The term is b2nodematch. A positive coefficient on b2nodematch would indicate that students are members of similar kinds of clubs, above what can be explained by other terms in the model.\nBut how do we sum up the homophilous two-paths? In the simplest case, we can sum up all of the two-paths that match on the attribute of interest. This is the default specification. We may, however, have good reason to incorporate some discounting, so that adding one more two-path (for a given edge) only marginally increases the count, once we reach a certain threshold. For example, if student \\(i\\) is a member of the football team, (\\(i\\)-football edge), then if \\(i\\) is also a member of the wrestling team, that would be a strong signal of matching on club type (both team sports). But adding another team sport membership, say \\(i\\) is also a member of the baseball team, may not offer quite as much information; as we already know that \\(i\\) joins team sports. We may then want to count the second two-path less than the first.\nWe can control the discounting using a beta parameter, which raises the count of two-paths (for a given edge) to beta. Setting beta to 1 would yield the number of two-paths (for an edge) where there is a match on the club attribute of interest (so the \\(i\\)-football edge would contribute a count of 2). Setting beta to 0 gives us the other extreme: showing if the given edge is involved in at least one homophilous two-path (so the \\(i\\)-football edge would contribute a count of 1). The count of two-paths, raised to beta, is then summed over all edges and divided by 2.\nWe set beta to .25, but we could imagine using a range of values (estimating the model each time), using model fit statistics to evaluate the choice of beta.\n\nset.seed(1007)\n\nmod4 &lt;- ergm(bnet96 ~ edges + \n                Offset(~nodemix(\"studentgender_clubgender\", \n                              levels2 = gender_terms), \n                      coef = offset_coefs_gender) + \n                Offset(~nodemix(\"studentgrade96_clubgrade\", \n                              levels2 = grade_terms), \n                      coef = offset_coefs_grade) + \n                b2factor(\"club_type_detailed\", levels = -2) + \n                b2nodematch(\"club_type_detailed\", beta = .25) + \n                b2factor(\"club_profile\", levels = c(3, 1, 4)) + \n                b2nodematch(\"club_profile\", beta = .25), \n              control = control.ergm(MCMC.burnin = 20000,\n                                     MCMC.samplesize = 3000))\n\nThe model is now being fit using MCMC techniques. This means that we should check if the model is fitting well. Use the mcmc.diagnostics() function.\nQ2 How would you intepret the below results (not plotted here)?\n\nmcmc.diagnostics(mod4)\n\nGo ahead a summarize model 4:\n\nsummary(mod4)\n\nCall:\nergm(formula = bnet96 ~ edges + Offset(~nodemix(\"studentgender_clubgender\", \n    levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(\"studentgrade96_clubgrade\", \n    levels2 = grade_terms), coef = offset_coefs_grade) + b2factor(\"club_type_detailed\", \n    levels = -2) + b2nodematch(\"club_type_detailed\", beta = 0.25) + \n    b2factor(\"club_profile\", levels = c(3, 1, 4)) + b2nodematch(\"club_profile\", \n    beta = 0.25), control = control.ergm(MCMC.burnin = 20000, \n    MCMC.samplesize = 3000))\n\nMonte Carlo Maximum Likelihood Results:\n\n                                                  Estimate Std. Error MCMC %\nedges                                            -3.596083   0.075080      0\nb2factor.club_type_detailed.Academic Competition -0.323232   0.085331      0\nb2factor.club_type_detailed.Ethnic Interest      -0.844453   0.169750      0\nb2factor.club_type_detailed.Individual Sports    -0.296443   0.083610      0\nb2factor.club_type_detailed.Leadership           -0.171296   0.193888      0\nb2factor.club_type_detailed.Media                 0.028381   0.138718      0\nb2factor.club_type_detailed.Performance Art       0.129749   0.059862      0\nb2factor.club_type_detailed.Service               0.361836   0.059245      0\nb2factor.club_type_detailed.Team Sports           0.502725   0.075382      0\nb2nodematch.club_type_detailed                    0.413830   0.070045      0\nb2factor.club_profile.moderate                   -0.075008   0.059536      0\nb2factor.club_profile.high                       -0.538820   0.098625      0\nb2factor.club_profile.very_high                  -0.003789   0.124166      0\nb2nodematch.club_profile                          0.779436   0.088837      0\n                                                 z value Pr(&gt;|z|)    \nedges                                            -47.897  &lt; 1e-04 ***\nb2factor.club_type_detailed.Academic Competition  -3.788 0.000152 ***\nb2factor.club_type_detailed.Ethnic Interest       -4.975  &lt; 1e-04 ***\nb2factor.club_type_detailed.Individual Sports     -3.546 0.000392 ***\nb2factor.club_type_detailed.Leadership            -0.883 0.376978    \nb2factor.club_type_detailed.Media                  0.205 0.837889    \nb2factor.club_type_detailed.Performance Art        2.167 0.030198 *  \nb2factor.club_type_detailed.Service                6.107  &lt; 1e-04 ***\nb2factor.club_type_detailed.Team Sports            6.669  &lt; 1e-04 ***\nb2nodematch.club_type_detailed                     5.908  &lt; 1e-04 ***\nb2factor.club_profile.moderate                    -1.260 0.207713    \nb2factor.club_profile.high                        -5.463  &lt; 1e-04 ***\nb2factor.club_profile.very_high                   -0.031 0.975654    \nb2nodematch.club_profile                           8.774  &lt; 1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 118331  on 85358  degrees of freedom\n Residual Deviance:  20965  on 85344  degrees of freedom\n \nAIC: 20993  BIC: 21124  (Smaller is better. MC Std. Err. = 0.9958)\n\n\n\n\n\n\nWe now add the analogous terms (nodefactor and nodematch) for the first mode, students. Here we focus on student attributes, specifically for race. We will add b1factor and b1nodematch terms to the model (b1 indicating the first mode of a bipartite network). For b1factor, we test if there are differences in degree by racial groups (do white students join clubs at lower rates than Asian students?).\nFor b1nodematch, we test if clubs are segregated along racial lines. We will count the number of two paths, \\(i\\)-A-\\(j\\), where \\(i\\) and \\(j\\) are students of the same race and A is a club that both \\(i\\) and \\(j\\) are members of. Again, we can use the beta argument to set the discounting when summing up the two-paths that match on the attribute of interest. We will set beta to .15.\nTo help with estimation convergence, we will also set the reference category for the b1factor term to include Asian (1), Hispanic (3) and Native American (4) (basically collapsing some of the smaller categories into a single ‘other’ category). Finally, we will tweak the control parameters, increasing the burnin and `sample size. This can take a little while to run, so we might want to include parallel processing options to speed things up (here we set the number of processors to 4).\n\nmod5 &lt;- ergm(bnet96 ~ edges + \n               Offset(~nodemix(\"studentgender_clubgender\", \n                              levels2 = gender_terms), \n                      coef = offset_coefs_gender) + \n                Offset(~nodemix(\"studentgrade96_clubgrade\", \n                              levels2 = grade_terms), \n                      coef = offset_coefs_grade) + \n               b2factor(\"club_type_detailed\", levels = -2) + \n               b2nodematch(\"club_type_detailed\", beta = .25) + \n               b2factor(\"club_profile\", levels = c(3, 1, 4)) + \n               b2nodematch(\"club_profile\", beta = .25) +\n               b1factor(\"race\", levels = -c(1, 3, 4)) + \n               b1nodematch(\"race\", beta = .15), \n             control = control.ergm(MCMC.burnin = 30000, \n                                    MCMC.samplesize = 5000, \n                                    parallel = 4, \n                                    parallel.type = \"PSOCK\"))\nsummary(mod5)\n\nCall:\nergm(formula = bnet96 ~ edges + Offset(~nodemix(\"studentgender_clubgender\", \n    levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(\"studentgrade96_clubgrade\", \n    levels2 = grade_terms), coef = offset_coefs_grade) + b2factor(\"club_type_detailed\", \n    levels = -2) + b2nodematch(\"club_type_detailed\", beta = 0.25) + \n    b2factor(\"club_profile\", levels = c(3, 1, 4)) + b2nodematch(\"club_profile\", \n    beta = 0.25) + b1factor(\"race\", levels = -c(1, 3, 4)) + b1nodematch(\"race\", \n    beta = 0.15), control = control.ergm(MCMC.burnin = 30000, \n    MCMC.samplesize = 5000, parallel = 4, parallel.type = \"PSOCK\"))\n\nMonte Carlo Maximum Likelihood Results:\n\n                                                 Estimate Std. Error MCMC %\nedges                                            -6.83027    0.14712      0\nb2factor.club_type_detailed.Academic Competition -0.05914    0.05116      0\nb2factor.club_type_detailed.Ethnic Interest      -0.24224    0.11227      0\nb2factor.club_type_detailed.Individual Sports     0.39357    0.05757      0\nb2factor.club_type_detailed.Leadership            0.10532    0.13252      0\nb2factor.club_type_detailed.Media                 0.15606    0.09393      0\nb2factor.club_type_detailed.Performance Art       0.17197    0.03435      0\nb2factor.club_type_detailed.Service               0.14154    0.03296      0\nb2factor.club_type_detailed.Team Sports           0.95282    0.05173      0\nb2nodematch.club_type_detailed                    0.33851    0.06648      0\nb2factor.club_profile.moderate                    0.07439    0.04003      0\nb2factor.club_profile.high                       -0.10308    0.07348      0\nb2factor.club_profile.very_high                   0.16866    0.09232      0\nb2nodematch.club_profile                          0.69575    0.08628      0\nb1factor.race.black                              -1.11809    0.05122      0\nb1factor.race.white                              -1.51223    0.06228      0\nb1nodematch.race                                  5.04005    0.18534      0\n                                                 z value Pr(&gt;|z|)    \nedges                                            -46.426   &lt;1e-04 ***\nb2factor.club_type_detailed.Academic Competition  -1.156   0.2477    \nb2factor.club_type_detailed.Ethnic Interest       -2.158   0.0310 *  \nb2factor.club_type_detailed.Individual Sports      6.836   &lt;1e-04 ***\nb2factor.club_type_detailed.Leadership             0.795   0.4268    \nb2factor.club_type_detailed.Media                  1.661   0.0966 .  \nb2factor.club_type_detailed.Performance Art        5.006   &lt;1e-04 ***\nb2factor.club_type_detailed.Service                4.294   &lt;1e-04 ***\nb2factor.club_type_detailed.Team Sports           18.420   &lt;1e-04 ***\nb2nodematch.club_type_detailed                     5.092   &lt;1e-04 ***\nb2factor.club_profile.moderate                     1.858   0.0632 .  \nb2factor.club_profile.high                        -1.403   0.1607    \nb2factor.club_profile.very_high                    1.827   0.0677 .  \nb2nodematch.club_profile                           8.064   &lt;1e-04 ***\nb1factor.race.black                              -21.830   &lt;1e-04 ***\nb1factor.race.white                              -24.281   &lt;1e-04 ***\nb1nodematch.race                                  27.194   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 118331  on 85358  degrees of freedom\n Residual Deviance:  20437  on 85341  degrees of freedom\n \nAIC: 20471  BIC: 20630  (Smaller is better. MC Std. Err. = 1.304)\n\n\nInterpretation: It looks like black and white students are members of fewer clubs than Asian, Native American or Hispanic students (the reference), while there is segregation along racial lines (looking at the b1nodematch term). Students are unlikely to find themselves in clubs where there are few (or even no) students of the same race. For example, by chance we might expect Asian students (a small racial group) to often be in clubs with few other Asian students, but empirically this rarely happens. Looking at AIC and BIC, we see that the model fit is improved quite a bit from the previous model.\n\n\n\n\nDownload the same darta for 1997 and perform a similar analysis, i.e. fitting ERGMs with the same first and second mode terms. Can you note differences between the two years?\n\n\nbnet97 &lt;- readRDS(\"bnet97.rds\")\n\n\nPerform MCMC and GoF assessment on the models fitted for both years."
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms2.html#packages-needed",
    "href": "teaching/sna/material/10/10-ergms2.html#packages-needed",
    "title": "Social Network Analysis",
    "section": "",
    "text": "library(statnet) # also loads the ERGM package\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(intergraph)\nlibrary(patchwork)\nlibrary(networkdata)\n\n# download data (network object):\nbnet96 &lt;- readRDS(\"bnet96.rds\")"
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms2.html#model-1-only-edge-term",
    "href": "teaching/sna/material/10/10-ergms2.html#model-1-only-edge-term",
    "title": "Social Network Analysis",
    "section": "",
    "text": "We start with a simple model and just include a term for edges. The edges term is directly analogous to the edges term for one mode networks, except here we must remember that edges can only exist between actors of different modes, i.e. edges are only possible between women and dates for social events.\n\nmod1 &lt;- ergm(bnet96 ~ edges)\nsummary(mod1)\n\nCall:\nergm(formula = bnet96 ~ edges)\n\nMaximum Likelihood Results:\n\n      Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges -3.44427    0.01977      0  -174.3   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 118331  on 85358  degrees of freedom\n Residual Deviance:  23558  on 85357  degrees of freedom\n \nAIC: 23560  BIC: 23569  (Smaller is better. MC Std. Err. = 0)\n\n\nQ1 How do you interpret the coefficient here? Recall that the probability of a tie is then given by exp(-3.44427) / (1 + exp(-3.44427)) = .0309402."
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms2.html#model-2-gender-and-grade-specific-clubs",
    "href": "teaching/sna/material/10/10-ergms2.html#model-2-gender-and-grade-specific-clubs",
    "title": "Social Network Analysis",
    "section": "",
    "text": "We have some structural conditions to consider: some student-club combinations cannot occur, or occur at extremely low rates. We need to adjust our model for these cases, or run the risk of biasing the estimates. For example, girls are very unlikely to join boys sports teams (e.g., wrestling or football), while boys are unlikely to join girls sports teams (volleyball). There are similar structural issues with grade, as some clubs are restricted to certain grades. For example, students in grade 9, 10, 11 or 12 do not join eighth grade sports teams (e.g., 8th grade football).\nWe will include nodemix terms in the model for gender and grade, capturing the student-club combinations that are unlikely to exist. The two attributes of interest are studentgender_clubgender and studentgrade96_clubgrad.\nWe will start with gender. Remember that student gender is measured as male or female, while club gender is measured as boys, boys_girls, or girls. We want to print out the combinations of student-club genders (e.g., “male.boys”). By default, nodemix will print the terms sorted alphabetically, first by the second level (clubs) and then by the first level (students). Let’s create our vector of names (student-club combinations) with that ordering in mind.\n\nstudent_gender &lt;- sort(as.character(c(\"female\", \"male\")))\nclub_gender &lt;- sort(c(\"boys\", \"boys_girls\", \"girls\"))\n\ngender_levels &lt;- paste(rep(student_gender, times = length(club_gender)), \n                      rep(club_gender, each = length(student_gender)),\n                      sep = \".\" )\ndata.frame(gender_levels)\n\n      gender_levels\n1       female.boys\n2         male.boys\n3 female.boys_girls\n4   male.boys_girls\n5      female.girls\n6        male.girls\n\n\nThe terms of interest are female.boys and male.girls; as these are the rare events we want to adjust for, girls joining boys clubs and boys joining girls clubs. This corresponds to spots 1 and 6 from the summary statistics, so let’s create a vector holding that information.\n\ngender_terms &lt;- c(1, 6)\n\nAnd now we look at the grade attribute, studentgrade96_clubgrade. Student grade is measured as 8, 9, 10, 11 or 12. Club grade is measured as: eighth (just eighth graders), ninth (just ninth graders), ninth_tenth_eleventh (just ninth, tenth or eleventh graders), and ninth+ (no eighth graders). The value is all_grades if there are no restrictions on membership, in terms of grade. Let’s get all student-club combinations for grade and put them together to be consistent with the nodemix term (sorted by the second level and then by the first level):\n\nstudent_grades &lt;- sort(as.character(c(8:12)))\nclub_grades &lt;- sort(c(\"all_grades\", \"eight\", \"ninth\", \"ninth_tenth_eleventh\", \"ninth+\"))\n\ngrade_levels &lt;- paste(rep(student_grades, times = length(club_grades)), \n                      rep(club_grades, each = length(student_grades)),\n                      sep = \".\" )\ndata.frame(grade_levels)\n\n              grade_levels\n1            10.all_grades\n2            11.all_grades\n3            12.all_grades\n4             8.all_grades\n5             9.all_grades\n6                 10.eight\n7                 11.eight\n8                 12.eight\n9                  8.eight\n10                 9.eight\n11                10.ninth\n12                11.ninth\n13                12.ninth\n14                 8.ninth\n15                 9.ninth\n16 10.ninth_tenth_eleventh\n17 11.ninth_tenth_eleventh\n18 12.ninth_tenth_eleventh\n19  8.ninth_tenth_eleventh\n20  9.ninth_tenth_eleventh\n21               10.ninth+\n22               11.ninth+\n23               12.ninth+\n24                8.ninth+\n25                9.ninth+\n\n\nWe want to include terms for any student-club combination that should not exist; like 12th graders in a ninth grade club, 12.ninth. Based on the order from the summary statistics, this corresponds to: 6 (10.eighth), 7 (11.eighth), 8 (12.eighth), 10 (9.eighth), 11 (10.ninth), 12 (11.ninth), 13 (12.ninth), 14 (8.ninth), 18 (12.ninth_tenth_eleventh), 19 (8.ninth_tenth_eleventh) and 24 (8.ninth+).\n\ngrade_terms &lt;- c(6, 7, 8, 10, 11, 12, 13, 14, 18, 19, 24)\n\nWe an now estimate our model, including nodemix terms for studentgender_clubgender and studentgrade96_clubgrade. We will specify which terms to include using the levels2 argument, setting it to the vectors defined above. To simplify the estimation of the model, we will specify these terms using an Offset() function (although we could have estimated the coefficients within the model). When using Offset(), the coefficients are not estimated and are instead set using the values supplied in the coef argument. This is appropriate in our case as the coefficients are based on logical conditions (e.g., 12th graders do not join 9th grade clubs) and can be set a priori by the researcher. Here, we set the coefficients to -10 for every term. We set the coefficients to -10 as we want to estimate the models conditioned on the fact that these student-club combinations are very rare.\n\noffset_coefs_gender &lt;- rep(-10, length(gender_terms))\noffset_coefs_grade &lt;- rep(-10, length(grade_terms))\n\nmod2 &lt;- ergm(bnet96 ~ edges + \n               Offset(~ nodemix(\"studentgender_clubgender\", \n                              levels2 = gender_terms), \n                      coef = offset_coefs_gender) + \n               Offset(~ nodemix(\"studentgrade96_clubgrade\", \n                              levels2 = grade_terms), \n                      coef = offset_coefs_grade))\nsummary(mod2)\n\nCall:\nergm(formula = bnet96 ~ edges + Offset(~nodemix(\"studentgender_clubgender\", \n    levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(\"studentgrade96_clubgrade\", \n    levels2 = grade_terms), coef = offset_coefs_grade))\n\nMaximum Likelihood Results:\n\n      Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges -2.99179    0.01994      0    -150   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 118331  on 85358  degrees of freedom\n Residual Deviance:  21515  on 85357  degrees of freedom\n \nAIC: 21517  BIC: 21526  (Smaller is better. MC Std. Err. = 0)\n\n\nAll of the offset terms are set to -10, although they are not printed out here. We see that the edge coefficient is different than with mod1, suggesting the importance of adjusting our model for structural/logical constraints. Note also that the model fit should only be compared to other models with the same set of offset terms.\nWe used nodemix terms to capture structural conditions in the data, but we could imagine using nodemix terms to answer more substantive questions. We could test if certain types of students (e.g., girls) are more likely to join certain types of clubs (academic, leadership, etc.), and this is left as an exercise."
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms2.html#second-mode-terms-node-factor-homphily",
    "href": "teaching/sna/material/10/10-ergms2.html#second-mode-terms-node-factor-homphily",
    "title": "Social Network Analysis",
    "section": "",
    "text": "We now add nodefactor and homophily terms to our model. With two-mode networks, these terms can take two forms, with different terms for the first and second mode. Here we focus on the second mode, the clubs.\n\n\nnodefactor terms capture differences in degree across nodes, here clubs, with different attributes. We are interested in the main effect of club type (sports, academic, etc.) and club profile (low, moderate, high, very high) on membership. For example, do high profile clubs have more members than low profile clubs?\nThe term of interest is b2factor (b2 indicating the second mode of a bipartite network). We will include b2factor terms for each club attribute of interest, club_type_detailed and club_profile. We include a levels argument for club_profile to set the order that the results are printed. By default, the results are printed in alphabetical order. In this case, that would correspond to high (1), low (2), moderate (3), very high (4), with high excluded as the reference. But we want the results to run from moderate (3) to high (1) to very high (4), with low (2) excluded (as this is easier to interpret). For club_type_detailed, we use the levels argument to set the second category, academic interest, as the reference. We set levels to -2 to exclude only the second category.\n\nmod3 &lt;- ergm(bnet96 ~ edges + \n               Offset(~nodemix(\"studentgender_clubgender\", \n                              levels2 = gender_terms), \n                      coef = offset_coefs_gender) + \n               Offset(~nodemix(\"studentgrade96_clubgrade\", \n                              levels2 = grade_terms), \n                      coef = offset_coefs_grade) + \n                b2factor(\"club_type_detailed\", levels = -2) + \n                b2factor(\"club_profile\", levels = c(3, 1, 4)))\nsummary(mod3)\n\nCall:\nergm(formula = bnet96 ~ edges + Offset(~nodemix(\"studentgender_clubgender\", \n    levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(\"studentgrade96_clubgrade\", \n    levels2 = grade_terms), coef = offset_coefs_grade) + b2factor(\"club_type_detailed\", \n    levels = -2) + b2factor(\"club_profile\", levels = c(3, 1, \n    4)))\n\nMaximum Likelihood Results:\n\n                                                 Estimate Std. Error MCMC %\nedges                                            -2.82059    0.04035      0\nb2factor.club_type_detailed.Academic Competition -0.44051    0.08242      0\nb2factor.club_type_detailed.Ethnic Interest      -1.03174    0.16677      0\nb2factor.club_type_detailed.Individual Sports    -0.41835    0.08391      0\nb2factor.club_type_detailed.Leadership           -0.36924    0.18887      0\nb2factor.club_type_detailed.Media                -0.16730    0.14109      0\nb2factor.club_type_detailed.Performance Art       0.10725    0.06436      0\nb2factor.club_type_detailed.Service               0.31118    0.06262      0\nb2factor.club_type_detailed.Team Sports           0.45881    0.07798      0\nb2factor.club_profile.moderate                   -0.29070    0.05741      0\nb2factor.club_profile.high                       -0.94788    0.09396      0\nb2factor.club_profile.very_high                  -0.48185    0.11402      0\n                                                 z value Pr(&gt;|z|)    \nedges                                            -69.902   &lt;1e-04 ***\nb2factor.club_type_detailed.Academic Competition  -5.345   &lt;1e-04 ***\nb2factor.club_type_detailed.Ethnic Interest       -6.186   &lt;1e-04 ***\nb2factor.club_type_detailed.Individual Sports     -4.986   &lt;1e-04 ***\nb2factor.club_type_detailed.Leadership            -1.955   0.0506 .  \nb2factor.club_type_detailed.Media                 -1.186   0.2357    \nb2factor.club_type_detailed.Performance Art        1.666   0.0956 .  \nb2factor.club_type_detailed.Service                4.970   &lt;1e-04 ***\nb2factor.club_type_detailed.Team Sports            5.883   &lt;1e-04 ***\nb2factor.club_profile.moderate                    -5.064   &lt;1e-04 ***\nb2factor.club_profile.high                       -10.088   &lt;1e-04 ***\nb2factor.club_profile.very_high                   -4.226   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 118331  on 85358  degrees of freedom\n Residual Deviance:  21116  on 85346  degrees of freedom\n \nAIC: 21140  BIC: 21252  (Smaller is better. MC Std. Err. = 0)\n\n\nWe see, for example, that competitive academic clubs and individual sports tend to have fewer members than academic interest clubs (the reference), while service clubs (like National Honors Society) and team sports tend to be large. We also see that high profile clubs have, if anything, fewer members than low profile clubs.\nMore formally, we can interpret the coefficient on individual sports (for example) as follows: the odds of a student being part of an individual sports team is exp(-0.41835) times lower than the odds of being part of an academic interest club. It is worth emphasizing that b2factor is based only on club attributes, and is thus different from nodemix (see above), which incorporates attributes from both modes.\n\n\n\nHomophily is more complicated with two-mode networks than with one-mode networks. This is the case as there are no direct ties from nodes of the same type; in our case, there are no ties from students to students or from clubs to clubs. So, if we are interested in homophily on a club attribute, say club type, we cannot ask if team sports are tied to other team sports, as there are no ties between clubs. Instead, we must ask if similar clubs are linked together through students; e.g., do students in team sports tend to be members of other team sports?\nWe must counts two-paths, based on homophily on the attribute of interest. The basic idea is to sum up the number of times that we see A-\\(i\\)-B, where A and B are clubs with the same attribute (e.g., both team sports) and \\(i\\) is a student in both A and B. The count is technically over half the number of two-paths, to avoid double counting (as A-\\(i\\)-B is substantively the same as B-\\(i\\)-A). The term is b2nodematch. A positive coefficient on b2nodematch would indicate that students are members of similar kinds of clubs, above what can be explained by other terms in the model.\nBut how do we sum up the homophilous two-paths? In the simplest case, we can sum up all of the two-paths that match on the attribute of interest. This is the default specification. We may, however, have good reason to incorporate some discounting, so that adding one more two-path (for a given edge) only marginally increases the count, once we reach a certain threshold. For example, if student \\(i\\) is a member of the football team, (\\(i\\)-football edge), then if \\(i\\) is also a member of the wrestling team, that would be a strong signal of matching on club type (both team sports). But adding another team sport membership, say \\(i\\) is also a member of the baseball team, may not offer quite as much information; as we already know that \\(i\\) joins team sports. We may then want to count the second two-path less than the first.\nWe can control the discounting using a beta parameter, which raises the count of two-paths (for a given edge) to beta. Setting beta to 1 would yield the number of two-paths (for an edge) where there is a match on the club attribute of interest (so the \\(i\\)-football edge would contribute a count of 2). Setting beta to 0 gives us the other extreme: showing if the given edge is involved in at least one homophilous two-path (so the \\(i\\)-football edge would contribute a count of 1). The count of two-paths, raised to beta, is then summed over all edges and divided by 2.\nWe set beta to .25, but we could imagine using a range of values (estimating the model each time), using model fit statistics to evaluate the choice of beta.\n\nset.seed(1007)\n\nmod4 &lt;- ergm(bnet96 ~ edges + \n                Offset(~nodemix(\"studentgender_clubgender\", \n                              levels2 = gender_terms), \n                      coef = offset_coefs_gender) + \n                Offset(~nodemix(\"studentgrade96_clubgrade\", \n                              levels2 = grade_terms), \n                      coef = offset_coefs_grade) + \n                b2factor(\"club_type_detailed\", levels = -2) + \n                b2nodematch(\"club_type_detailed\", beta = .25) + \n                b2factor(\"club_profile\", levels = c(3, 1, 4)) + \n                b2nodematch(\"club_profile\", beta = .25), \n              control = control.ergm(MCMC.burnin = 20000,\n                                     MCMC.samplesize = 3000))\n\nThe model is now being fit using MCMC techniques. This means that we should check if the model is fitting well. Use the mcmc.diagnostics() function.\nQ2 How would you intepret the below results (not plotted here)?\n\nmcmc.diagnostics(mod4)\n\nGo ahead a summarize model 4:\n\nsummary(mod4)\n\nCall:\nergm(formula = bnet96 ~ edges + Offset(~nodemix(\"studentgender_clubgender\", \n    levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(\"studentgrade96_clubgrade\", \n    levels2 = grade_terms), coef = offset_coefs_grade) + b2factor(\"club_type_detailed\", \n    levels = -2) + b2nodematch(\"club_type_detailed\", beta = 0.25) + \n    b2factor(\"club_profile\", levels = c(3, 1, 4)) + b2nodematch(\"club_profile\", \n    beta = 0.25), control = control.ergm(MCMC.burnin = 20000, \n    MCMC.samplesize = 3000))\n\nMonte Carlo Maximum Likelihood Results:\n\n                                                  Estimate Std. Error MCMC %\nedges                                            -3.596083   0.075080      0\nb2factor.club_type_detailed.Academic Competition -0.323232   0.085331      0\nb2factor.club_type_detailed.Ethnic Interest      -0.844453   0.169750      0\nb2factor.club_type_detailed.Individual Sports    -0.296443   0.083610      0\nb2factor.club_type_detailed.Leadership           -0.171296   0.193888      0\nb2factor.club_type_detailed.Media                 0.028381   0.138718      0\nb2factor.club_type_detailed.Performance Art       0.129749   0.059862      0\nb2factor.club_type_detailed.Service               0.361836   0.059245      0\nb2factor.club_type_detailed.Team Sports           0.502725   0.075382      0\nb2nodematch.club_type_detailed                    0.413830   0.070045      0\nb2factor.club_profile.moderate                   -0.075008   0.059536      0\nb2factor.club_profile.high                       -0.538820   0.098625      0\nb2factor.club_profile.very_high                  -0.003789   0.124166      0\nb2nodematch.club_profile                          0.779436   0.088837      0\n                                                 z value Pr(&gt;|z|)    \nedges                                            -47.897  &lt; 1e-04 ***\nb2factor.club_type_detailed.Academic Competition  -3.788 0.000152 ***\nb2factor.club_type_detailed.Ethnic Interest       -4.975  &lt; 1e-04 ***\nb2factor.club_type_detailed.Individual Sports     -3.546 0.000392 ***\nb2factor.club_type_detailed.Leadership            -0.883 0.376978    \nb2factor.club_type_detailed.Media                  0.205 0.837889    \nb2factor.club_type_detailed.Performance Art        2.167 0.030198 *  \nb2factor.club_type_detailed.Service                6.107  &lt; 1e-04 ***\nb2factor.club_type_detailed.Team Sports            6.669  &lt; 1e-04 ***\nb2nodematch.club_type_detailed                     5.908  &lt; 1e-04 ***\nb2factor.club_profile.moderate                    -1.260 0.207713    \nb2factor.club_profile.high                        -5.463  &lt; 1e-04 ***\nb2factor.club_profile.very_high                   -0.031 0.975654    \nb2nodematch.club_profile                           8.774  &lt; 1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 118331  on 85358  degrees of freedom\n Residual Deviance:  20965  on 85344  degrees of freedom\n \nAIC: 20993  BIC: 21124  (Smaller is better. MC Std. Err. = 0.9958)"
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms2.html#first-mode-terms-node-factor-homphily",
    "href": "teaching/sna/material/10/10-ergms2.html#first-mode-terms-node-factor-homphily",
    "title": "Social Network Analysis",
    "section": "",
    "text": "We now add the analogous terms (nodefactor and nodematch) for the first mode, students. Here we focus on student attributes, specifically for race. We will add b1factor and b1nodematch terms to the model (b1 indicating the first mode of a bipartite network). For b1factor, we test if there are differences in degree by racial groups (do white students join clubs at lower rates than Asian students?).\nFor b1nodematch, we test if clubs are segregated along racial lines. We will count the number of two paths, \\(i\\)-A-\\(j\\), where \\(i\\) and \\(j\\) are students of the same race and A is a club that both \\(i\\) and \\(j\\) are members of. Again, we can use the beta argument to set the discounting when summing up the two-paths that match on the attribute of interest. We will set beta to .15.\nTo help with estimation convergence, we will also set the reference category for the b1factor term to include Asian (1), Hispanic (3) and Native American (4) (basically collapsing some of the smaller categories into a single ‘other’ category). Finally, we will tweak the control parameters, increasing the burnin and `sample size. This can take a little while to run, so we might want to include parallel processing options to speed things up (here we set the number of processors to 4).\n\nmod5 &lt;- ergm(bnet96 ~ edges + \n               Offset(~nodemix(\"studentgender_clubgender\", \n                              levels2 = gender_terms), \n                      coef = offset_coefs_gender) + \n                Offset(~nodemix(\"studentgrade96_clubgrade\", \n                              levels2 = grade_terms), \n                      coef = offset_coefs_grade) + \n               b2factor(\"club_type_detailed\", levels = -2) + \n               b2nodematch(\"club_type_detailed\", beta = .25) + \n               b2factor(\"club_profile\", levels = c(3, 1, 4)) + \n               b2nodematch(\"club_profile\", beta = .25) +\n               b1factor(\"race\", levels = -c(1, 3, 4)) + \n               b1nodematch(\"race\", beta = .15), \n             control = control.ergm(MCMC.burnin = 30000, \n                                    MCMC.samplesize = 5000, \n                                    parallel = 4, \n                                    parallel.type = \"PSOCK\"))\nsummary(mod5)\n\nCall:\nergm(formula = bnet96 ~ edges + Offset(~nodemix(\"studentgender_clubgender\", \n    levels2 = gender_terms), coef = offset_coefs_gender) + Offset(~nodemix(\"studentgrade96_clubgrade\", \n    levels2 = grade_terms), coef = offset_coefs_grade) + b2factor(\"club_type_detailed\", \n    levels = -2) + b2nodematch(\"club_type_detailed\", beta = 0.25) + \n    b2factor(\"club_profile\", levels = c(3, 1, 4)) + b2nodematch(\"club_profile\", \n    beta = 0.25) + b1factor(\"race\", levels = -c(1, 3, 4)) + b1nodematch(\"race\", \n    beta = 0.15), control = control.ergm(MCMC.burnin = 30000, \n    MCMC.samplesize = 5000, parallel = 4, parallel.type = \"PSOCK\"))\n\nMonte Carlo Maximum Likelihood Results:\n\n                                                 Estimate Std. Error MCMC %\nedges                                            -6.83027    0.14712      0\nb2factor.club_type_detailed.Academic Competition -0.05914    0.05116      0\nb2factor.club_type_detailed.Ethnic Interest      -0.24224    0.11227      0\nb2factor.club_type_detailed.Individual Sports     0.39357    0.05757      0\nb2factor.club_type_detailed.Leadership            0.10532    0.13252      0\nb2factor.club_type_detailed.Media                 0.15606    0.09393      0\nb2factor.club_type_detailed.Performance Art       0.17197    0.03435      0\nb2factor.club_type_detailed.Service               0.14154    0.03296      0\nb2factor.club_type_detailed.Team Sports           0.95282    0.05173      0\nb2nodematch.club_type_detailed                    0.33851    0.06648      0\nb2factor.club_profile.moderate                    0.07439    0.04003      0\nb2factor.club_profile.high                       -0.10308    0.07348      0\nb2factor.club_profile.very_high                   0.16866    0.09232      0\nb2nodematch.club_profile                          0.69575    0.08628      0\nb1factor.race.black                              -1.11809    0.05122      0\nb1factor.race.white                              -1.51223    0.06228      0\nb1nodematch.race                                  5.04005    0.18534      0\n                                                 z value Pr(&gt;|z|)    \nedges                                            -46.426   &lt;1e-04 ***\nb2factor.club_type_detailed.Academic Competition  -1.156   0.2477    \nb2factor.club_type_detailed.Ethnic Interest       -2.158   0.0310 *  \nb2factor.club_type_detailed.Individual Sports      6.836   &lt;1e-04 ***\nb2factor.club_type_detailed.Leadership             0.795   0.4268    \nb2factor.club_type_detailed.Media                  1.661   0.0966 .  \nb2factor.club_type_detailed.Performance Art        5.006   &lt;1e-04 ***\nb2factor.club_type_detailed.Service                4.294   &lt;1e-04 ***\nb2factor.club_type_detailed.Team Sports           18.420   &lt;1e-04 ***\nb2nodematch.club_type_detailed                     5.092   &lt;1e-04 ***\nb2factor.club_profile.moderate                     1.858   0.0632 .  \nb2factor.club_profile.high                        -1.403   0.1607    \nb2factor.club_profile.very_high                    1.827   0.0677 .  \nb2nodematch.club_profile                           8.064   &lt;1e-04 ***\nb1factor.race.black                              -21.830   &lt;1e-04 ***\nb1factor.race.white                              -24.281   &lt;1e-04 ***\nb1nodematch.race                                  27.194   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 118331  on 85358  degrees of freedom\n Residual Deviance:  20437  on 85341  degrees of freedom\n \nAIC: 20471  BIC: 20630  (Smaller is better. MC Std. Err. = 1.304)\n\n\nInterpretation: It looks like black and white students are members of fewer clubs than Asian, Native American or Hispanic students (the reference), while there is segregation along racial lines (looking at the b1nodematch term). Students are unlikely to find themselves in clubs where there are few (or even no) students of the same race. For example, by chance we might expect Asian students (a small racial group) to often be in clubs with few other Asian students, but empirically this rarely happens. Looking at AIC and BIC, we see that the model fit is improved quite a bit from the previous model."
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms2.html#exercise",
    "href": "teaching/sna/material/10/10-ergms2.html#exercise",
    "title": "Social Network Analysis",
    "section": "",
    "text": "Download the same darta for 1997 and perform a similar analysis, i.e. fitting ERGMs with the same first and second mode terms. Can you note differences between the two years?\n\n\nbnet97 &lt;- readRDS(\"bnet97.rds\")\n\n\nPerform MCMC and GoF assessment on the models fitted for both years."
  },
  {
    "objectID": "teaching/sna/material/05/05-ego-nets.html",
    "href": "teaching/sna/material/05/05-ego-nets.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "library(egor)\nlibrary(igraph)\nlibrary(tidyverse)\nlibrary(patchwork)\n\nIn this practical we will\n\nbegin by covering the basics of ego network data, utilizing the egor package to manipulate, construct and visualize ego networks\n\nfocus on analyzing substantive questions related to homophily, the tendency for similar actors to interact at higher rates than dissimilar actors, on different demographic dimensions.\nconsider an example where the ego network properties are used to predict other outcomes of interest, like happiness\n\n\n\nThe example ego network data come from the 1985 General Social Survey (GSS), a national survey of American adults done face-to-face. The aim of the surveys are to\n\ntrack changes in social attitudes, behaviors, and attributes over time\nmeasure opinions on a wide range of social issues like race, religion, politics, crime, work, and family.\nprovide data for social science research\n\nWe work with ego network data from the GSS that has been preprocessed into three different files:\n\na file with the ego attributes\na file with the alter attributes\na file with the alter-alter ties (1 means that the alters know each other, 2 means they are especially close)\n\nLet’s go ahead and read in the three files, starting with the ego attribute data.\nRead in the three data file:\n\nego_dat &lt;- read.csv(file = \"data/gss1985_ego_dat.csv\" , stringsAsFactors = F) \nalter_dat &lt;- read.csv(file = \"data/gss1985_alter_dat.csv\", stringsAsFactors = F)\nalteralter_dat &lt;- read.csv(file = \"data/gss1985_alteralter_dat.csv\")\n\n\n\nExplore the three different data sets. Do you understand the structure and content of each one? Can you see similarities and differences between the data sets? Focus specifically on these columns:\n\n c(\"CASEID\", \"AGE\", \"EDUC\", \"RACE\", \"SEX\", \"HAPPY\", \"NUMGIVEN\")\n\n\n\nShow solution\nnrow(ego_dat)\nnrow(alter_dat)\nego_dat[1:10, c(\"CASEID\", \"AGE\", \"EDUC\", \"RACE\", \"SEX\", \"HAPPY\", \"NUMGIVEN\")]\n\n# CASEID, is the unique id for each respondent\n# for example: we can see that respondent 1 (CASEID = 19850001) names 5 alters. \n# The first alter (ALTERID = 1) is 32, has 18 years of education, and is not kin to ego. \n# NUMGIVEN is the number of alters given, there are NA's here that need to be removed\n# Note that the number of rows in the two data frames is not the same\n\nalter_dat[1:10, c(\"CASEID\", \"ALTERID\", \"AGE\", \"EDUC\", \"RACE\", \"SEX\", \"KIN\")] \n# Each row corresponds to a different named alter. \n# Each alter is denoted by an alter id (ALTERID), unique to that respondent (based on CASEID). \n# We see similar attributes as with the ego data.\n# There is also information on the relationship between ego and each alter. \n\nalteralter_dat[1:10, ]\n# this data frame captures the ties between the named alters (as reported on by the respondent)\n# We see four columns. The first column defines the relevant ego using CASEID. \n# ALTER1 defines the first alter in the dyad and ALTER2 defines the second. \n\n\nAs noted above, there are missing values above that need to be removed:\n\nego_dat &lt;- ego_dat[!is.na(ego_dat$NUMGIVEN), ]\n\n\n\n\n\nFirst challenge in analyzing ego network data is that we must transform traditional survey data into something that has the structure of a network, so that we can then utilize packages like igraph and sna. Our survey data will not look like traditional network inputs (matrices, edgelists, etc.) and each survey is likely to be different, complicating the task of putting together the ego networks.\nLuckily, the egor package has made the task of constructing ego networks from survey data much easier. We will utilize the basic functionality of this package throughout the tutorial.\nThe egor function assumes that you are inputting the data using three separate files. The main arguments are:\n\nalters = alter attributes data frame\negos = ego attributes data frame\naaties = alter-alter tie data frame\nalter_design = list of arguments to specify nomination information from survey\nego_design = list of arguments to specify survey design of study\nID.vars = list of variable names corresponding to key columns:\n\nego = variable name for id of ego\nalter = variable name for id of alter (in alter data)\nsource = variable name for ‘sender’ of tie in alter-alter data\ntarget = variable name for ‘receiver’ of tie in alter-alter data\n\n\nWe will use the three data frames read in above as the main inputs. We will also tell R that CASEID is the ego id variable and ALTERID is the id variable for alters, while ALTER1 and ALTER2 are the source/target variables in the alter-alter data frame. We also note that the maximum number of alters was set to 5:\n\negonetlist &lt;-  egor(alters = alter_dat, egos = ego_dat, \n                    aaties = alteralter_dat, alter_design = list(max = 5), \n                    ID.vars = list(ego = \"CASEID\", alter =\"ALTERID\", \n                                   source = \"ALTER1\", target = \"ALTER2\")) \negonetlist\n\n# EGO data (active): 1,531 × 13\n  .egoID   AGE  EDUC RACE  SEX   RELIG AGE_CATEGORICAL EDUC_CATEGORICAL NUMGIVEN\n*  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt;               &lt;int&gt;\n1 1.99e7    33    16 white male  jewi… 30s             College                 6\n2 1.99e7    49    19 white male  cath… 40s             Post Graduate           6\n3 1.99e7    23    16 white fema… jewi… 20s             College                 5\n4 1.99e7    26    20 white fema… jewi… 20s             Post Graduate           5\n5 1.99e7    24    17 white fema… cath… 20s             Post Graduate           5\n# ℹ 1,526 more rows\n# ℹ 4 more variables: HAPPY &lt;int&gt;, HEALTH &lt;int&gt;, PARTYID &lt;int&gt;, WTSSALL &lt;dbl&gt;\n# ALTER data: 4,483 × 12\n  .altID   .egoID   AGE  EDUC RACE  SEX   RELIG AGE_CATEGORICAL EDUC_CATEGORICAL\n*  &lt;int&gt;    &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt;           \n1      1 19850001    32    18 white male  jewi… 30s             Post Graduate   \n2      2 19850001    29    16 white fema… prot… 20s             College         \n3      3 19850001    32    18 white male  jewi… 30s             Post Graduate   \n# ℹ 4,480 more rows\n# ℹ 3 more variables: TALKTO &lt;int&gt;, SPOUSE &lt;int&gt;, KIN &lt;int&gt;\n# AATIE data: 4,880 × 4\n    .egoID .srcID .tgtID WEIGHT\n*    &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n1 19850001      1      2      2\n2 19850001      1      3      1\n3 19850001      1      4      1\n# ℹ 4,877 more rows\n\n\negor objects are constructed as tibbles, which are data frames built using the tidyverse logic. For those versed in the tidyverse, one can take advantage of all the functions, calls, etc. that go along with those kinds of objects. It is, however, not strictly necessary to know the syntax of the tidyverse to work with these objects. Let’s take a look at the egor object.\n\nnames(egonetlist) \n\n[1] \"ego\"   \"alter\" \"aatie\"\n\n\nWe see that the elements are made up of our three data frames. For example, we take the first five columns of the ego data and alter attributes, extracted from the egor object:\n\negonetlist[[\"ego\"]][, 1:5]\n\n# A tibble: 1,531 × 5\n     .egoID   AGE  EDUC RACE  SEX   \n      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n 1 19850001    33    16 white male  \n 2 19850002    49    19 white male  \n 3 19850003    23    16 white female\n 4 19850004    26    20 white female\n 5 19850005    24    17 white female\n 6 19850006    45    17 white male  \n 7 19850007    44    18 white female\n 8 19850008    56    12 white female\n 9 19850009    85     7 white female\n10 19850010    65    12 white female\n# ℹ 1,521 more rows\n\negonetlist[[\"alter\"]][, 1:5]\n\n# A tibble: 4,483 × 5\n   .altID   .egoID   AGE  EDUC RACE \n    &lt;int&gt;    &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;\n 1      1 19850001    32    18 white\n 2      2 19850001    29    16 white\n 3      3 19850001    32    18 white\n 4      4 19850001    35    16 white\n 5      5 19850001    29    13 white\n 6      1 19850002    42    12 white\n 7      2 19850002    44    18 white\n 8      3 19850002    45    16 white\n 9      4 19850002    40    12 white\n10      5 19850002    50    18 white\n# ℹ 4,473 more rows\n\n\nNote that the id variable for ego has been renamed to .egoID (it was CASEID on the original data). We also see see that the alter id has been renamed .altID (from ALTERID on the original data).\nAnd now we look at the alter-alter ties where we can see that the column names for the alter-alter ties have also been renamed from the input data. The variables are now .srcID and .tgtID (rather than ALTER1 and ALTER2, as on the original data).\n\negonetlist[[\"aatie\"]]\n\n# A tibble: 4,880 × 4\n     .egoID .srcID .tgtID WEIGHT\n      &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n 1 19850001      1      2      2\n 2 19850001      1      3      1\n 3 19850001      1      4      1\n 4 19850001      1      5      1\n 5 19850001      2      3      2\n 6 19850001      2      4      2\n 7 19850001      2      5      2\n 8 19850001      3      4      1\n 9 19850001      3      5      1\n10 19850001      4      5      1\n# ℹ 4,870 more rows\n\n\n\n\nCalculate density on the egor object using the function ego_density(). Note that all ego networks of size 0 or 1 will have NAs for density).\n\n\nShow solution\ndens &lt;- ego_density(egonetlist)\nhead(dens)\n\n# For example, respondent 1 (19850001) has 5 alters and all 10 possible ties exist \n# (density = 1), \n# while respondent 2 (1950002) has 5 alters but only 8 ties exist (density = .8). \n# To check this you can run the following:\nalteralter_dat[alteralter_dat$CASEID == 19850001, ]\nalteralter_dat[alteralter_dat$CASEID == 19850002, ]\n\n\n\n\n\n\nWe not plot the networks using igraph. First step is to convert the information in the egor object to igraph objects. We do this using the as_igraph() function. Let’s take a look at the first three ego networks.\n\nego_nets &lt;- as_igraph(egonetlist)\nego_nets[1:3] \n\n$`19850001`\nIGRAPH 5f36126 UN-- 5 10 -- \n+ attr: .egoID (g/n), name (v/c), AGE (v/n), EDUC (v/n), RACE (v/c),\n| SEX (v/c), RELIG (v/c), AGE_CATEGORICAL (v/c), EDUC_CATEGORICAL\n| (v/c), TALKTO (v/n), SPOUSE (v/n), KIN (v/n), WEIGHT (e/n)\n+ edges from 5f36126 (vertex names):\n [1] 1--2 1--3 1--4 1--5 2--3 2--4 2--5 3--4 3--5 4--5\n\n$`19850002`\nIGRAPH 15ba0c9 UN-- 5 8 -- \n+ attr: .egoID (g/n), name (v/c), AGE (v/n), EDUC (v/n), RACE (v/c),\n| SEX (v/c), RELIG (v/c), AGE_CATEGORICAL (v/c), EDUC_CATEGORICAL\n| (v/c), TALKTO (v/n), SPOUSE (v/n), KIN (v/n), WEIGHT (e/n)\n+ edges from 15ba0c9 (vertex names):\n[1] 1--2 1--3 1--4 1--5 2--4 3--4 3--5 4--5\n\n$`19850003`\nIGRAPH d2d7e9e UN-- 5 6 -- \n+ attr: .egoID (g/n), name (v/c), AGE (v/n), EDUC (v/n), RACE (v/c),\n| SEX (v/c), RELIG (v/c), AGE_CATEGORICAL (v/c), EDUC_CATEGORICAL\n| (v/c), TALKTO (v/n), SPOUSE (v/n), KIN (v/n), WEIGHT (e/n)\n+ edges from d2d7e9e (vertex names):\n[1] 1--2 1--3 1--4 2--3 2--4 3--4\n\n\nNote that we would use as_network() function if we wanted to construct networks in the network format.\nWe have a list of ego networks (in the igraph format), with each ego network in a different element in the list. We can see that the information on the alters was automatically passed to the igraph objects, as was the information on the weights for the alter-alter ties. Note that by default the igraph objects will not include ego. Ego is often (but not always) excluded from visualizations and calculations because ego is, by definition, tied to all alters. Including ego thus offers little additional structural information. We will consider measures below that incorporate both ego and alter information.\nAs with all igraph objects, we can extract useful information, like the attributes of the nodes. As an example, let’s extract information on sex of alters from the first ego network.\n\nvertex_attr(ego_nets[[1]], \"SEX\")\n\n[1] \"male\"   \"female\" \"male\"   \"male\"   \"female\"\n\n# Note that this is the same information as:\n# alter_dat[alter_dat$CASEID == 19850001, \"SEX\"]\n\nNow let’s plot a few networks focusing on the first 6 ego networks:\n\n# lapply() will perform a given function, here plot(), \n# over every element of an input list, here the first 6 elements of ego_nets.\n# note that you also can use purrr::walk()\npar(mfrow = c(2, 3))\nlapply(ego_nets[1:6], plot)\n\n\n\n\n\n\n\n\n\n\n\nNow that we have a list of networks, we can apply the same function to each network using a single line of code, again with the help of lapply().\n\n\nFind the mean number of nodes and ties using vcount() and ecount() function for all networks. Try reproducing the plots shown below. Hint: You can use lapply() again here.\n\n\n\n\n\n\n\n\n\n\n\nShow solution\n# nodes\nnetwork_sizes &lt;- lapply(ego_nets, vcount)\nnetwork_sizes &lt;- unlist(network_sizes)\nmean(network_sizes, na.rm = T)\n\n# ties\nnetwork_edge_counts &lt;- lapply(ego_nets, ecount)\nnetwork_edge_counts &lt;- unlist(network_edge_counts)\nmean(network_edge_counts, na.rm = T)\n\n# Create data frames\ndf1 &lt;- data.frame(NetworkSize = network_sizes)\ndf2 &lt;- data.frame(EdgeCount = network_edge_counts)\n\n# Create histograms (ggplot approach)\np1 &lt;- ggplot(df1, aes(x = NetworkSize)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"skyblue\") +\n  ggtitle(\"Histogram of Network Sizes\") +\n  xlab(\"# of Nodes\") +\n  theme_minimal()\n\np2 &lt;- ggplot(df2, aes(x = EdgeCount)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"tan2\") +\n  ggtitle(\"Histogram of Edge Counts\") +\n  xlab(\"# of Edges\") +\n  theme_minimal()\n\np1 + p2 # needs patchwork\n\n\n\n\n\nUse edge_density and apply to every ego network in the data to find the density of all networks. Create a histogram of these densities as in exercise 3.\n\n\nShow solution\ndensities &lt;- lapply(ego_nets, edge_density)\ndensities &lt;- unlist(densities)\nhist(densities)\n\n\n\n\n\nThere are also inbuilt functions in the egor package that can be used to analyze the ego networks, for example ego_density, composition, alts_diversity_count, count_dyads, and comp_ei. Explore these functions by yourself. Note that you then need to work with the egor object and not the igraph object.\n\n\n\n\nThe EI index (External-Internal index) in ego network compositional analysis is a measure of homophily (similarity) or heterophily (difference) in ego networks. It tells you whether ego’s alters are more similar or dissimilar to the ego based on a categorical attribute (e.g., gender, ethnicity, etc.). The formula is given as: \\[EI = \\frac{E - I}{E + I}\\] where:\n\nE = Number of external ties (alters with a different attribute value than ego)\nI = Number of internal ties (alters with the same attribute value as ego)\n\nThe measure is interpreted as follows:\n\n+1: all alters are different from ego (maximum heterophily)\n0: equal number of similar and different alters (neutral)\n–1: all alters are the same as ego (maximum homophily)\n\nLet’s start by looking at the level of homophily based on the attribute SEX:\n\ncomp_ei(egonetlist, alt.attr = \"SEX\", ego.attr = \"SEX\")\n\n# A tibble: 1,531 × 2\n     .egoID    ei\n      &lt;int&gt; &lt;dbl&gt;\n 1 19850001  -0.2\n 2 19850002  -0.2\n 3 19850003  -1  \n 4 19850004   0.2\n 5 19850005   0.2\n 6 19850006  -0.5\n 7 19850007  -0.6\n 8 19850008   0.2\n 9 19850009   0  \n10 19850010  -1  \n# ℹ 1,521 more rows\n\n\nFor each ego, you know have an index that tells you the level of homophily based on the attribute “SEX”. For example, the first ego 19850001 tends to have slightly more homophilious ties compared to heterophilous ties. To get a better idea of this measure over all networks you can compute the proportion of negative/positive values or condition on (group by) another attribute. For example:\n\ncomp_ei(egonetlist, alt.attr = \"SEX\", ego.attr = \"SEX\") %&gt;% count(ei &lt; 0)\n\n# A tibble: 3 × 2\n  `ei &lt; 0`     n\n  &lt;lgl&gt;    &lt;int&gt;\n1 FALSE      599\n2 TRUE       795\n3 NA         137\n\n\nNote that the NA’s are cause because of some missing values in the alter data set.\n\n\nDo the same thing, but only consider ties that are based on variable RACE. What can you conclude?\n\n\nShow solution\ncomp_ei(egonetlist, alt.attr = \"RACE\", ego.attr = \"RACE\")\n\n\nHomphily based on SEX and RACE offers very different stories. We see that race is a much more salient dimension than gender, with many respondents matching perfectly with all members of their network along racial lines, but much less so with gender, where differences between ego and alter are more common.\n\n\n\n\nAbove we examined the properties of the ego networks, focusing mostly on racial and gender homophily. There are a number of other properties we could explore in more detail, like density or network size. For example, we might want to predict network size as a function of race, gender or other demographic characteristics.\nWe can also use properties of the ego networks as predictors of other outcomes of interest. For example, let’s try and predict the variable HAPPY using the features of the ego networks. Are individuals with larger ego networks happier?\nHAPPY is coded as a 1 (very happy), 2 (pretty happy), 3 (not too happy). Let’s add a label to the variable and reorder it to run from not happy to happy:\n\nego_dat$HAPPY_FACTOR &lt;- factor(ego_dat$HAPPY, levels = c(3, 2, 1), \n                            labels = c(\"not too happy\", \"pretty happy\", \n                                     \"very happy\"))\n\nWe also turn our race and sex variables into factors. We set white as the first category in our race variable.\n\nego_dat$RACE_FACTOR &lt;- factor(ego_dat$RACE, levels = c(\"white\", \"asian\", \n                                                       \"black\", \"hispanic\", \n                                                       \"other\")) \nego_dat$SEX_FACTOR &lt;- factor(ego_dat$SEX)\n\nLet’s also save density\n\ndens &lt;- ego_density(egonetlist)\nego_dat$DENSITY &lt;- dens[[\"density\"]] #  getting values out of tibble format\n\nHAPPY is an ordinal variable. With ordinal outcome variables, it is best to utilize ordered logistic regression (or a similar model). We will need the polr() function in the MASS package to run these models.\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\ndens &lt;- ego_density(egonetlist)\nego_dat$DENSITY &lt;- dens[[\"density\"]] #  getting values out of tibble format\n\nLet’s create a data frame that has no missing data on any of the variables we want to include in the full model. The outcome of interest is HAPPY_FACTOR and the main predictors are ego network size (NUMGIVEN) and density (DENSITY) . We also include a number of demographic controls:\n\nego_dat_nomiss &lt;- na.omit(ego_dat[, c(\"HAPPY_FACTOR\", \"NUMGIVEN\", \"DENSITY\", \n                                     \"EDUC\", \"AGE\", \"RACE_FACTOR\", \n                                     \"SEX_FACTOR\")])\n\nNow we run the ordered logistic regression predicting happiness. For our first model we will predict happiness as a function of our two structural network features, ego network size and density.\n\nsummary(happy_mod1 &lt;- polr(HAPPY_FACTOR ~ NUMGIVEN + DENSITY, \n                           data = ego_dat_nomiss)) \n\n\nRe-fitting to get Hessian\n\n\nCall:\npolr(formula = HAPPY_FACTOR ~ NUMGIVEN + DENSITY, data = ego_dat_nomiss)\n\nCoefficients:\n           Value Std. Error t value\nNUMGIVEN 0.08399    0.04753   1.767\nDENSITY  0.47660    0.20388   2.338\n\nIntercepts:\n                           Value   Std. Error t value\nnot too happy|pretty happy -1.4375  0.2700    -5.3240\npretty happy|very happy     1.5709  0.2697     5.8236\n\nResidual Deviance: 2105.42 \nAIC: 2113.42 \n\n\nThe results suggest that respondents with dense networks report higher levels of happiness, while ego network size (NUMGIVEN) is not a significant predictor of happiness, controlling for density. The initial results would suggest that it is less about the number of people in your ego network that matters for happiness, and more about whether they know each other.\n\n\nAdd the control variables into the model and interpret the results.\n\n\nShow solution\nsummary(happy_mod2 &lt;- polr(HAPPY_FACTOR ~ NUMGIVEN + DENSITY + EDUC + AGE + \n                             RACE_FACTOR + SEX_FACTOR, \n                           data = ego_dat_nomiss))\n\n\nTo summarize the final model fit: density is still a significant predictor of happiness. Individuals with alters who are interconnected consistently report higher levels of happiness, showing the potential benefits of being part of an integrated social group. We also see that individuals with higher education tend to report higher levels of happiness, while those individuals identifying as black report lower levels of happiness."
  },
  {
    "objectID": "teaching/sna/material/05/05-ego-nets.html#data",
    "href": "teaching/sna/material/05/05-ego-nets.html#data",
    "title": "Social Network Analysis",
    "section": "",
    "text": "The example ego network data come from the 1985 General Social Survey (GSS), a national survey of American adults done face-to-face. The aim of the surveys are to\n\ntrack changes in social attitudes, behaviors, and attributes over time\nmeasure opinions on a wide range of social issues like race, religion, politics, crime, work, and family.\nprovide data for social science research\n\nWe work with ego network data from the GSS that has been preprocessed into three different files:\n\na file with the ego attributes\na file with the alter attributes\na file with the alter-alter ties (1 means that the alters know each other, 2 means they are especially close)\n\nLet’s go ahead and read in the three files, starting with the ego attribute data.\nRead in the three data file:\n\nego_dat &lt;- read.csv(file = \"data/gss1985_ego_dat.csv\" , stringsAsFactors = F) \nalter_dat &lt;- read.csv(file = \"data/gss1985_alter_dat.csv\", stringsAsFactors = F)\nalteralter_dat &lt;- read.csv(file = \"data/gss1985_alteralter_dat.csv\")\n\n\n\nExplore the three different data sets. Do you understand the structure and content of each one? Can you see similarities and differences between the data sets? Focus specifically on these columns:\n\n c(\"CASEID\", \"AGE\", \"EDUC\", \"RACE\", \"SEX\", \"HAPPY\", \"NUMGIVEN\")\n\n\n\nShow solution\nnrow(ego_dat)\nnrow(alter_dat)\nego_dat[1:10, c(\"CASEID\", \"AGE\", \"EDUC\", \"RACE\", \"SEX\", \"HAPPY\", \"NUMGIVEN\")]\n\n# CASEID, is the unique id for each respondent\n# for example: we can see that respondent 1 (CASEID = 19850001) names 5 alters. \n# The first alter (ALTERID = 1) is 32, has 18 years of education, and is not kin to ego. \n# NUMGIVEN is the number of alters given, there are NA's here that need to be removed\n# Note that the number of rows in the two data frames is not the same\n\nalter_dat[1:10, c(\"CASEID\", \"ALTERID\", \"AGE\", \"EDUC\", \"RACE\", \"SEX\", \"KIN\")] \n# Each row corresponds to a different named alter. \n# Each alter is denoted by an alter id (ALTERID), unique to that respondent (based on CASEID). \n# We see similar attributes as with the ego data.\n# There is also information on the relationship between ego and each alter. \n\nalteralter_dat[1:10, ]\n# this data frame captures the ties between the named alters (as reported on by the respondent)\n# We see four columns. The first column defines the relevant ego using CASEID. \n# ALTER1 defines the first alter in the dyad and ALTER2 defines the second. \n\n\nAs noted above, there are missing values above that need to be removed:\n\nego_dat &lt;- ego_dat[!is.na(ego_dat$NUMGIVEN), ]"
  },
  {
    "objectID": "teaching/sna/material/05/05-ego-nets.html#creating-the-network-using-egor",
    "href": "teaching/sna/material/05/05-ego-nets.html#creating-the-network-using-egor",
    "title": "Social Network Analysis",
    "section": "",
    "text": "First challenge in analyzing ego network data is that we must transform traditional survey data into something that has the structure of a network, so that we can then utilize packages like igraph and sna. Our survey data will not look like traditional network inputs (matrices, edgelists, etc.) and each survey is likely to be different, complicating the task of putting together the ego networks.\nLuckily, the egor package has made the task of constructing ego networks from survey data much easier. We will utilize the basic functionality of this package throughout the tutorial.\nThe egor function assumes that you are inputting the data using three separate files. The main arguments are:\n\nalters = alter attributes data frame\negos = ego attributes data frame\naaties = alter-alter tie data frame\nalter_design = list of arguments to specify nomination information from survey\nego_design = list of arguments to specify survey design of study\nID.vars = list of variable names corresponding to key columns:\n\nego = variable name for id of ego\nalter = variable name for id of alter (in alter data)\nsource = variable name for ‘sender’ of tie in alter-alter data\ntarget = variable name for ‘receiver’ of tie in alter-alter data\n\n\nWe will use the three data frames read in above as the main inputs. We will also tell R that CASEID is the ego id variable and ALTERID is the id variable for alters, while ALTER1 and ALTER2 are the source/target variables in the alter-alter data frame. We also note that the maximum number of alters was set to 5:\n\negonetlist &lt;-  egor(alters = alter_dat, egos = ego_dat, \n                    aaties = alteralter_dat, alter_design = list(max = 5), \n                    ID.vars = list(ego = \"CASEID\", alter =\"ALTERID\", \n                                   source = \"ALTER1\", target = \"ALTER2\")) \negonetlist\n\n# EGO data (active): 1,531 × 13\n  .egoID   AGE  EDUC RACE  SEX   RELIG AGE_CATEGORICAL EDUC_CATEGORICAL NUMGIVEN\n*  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt;               &lt;int&gt;\n1 1.99e7    33    16 white male  jewi… 30s             College                 6\n2 1.99e7    49    19 white male  cath… 40s             Post Graduate           6\n3 1.99e7    23    16 white fema… jewi… 20s             College                 5\n4 1.99e7    26    20 white fema… jewi… 20s             Post Graduate           5\n5 1.99e7    24    17 white fema… cath… 20s             Post Graduate           5\n# ℹ 1,526 more rows\n# ℹ 4 more variables: HAPPY &lt;int&gt;, HEALTH &lt;int&gt;, PARTYID &lt;int&gt;, WTSSALL &lt;dbl&gt;\n# ALTER data: 4,483 × 12\n  .altID   .egoID   AGE  EDUC RACE  SEX   RELIG AGE_CATEGORICAL EDUC_CATEGORICAL\n*  &lt;int&gt;    &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt;           \n1      1 19850001    32    18 white male  jewi… 30s             Post Graduate   \n2      2 19850001    29    16 white fema… prot… 20s             College         \n3      3 19850001    32    18 white male  jewi… 30s             Post Graduate   \n# ℹ 4,480 more rows\n# ℹ 3 more variables: TALKTO &lt;int&gt;, SPOUSE &lt;int&gt;, KIN &lt;int&gt;\n# AATIE data: 4,880 × 4\n    .egoID .srcID .tgtID WEIGHT\n*    &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n1 19850001      1      2      2\n2 19850001      1      3      1\n3 19850001      1      4      1\n# ℹ 4,877 more rows\n\n\negor objects are constructed as tibbles, which are data frames built using the tidyverse logic. For those versed in the tidyverse, one can take advantage of all the functions, calls, etc. that go along with those kinds of objects. It is, however, not strictly necessary to know the syntax of the tidyverse to work with these objects. Let’s take a look at the egor object.\n\nnames(egonetlist) \n\n[1] \"ego\"   \"alter\" \"aatie\"\n\n\nWe see that the elements are made up of our three data frames. For example, we take the first five columns of the ego data and alter attributes, extracted from the egor object:\n\negonetlist[[\"ego\"]][, 1:5]\n\n# A tibble: 1,531 × 5\n     .egoID   AGE  EDUC RACE  SEX   \n      &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; \n 1 19850001    33    16 white male  \n 2 19850002    49    19 white male  \n 3 19850003    23    16 white female\n 4 19850004    26    20 white female\n 5 19850005    24    17 white female\n 6 19850006    45    17 white male  \n 7 19850007    44    18 white female\n 8 19850008    56    12 white female\n 9 19850009    85     7 white female\n10 19850010    65    12 white female\n# ℹ 1,521 more rows\n\negonetlist[[\"alter\"]][, 1:5]\n\n# A tibble: 4,483 × 5\n   .altID   .egoID   AGE  EDUC RACE \n    &lt;int&gt;    &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;\n 1      1 19850001    32    18 white\n 2      2 19850001    29    16 white\n 3      3 19850001    32    18 white\n 4      4 19850001    35    16 white\n 5      5 19850001    29    13 white\n 6      1 19850002    42    12 white\n 7      2 19850002    44    18 white\n 8      3 19850002    45    16 white\n 9      4 19850002    40    12 white\n10      5 19850002    50    18 white\n# ℹ 4,473 more rows\n\n\nNote that the id variable for ego has been renamed to .egoID (it was CASEID on the original data). We also see see that the alter id has been renamed .altID (from ALTERID on the original data).\nAnd now we look at the alter-alter ties where we can see that the column names for the alter-alter ties have also been renamed from the input data. The variables are now .srcID and .tgtID (rather than ALTER1 and ALTER2, as on the original data).\n\negonetlist[[\"aatie\"]]\n\n# A tibble: 4,880 × 4\n     .egoID .srcID .tgtID WEIGHT\n      &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n 1 19850001      1      2      2\n 2 19850001      1      3      1\n 3 19850001      1      4      1\n 4 19850001      1      5      1\n 5 19850001      2      3      2\n 6 19850001      2      4      2\n 7 19850001      2      5      2\n 8 19850001      3      4      1\n 9 19850001      3      5      1\n10 19850001      4      5      1\n# ℹ 4,870 more rows\n\n\n\n\nCalculate density on the egor object using the function ego_density(). Note that all ego networks of size 0 or 1 will have NAs for density).\n\n\nShow solution\ndens &lt;- ego_density(egonetlist)\nhead(dens)\n\n# For example, respondent 1 (19850001) has 5 alters and all 10 possible ties exist \n# (density = 1), \n# while respondent 2 (1950002) has 5 alters but only 8 ties exist (density = .8). \n# To check this you can run the following:\nalteralter_dat[alteralter_dat$CASEID == 19850001, ]\nalteralter_dat[alteralter_dat$CASEID == 19850002, ]"
  },
  {
    "objectID": "teaching/sna/material/05/05-ego-nets.html#plotting-the-ego-nets",
    "href": "teaching/sna/material/05/05-ego-nets.html#plotting-the-ego-nets",
    "title": "Social Network Analysis",
    "section": "",
    "text": "We not plot the networks using igraph. First step is to convert the information in the egor object to igraph objects. We do this using the as_igraph() function. Let’s take a look at the first three ego networks.\n\nego_nets &lt;- as_igraph(egonetlist)\nego_nets[1:3] \n\n$`19850001`\nIGRAPH 5f36126 UN-- 5 10 -- \n+ attr: .egoID (g/n), name (v/c), AGE (v/n), EDUC (v/n), RACE (v/c),\n| SEX (v/c), RELIG (v/c), AGE_CATEGORICAL (v/c), EDUC_CATEGORICAL\n| (v/c), TALKTO (v/n), SPOUSE (v/n), KIN (v/n), WEIGHT (e/n)\n+ edges from 5f36126 (vertex names):\n [1] 1--2 1--3 1--4 1--5 2--3 2--4 2--5 3--4 3--5 4--5\n\n$`19850002`\nIGRAPH 15ba0c9 UN-- 5 8 -- \n+ attr: .egoID (g/n), name (v/c), AGE (v/n), EDUC (v/n), RACE (v/c),\n| SEX (v/c), RELIG (v/c), AGE_CATEGORICAL (v/c), EDUC_CATEGORICAL\n| (v/c), TALKTO (v/n), SPOUSE (v/n), KIN (v/n), WEIGHT (e/n)\n+ edges from 15ba0c9 (vertex names):\n[1] 1--2 1--3 1--4 1--5 2--4 3--4 3--5 4--5\n\n$`19850003`\nIGRAPH d2d7e9e UN-- 5 6 -- \n+ attr: .egoID (g/n), name (v/c), AGE (v/n), EDUC (v/n), RACE (v/c),\n| SEX (v/c), RELIG (v/c), AGE_CATEGORICAL (v/c), EDUC_CATEGORICAL\n| (v/c), TALKTO (v/n), SPOUSE (v/n), KIN (v/n), WEIGHT (e/n)\n+ edges from d2d7e9e (vertex names):\n[1] 1--2 1--3 1--4 2--3 2--4 3--4\n\n\nNote that we would use as_network() function if we wanted to construct networks in the network format.\nWe have a list of ego networks (in the igraph format), with each ego network in a different element in the list. We can see that the information on the alters was automatically passed to the igraph objects, as was the information on the weights for the alter-alter ties. Note that by default the igraph objects will not include ego. Ego is often (but not always) excluded from visualizations and calculations because ego is, by definition, tied to all alters. Including ego thus offers little additional structural information. We will consider measures below that incorporate both ego and alter information.\nAs with all igraph objects, we can extract useful information, like the attributes of the nodes. As an example, let’s extract information on sex of alters from the first ego network.\n\nvertex_attr(ego_nets[[1]], \"SEX\")\n\n[1] \"male\"   \"female\" \"male\"   \"male\"   \"female\"\n\n# Note that this is the same information as:\n# alter_dat[alter_dat$CASEID == 19850001, \"SEX\"]\n\nNow let’s plot a few networks focusing on the first 6 ego networks:\n\n# lapply() will perform a given function, here plot(), \n# over every element of an input list, here the first 6 elements of ego_nets.\n# note that you also can use purrr::walk()\npar(mfrow = c(2, 3))\nlapply(ego_nets[1:6], plot)"
  },
  {
    "objectID": "teaching/sna/material/05/05-ego-nets.html#descriptives-on-all-ego-nets",
    "href": "teaching/sna/material/05/05-ego-nets.html#descriptives-on-all-ego-nets",
    "title": "Social Network Analysis",
    "section": "",
    "text": "Now that we have a list of networks, we can apply the same function to each network using a single line of code, again with the help of lapply().\n\n\nFind the mean number of nodes and ties using vcount() and ecount() function for all networks. Try reproducing the plots shown below. Hint: You can use lapply() again here.\n\n\n\n\n\n\n\n\n\n\n\nShow solution\n# nodes\nnetwork_sizes &lt;- lapply(ego_nets, vcount)\nnetwork_sizes &lt;- unlist(network_sizes)\nmean(network_sizes, na.rm = T)\n\n# ties\nnetwork_edge_counts &lt;- lapply(ego_nets, ecount)\nnetwork_edge_counts &lt;- unlist(network_edge_counts)\nmean(network_edge_counts, na.rm = T)\n\n# Create data frames\ndf1 &lt;- data.frame(NetworkSize = network_sizes)\ndf2 &lt;- data.frame(EdgeCount = network_edge_counts)\n\n# Create histograms (ggplot approach)\np1 &lt;- ggplot(df1, aes(x = NetworkSize)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"skyblue\") +\n  ggtitle(\"Histogram of Network Sizes\") +\n  xlab(\"# of Nodes\") +\n  theme_minimal()\n\np2 &lt;- ggplot(df2, aes(x = EdgeCount)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"tan2\") +\n  ggtitle(\"Histogram of Edge Counts\") +\n  xlab(\"# of Edges\") +\n  theme_minimal()\n\np1 + p2 # needs patchwork\n\n\n\n\n\nUse edge_density and apply to every ego network in the data to find the density of all networks. Create a histogram of these densities as in exercise 3.\n\n\nShow solution\ndensities &lt;- lapply(ego_nets, edge_density)\ndensities &lt;- unlist(densities)\nhist(densities)\n\n\n\n\n\nThere are also inbuilt functions in the egor package that can be used to analyze the ego networks, for example ego_density, composition, alts_diversity_count, count_dyads, and comp_ei. Explore these functions by yourself. Note that you then need to work with the egor object and not the igraph object."
  },
  {
    "objectID": "teaching/sna/material/05/05-ego-nets.html#homophily-ego-alter-attributes",
    "href": "teaching/sna/material/05/05-ego-nets.html#homophily-ego-alter-attributes",
    "title": "Social Network Analysis",
    "section": "",
    "text": "The EI index (External-Internal index) in ego network compositional analysis is a measure of homophily (similarity) or heterophily (difference) in ego networks. It tells you whether ego’s alters are more similar or dissimilar to the ego based on a categorical attribute (e.g., gender, ethnicity, etc.). The formula is given as: \\[EI = \\frac{E - I}{E + I}\\] where:\n\nE = Number of external ties (alters with a different attribute value than ego)\nI = Number of internal ties (alters with the same attribute value as ego)\n\nThe measure is interpreted as follows:\n\n+1: all alters are different from ego (maximum heterophily)\n0: equal number of similar and different alters (neutral)\n–1: all alters are the same as ego (maximum homophily)\n\nLet’s start by looking at the level of homophily based on the attribute SEX:\n\ncomp_ei(egonetlist, alt.attr = \"SEX\", ego.attr = \"SEX\")\n\n# A tibble: 1,531 × 2\n     .egoID    ei\n      &lt;int&gt; &lt;dbl&gt;\n 1 19850001  -0.2\n 2 19850002  -0.2\n 3 19850003  -1  \n 4 19850004   0.2\n 5 19850005   0.2\n 6 19850006  -0.5\n 7 19850007  -0.6\n 8 19850008   0.2\n 9 19850009   0  \n10 19850010  -1  \n# ℹ 1,521 more rows\n\n\nFor each ego, you know have an index that tells you the level of homophily based on the attribute “SEX”. For example, the first ego 19850001 tends to have slightly more homophilious ties compared to heterophilous ties. To get a better idea of this measure over all networks you can compute the proportion of negative/positive values or condition on (group by) another attribute. For example:\n\ncomp_ei(egonetlist, alt.attr = \"SEX\", ego.attr = \"SEX\") %&gt;% count(ei &lt; 0)\n\n# A tibble: 3 × 2\n  `ei &lt; 0`     n\n  &lt;lgl&gt;    &lt;int&gt;\n1 FALSE      599\n2 TRUE       795\n3 NA         137\n\n\nNote that the NA’s are cause because of some missing values in the alter data set.\n\n\nDo the same thing, but only consider ties that are based on variable RACE. What can you conclude?\n\n\nShow solution\ncomp_ei(egonetlist, alt.attr = \"RACE\", ego.attr = \"RACE\")\n\n\nHomphily based on SEX and RACE offers very different stories. We see that race is a much more salient dimension than gender, with many respondents matching perfectly with all members of their network along racial lines, but much less so with gender, where differences between ego and alter are more common."
  },
  {
    "objectID": "teaching/sna/material/05/05-ego-nets.html#ego-networks-as-predictors",
    "href": "teaching/sna/material/05/05-ego-nets.html#ego-networks-as-predictors",
    "title": "Social Network Analysis",
    "section": "",
    "text": "Above we examined the properties of the ego networks, focusing mostly on racial and gender homophily. There are a number of other properties we could explore in more detail, like density or network size. For example, we might want to predict network size as a function of race, gender or other demographic characteristics.\nWe can also use properties of the ego networks as predictors of other outcomes of interest. For example, let’s try and predict the variable HAPPY using the features of the ego networks. Are individuals with larger ego networks happier?\nHAPPY is coded as a 1 (very happy), 2 (pretty happy), 3 (not too happy). Let’s add a label to the variable and reorder it to run from not happy to happy:\n\nego_dat$HAPPY_FACTOR &lt;- factor(ego_dat$HAPPY, levels = c(3, 2, 1), \n                            labels = c(\"not too happy\", \"pretty happy\", \n                                     \"very happy\"))\n\nWe also turn our race and sex variables into factors. We set white as the first category in our race variable.\n\nego_dat$RACE_FACTOR &lt;- factor(ego_dat$RACE, levels = c(\"white\", \"asian\", \n                                                       \"black\", \"hispanic\", \n                                                       \"other\")) \nego_dat$SEX_FACTOR &lt;- factor(ego_dat$SEX)\n\nLet’s also save density\n\ndens &lt;- ego_density(egonetlist)\nego_dat$DENSITY &lt;- dens[[\"density\"]] #  getting values out of tibble format\n\nHAPPY is an ordinal variable. With ordinal outcome variables, it is best to utilize ordered logistic regression (or a similar model). We will need the polr() function in the MASS package to run these models.\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\ndens &lt;- ego_density(egonetlist)\nego_dat$DENSITY &lt;- dens[[\"density\"]] #  getting values out of tibble format\n\nLet’s create a data frame that has no missing data on any of the variables we want to include in the full model. The outcome of interest is HAPPY_FACTOR and the main predictors are ego network size (NUMGIVEN) and density (DENSITY) . We also include a number of demographic controls:\n\nego_dat_nomiss &lt;- na.omit(ego_dat[, c(\"HAPPY_FACTOR\", \"NUMGIVEN\", \"DENSITY\", \n                                     \"EDUC\", \"AGE\", \"RACE_FACTOR\", \n                                     \"SEX_FACTOR\")])\n\nNow we run the ordered logistic regression predicting happiness. For our first model we will predict happiness as a function of our two structural network features, ego network size and density.\n\nsummary(happy_mod1 &lt;- polr(HAPPY_FACTOR ~ NUMGIVEN + DENSITY, \n                           data = ego_dat_nomiss)) \n\n\nRe-fitting to get Hessian\n\n\nCall:\npolr(formula = HAPPY_FACTOR ~ NUMGIVEN + DENSITY, data = ego_dat_nomiss)\n\nCoefficients:\n           Value Std. Error t value\nNUMGIVEN 0.08399    0.04753   1.767\nDENSITY  0.47660    0.20388   2.338\n\nIntercepts:\n                           Value   Std. Error t value\nnot too happy|pretty happy -1.4375  0.2700    -5.3240\npretty happy|very happy     1.5709  0.2697     5.8236\n\nResidual Deviance: 2105.42 \nAIC: 2113.42 \n\n\nThe results suggest that respondents with dense networks report higher levels of happiness, while ego network size (NUMGIVEN) is not a significant predictor of happiness, controlling for density. The initial results would suggest that it is less about the number of people in your ego network that matters for happiness, and more about whether they know each other.\n\n\nAdd the control variables into the model and interpret the results.\n\n\nShow solution\nsummary(happy_mod2 &lt;- polr(HAPPY_FACTOR ~ NUMGIVEN + DENSITY + EDUC + AGE + \n                             RACE_FACTOR + SEX_FACTOR, \n                           data = ego_dat_nomiss))\n\n\nTo summarize the final model fit: density is still a significant predictor of happiness. Individuals with alters who are interconnected consistently report higher levels of happiness, showing the potential benefits of being part of an integrated social group. We also see that individuals with higher education tend to report higher levels of happiness, while those individuals identifying as black report lower levels of happiness."
  },
  {
    "objectID": "teaching/sna/material/05/05-ego-nets.html#footnotes",
    "href": "teaching/sna/material/05/05-ego-nets.html#footnotes",
    "title": "Social Network Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis worksheet is inspired and adapted from this source↩︎"
  },
  {
    "objectID": "teaching/sna/material/04/04-descriptives-2.html",
    "href": "teaching/sna/material/04/04-descriptives-2.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "library(igraph)\nlibrary(networkdata)\nlibrary(netUtils)\nlibrary(netrankr)"
  },
  {
    "objectID": "teaching/sna/material/04/04-descriptives-2.html#reciprocity",
    "href": "teaching/sna/material/04/04-descriptives-2.html#reciprocity",
    "title": "Social Network Analysis",
    "section": "Reciprocity",
    "text": "Reciprocity\nLoad the “Hightech manager” dataset\n\ndata(\"ht_advice\")\ndata(\"ht_friends\")\ndata(\"ht_reports\")\n\nThese data were collected from the managers of a high-tech company. Each manager was asked “To whom do you go to for advice?” and “Who is your friend?”. Data for the item “To whom do you report?” was taken from company documents.\n\nExercise 1:\n\nBefore computing the reciprocity, think about where you expect high and where low reciprocity\nCompute the reciprocity of each network. Does it fit you intuition?\n\n\n\nShow solution\nreciprocity(ht_advice)\nreciprocity(ht_friends)\nreciprocity(ht_reports)"
  },
  {
    "objectID": "teaching/sna/material/04/04-descriptives-2.html#triad-census",
    "href": "teaching/sna/material/04/04-descriptives-2.html#triad-census",
    "title": "Social Network Analysis",
    "section": "Triad census",
    "text": "Triad census\n\nOne of the many applications of the triad census is to compare a set of networks. Networks with a similar triad census profile are said to be structurally similar.\n\nExercise 2:\nCompute the triad census for each of the three “Hightech manager” dataset and interpret the results.\n\n\nShow solution\ntriad_census(ht_advice)\ntriad_census(ht_friends)\ntriad_census(ht_reports)"
  },
  {
    "objectID": "teaching/sna/material/04/04-descriptives-2.html#centrality",
    "href": "teaching/sna/material/04/04-descriptives-2.html#centrality",
    "title": "Social Network Analysis",
    "section": "Centrality",
    "text": "Centrality\n\nExploring implemented indices\n(interactive session)\nigraph contains the following 10 indices:\n\ndegree (degree())\nweighted degree (strength())\nbetweenness (betweenness())\ncloseness (closeness())\neigenvector (eigen_centrality())\nalpha centrality (alpha_centrality())\npower centrality (power_centrality())\nPageRank (page_rank())\neccentricity (eccentricity())\nhubs and authorities (authority_score() and hub_score())\nsubgraph centrality (subgraph_centrality())\n\nTo illustrate some of the indices, we use the “dbces11” graph which is part of the netrankr package.\n\ndata(\"dbces11\")\n\n\n\nRise of the Medici\n\ndata(\"flo_marriage\")\n\nThe figure below shows part of the marriage network of Florentine families around 1430. The node size corresponds to the number of prior seats and the size of the label to their wealth. \nThe network includes families who were locked in a struggle for political control of the city of Florence. Two factions were dominant in this struggle: one revolved around the infamous Medicis, the other around the powerful Strozzis.\nThe network is frequently used to illustrate how a central position in a network can have beneficial outcomes for actors.\n\n\nExercise 3\n\nCalculate degree, betweenness, closeness, and eigenvector centrality\nArgue for each index, why it might be beneficial to have a high rank in this particular network and setting\n\n\n\nShow solution\nsort(degree(flo_marriage), decreasing = TRUE)\nsort(betweenness(flo_marriage), decreasing = TRUE)\nsort(closeness(flo_marriage), decreasing = TRUE)\nsort(eigen_centrality(flo_marriage)$vector, decreasing = TRUE)\n\n\nExtra: Choose one index and with the help of SNAhelper visualize the network such that the node sizes are proportional to centrality values.\n\n\nPageRank\nload the ATP or WTA dataset from the networkdata package. Both contain a list of igraph objects of tennis results from 1968 to 2021. The last season (2021) can be accessed via atp[[54]] or wta[[54]].\n\ndata(atp)\nstr(atp[[54]])\n\n-------------------------------------------------------\nATP SEASON 2021 (directed, weighted, one-mode network)\n-------------------------------------------------------\nNodes: 393, Edges: 2609, Density: 0.0169, Components: 14, Isolates: 0\n-Graph Attributes:\n  name(c)\n---\n-Vertex Attributes:\n name(c): Frances Tiafoe, Jan Lennard Struff, Sumit Nagal, John Millman, ...\n hand(c): R, R, R, R, R, R, L, R, R, R, R, R, R, R, L, R, R, R, L, R, R, ...\n age(n): 23, 31, 24, 32, 21, 34, 27, 22, 28, 29, 25, 29, 33, 34, 28, 29, ...\n country(c): USA, GER, IND, AUS, CZE, KAZ, GER, SRB, USA, BLR, COL, AUS, ...\n---\n-Edge Attributes:\n surface(c): Clay, Hard, Hard, Hard, Hard, Clay, Hard, Clay, Hard, Clay, ...\n weight(n): 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n---\n-Edges (first 10): \n Adolfo Daniel Vallejo-&gt;Tom Kocevar Desman Adrian Andreev-&gt;Alex De\nMinaur Adrian Andreev-&gt;Alexei Popyrin Adrian Andreev-&gt;Gerardo Lopez\nVillasenor Adrian Andreev-&gt;Miomir Kecmanovic Adrian Mannarino-&gt;Albert\nRamos Adrian Mannarino-&gt;Alexander Zverev Adrian Mannarino-&gt;Aljaz Bedene\nAdrian Mannarino-&gt;Andy Murray Adrian Mannarino-&gt;Arthur Cazaux\n\n\nA directed edge points from the loser to the winner of a match. The network is weighted and a weight corresponds to the number of times a player lost to another on a specific surface.\n\n\nExercise 4\n\nWhat do weighted in- and out-degree, and degree measure mean in the network?\nWhy could Page Rank be a good index to measure player strength?\nWho was the best player in 2021 according to Page Rank?\nPlot the relation between matches won and Page rank. What can you observe?\nTry to redo the analysis for a specific surface (Clay, Hard, Grass)\n\n\n\n\n\n\n\nTip\n\n\n\nthe igraph functions you need are page_rank() and strength(). Check the help for how they handle weights and direction.\nTo extract the network for a specific surface, check out subgraph_from_edges()\n\n\n\n\nShow solution\ng &lt;- atp[[54]]\n#top ten in 2021\nsort(page_rank(g)$vector,decreasing = TRUE)[1:10]\nplot(degree(g, mode = \"in\"),page_rank(g)$vector)\n# example: extract clay networks\ng1 &lt;- subgraph_from_edges(g,which(E(g)$surface==\"Clay\"))\n#top ten in 2021\nsort(page_rank(g1)$vector,decreasing = TRUE)[1:10]"
  },
  {
    "objectID": "teaching/sna/material/04/04-descriptives-2.html#exercise-clustering",
    "href": "teaching/sna/material/04/04-descriptives-2.html#exercise-clustering",
    "title": "Social Network Analysis",
    "section": "Exercise: Clustering",
    "text": "Exercise: Clustering\nPractice the clustering workflow with (a) network(s) from the networkdata package. When choosing a directed and/or weighted network, make sure to set parameters appropriately."
  },
  {
    "objectID": "teaching/sna/material/03/03-descriptives-1.html",
    "href": "teaching/sna/material/03/03-descriptives-1.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "library(igraph)\nlibrary(networkdata)\nlibrary(netUtils)"
  },
  {
    "objectID": "teaching/sna/material/03/03-descriptives-1.html#structural-features",
    "href": "teaching/sna/material/03/03-descriptives-1.html#structural-features",
    "title": "Social Network Analysis",
    "section": "Structural features",
    "text": "Structural features\n\nExercise: Degree Distribution\n\npa &lt;- sample_pa(1500, power = 1.5, m = 3, directed = FALSE)\ner &lt;- sample_gnp(1500, p = 0.003)\n\n\nVerify that the networks have approximately the same density\nPlot the degree distributions of both networks and check for skewness\ncalculate the degrees (degree()) and order the sequence decreasingly (order())\nFor both networks, create a loop (i=1,…, 50) which deletes the top i vertices (delete_vertices()) in the ordered degree sequence and compute the diameter of the resulting networks\nWhat can you observe?\n\nSolution\n\n\nShow solution\n#calculate the density (edge_density is the same as graph.density)\nedge_density(pa)\nedge_density(er)\n\n# for the preferential attachment network, \n# we plot the degree distribution on a log-log scale\n# a straight-ish line should appear \nplot(degree_distribution(pa), log=\"xy\")\nplot(degree_distribution(er))\n\n#calculate the degrees explicitly\ndeg_er &lt;- order(degree(er),decreasing = TRUE)\ndeg_pa &lt;- order(degree(pa),decreasing = TRUE)\n\n# iterate through the top 50 nodes, delete them and calculate the diameter\nres_er &lt;- rep(0,50)\nres_pa &lt;- rep(0,50)\n\nfor(i in 1:50){\n  g1 &lt;- delete_vertices(er,deg_er[1:i])\n  g2 &lt;- delete_vertices(pa,deg_pa[1:i])\n  res_er[i] &lt;- diameter(g1)\n  res_pa[i] &lt;- diameter(g2)\n}\n\nplot(res_pa,type = \"l\", col=\"red\",ylab=\"diameter\")\nlines(res_er,col=\"black\")\n\n\nYou’ll notice that the diameter increases very quickly for a preferential attachment network. The diameter of the random network on the other hand remains constant. If you would redo the analysis for the preferential attachment network and remove random nodes, you’ll notice that the diameter also remains constant. A preferential network is thus said to be fragile to targeted attacks but robust to random attacks.\n\n\nExercise: Density, Transitivity, Distances\nReal world networks tend to have a high transitivity (transitivity(type=\"global\")), low average distance (mean_distance()) and a low density (graph.density()).\n\nExperiment with sample_pa() and sample_gnp() and compute the three stats. Can all three criteria be fullfiled? Why/why not?\n\n\n\nHint\n# Parameters\nn &lt;- 1000       # number of nodes\nm &lt;- 5          # edges per new node in PA\np &lt;- 0.01       # connection probability for GNP\n\n# Preferential Attachment \ng_pa &lt;- sample_pa(n, power = 1, m = m, directed = FALSE)\n\n# Erdős–Rényi (GNP)\ng_gnp &lt;- sample_gnp(n, p, directed = FALSE)\n\n# Compute statistics\ncompute_stats &lt;- function(graph) {\n  trans &lt;- transitivity(graph, type = \"global\")\n  dist &lt;- mean_distance(graph, directed = FALSE, unconnected = TRUE)\n  dens &lt;- edge_density(graph)\n  return(c(transitivity = trans, mean_distance = dist, density = dens))\n}\n\n# Get stats\nstats_pa &lt;- compute_stats(g_pa)\nstats_gnp &lt;- compute_stats(g_gnp)\n\n# Combine into a data frame\nresults &lt;- data.frame(\n  Model = c(\"Preferential Attachment\", \"Erdős–Rényi (GNP)\"),\n  Transitivity = c(stats_pa[\"transitivity\"], stats_gnp[\"transitivity\"]),\n  Mean_Distance = c(stats_pa[\"mean_distance\"], stats_gnp[\"mean_distance\"]),\n  Density = c(stats_pa[\"density\"], stats_gnp[\"density\"])\n)\n\nresults"
  },
  {
    "objectID": "project/nexus/index.html",
    "href": "project/nexus/index.html",
    "title": "NEXUS1492 - Reconstructing Archaeological Networks",
    "section": "",
    "text": "Termeh Shafie"
  },
  {
    "objectID": "project/nexus/index.html#project-summary",
    "href": "project/nexus/index.html#project-summary",
    "title": "NEXUS1492 - Reconstructing Archaeological Networks",
    "section": "Project Summary",
    "text": "Project Summary\nBetween 2013-2018, I worked in international ERC-Synergy research project NEXUS1492 investigates the impacts of colonial encounters in the Caribbean through the reconstruction and modelling of archaeological networks.\nYou can read more about the project and its output here."
  },
  {
    "objectID": "project/nexus/index.html#references",
    "href": "project/nexus/index.html#references",
    "title": "NEXUS1492 - Reconstructing Archaeological Networks",
    "section": "References",
    "text": "References\n\nShafie T., Schoch D., Mans J., Hofman C., Brandes U., (2017). Hypergraph Representations: A Study of Carib Attacks on Colonial Forces, 1509-1700. Journal of Historical Network Research,1(1), 52-70. Link\nLaffoon, J.E., Sonnemann, T.F., Shafie, T., Hofman, C.L., Brandes, U. and Davies, G.R., (2017). Investigating human geographic origins using dual-isotope (87Sr/86Sr, δ18O) assignment approaches. PloS one, 12(2), p.e0172562. Link\nAmati, V., Shafie, T., Brandes U., (2018) Reconstructing Archaeological Networks with Structural Holes. Journal of Archaeological Method and Theory volume 25, 226–253. Link\nAmati, V., Mol, A., Shafie, T., Hofman, C., Brandes U., (2020). A Framework for Reconstructing Archaeological Networks Using Exponential Random Graph Models. Journal of Archaeological Method and Theory volume 27, 192–219. Link"
  },
  {
    "objectID": "project/rmm/index.html",
    "href": "project/rmm/index.html",
    "title": "Multigraph Representation of Network Data",
    "section": "",
    "text": "Termeh Shafie"
  },
  {
    "objectID": "project/rmm/index.html#project-summary",
    "href": "project/rmm/index.html#project-summary",
    "title": "Multigraph Representation of Network Data",
    "section": "Project summary",
    "text": "Project summary\nMultigraphs are network representations in which multiple edges and edge loops (self edges) are permitted. Multigraph data structure can be observed directly but the possibility to obtain multigraphs using blocking, aggregation and scaling makes them widely applicable.\nFor the past decade, I’ve been working on a statistical framework for analyzing network data using this representation. I have developed different multigraph multigraph models and derived several statistics under these models that can be used to analyse local and global network properties in order to convey important social phenomena and processes. The latest contribution in this framework is formal goodness of fit tests for the developed probability models for random multigraphs.\nThe proposed framework is in full implemented in the R package ‘multigraphr’ and a description of various functions implemented in the package are given in the following. More details are provided in the package vignetteand the references listed."
  },
  {
    "objectID": "project/rmm/index.html#r-package-multigraphr",
    "href": "project/rmm/index.html#r-package-multigraphr",
    "title": "Multigraph Representation of Network Data",
    "section": "R package multigraphr",
    "text": "R package multigraphr\n\nPackage overview\n  \nThis package introduces the multigraph framework for analyzing social network data. Brief description of various functions implemented in the package are given in the following but more details are provided in the package vignettes and the references listed.\n\n\nInstallation\nYou can install the released version of multigraphr from CRAN with:\ninstall.packages(\"multigraphr\")\nThe development version from GitHub with:\n# install.packages(\"devtools\")\ndevtools::install_github(\"termehs/multigraphr\")"
  },
  {
    "objectID": "project/rmm/index.html#multigraphs-and-applicability",
    "href": "project/rmm/index.html#multigraphs-and-applicability",
    "title": "Multigraph Representation of Network Data",
    "section": "Multigraphs and applicability",
    "text": "Multigraphs and applicability\nMultigraphs are network representations in which multiple edges and edge loops (self edges) are permitted. These data structures can be either directly observed or aggregated by classifying or cross-classifying node attributes into meta nodes. For the latter case, within group edges correspond to self-edges. See example below where the original graph with 15 nodes and 12 edges (left) is aggregated based on node categories into a small multigraph with 4 nodes (right).\n\nEdge aggregation can also be used to obtain multigraphs. Assume that we study a graph with three different types of relations over three periods of time: \nIf we aggregate over time periods, we obtain for each edge category a multigraph for the total time period of three days:\n\nFor more details on these kinds of aggregations, see Shafie (2015;2016)."
  },
  {
    "objectID": "project/rmm/index.html#multigraph-representation-of-network-data",
    "href": "project/rmm/index.html#multigraph-representation-of-network-data",
    "title": "Multigraph Representation of Network Data",
    "section": "Multigraph representation of network data",
    "text": "Multigraph representation of network data\nMultigraphs are represented by their edge multiplicity sequence M with elements M(i,j), denoting the number of edges at vertex pair sites (i,j) ordered according to (1,1) &lt; (1,2) &lt;···&lt; (1,n) &lt; (2,2) &lt; (2,3) &lt;···&lt; (n,n), where n is number of nodes. The number of vertex pair sites is given by r = n(n+1)/2.\n\nRandom multigraph models\nTwo probability models for generating undirected random multigraphs are implemented in the package together with several statistics under these two models. Moreover, functions for goodness of fit tests are available for the presented models.\nNote that some of the functions are only practical for small scale multigraphs.\nThe first model is obtained by random stub matching (RSM) given observed degree sequence of a multigraphs, so that edge assignments to vertex pair sites are dependent. The second is obtained by independent edge assignments (IEA) according to a common probability distribution. There are two ways in which an approximate IEA model can be obtained from an RSM model, thus facilitating the structural analysis. These two ways are\n\nindependent stub assignment (ISA)\nindependent edge assignment of stubs (IEAS)\n\n(Shafie, 2016).\n\n\nExample\n\nlibrary('multigraphr')\n\nConsider a small graph on 3 nodes and the following adjacency matrix:\n\nA &lt;-  matrix(c(1, 1, 0, \n               1, 2, 2, \n               0, 2, 0), \n             nrow = 3, ncol = 3)\nA\n\n     [,1] [,2] [,3]\n[1,]    1    1    0\n[2,]    1    2    2\n[3,]    0    2    0\n\n\nThe degree sequence of the multigraph has double counted diagonals (edge stubs for loops) and is given by\n\nD &lt;- get_degree_seq(adj = A, type = 'graph')\nD\n\n[1] 3 7 2\n\n\nso that number of edges in the multigraph is half the sum of the degree sequence which is equal to 6.\nThe RSM model given observed degree sequence shows the sample space consists of 7 possible multigraphs, as represented by their multiplicity sequence m.seq (each row correspond to the edge multiplicity sequence of a unique multigraph):\n\nrsm_1 &lt;- rsm_model(deg.seq = D)\nrsm_1$m.seq\n\n  M11 M12 M13 M22 M23 M33\n1   1   1   0   3   0   1\n2   1   1   0   2   2   0\n3   1   0   1   3   1   0\n4   0   3   0   2   0   1\n5   0   3   0   1   2   0\n6   0   2   1   2   1   0\n7   0   1   2   3   0   0\n\n\nwith probabilities associated with each multigraph, together with statistics ‘number of loops’, ‘number of multiple edges’ and ‘simple graphs or not’:\n\nrsm_1$prob.dists\n\n    prob.rsm loops multiedges simple\n1 0.03030303     5          1      0\n2 0.18181818     3          3      0\n3 0.06060606     4          2      0\n4 0.06060606     3          3      0\n5 0.24242424     1          5      0\n6 0.36363636     2          4      0\n7 0.06060606     3          3      0\n\n\nConsider using the IEA model to approximate the RSM model so that edge assignment probabilities are functions of observed degree sequence. Note that the sample space for multigraphs is much bigger than for the RSM model so the multiplicity sequences are not printed (they can be found using the function get_edgemultip_seq for very small multigraphs and their probabilities can be found using the multinomial distribution). The following shows the number of multigraphs under either of the IEA models:\n\nieas_1 &lt;-   iea_model(adj = A , type = 'graph',  model = 'IEAS', K = 0, apx = TRUE)\nieas_1$nr.multigraphs\n\n[1] 462"
  },
  {
    "objectID": "project/rmm/index.html#statistics-to-analyze-structural-properties",
    "href": "project/rmm/index.html#statistics-to-analyze-structural-properties",
    "title": "Multigraph Representation of Network Data",
    "section": "Statistics to analyze structural properties",
    "text": "Statistics to analyze structural properties\nThese statistics include number of loops (indicator of e.g. homophily) and number of multiple edges (indicator of e.g. multiplexity/interlocking), which are implemented in the package together with their probability distributions, moments and interval estimates under the different multigraph models.\n\nExample (cont’d)\nUnder the RSM model, the first two moments and interval estimates of the statistics M1 = ‘number of loops’ and M2 = ‘number of multiple edges’ are given by\n\nrsm_1$M\n\n             M1    M2\nExpected  2.273 3.727\nVariance  0.986 0.986\nUpper 95% 4.259 5.713\nLower 95% 0.287 1.741\n\n\nwhich are calculated using the numerically found probability distributions under RSM (no analytical solutions exist for these moments).\nUnder the IEA models (IEAS or ISA), moments of these statistics, together with the complexity statistic \\(R_k\\) representing the sequence of frequencies of edge sites with multiplicities 0,1,…,k, are found using derived formulas. Thus, there is no limit on multigraph size to use these. When the IEAS model is used to approximate the RSM model as shown above:\n\nieas_1$M\n\n              M1    M2\nObserved   3.000 3.000\nExpected   2.273 3.727\nVariance   1.412 1.412\nUpper 95%  4.649 6.104\nLower 95% -0.104 1.351\n\nieas_1$R\n\n             R0     R1     R2\nObserved  2.000  2.000  2.000\nExpected  2.674  1.588  1.030\nVariance  0.575  1.129  0.760\nUpper 95% 4.191  3.713  2.773\nLower 95% 1.156 -0.537 -0.713\n\n\nWhen the ISA model is used to approximate the RSM model (see above):\n\nisa_1 &lt;-   iea_model(adj = A , type = 'graph',  \n                     model = 'ISA', K = 0, apx = TRUE)\nisa_1$M\n\n             M1    M2\nObserved  3.000 3.000\nExpected  2.583 3.417\nVariance  1.471 1.471\nUpper 95% 5.009 5.842\nLower 95% 0.158 0.991\n\nisa_1$R\n\n             R0     R1     R2\nObserved  2.000  2.000  2.000\nExpected  2.599  1.703  1.018\nVariance  0.622  1.223  0.748\nUpper 95% 4.176  3.915  2.748\nLower 95% 1.021 -0.509 -0.711\n\n\nThe IEA models can also be used independent of the RSM model. For example, the IEAS model can be used where edge assignment probabilities are estimated using the observed edge multiplicities (maximum likelihood estimates):\n\nieas_2 &lt;-   iea_model(adj = A , type = 'graph',  \n                      model = 'IEAS', K = 0, apx = FALSE)\nieas_2$M\n\n             M1    M2\nObserved  3.000 3.000\nExpected  3.000 3.000\nVariance  1.500 1.500\nUpper 95% 5.449 5.449\nLower 95% 0.551 0.551\n\nieas_2$R\n\n             R0     R1     R2\nObserved  2.000  2.000  2.000\nExpected  2.845  1.331  1.060\nVariance  0.434  0.805  0.800\nUpper 95% 4.163  3.125  2.849\nLower 95% 1.528 -0.464 -0.729\n\n\nThe ISA model can also be used independent of the RSM model. Then, a sequence containing the stub assignment probabilities (for example based on prior belief) should be given as argument:\n\nisa_2 &lt;-   iea_model(adj = A , type = 'graph',  \n                     model = 'ISA', K = 0, apx = FALSE, p.seq = c(1/3, 1/3, 1/3))\nisa_2$M\n\n              M1    M2\nObserved   3.000 3.000\nExpected   2.000 4.000\nVariance   1.333 1.333\nUpper 95%  4.309 6.309\nLower 95% -0.309 1.691\n\nisa_2$R\n\n             R0     R1     R2\nObserved  2.000  2.000  2.000\nExpected  2.144  2.248  1.160\nVariance  0.632  1.487  0.710\nUpper 95% 3.734  4.687  2.845\nLower 95% 0.554 -0.190 -0.525\n\n\nThe interval estimates can then be visualized to detect discrepancies between observed and expected values thus indicating social mechanisms at play in the generation of edges, and to detect interval overlap and potential interdependence between different types of edges (see Shafie 2015,2016; Shafie & Schoch 2021)."
  },
  {
    "objectID": "project/rmm/index.html#goodness-of-fit-tests",
    "href": "project/rmm/index.html#goodness-of-fit-tests",
    "title": "Multigraph Representation of Network Data",
    "section": "Goodness of fit tests",
    "text": "Goodness of fit tests\nGoodness of fits tests of multigraph models using Pearson (S) and information divergence (A) test statistics under the random stub matching (RSM) and by independent edge assignments (IEA) model, where the latter is either independent edge assignments of stubs (IEAS) or independent stub assignment (ISA). The tests are performed using goodness-of-fit measures between the edge multiplicity sequence of a specified model or an observed multigraph, and the expected multiplicity sequence according to a simple or composite hypothesis."
  },
  {
    "objectID": "project/rmm/index.html#simulated-goodness-of-fit-tests",
    "href": "project/rmm/index.html#simulated-goodness-of-fit-tests",
    "title": "Multigraph Representation of Network Data",
    "section": "Simulated goodness of fit tests",
    "text": "Simulated goodness of fit tests\nProbability distributions of test statistics, summary of tests, moments of tests statistics, adjusted test statistics, critical values, significance level according to asymptotic distribution, and power of tests can be examined using gof_sim given a specified model from which we simulate observed values from, and a null or non-null hypothesis from which we calculate expected values from. This in order to investigate the behavior of the null and non-null distributions of the test statistics and their fit to to asymptotic chi-square distributions.\n\nExample\nSimulated goodness of fit tests for multigraphs with n=4 nodes and m=10 edges.\n(1) Testing a simple IEAS hypothesis with degree sequence (6,6,6,2) against a RSM model with degrees (8,8,2,2):\n\ngof1 &lt;- gof_sim(m = 10, model = 'IEAS', deg.mod = c(8,8,2,2), \n                hyp = 'IEAS', deg.hyp = c(6,6,6,2))\n\n(2) Testing a correctly specified simple IEAS hypothesis with degree sequence (14,2,2,2):\n\ngof2 &lt;- gof_sim(m = 10, model = 'IEAS', deg.mod = c(14,2,2,2), \n                hyp = 'IEAS', deg.hyp = c(14,2,2,2))\n\nThe non-null (gof1) and null (gof2) distributions of the test statistics together with their asymptotic chi2-distribution can be visualized using ggplot2:\n \n(3) Testing a composite IEAS hypothesis against a RSM model with degree sequence (14,2,2,2):\n\ngof3 &lt;- gof_sim(m = 10, model = 'RSM', deg.mod = c(14,2,2,2), \n                hyp = 'IEAS', deg.hyp = 0)\n\n(4) Testing a composite ISA hypothesis against a ISA model with degree sequence (14,2,2,2):\n\ngof4 &lt;- gof_sim(m = 10, model = 'ISA', deg.mod = c(14,2,2,2), \n                hyp = 'ISA', deg.hyp = 0)\n\nThe non-null (gof3) and null (gof4) distributions of the test statistics can then be visualized as shown above to check their fit to the asymptotic χ²-distribution."
  },
  {
    "objectID": "project/rmm/index.html#performing-the-goodness-of-fit-test-on-your-data",
    "href": "project/rmm/index.html#performing-the-goodness-of-fit-test-on-your-data",
    "title": "Multigraph Representation of Network Data",
    "section": "Performing the goodness of fit test on your data",
    "text": "Performing the goodness of fit test on your data\nUse function gof_test to test whether the observed data follows IEA approximations of the RSM model. The null hypotheses can be simple or composite, although the latter is not recommended for small multigraphs as it is difficult to detect a false composite hypothesis under an RSM model and under IEA models (this can be checked and verified using gof_sim to simulate these cases).\nNon-rejection of the null implies that the approximations fit the data, thus implying that above statistics under the IEA models can be used to further analyze the observed network. Consider the following multigraph from the well known Florentine family network with marital. This multigraphs is aggregated based on the three actor attributes wealth (W), number of priorates (P) and total number of ties (T) which are all dichotomized to reflect high or low economic, political and social influence (details on the aggregation can be found in Shafie, 2015):\n\nThe multiplicity sequence represented as an upper triangular matrix for this mutigrpah is given by\n\nflor_m &lt;- t(matrix(c (0, 0, 1, 0, 0, 0, 0, 0,\n                      0, 0, 0, 0, 0, 0, 0, 0,\n                      0, 0, 0, 2, 0, 0, 1, 5,\n                      0, 0, 0, 0, 0, 0, 1, 1,\n                      0, 0, 0, 0, 0, 0, 1, 2,\n                      0, 0, 0, 0, 0, 0, 2, 1,\n                      0, 0, 0, 0, 0, 0, 0, 2,\n                      0, 0, 0, 0, 0, 0, 0, 1), nrow= 8, ncol=8))\n\nThe equivalence of adjacency matrix for the multigraph is given by\n\nflor_adj &lt;- flor_m+t(flor_m)\nflor_adj \n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]\n[1,]    0    0    1    0    0    0    0    0\n[2,]    0    0    0    0    0    0    0    0\n[3,]    1    0    0    2    0    0    1    5\n[4,]    0    0    2    0    0    0    1    1\n[5,]    0    0    0    0    0    0    1    2\n[6,]    0    0    0    0    0    0    2    1\n[7,]    0    0    1    1    1    2    0    2\n[8,]    0    0    5    1    2    1    2    2\n\n\nwith the diagonal representing the loops double counted (Shafie, 2016). The function get_degree_seq can now be used to find the degree sequence for this multigraph:\n\nflor_d &lt;- get_degree_seq(adj = flor_adj, type = 'multigraph')\nflor_d\n\n[1]  1  0  9  4  3  3  7 13\n\n\nNow we test whether the observed network fits the IEAS or the ISA model. The \\(p\\)-values for testing whether there is a significant difference between observed and expected edge multiplicity values according to the two approximate IEA models are given in the output tables below. Note that the asymptotic χ²-distribution has \\(r-1 = (n(n+1)/2) - 1 =35\\) degrees of freedom.\n\nflor_ieas_test &lt;- gof_test(flor_adj, 'multigraph', 'IEAS', flor_d, 35)\nflor_ieas_test\n\n  Stat dof Stat(obs) p-value\n1    S  35    15.762   0.998\n2    A  35    18.905   0.988\n\n\n\nflor_isa_test &lt;- gof_test(flor_adj, 'multigraph', 'ISA', flor_d, 35)\nflor_isa_test \n\n  Stat dof Stat(obs) p-value\n1    S  35    16.572   0.997\n2    A  35    19.648   0.983\n\n\nThe results show that we have strong evidence for the null such that we fail to reject it. Thus, there is not a significant difference between the observed and the expected edge multiplicity sequence according on the two IEA models. Statistics derived under these models presented above can thus be used to analyze the structure of these multigraphs."
  },
  {
    "objectID": "project/rmm/index.html#references",
    "href": "project/rmm/index.html#references",
    "title": "Multigraph Representation of Network Data",
    "section": "References",
    "text": "References\n\nShafie, T. (2015). A multigraph approach to social network analysis. Journal of Social Structure, 16. Link\nShafie, T. (2016). Analyzing local and global properties of multigraphs. The Journal of Mathematical Sociology, 40(4), 239-264. Link\nFrank, O., Shafie, T., (2018). Random Multigraphs and Aggregated Triads with Fixed Degrees. Network Science, 6(2), 232-250. Link\nShafie, T., Schoch, D. (2021) Multiplexity analysis of networks using multigraph representations. Statistical Methods & Applications 30, 1425–1444. Link\nShafie, T. (2022). Goodness of fit tests for random multigraph models, Journal of Applied Statistics. Link"
  },
  {
    "objectID": "project/movienetworks/index.html",
    "href": "project/movienetworks/index.html",
    "title": "Gender Dependent Structures in Charachter Networks",
    "section": "",
    "text": "Termeh Shafie"
  },
  {
    "objectID": "project/movienetworks/index.html#project-summary",
    "href": "project/movienetworks/index.html#project-summary",
    "title": "Gender Dependent Structures in Charachter Networks",
    "section": "Project Summary",
    "text": "Project Summary\nSince 2019, I’m sporadically working on a project with Pete Jones which aims to apply existing and novel quantitative methods (e.g. relational event models, entropy based natural language processing and multigraph representations), to study the gendered inequalities in popular cinema. See Pete’s website for more information and some awesome packages such as charinet which he has developed on the topic."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "",
    "section": "",
    "text": "Hello World\n\n\n\ntest\n\n\n\nThis is a test entry…\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#general",
    "href": "index.html#general",
    "title": "Termeh Shafie",
    "section": "General",
    "text": "General\n\n09.09.2024: Netropy 0.2.0\nThe new version of netropy is published on CRAN.\n\n\n05.06.2024: Inaugural Professorial Lecture 🤓\nI’ll be giving my inaugural professorial lecture. which is open for anyone who wants join. Expect a lot of big words, personal anecdotes, some mildly amusing jokes, and maybe even a technical hiccup or two 🤓📚✨  Join afterward for finger food and drinks 🥂  📅 Date: June 5th 📍 Venue: Data Theatre ZT1204, University of Konstanz\n\n\n20.02.2024: Multigraphr 0.2.0\nThe new version of multigraphr is published on CRAN. No user facing changes, just improved performance thanks to David Schoch.\n\n\n10.08.2023: Maternity Leave 🐣\nOn parental leave until end of the year.\n\n\n27.07.2023: Professor of Computational Social Science and Data Science\nToday I start my new position as a Professor at the Department of Politics and Public Administration, University of Konstanz."
  },
  {
    "objectID": "index.html#recent-publications",
    "href": "index.html#recent-publications",
    "title": "Termeh Shafie",
    "section": "Recent Publications",
    "text": "Recent Publications\n\n\n\n\n\n\nTitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nA complex systems view on physical activity with actionable insights for behaviour change\n\n\nAug 4, 2025\n\n\n\n\n\n\nThe interplay of structural features and observed dissimilarities among centrality indices\n\n\nDec 6, 2023\n\n\n\n\n\n\nGoodness of fit tests for random multigraph models\n\n\nJul 21, 2022\n\n\n\n\n\n\nNo matching items\n\n\nall publications"
  },
  {
    "objectID": "index.html#latest-talks",
    "href": "index.html#latest-talks",
    "title": "Termeh Shafie",
    "section": "Latest Talks",
    "text": "Latest Talks\n\n\n\n\n\n\n\n\n\n\n“The Interplay of Structural Features and Observed Dissimilarities Among Centrality Indices”\n\n\n\n\n\nJun 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Analysis & Modeling of Multivariate Networks\n\n\n\n\n\nJun 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n“Statistical Analysis of Multivariate Egocentric Networks”\n\n\n\n\n\nJul 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n“Statistical Entropy Analysis of Network Data”\n\n\n\n\n\nMay 5, 2022\n\n\n\n\n\nNo matching items\n\n\nmore talks"
  },
  {
    "objectID": "index.html#r-packages",
    "href": "index.html#r-packages",
    "title": "Termeh Shafie",
    "section": "R packages",
    "text": "R packages\n\n\n\n\n\n\n\n\n\nLatest published version 0.2.0 on CRAN is up to date.\n\n\n\n\n\n\n\n\n\n\n\nLatest published version 0.2.0 on CRAN is up to date.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#github-activity",
    "href": "index.html#github-activity",
    "title": "Termeh Shafie",
    "section": "GitHub Activity",
    "text": "GitHub Activity"
  },
  {
    "objectID": "index.html#social-media",
    "href": "index.html#social-media",
    "title": "Termeh Shafie",
    "section": "Social Media",
    "text": "Social Media\n\n \n\n    Bluesky\n\n\n\n \n\n\n\n\n    Toots"
  },
  {
    "objectID": "talks/SUNBELT2024/index.html",
    "href": "talks/SUNBELT2024/index.html",
    "title": "“The Interplay of Structural Features and Observed Dissimilarities Among Centrality Indices”",
    "section": "",
    "text": "The XLIV Social Networks Conference of INSNA\nTermeh Shafie\n\nJun 29, 2024\n\nAbstract\nAn abundance of centrality indices has been proposed which capture the importance of nodes in a network based on different structural features. While there remains a persistent belief that similarities in outcomes of indices is contingent on their technical definitions, a growing body of research shows that structural features affect observed similarities more than technicalities. We conduct a series of experiments on artificial networks to trace the influence of specific structural features on the similarity of indices which confirm previous results in the literature. Our analysis on 1163 real-world networks, however, shows that little of the observations on synthetic networks convincingly carry over to empirical settings. Our findings suggest that although it seems clear that (dis)similarities among centralities depend on structural properties of the network, using correlation type analyses do not seem to be a promising approach to uncover such connections.\n\n\n\n\n\nSlides"
  },
  {
    "objectID": "talks/EUSN2019/index.html",
    "href": "talks/EUSN2019/index.html",
    "title": "“Gender Dependent Structures of Dialogue Networks in Films”",
    "section": "",
    "text": "The 4th European Conference on Social Networks\nTermeh Shafie, Pete Jones\n\nSep 9, 2019\n\nAbstract\nWe present a novel approach to empirically analyse gender representation in films by using entropy tools. Multivariate entropy analysis is a general statistical method for analysing and testing compli- cated dependence structures in data consisting of repeated observations of variables with a common domain and discrete finite range spaces. Only nominal scale is required for each variable, thus making it very suitable for extracting semantic information from dialogues and scripts. Variables on ordinal or numerical scales can also be used, but they should be aggregated so that their ranges match the number of available repeated observations. Using a corpus of movie dialogue networks, we illustrate how these tools can enhance the analysis of gender representations in films by identifying factors associated with certain tropes of gendered conversation. The observed dialogue networks are transformed into a multidimensional data set comprising multiple node and edge attributes, where the latter is coded according to conversational content between pairs of same sex characters. Under the assumptions that associations between conversational content are shared by members of the same gender group, dialogues are used to reflect a meaning structure that characterize this group (stereotypical and non-stereotypical gender portrayals). Furthermore, we use the output of the entropy analyses to specify different structural models to be tested against null distributions according to stochastic blockmodels and random multigraph models. This application yields a deeper insight into the gendered nature of dialogue in film, an area in which little systematic empirical research exists. Moreover, the multivariate entropy analysis allows for the relationships between textual attributes, production-level attributes and endogenous network attributes to be explored together. This provides a novel opportunity to situate the character networks within their industrial context in a way that provides a new layer to the development of network tools for analysing fictional narrative texts.\n\n\n\n\n\nSlides"
  },
  {
    "objectID": "talks/SUNBELT2022/index.html",
    "href": "talks/SUNBELT2022/index.html",
    "title": "“Statistical Analysis of Multivariate Egocentric Networks”",
    "section": "",
    "text": "The XLII Social Networks Conference of INSNA\nTermeh Shafie\n\nJul 15, 2022\n\nAbstract\nIn this presentation we take a closer look at how multigraph representations (where multiple relations and edge loops are permitted) can be used to analyse structural and compositional features of egocentric networks with perceived alter-to-alter ties included. We aggregate the ego nets based on single or combined exogenous variables and several measures are used to uncover underlying social processes and phenomena with respect to the endogenous variables. Using theoretical underpinnings, we specify hypotheses that are tested using statistics under different probability models for multigraphs. The framework is exemplified and applied to the well-known data on Krackhardt’s high tech managers.\n\n\n\n\n\nSlides"
  },
  {
    "objectID": "talks/NETWORKS2021/index.html",
    "href": "talks/NETWORKS2021/index.html",
    "title": "“Goodness of Fit Tests for Random Multigraph Models”",
    "section": "",
    "text": "Network 2021 - A Joint Sunbelt and NetSci Conference\nTermeh Shafie\n\nJul 6, 2021\n\nAbstract\nGoodness of fit tests for two probabilistic multigraph models are presented. The first model is obtained by random stub matching given fixed degrees (RSM) so that edge assignments to vertex pair sites are dependent, and the second is obtained by independent edge assignments (IEA) according to a common probability distribution. The tests are performed using goodness of fit measures between the edge multiplicity sequence of an observed multigraph, and the expected multiplicity sequence according to a simple or composite hypothesis. Test statistics of Pearson type and of likelihood ratio type are used, and the expected values of the Pearson statistic under the different models are derived. Illustrations of test performances are given and the results indicate that even for small number of edges, the null distributions of both statistics are well approximated by their asymptotic χ2-distribution. The non-null distributions of the test statistics can be well approximated by proposed adjusted χ2-distributions used for power approximations. The influence of RSM on both test statistics is substantial for small number of edges and implies a shift of their distributions towards smaller values compared to what holds true for the null distributions under IEA.\n\n\n\n\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "talks/WiNS2022/index.html",
    "href": "talks/WiNS2022/index.html",
    "title": "“Statistical Entropy Analysis of Network Data”",
    "section": "",
    "text": "The Women in Network Science (WiNS) seminar\nTermeh Shafie\n\nMay 5, 2022\n\nAbstract\nIn multivariate statistics, there is an abundance of different measures of centrality and spread, many of which cannot be applied on variables measured on nominal or ordinal scale. Since network data in majority comprises such variables, alternative measures for analysing spread, flatness and association is needed. This is also of particular relevance given the special feature of interdependent observations in networks. In this presentation, multivariate entropy analysis is introduced and demonstrated as a general statistical method for finding, analysing and testing complicated dependence structures such as partial and conditional independencies, redundancies and functional dependencies. For example, consider the joint entropies of all pairs of variables which are used to construct a sequence of association graphs that represent variables by nodes and pairwise dependences above decreasing thresholds by links (cf. graphical models). By successively lowering the threshold from the maximum joint entropy to smaller occurring values, the sequence of graphs get more and more links. Connected components that are cliques represent dependent subsets of variables, and different components represent independent subsets of variables. Conditional independence between subsets of variables can be identified by omitting the subset corresponding to the conditioning variables. By comparing such graphs given different thresholds and with different components and cliques, specific structural models of multivariate dependence can be suggested and tested by divergence measures of goodness of fit. The roles of various entropy based measures are further highlighted and illustrated by applications on social network data. These applications show that important social phenomena and processes are often identified using these tools. The proposed framework is implemented in the R package ‘netropy’ and examples of using functions from this package are also presented.\n\n\n\n\n\nSlides\n\n\nExamples\n\n\nVideo"
  },
  {
    "objectID": "publications/ergm_framework_arch/index.html",
    "href": "publications/ergm_framework_arch/index.html",
    "title": "A Framework for Reconstructing Archaeological Networks using Exponential Random Graph Models",
    "section": "",
    "text": "Viviana Amati, Angus Mol, Termeh Shafie, Corinne Hofman, Ulrik Brandes\n\n2019-08-25"
  },
  {
    "objectID": "publications/ergm_framework_arch/index.html#abstract",
    "href": "publications/ergm_framework_arch/index.html#abstract",
    "title": "A Framework for Reconstructing Archaeological Networks using Exponential Random Graph Models",
    "section": "Abstract",
    "text": "Abstract\nReconstructing ties between archaeological contexts may contribute to explain and describe a variety of past social phenomena. Several models have been formulated to infer the structure of such archaeological networks. The underlying propositions about mechanisms regulating the formation of ties in the past are often articulated on a dyadic basis and therefore rarely account for dependencies among ties. Here, we present a general framework in which we combine exponential random graph models with archaeological substantiations of mechanisms that may be responsible for network formation to account for tie dependence. We use data collected over a set of sites in the Caribbean during the period AD 100 - 400 to illustrate the steps to obtain a network reconstruction.\nJournal of Archaeological Method and Theory 27, 192-219"
  },
  {
    "objectID": "publications/multigraph_approach/index.html",
    "href": "publications/multigraph_approach/index.html",
    "title": "A Multigraph Approach to Social Network Analysis",
    "section": "",
    "text": "Termeh Shafie\n\n2015-06-01"
  },
  {
    "objectID": "publications/multigraph_approach/index.html#abstract",
    "href": "publications/multigraph_approach/index.html#abstract",
    "title": "A Multigraph Approach to Social Network Analysis",
    "section": "Abstract",
    "text": "Abstract\nMultigraphs are graphs where multiple edges and edge loops are permitted. The main purpose of this article is to show the versatility of a multigraph approach when analysing social networks. Multigraph data structures are described and it is exemplified how they naturally occur in many contexts but also how they can be constructed by different kinds of aggregation in graphs. Special attention is given to a random multigraph model based on independent edge assignments to sites of vertex pairs and some useful measures of the local and global structure under this model are presented. Further, it is shown how some general measures of simplicity and complexity of multigraphs are easily handled under the present model.\n‘Journal of Social Structure 16, 1-22’"
  },
  {
    "objectID": "publications/sna_chapter/index.html",
    "href": "publications/sna_chapter/index.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "Termeh Shafie\n\n2020-06-17"
  },
  {
    "objectID": "publications/sna_chapter/index.html#abstract",
    "href": "publications/sna_chapter/index.html#abstract",
    "title": "Social Network Analysis",
    "section": "Abstract",
    "text": "Abstract\nSocial networks comprise a set of nodes or actors (representing, e.g., individuals, groups, organisations) that are pairwise connected by edges or ties (representing, e.g., relationships, interactions, communication). The social systems arising exhibit patterns of interest, and social network analysis is the study of how and why these patterns emerge, sustain, and evolve. Of primary interest is thus to understand and describe the social processes that support the observed structure. These processes are founded in theories about network representation and theories about observed social phenomena. The benefit from network conceptualisation is thus obtained by outlining the association and distinction between these theories. This entry serves as an introduction to fundamental network concepts and analytical approaches, their potential for studying social phenomena, and a description of why they are central to theoretical constructs. This entry also provides a short introduction to statistical network modelling for cross-sectional and longitudinal network data.\nSAGE Research Methods Foundations"
  },
  {
    "objectID": "publications/complexity_multigraphs/index.html",
    "href": "publications/complexity_multigraphs/index.html",
    "title": "Complexity of Families of Multigraphs",
    "section": "",
    "text": "Ove Frank, Termeh Shafie\n\n2012-12-01"
  },
  {
    "objectID": "publications/complexity_multigraphs/index.html#abstract",
    "href": "publications/complexity_multigraphs/index.html#abstract",
    "title": "Complexity of Families of Multigraphs",
    "section": "Abstract",
    "text": "Abstract\nThis article describes families of finite multigraphs with labeled or unlabeled edges and vertices. It shows how size and complexity vary for different types of equivalence classes of graphs defined by ignoring only edge labels or ignoring both edge and vertex labels. Complexity is quantified by the distribution of edge multiplicities, and different complexity measures are discussed. Basic occupancy models for multigraphs are used to illustrate different graph distributions on isomorphism and complexity. The loss of information caused by ignoring edge and vertex labels is quantified by entropy and joint information that provide tools for studying properties of and relations between different graph families.\nIn 2012 JSM Proceedings: Papers Presented at the Joint Statistical Meetings, San Diego, California, July 28-August 2, 2012, and Other ASA-sponsored Conferences, American Statistical Association, 2012"
  },
  {
    "objectID": "publications/gof_multigraph/index.html",
    "href": "publications/gof_multigraph/index.html",
    "title": "Goodness of fit tests for random multigraph models",
    "section": "",
    "text": "Termeh Shafie\n\n2022-07-21"
  },
  {
    "objectID": "publications/gof_multigraph/index.html#abstract",
    "href": "publications/gof_multigraph/index.html#abstract",
    "title": "Goodness of fit tests for random multigraph models",
    "section": "Abstract",
    "text": "Abstract\nGoodness of fit tests for two probabilistic multigraph models are presented. The first model is random stub matching given fixed degrees (RSM) so that edge assignments to vertex pair sites are dependent, and the second is independent edge assignments (IEA) according to a common probability distribution. Tests are performed using goodness of fit measures between the edge multiplicity sequence of an observed multigraph, and the expected one according to a simple or composite hypothesis. Test statistics of Pearson type and of likelihood ratio type are used, and the expected values of the Pearson statistic under the different models are derived. Test performances based on simulations indicate that even for small number of edges, the null distributions of both statistics are well approximated by their asymptotic χ2-distribution. The non-null distributions of the test statistics can be well approximated by proposed adjusted χ2-distributions used for power approximations. The influence of RSM on both test statistics is substantial for small number of edges and implies a shift of their distributions towards smaller values compared to what holds true for the null distributions under IEA. Two applications on social networks are included to illustrate how the tests can guide in the analysis of social structure.\nJournal of Applied Statistics"
  },
  {
    "objectID": "publications/hypergraph/index.html",
    "href": "publications/hypergraph/index.html",
    "title": "Hypergraph Representations: A Study of Carib Attacks on Colonial Forces, 1509-1700",
    "section": "",
    "text": "Termeh Shafie, David Schoch, Jimmy Mans, Corinne Hofman, Ulrik Brandes\n\n2017-10-25"
  },
  {
    "objectID": "publications/hypergraph/index.html#abstract",
    "href": "publications/hypergraph/index.html#abstract",
    "title": "Hypergraph Representations: A Study of Carib Attacks on Colonial Forces, 1509-1700",
    "section": "Abstract",
    "text": "Abstract\nNetwork data consisting of recorded historical events can be represented as hyper-graphs where the ties or events can connect any number of nodes or event related attributes. In this paper, we perform a centrality analysis of a directed hypergraph representing attacks by indigenous peoples from the Lesser Antilles on European colonial settlements, 1509-1700. The results of central attacks with respect to at- tacked colonial force, member of attack alliances, and year and location of attack are discussed and compared to a non-relational exploratory analysis of the data. This comparison points to the importance of a mixed methods approach to enhance the analysis and to obtain a complementary understanding of a network study.\npublication: ‘Journal of Historical Network Research 1(1), 52-70’"
  },
  {
    "objectID": "publications/multiplexity/index.html",
    "href": "publications/multiplexity/index.html",
    "title": "Multiplexity Analysis of Networks using Multigraph Representations",
    "section": "",
    "text": "Termeh Shafie, David Schoch\n\n2021-09-30"
  },
  {
    "objectID": "publications/multiplexity/index.html#abstract",
    "href": "publications/multiplexity/index.html#abstract",
    "title": "Multiplexity Analysis of Networks using Multigraph Representations",
    "section": "Abstract",
    "text": "Abstract\nMultivariate networks comprising several compositional and structural variables can be represented as multigraphs by various forms of aggregations based on vertex attributes. We propose a framework to perform exploratory and confirmatory multiplexity analysis of aggregated multigraphs in order to find relevant associations between vertex and edge attributes. The exploration is performed by comparing frequencies of the different edges within and between aggregated vertex categories, while the confirmatory analysis is performed using derived complexity or multiplexity statistics under different random multigraph models. These statistics are defined by the distribution of edge multiplicities and provide information on the covariation and dependencies of different edges given vertex attributes. The presented approach highlights the need to further analyse and model structural dependencies with respect to edge entrainment. We illustrate the approach by applying it on a well known multivariate network dataset which has previously been analysed in the context of multiplexity.\npublication: Statistical Methods & Applications 30, 1425–1444"
  },
  {
    "objectID": "publications/multivariate_entropy_analysis/index.html",
    "href": "publications/multivariate_entropy_analysis/index.html",
    "title": "Multivariate Entropy Analysis of Network Data",
    "section": "",
    "text": "Over Frank, Termeh Shafie\n\n2016-01-02"
  },
  {
    "objectID": "publications/multivariate_entropy_analysis/index.html#abstract",
    "href": "publications/multivariate_entropy_analysis/index.html#abstract",
    "title": "Multivariate Entropy Analysis of Network Data",
    "section": "Abstract",
    "text": "Abstract\nMultigraphs with numerical or qualitative attributes defined on vertices and edges can benefit from systematic methods based on multivariate entropies for describing and analysing the interdependencies that are present between vertex and edge attributes. This is here illustrated by application of these tools to a subset of data on the social relations among Renaissance Florentine families collected by John Padgett. Using multivariate entropies we show how it is possible to systematically check for tendencies in data that can be described as independencies or conditional independencies, or as dependencies allowing certain combinations of variables to predict other variables. We also show how different structural models can be tested by divergence measures obtained from the multivariate entropies.\nBulletin of Sociological Methodology/Bulletin de Méthodologie Sociologique 120(1), 45-63"
  },
  {
    "objectID": "publications/reconstructing_arch_nets/index.html",
    "href": "publications/reconstructing_arch_nets/index.html",
    "title": "Reconstructing Archaeological Networks with Structural Holes",
    "section": "",
    "text": "Viviana Amati, Termeh Shafie, Ulrik Brandes\n\n2018-03-25"
  },
  {
    "objectID": "publications/reconstructing_arch_nets/index.html#abstract",
    "href": "publications/reconstructing_arch_nets/index.html#abstract",
    "title": "Reconstructing Archaeological Networks with Structural Holes",
    "section": "Abstract",
    "text": "Abstract\nModel-based reconstruction is an approach to infer network structures where they cannot be observed. For archaeological networks, several models based on assumptions concerning distance among sites, site size, or costs and benefits have been proposed to infer missing ties. Since these assumptions are formulated at a dyadic level, they do not provide means to express dependencies among ties and therefore include less plausible network scenarios. In this paper we investigate the use of network models that explicitly incorporate tie dependence. In particular, we consider exponential random graph models, and show how they can be applied to reconstruct networks coherent with Burt’s arguments on closure and structural holes (Burt 2001). The approach is illustrated on data from the Middle Bronze Age in the Aegean. authors:\nJournal of Archaeological Method and Theory 25(1), 226-253"
  },
  {
    "objectID": "publications/interplay_str_cent/index.html",
    "href": "publications/interplay_str_cent/index.html",
    "title": "The interplay of structural features and observed dissimilarities among centrality indices",
    "section": "",
    "text": "David Schoch, Termeh Shafie\n\n2023-12-06"
  },
  {
    "objectID": "publications/interplay_str_cent/index.html#abstract",
    "href": "publications/interplay_str_cent/index.html#abstract",
    "title": "The interplay of structural features and observed dissimilarities among centrality indices",
    "section": "Abstract",
    "text": "Abstract\nAn abundance of centrality indices has been proposed which capture the importance of nodes in a network based on different structural features. While there remains a persistent belief that similarities in outcomes of indices is contingent on their technical definitions, a growing body of research shows that structural features affect observed similarities more than technicalities. We conduct a series of experiments on artificial networks to trace the influence of specific structural features on the similarity of indices which confirm previous results in the literature. Our analysis on 1163 real-world networks, however, shows that little of the observations on synthetic networks convincingly carry over to empirical settings. Our findings suggest that although it seems clear that (dis)similarities among centralities depend on structural properties of the network, using correlation type analyses do not seem to be a promising approach to uncover such connections.\nSocial Networks"
  },
  {
    "objectID": "publications/aggregated_triads/index.html",
    "href": "publications/aggregated_triads/index.html",
    "title": "Random Multigraphs and Aggregated Triads with Fixed Degrees",
    "section": "",
    "text": "Ove Frank, Termeh Shafie\n\n2018-01-25"
  },
  {
    "objectID": "publications/aggregated_triads/index.html#abstract",
    "href": "publications/aggregated_triads/index.html#abstract",
    "title": "Random Multigraphs and Aggregated Triads with Fixed Degrees",
    "section": "Abstract",
    "text": "Abstract\nRandom multigraphs with fixed degrees are obtained by the configuration model or by so called random stub matching. New combinatorial results are given for the global probability distribution of edge multiplicities and its marginal local distributions of loops and edges. The number of multigraphs on triads is determined for arbitrary degrees, and aggregated triads are shown to be useful for analyzing regular and almost regular multigraphs. Relationships between entropy and complexity are given and numerically illustrated for multigraphs with different number of vertices and specified average and variance for the degrees.\nNetwork Science 6(2), 232-250"
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "A complex systems view on physical activity with actionable insights for behaviour change\n\n\nA perspective that takes a complex systems view on physical activity.\n\n\n\n\n\n\n\n\n\n\n\n\nThe interplay of structural features and observed dissimilarities among centrality indices\n\n\nThe association of network topology with dissimilarities of indices is assessed\n\n\n\n\n\n\n\n\n\n\n\n\nGoodness of fit tests for random multigraph models\n\n\nGoodness of fit tests for different probability models for random multigraphs.\n\n\n\n\n\n\n\n\n\n\n\n\nMultiplexity Analysis of Networks using Multigraph Representations\n\n\nA method for performing multiplexity analysis in social networks with several node covariates is presented.\n\n\n\n\n\n\n\n\n\n\n\n\nSocial Network Analysis\n\n\nA review chapter on social network analysis aimed towards undergraduate students.\n\n\n\n\n\n\n\n\n\n\n\n\nA Framework for Reconstructing Archaeological Networks using Exponential Random Graph Models\n\n\nWe present a general framework in which we combine exponential random graph models with archaeological substantiations of mechanisms for network formation.\n\n\n\n\n\n\n\n\n\n\n\n\nReconstructing Archaeological Networks with Structural Holes\n\n\nWe consider exponential random graph models, and show how they can be applied to reconstruct networks coherent with Burt’s arguments on closure and structural holes.\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Multigraphs and Aggregated Triads with Fixed Degrees\n\n\nNew combinatorial results are given for the global probability distribution of edge multiplicities and its marginal local distributions of loops and edges.\n\n\n\n\n\n\n\n\n\n\n\n\nHypergraph Representations: A Study of Carib Attacks on Colonial Forces, 1509-1700\n\n\nWe perform a centrality analysis of a directed hypergraph representing attacks by indigenous peoples from the Lesser Antilles on European colonial settlements, 1509-1700.\n\n\n\n\n\n\n\n\n\n\n\n\nInvestigating Human Geographic Origins using Dual-Isotope (87Sr/86Sr, d18O) Assignment approaches\n\n\nWe develop and test more standardized and quantitative approaches to geographic assignment of individual origins using multivariate isotopic data.\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Local and Global Properties of Multigraphs\n\n\nThe local and global structures of undirected multigraphs under two random multigraph models are analyzed and compared.\n\n\n\n\n\n\n\n\n\n\n\n\nNation Building and Social Signaling in Southern Ontario AD 1350-1650\n\n\nSocial network analysis is used to demonstrates the signaling practices reflecting regional patterns.\n\n\n\n\n\n\n\n\n\n\n\n\nMultivariate Entropy Analysis of Network Data\n\n\nWe show how it is possible to systematically check for tendencies in data, such as independencies or conditional independencies, using multivariate entropies.\n\n\n\n\n\n\n\n\n\n\n\n\nData Protection for Online Social Networks and P-Stability for Graphs\n\n\nWe consider different approaches for data privacy in online social networks and for developing graph protection.\n\n\n\n\n\n\n\n\n\n\n\n\nA Multigraph Approach to Social Network Analysis\n\n\nThe theoretical background for analyzing multivariate social networks using multigraph representations is introduced.\n\n\n\n\n\n\n\n\n\n\n\n\nComplexity of Families of Multigraphs\n\n\nComplexity measured for multigraphs are specified and their applicability is discussed.\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/pa_complexsystems/index.html",
    "href": "publications/pa_complexsystems/index.html",
    "title": "A complex systems view on physical activity with actionable insights for behaviour change",
    "section": "",
    "text": "Julia Schüler, Matti T. J. Heino, Natàlia Balagué, Angel M. Chater, Markus Gruber, Martina Kanning, Daniel Keim, Daniela Mier, Maria Moreno-Villanueva, Fridtjof W. Nussbeck, Jens Pruessner, Termeh Shafie, Michael Schwenk, Maik Bieleke\n\n2025-08-04"
  },
  {
    "objectID": "publications/pa_complexsystems/index.html#abstract",
    "href": "publications/pa_complexsystems/index.html#abstract",
    "title": "A complex systems view on physical activity with actionable insights for behaviour change",
    "section": "Abstract",
    "text": "Abstract\nPhysical inactivity and its associated health and economic burdens continue to rise despite decades of interdisciplinary research aimed at promoting physical activity. This Perspective takes a complex systems view on physical activity, proposing that at least two layers of complexity should be considered: (1) interactions between various physiological, psychological, social and environmental systems; and (2) their dynamic interactions across time. To address this complexity, all stages of the research process—from theory and measurement to study design, analysis and interventions—must be aligned with a complex systems perspective. This alignment requires intensive interdisciplinary collaboration and an integration of basic and applied research beyond current research practices to create transdisciplinary solutions. We offer actionable insights that bridge the gap between abstract theoretical approaches (for example, complex systems and attractor landscape frameworks of behaviour change) and practical research on physical activity, thereby laying a foundation for more effective behaviour change interventions.\nNature Human Behaviour"
  },
  {
    "objectID": "publications/global_local_multigraphs/index.html",
    "href": "publications/global_local_multigraphs/index.html",
    "title": "Analyzing Local and Global Properties of Multigraphs",
    "section": "",
    "text": "Termeh Shafie\n\n2016-09-25"
  },
  {
    "objectID": "publications/global_local_multigraphs/index.html#abstract",
    "href": "publications/global_local_multigraphs/index.html#abstract",
    "title": "Analyzing Local and Global Properties of Multigraphs",
    "section": "Abstract",
    "text": "Abstract\nThe local structure of undirected multigraphs under two random multigraph models is analyzed and compared. The first model generates multigraphs by randomly coupling pairs of stubs according to a fixed degree sequence so that edge assignments to vertex pair sites are dependent. The second model is a simplification that ignores the dependency between the edge assignments. It is investigated when this ignorance is justified so that the simplified model can be used as an approximation, thus facilitating the structural analysis of network data with multiple relations and loops. The comparison is based on the local properties of multigraphs given by marginal distribution of edge multiplicities and some local properties that are aggregations of global properties.\nJournal of Mathematical Sociology 40(4), 239-264"
  },
  {
    "objectID": "publications/data_protection/index.html",
    "href": "publications/data_protection/index.html",
    "title": "Data Protection for Online Social Networks and P-Stability for Graphs",
    "section": "",
    "text": "Vicenç Torra, Termeh Shafie, Julián Salas\n\n2016-01-01"
  },
  {
    "objectID": "publications/data_protection/index.html#abstract",
    "href": "publications/data_protection/index.html#abstract",
    "title": "Data Protection for Online Social Networks and P-Stability for Graphs",
    "section": "Abstract",
    "text": "Abstract\nGraphs can be used as a model for online social networks. In this framework, vertices represent individuals and edges relationships between individuals. In recent years, different approaches have been considered to offer data privacy to online social networks and for developing graph protection. Perturbative approaches are formally defined in terms of perturbation and modification of graphs. In this paper, we discuss the concept of P -stability on graphs and its relation to data privacy. The concept of P-stability is rooted in the number of graphs given a fixed degree sequence. In this paper, we show that for any graph there exists a class of P-stable graphs. This result implies that there is a fully polynomial randomized approximation for graph masking for the graphs in the class. In order to further refine the classification of a given graph, we introduce the concept of natural class of a graph. It is based on a class of scale-free networks.\nIEEE Transactions on Emerging Topics in Computing 4(3), 374-381"
  },
  {
    "objectID": "publications/isotopes/index.html",
    "href": "publications/isotopes/index.html",
    "title": "Investigating Human Geographic Origins using Dual-Isotope (87Sr/86Sr, d18O) Assignment approaches",
    "section": "",
    "text": "Jason E Laffoon, Till F Sonnemann, Termeh Shafie, Corinne L Hofman, Ulrik Brandes, Gareth R Davies\n\n2017-02-25"
  },
  {
    "objectID": "publications/isotopes/index.html#abstract",
    "href": "publications/isotopes/index.html#abstract",
    "title": "Investigating Human Geographic Origins using Dual-Isotope (87Sr/86Sr, d18O) Assignment approaches",
    "section": "Abstract",
    "text": "Abstract\nSubstantial progress in the application of multiple isotope analyses has greatly improved the ability to identify nonlocal individuals amongst archaeological populations over the past decades. More recently the development of large scale models of spatial isotopic variation (isoscapes) has contributed to improved geographic assignments of human and animal origins. Persistent challenges remain, however, in the accurate identification of individual geographic origins from skeletal isotope data in studies of human (and animal) migration and provenance. In an attempt to develop and test more standardized and quantitative approaches to geographic assignment of individual origins using isotopic data two methods, combining 87Sr/86Sr and d18O isoscapes, are examined for the Circum-Caribbean region 1) an Interval approach using a defined range of fixed isotopic variation per location and 2) a Likelihood assignment approach using univariate and bivariate probability density functions. These two methods are tested with enamel isotope data from a modern sample of known origin from Caracas, Venezuela and further explored with two archaeological samples of unknown origin recovered from Cuba and Trinidad. The results emphasize both the potential and limitation of the different approaches. Validation tests on the known origin sample exclude most areas of the Circum-Caribbean region and correctly highlight Caracas as a possible place of origin with both approaches. The positive validation results clearly demonstrate the overall efficacy of a dual-isotope approach to geoprovenance. The accuracy and precision of geographic assignments may be further improved by better understanding of the relationships between environmental and biological isotope variation; continued development and refinement of relevant isoscapes; and the eventual incorporation of a broader array of isotope proxy data.\nPLoS ONE 12(2), e0172562"
  },
  {
    "objectID": "publications/ontario_sna/index.html",
    "href": "publications/ontario_sna/index.html",
    "title": "Nation Building and Social Signaling in Southern Ontario AD 1350-1650",
    "section": "",
    "text": "John P Hart, Termeh Shafie, Jennifer Birch, Susan Dermarkar, Ronald F Williamson\n\n2016-05-25"
  },
  {
    "objectID": "publications/ontario_sna/index.html#abstract",
    "href": "publications/ontario_sna/index.html#abstract",
    "title": "Nation Building and Social Signaling in Southern Ontario AD 1350-1650",
    "section": "Abstract",
    "text": "Abstract\nPottery is a mainstay of archaeological analysis worldwide. Often, high proportions of the pottery recovered from a given site are decorated in some manner. In northern Iroquoia, late pre-contact pottery and early contact decoration commonly occur on collars—thick bands of clay that encircle a pot and extend several centimeters down from the lip. These decorations constitute signals that conveyed information about a pot’s user(s). In southern Ontario the period A.D. 1350 to 1650 witnessed substantial changes in socio-political and settlement systems that included population movement, coalescence of formerly separate communities into large villages and towns, waxing and waning of regional strife, the formation of nations, and finally the development of three confederacies that each occupied distinct, constricted areas. Social network analysis demonstrates that signaling practices changed to reflect these regional patterns. Networks become more consolidated through time ultimately resulting in a “small world” network with small degrees of separation between sites reflecting the integration of communities within and between the three confederacies.\nPLoS ONE 11(5), e0156178"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "“The Interplay of Structural Features and Observed Dissimilarities Among Centrality Indices”\n\n\nThe XLIV Social Networks Conference of INSNA\n\n\n\nJun 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Analysis & Modeling of Multivariate Networks\n\n\nInaugural Lecture, University of Konstanz\n\n\n\nJun 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Analyzing Social Structure using Multigraph Representations”\n\n\nPresentation @ Centre Marc Bloch, Berlin\n\n\n\nNov 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Statistical Analysis of Multivariate Egocentric Networks”\n\n\nThe XLII Social Networks Conference of INSNA\n\n\n\nJul 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Statistical Entropy Analysis of Network Data”\n\n\nThe Women in Network Science (WiNS) seminar\n\n\n\nMay 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiplexity Analysis of Networks using Multigraph Representations\n\n\nThe 5th European Conference on Social Networks\n\n\n\nSep 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Goodness of Fit Tests for Random Multigraph Models”\n\n\nNetwork 2021 - A Joint Sunbelt and NetSci Conference\n\n\n\nJul 6, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n“Gender Dependent Structures of Dialogue Networks in Films”\n\n\nThe 4th European Conference on Social Networks\n\n\n\nSep 9, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/Inaugural-KN/index.html",
    "href": "talks/Inaugural-KN/index.html",
    "title": "Statistical Analysis & Modeling of Multivariate Networks",
    "section": "",
    "text": "Inaugural Lecture, University of Konstanz\nTermeh Shafie\n\nJun 5, 2024\n\n\n\n\nSlides"
  },
  {
    "objectID": "talks/CMB2022/index.html",
    "href": "talks/CMB2022/index.html",
    "title": "“Analyzing Social Structure using Multigraph Representations”",
    "section": "",
    "text": "Presentation @ Centre Marc Bloch, Berlin\nTermeh Shafie\n\nNov 2, 2022\n\nAbstract\nMultivariate networks consist of a vertex set with at least one type of edge between pairs of vertices and with numerical and/or qualitative attributes on the vertices and the edges. These networks provide a more accurate representation of social structure than univariate networks, but analysing them introduce technical and computational complexity. In this presentation, we consider exploratory and confirmatory analysis of multivariate networks represented as multigraphs. Multigraph data structure is described with examples of their natural appearance, together with a description of the possibility to obtain multigraphs using blocking, aggregation and scaling. Two random multigraph models are presented and several statistics under these models are derived. It is shown how these statistics can be used to analyse local and global network properties in order to convey important social phenomena and processes. Applications are used to illustrate the applicability of the presented approach, including when analysing the gendered inequalities in popular cinema by using dialogue networks. Moreover, some examples are provided on how to use the package ‘multigraphr’ in the statistical software R to perform the analysis.\n\n\n\n\n\nSlides"
  },
  {
    "objectID": "talks/EUSN2021/index.html",
    "href": "talks/EUSN2021/index.html",
    "title": "Multiplexity Analysis of Networks using Multigraph Representations",
    "section": "",
    "text": "The 5th European Conference on Social Networks\nTermeh Shafie, David Schoch\n\nSep 7, 2021\n\nAbstract\nMultivariate networks comprising several compositional and structural variables can be represented as multigraphs by various forms of aggregations based on vertex attributes. We propose a framework to perform exploratory and confirmatory multiplexity analysis of aggregated multigraphs in order to find relevant associations between vertex and edge attributes. The exploration is performed by comparing the frequencies of the different edges within and between aggregated vertex categories, while the confirmatory analysis is performed using derived complexity or multiplexity statistics under different random multigraph models. These statistics are defined by the distribution of edge multiplicities and provide information on the covariation and dependencies of different edges given vertex attributes. The presented approach highlights the need to further analyse and model structural dependencies with respect to edge entrainment. We illustrate the approach by applying it on a well known multivariate network dataset which has previously been analysed in the context of multiplexity.\n\n\n\n\n\nSlides"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Termeh Shafie",
    "section": "",
    "text": "👋 Hi, I’m Termeh\nI’m a statistician with more than a decade of experience in developing methods and models to analyze multivariate social networks. I’m also the developer and maintainer of two R packages on the topic.\nInterdisciplinary applications are close to heart and I have had many such collaborations in different branches of computational social science and digital humanities. You can find information on some of the methodological and empirical projects that I have worked on or currently work on here.\nA bit of random trivia about me: my Erdős number is 3 and I have an irrational obsession with Pokémon."
  },
  {
    "objectID": "about/index.html#education",
    "href": "about/index.html#education",
    "title": "Termeh Shafie",
    "section": "Education",
    "text": "Education\n\nPhD in Statistics | 2013 | Stockholm University | Stockholm, Sweden\nLicentiate of Philosophy in Statistic | 2008 | Umeå University | Umeå Sweden\nMaster of Social Science (major in Statistics) | 2006 | Umeå University | Umeå Sweden\nMaster of Science in Public Administration and Economics | 2006 | Umeå University | Umeå Sweden"
  },
  {
    "objectID": "about/index.html#experience",
    "href": "about/index.html#experience",
    "title": "Termeh Shafie",
    "section": "Experience",
    "text": "Experience\n\nProfessor of Computational Social Science and Data Science (W2) | July 2023 – | Department of Politics and Public Administration | Center for Data and Methods | University of Konstanz\nInterim Professor | Apr 2023 – July 2023 | Department of Politics and Public Administration | University of Konstanz\nSenior Researcher | Jul 2022 – Mar 2023 | Department of Computational Social Science GESIS – Leibniz Institute for the Social Sciences\nLecturer in Social Statistics | Jul 2018 – Apr 2022 | Department of Social Statistics | The University of Manchester\nPostdoctoral Researcher | Nov 2017 – Jun 2018 | Department of Humanities, Social and Political Sciences | ETH Zürich\nPostdoctoral Researcher | Sep 2013 – Oct 2017 | Department of Computer and Information Science | University of Konstanz"
  },
  {
    "objectID": "blog/posts/test/index.html",
    "href": "blog/posts/test/index.html",
    "title": "Hello World",
    "section": "",
    "text": "Termeh Shafie"
  },
  {
    "objectID": "project/convoys/index.html",
    "href": "project/convoys/index.html",
    "title": "Network of Interconnected Convoys",
    "section": "",
    "text": "Termeh Shafie"
  },
  {
    "objectID": "project/convoys/index.html#more-info-coming-soon",
    "href": "project/convoys/index.html#more-info-coming-soon",
    "title": "Network of Interconnected Convoys",
    "section": "More info coming soon",
    "text": "More info coming soon"
  },
  {
    "objectID": "project/seand/index.html",
    "href": "project/seand/index.html",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "",
    "text": "Termeh Shafie"
  },
  {
    "objectID": "project/seand/index.html#project-summary",
    "href": "project/seand/index.html#project-summary",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "Project summary",
    "text": "Project summary\nIn multivariate statistics, there is an abundance of different measures of centrality and spread, many of which cannot be applied on variables measured on nominal or ordinal scale. Since network data in majority comprises such variables, alternative measures for analyzing spread, flatness and association is needed. This is also of particular relevance given the special feature of interdependent observations in networks.\nMultivariate entropy analysis is a general statistical method for analyzing and finding dependence structure in data consisting of repeated observations of variables with a common domain and with discrete finite range spaces. Only nominal scale is required for each variable, so only the size of the variable’s range space is important but not its actual values. Variables on ordinal or numerical scales, even continuous numerical scales, can be used, but they should be aggregated so that their ranges match the number of available repeated observations. By investigating the frequencies of occurrences of joint variable outcomes, complicated dependence structures, partial independence and conditional independence as well as redundancies and functional dependence can be found.\nSince 2015, I am working with Ove Frank and Krzysztof Nowicki on a project in which we build a systematic framework for using statistical entropy tools to analyze network data.\nThe proposed framework is implemented in the R package ‘netropy’ and a description of various functions implemented in the package are given in the following. More details are provided in the package vignettes and the references listed."
  },
  {
    "objectID": "project/seand/index.html#r-package-netropy",
    "href": "project/seand/index.html#r-package-netropy",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "R package netropy",
    "text": "R package netropy\n\n\nPackage overview\n  \nThis package introduces these entropy tools in the context of network data. Brief description of various functions implemented in the package are given in the following but more details are provided in the package vignettes and the references listed.\n\n\nInstallation\nYou can install the released version of netropy from CRAN with:\ninstall.packages(\"netropy\")\nThe development version from GitHub with:\n# install.packages(\"devtools\")\ndevtools::install_github(\"termehs/netropy\")\nTo load the package:\n\nlibrary('netropy')\n\n\n\nLoading internal data\nThe different entropy tools are explained and illustrated by exploring data from a network study of a corporate law firm, which has previously been analysed by several authors (link). The data set is included in the package as a list with objects representing adjacency matrices for each of the three networks advice (directed), friendship (directed) and co-work (undirected), together with a data frame comprising 8 attributes on each of the 71 lawyers.\nTo load the data, extract each object and assign the correct names to them:\n\ndata(lawdata) \nadj.advice &lt;- lawdata[[1]]\nadj.friend &lt;- lawdata[[2]]\nadj.cowork &lt;-lawdata[[3]]\ndf.att &lt;- lawdata[[4]]"
  },
  {
    "objectID": "project/seand/index.html#variable-domains-and-data-editing",
    "href": "project/seand/index.html#variable-domains-and-data-editing",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "Variable domains and data editing",
    "text": "Variable domains and data editing\nA requirement for the applicability of these entropy tools is the specification of discrete variables with finite range spaces on the same domain: either node attributes/vertex variables, edges/dyad variables or triad variables. These can be either observed or transformed as shown in the following using the above example data set.\nWe have 8 vertex variables with 71 observations, two of which (years and age) are numerical and needs categorization based on their cumulative distributions. This categorization is in details described in the vignette “variable domains and data editing”. Here we just show the new dataframe created (note that variable senior is omitted as it only comprises unique values and that we edit all variable to start from 0):\n\natt.var &lt;-\n  data.frame(\n    status   = df.att$status-1,\n    gender   = df.att$gender,\n    office   = df.att$office-1,\n    years    = ifelse(df.att$years &lt;= 3,0,\n                      ifelse(df.att$years &lt;= 13,1,2)),\n    age      = ifelse(df.att$age &lt;= 35,0,\n                      ifelse(df.att$age &lt;= 45,1,2)),\n    practice = df.att$practice,\n    lawschool= df.att$lawschool-1\n    )\nhead(att.var)\n\n  status gender office years age practice lawschool\n1      0      1      0     2   2        1         0\n2      0      1      0     2   2        0         0\n3      0      1      1     1   2        1         0\n4      0      1      0     2   2        0         2\n5      0      1      1     2   2        1         1\n6      0      1      1     2   2        1         0\n\n\nThese vertex variables can be transformed into dyad variables by using the function get_dyad_var(). Observed node attributes in the dataframe att_var are then transformed into pairs of individual attributes. For example, status with binary outcomes is transformed into dyads having 4 possible outcomes (0,0), (0,1), (1,0), (1,1):\n\ndyad.status    &lt;- get_dyad_var(att.var$status, type = 'att')\ndyad.gender    &lt;- get_dyad_var(att.var$gender, type = 'att')\ndyad.office    &lt;- get_dyad_var(att.var$office, type = 'att')\ndyad.years     &lt;- get_dyad_var(att.var$years, type = 'att')\ndyad.age       &lt;- get_dyad_var(att.var$age, type = 'att')\ndyad.practice  &lt;- get_dyad_var(att.var$practice, type = 'att')\ndyad.lawschool &lt;- get_dyad_var(att.var$lawschool, type = 'att')\n\nSimilarly, dyad variables can be created based on observed ties. For the undirected edges, we use indicator variables read directly from the adjacency matrix for the dyad in question, while for the directed ones (advice and friendship) we have pairs of indicators representing sending and receiving ties with 4 possible outcomes :\n\ndyad.cwk    &lt;- get_dyad_var(adj.cowork, type = 'tie')\ndyad.adv    &lt;- get_dyad_var(adj.advice, type = 'tie')\ndyad.frn    &lt;- get_dyad_var(adj.friend, type = 'tie')\n\nAll 10 dyad variables are merged into one data frame for subsequent entropy analysis:\n\ndyad.var &lt;-\n  data.frame(cbind(status   = dyad.status$var,\n                  gender    = dyad.gender$var,\n                  office    = dyad.office$var,\n                  years     = dyad.years$var,\n                  age       = dyad.age$var,\n                  practice  = dyad.practice$var,\n                  lawschool = dyad.lawschool$var,\n                  cowork    = dyad.cwk$var,\n                  advice    = dyad.adv$var,\n                  friend    = dyad.frn$var)\n                  )\nhead(dyad.var)\n\n  status gender office years age practice lawschool cowork advice friend\n1      3      3      0     8   8        1         0      0      3      2\n2      3      3      3     5   8        3         0      0      0      0\n3      3      3      3     5   8        2         0      0      1      0\n4      3      3      0     8   8        1         6      0      1      2\n5      3      3      0     8   8        0         6      0      1      1\n6      3      3      1     7   8        1         6      0      1      1\n\n\nA similar function get_triad_var() is implemented for transforming vertex variables and different relation types into triad variables. This is described in more detail in the vignette “variable domains and data editing”."
  },
  {
    "objectID": "project/seand/index.html#univariate-bivariate-and-trivariate-entropies",
    "href": "project/seand/index.html#univariate-bivariate-and-trivariate-entropies",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "Univariate, bivariate and trivariate entropies",
    "text": "Univariate, bivariate and trivariate entropies\nThe function entropy_bivar() computes the bivariate entropies of all pairs of variables in the dataframe. The output is given as an upper triangular matrix with cells giving the bivariate entropies of row and column variables. The diagonal thus gives the univariate entropies for each variable in the dataframe:\n\nH2 &lt;- entropy_bivar(dyad.var)\nH2\n\n          status gender office years   age practice lawschool cowork advice\nstatus     1.493  2.868  3.640 3.370 3.912    3.453     4.363  2.092  2.687\ngender        NA  1.547  3.758 3.939 4.274    3.506     4.439  2.158  2.785\noffice        NA     NA  2.239 4.828 4.901    4.154     5.058  2.792  3.388\nyears         NA     NA     NA 2.671 4.857    4.582     5.422  3.268  3.868\nage           NA     NA     NA    NA 2.801    4.743     5.347  3.411  4.028\npractice      NA     NA     NA    NA    NA    1.962     4.880  2.530  3.127\nlawschool     NA     NA     NA    NA    NA       NA     2.953  3.567  4.186\ncowork        NA     NA     NA    NA    NA       NA        NA  0.615  1.687\nadvice        NA     NA     NA    NA    NA       NA        NA     NA  1.248\nfriend        NA     NA     NA    NA    NA       NA        NA     NA     NA\n          friend\nstatus     2.324\ngender     2.415\noffice     3.044\nyears      3.483\nage        3.637\npractice   2.831\nlawschool  3.812\ncowork     1.456\nadvice     1.953\nfriend     0.881\n\n\nBivariate entropies can be used to detect redundant variables that should be omitted from the dataframe for further analysis. This occurs when the univariate entropy for a variable is equal to the bivariate entropies for pairs including that variable. As seen above, the dataframe dyad.var has no redundant variables. This can also be checked using the function redundancy() which yields a binary matrix as output indicating which row and column variables are hold the same information:\n\nredundancy(dyad.var)\n\nNULL\n\n\nMore examples of using the function redundancy() is given in the vignette “univariate bivariate and trivariate entropies”.\nTrivariate entropies can be computed using the function entropy_trivar() which returns a dataframe with the first three columns representing possible triples of variables V1,V2, and V3 from the dataframe in question, and their entropies H(V1,V2,V3) as the fourth column. We illustrated this on the dataframe dyad.var:\n\nH3 &lt;- entropy_trivar(dyad.var)\nhead(H3, 10) # view first 10 rows of dataframe\n\n       V1     V2        V3 H(V1,V2,V3)\n1  status gender    office       4.938\n2  status gender     years       4.609\n3  status gender       age       5.129\n4  status gender  practice       4.810\n5  status gender lawschool       5.664\n6  status gender    cowork       3.464\n7  status gender    advice       4.048\n8  status gender    friend       3.685\n9  status office     years       5.321\n10 status office       age       5.721"
  },
  {
    "objectID": "project/seand/index.html#joint-entropy-and-association-graphs",
    "href": "project/seand/index.html#joint-entropy-and-association-graphs",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "Joint entropy and association graphs",
    "text": "Joint entropy and association graphs\nJoint entropies is a non-negative measure of association among pairs of variables. It is equal to 0 if and only if two variables are completely independent of each other.\nThe function joint_entropy() computes the joint entropies between all pairs of variables in a given dataframe and returns a list consisting of the upper triangular joint entropy matrix (univariate entropies in the diagonal) and a dataframe giving the frequency distributions of unique joint entropy values. A function argument specifies the precision given in number of decimals for which the frequency distribution of unique entropy values is created (default is 3). Applying the function on the dataframe dyad.var with two decimals:\n\nJ &lt;- joint_entropy(dyad.var, 2)\nJ$matrix\n\n          status gender office years  age practice lawschool cowork advice\nstatus      1.49   0.17   0.09  0.79 0.38     0.00      0.08   0.02   0.05\ngender        NA   1.55   0.03  0.28 0.07     0.00      0.06   0.00   0.01\noffice        NA     NA   2.24  0.08 0.14     0.05      0.13   0.06   0.10\nyears         NA     NA     NA  2.67 0.61     0.05      0.20   0.02   0.05\nage           NA     NA     NA    NA 2.80     0.02      0.41   0.01   0.02\npractice      NA     NA     NA    NA   NA     1.96      0.04   0.05   0.08\nlawschool     NA     NA     NA    NA   NA       NA      2.95   0.00   0.01\ncowork        NA     NA     NA    NA   NA       NA        NA   0.62   0.18\nadvice        NA     NA     NA    NA   NA       NA        NA     NA   1.25\nfriend        NA     NA     NA    NA   NA       NA        NA     NA     NA\n          friend\nstatus      0.05\ngender      0.01\noffice      0.08\nyears       0.07\nage         0.05\npractice    0.01\nlawschool   0.02\ncowork      0.04\nadvice      0.18\nfriend      0.88\n\nJ$freq\n\n      j  #(J = j) #(J &gt;= j)\n1  0.79         1         1\n2  0.61         1         2\n3  0.41         1         3\n4  0.38         1         4\n5  0.28         1         5\n6   0.2         1         6\n7  0.18         2         8\n8  0.17         1         9\n9  0.14         1        10\n10 0.13         1        11\n11  0.1         1        12\n12 0.09         1        13\n13 0.08         4        17\n14 0.07         2        19\n15 0.06         2        21\n16 0.05         7        28\n17 0.04         2        30\n18 0.03         1        31\n19 0.02         5        36\n20 0.01         5        41\n21    0         4        45\n\n\nAs seen, the strongest association is between the variables status and years with joint entropy values of 0.79. We have independence (joint entropy value of 0) between two pairs of variables: (status,practice), (practise,gender), (cowork,gender),and (cowork,lawschool).\nThese results can be illustrated in a association graph using the function assoc_graph() which returns a ggraph object in which nodes represent variables and links represent strength of association (thicker links indicate stronger dependence). To use the function we need to load the ggraph library and to determine a threshold which the graph drawn is based on. We set it to 0.15 so that we only visualize the strongest associations\n\nlibrary(ggraph)\nassoc_graph(dyad.var, 0.15)\n\n\n\n\n\n\n\n\nGiven this threshold, we see isolated and disconnected nodes representing independent variables. We note strong dependence between the three dyadic variables status,years and age, but also a somewhat strong dependence among the three variables lawschool, years and age, and the three variables status, years and gender. The association graph can also be interpreted as a tendency for relations cowork and friend to be independent conditionally on relation advice, that is, any dependence between dyad variables cowork and friend is explained by advice.\nA threshold that gives a graph with reasonably many small independent or conditionally independent subsets of variables can be considered to represent a multivariate model for further testing.\nMore details and examples of joint entropies and association graphs are given in the vignette “joint entropies and association graphs”."
  },
  {
    "objectID": "project/seand/index.html#prediction-power-based-on-expected-conditional-entropies",
    "href": "project/seand/index.html#prediction-power-based-on-expected-conditional-entropies",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "Prediction power based on expected conditional entropies",
    "text": "Prediction power based on expected conditional entropies\nThe function prediction_power() computes prediction power when pairs of variables in a given dataframe are used to predict a third variable from the same dataframe. The variable to be predicted and the dataframe in which this variable also is part of is given as input arguments, and the output is an upper triangular matrix giving the expected conditional entropies of pairs of row and column variables (denoted \\(X\\) and \\(Y\\)) of the matrix, i.e. EH(Z|X,Y) where \\(Z\\) is the variable to be predicted. The diagonal gives EH(Z|X) , that is when only one variable as a predictor. Note that NA’s are in the row and column representing the variable being predicted.\nAssume we are interested in predicting variable status (that is whether a lawyer in the data set is an associate or partner). This is done by running the following syntax\n\nprediction_power('status', dyad.var)\n\n          status gender office years   age practice lawschool cowork advice\nstatus        NA     NA     NA    NA    NA       NA        NA     NA     NA\ngender        NA  1.375  1.180 0.670 0.855    1.304     1.225  1.306  1.263\noffice        NA     NA  2.147 0.493 0.820    1.374     1.245  1.373  1.325\nyears         NA     NA     NA 2.265 0.573    0.682     0.554  0.691  0.667\nage           NA     NA     NA    NA 1.877    1.089     0.958  1.087  1.052\npractice      NA     NA     NA    NA    NA    2.446     1.388  1.459  1.410\nlawschool     NA     NA     NA    NA    NA       NA     3.335  1.390  1.337\ncowork        NA     NA     NA    NA    NA       NA        NA  2.419  1.400\nadvice        NA     NA     NA    NA    NA       NA        NA     NA  2.781\nfriend        NA     NA     NA    NA    NA       NA        NA     NA     NA\n          friend\nstatus        NA\ngender     1.270\noffice     1.334\nyears      0.684\nage        1.058\npractice   1.427\nlawschool  1.350\ncowork     1.411\nadvice     1.407\nfriend     3.408\n\n\nFor better readability, the powers of different predictors can be conveniently compared by using prediction plots that display a color matrix with rows for \\(X\\) and columns for \\(Y\\) with darker colors in the cells when we have higher prediction power for \\(Z\\).\nMore details and examples of expected conditional entropies and prediction power are given in the package vignette."
  },
  {
    "objectID": "project/seand/index.html#divergence-tests-of-goodness-of-fit",
    "href": "project/seand/index.html#divergence-tests-of-goodness-of-fit",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "Divergence Tests of Goodness of Fit",
    "text": "Divergence Tests of Goodness of Fit\nOccurring cliques in association graphs represent connected components of dependent variables, and by comparing the graphs for different thresholds, specific structural models of multivariate dependence can be suggested and tested. The function div_gof() allows such hypothesis tests for pairwise independence of \\(X\\) and \\(Y\\): \\(X \\bot Y\\), and pairwise independence conditional a third variable \\(Z\\): \\(X\\bot Y|Z\\).\nTo test friend\\(\\bot\\) cowork\\(|\\)advice, that is whether dyad variable friend is independent of cowork given advice we use the function as shown below:\n\ndiv_gof(dat = dyad.var, var1 = \"friend\", var2 = \"cowork\", var_cond = \"advice\")\n\n     D df(D)\n1 0.94    12\n\n\nNot specifying argument var_cond would instead test friend\\(\\bot\\)cowork without any conditioning."
  },
  {
    "objectID": "project/seand/index.html#references",
    "href": "project/seand/index.html#references",
    "title": "Statistical Entropy Analysis of Network Data",
    "section": "References",
    "text": "References\nParts of the theoretical background is provided in the package vignettes, but for more details, consult the following literature:\n\nFrank, O., & Shafie, T. (2016). Multivariate entropy analysis of network data. Bulletin of Sociological Methodology/Bulletin de Méthodologie Sociologique, 129(1), 45-63. link"
  },
  {
    "objectID": "project/index.html",
    "href": "project/index.html",
    "title": "Projects",
    "section": "",
    "text": "Statistical Entropy Analysis of Network Data\n\n\nIn this project, a general framework for using statistical entropies to capture interdependencies among node and tie variables in multivariate networks is developed.\n\n\n\n\n\n\n\n\n\n\n\n\nMultigraph Representation of Network Data\n\n\nThe exploratory and confirmatory statistical analysis of multivariate social networks represented as multigraphs.\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork of Interconnected Convoys\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGender Dependent Structures in Charachter Networks\n\n\nUsing network analysis to analyze gender representation in popular cinema.\n\n\n\n\n\n\n\n\n\n\n\n\nNEXUS1492 - Reconstructing Archaeological Networks\n\n\nReconstructing Archaeological Networks And Their Transformations Across The Historical Divide.\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/sna/index.html",
    "href": "teaching/sna/index.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "The course text book can be found here: R4SNA (work in progress)\nSchedule\n\n\n\n\nslides\npractical\ndata\nworksheet\n\n\n\n\n1: Introduction\n\n\n\n.qmd\n\n\n2: The Language of Networks\n\n\n\n.qmd\n\n\n3: Network Concepts and Descriptives I\n\n\n.zip\n.qmd\n\n\n4: Network Concepts and Descriptives II\n\n \n\n.qmd\n\n\n5: Beyond ‘Standard’ Networks\n\n \n.zip\n.qmd\n\n\n6: Network Visualization I\n\n\n\n.qmd\n\n\n7: Network Visualization II\n\n\n\n.qmd\n\n\n8: Network Modelling: Introduction\n\n\n\n.qmd\n\n\n9: Random Graph Models\n\n\n\n.qmd\n\n\n10: Exponential Random Graph Models (ERGMs)\n\n \n.zip\n.qmd .qmd\n\n\n11: Stochastic Actor Oriented Models (SAOMs)\n\n \n.zip .zip\n.qmd .qmd\n\n\n\n\n\n\n\nR Packages\nThroughout the course we will use a variety of different packages of doing network analysis, modeling and visualization. Make sure to install them all and have them ready to load when needed:\n\ninstall.packages(\"igraph\")   \ninstall.packages(\"statnet\")  #installs ergm, network, and sna\ninstall.packages(\"snahelper\")\ninstall.packages(\"netUtils\")\ninstall.packages(\"ggraph\")\ninstall.packages(\"backbone\")\ninstall.packages(\"netrankr\")\ninstall.packages(\"signnet\")\ninstall.packages(\"egor\")\ninstall.packages(\"intergraph\")\ninstall.packages(\"graphlayouts\")\ninstall.packages(\"visNetwork\")\ninstall.packages(\"patchwork\")\ninstall.packages(\"edgebundle\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"gganimate\")\ninstall.packages(\"ggforce\")\ninstall.packages(\"rsiena\")\n\ninstall.packages(\"remotes\")\nremotes::install_github(\"schochastics/networkdata\")"
  },
  {
    "objectID": "teaching/sna/material/04/04-descriptives-2-bm.html",
    "href": "teaching/sna/material/04/04-descriptives-2-bm.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "library(igraph)\nlibrary(networkdata)\nlibrary(netUtils)\nlibrary(ggraph)\nlibrary(kableExtra)\nlibrary(corrplot)\nlibrary(tidygraph)"
  },
  {
    "objectID": "teaching/sna/material/04/04-descriptives-2-bm.html#blockmodels-using-concor",
    "href": "teaching/sna/material/04/04-descriptives-2-bm.html#blockmodels-using-concor",
    "title": "Social Network Analysis",
    "section": "Blockmodels using CONCOR",
    "text": "Blockmodels using CONCOR\nThe example we considered in lecture concerns the relatively small network. What happens when we apply the method of iterated correlations (CONCOR) to a bigger network, something like the one shown below?\n\n\n\n\n\n\n\n\nFigure 1: An undirected graph.\n\n\n\n\n\nFirst save the function below for computing the correlation distance between pairs of nodes:\n\n#function:\ncorr.dist &lt;- function(x) {\n         r &lt;- nrow(x)\n         c &lt;- ncol(x)\n         r.c &lt;- matrix(0, r, r)\n         c.c &lt;- matrix(0, c, c)\n         r.m &lt;- rowMeans(x)\n         c.m &lt;- colMeans(x)\n         \n         for (i in 1: r) {\n              for (j in 1:r) {\n                   r.x &lt;- x[i, ] - r.m[i]\n                   r.y &lt;- x[j, ] - r.m[j]\n                   r.xy &lt;- r.x * r.y\n                   r.xx &lt;- r.x^2\n                   r.yy &lt;- r.y^2\n                   r.num &lt;- sum(r.xy)\n                   r.den &lt;- sqrt(sum(r.xx)) * sqrt(sum(r.yy))\n                   r.c[i, j] &lt;- round(r.num / r.den, 2)\n              }\n         }\n         rownames(r.c) &lt;- rownames(x)\n         colnames(r.c) &lt;- rownames(x)\n         return(r.c)\n}\n\nLet’s start by definig the graph:\n\nfr &lt;- c(rep(1, 5), rep(2, 5), rep(3, 5), rep(4, 3), rep(20, 3))\n    to &lt;- c(5:9, 10:14, 15:19, 1:3, 5, 21, 22)\n    edge.dat &lt;- data.frame(fr, to)\n    node.dat &lt;- data.frame(name = toupper(letters[union(fr, to)]))\n    gr &lt;- tbl_graph(edges = edge.dat, nodes = node.dat, directed = FALSE)\n    gr &lt;- as_tbl_graph(simplify(gr)) \n\nWell, we can begin by computing the correlation distance across all the \\(22\\) nodes in that network.\nNote that even before we do any iterated correlations of correlation matrices we can see that the peripheral, single-connection nodes \\(E, F, G, H\\), \\(I, J, K, L, M\\) and \\(N, O, P, Q, R\\) are perfectly structurally equivalent. This makes sense, because all the nodes in each of these three groups have identical neighborhoods, since they happen to be connected to the same central node \\(A\\) for the first group, \\(B\\) for the second group and \\(C\\) for the third group. Note also that \\(U\\) and \\(V\\) are structurally equivalent, since their neighborhoods are the same: Their single connection is to node \\(S\\).\n\na &lt;- matrix(as_adjacency_matrix(gr), nrow = length(V(gr)))\n    rownames(a) &lt;- V(gr)$name\n    colnames(a) &lt;- V(gr)$name\n    \nb &lt;- corr.dist(a)\n\nWhat happens when we take the correlation distance of the correlation distance matrix shown in Table 1 (a), and the correlation distance of the resulting matrix, and keep going until we only have zeros and ones? The results is Table 1 (b). This matrix seems to reveal a much deeper pattern of commonalities in structural positions across the nodes in Figure 1.\n\nb &lt;- corr.dist(a)\nc &lt;- b\nk &lt;- 1\nwhile (mean(abs(c)) != 1) {\n      c &lt;- corr.dist(c)\n      k &lt;- k + 1\n  }\n\n\n\n\n\n\n\n\n\n\n(a) Original Correlation Distance Matrix.\n\n\n\n\n\n\n\nA\nB\nC\nD\nT\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nP\nQ\nR\nS\nU\nV\n\n\n\n\nA\n1.00\n-0.15\n-0.15\n-0.24\n-0.19\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n0.05\n-0.13\n-0.13\n\n\nB\n-0.15\n1.00\n-0.15\n-0.24\n-0.19\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.24\n-0.13\n-0.13\n\n\nC\n-0.15\n-0.15\n1.00\n-0.24\n-0.19\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.13\n-0.24\n-0.13\n-0.13\n\n\nD\n-0.24\n-0.24\n-0.24\n1.00\n0.34\n0.55\n0.55\n0.55\n0.55\n0.55\n0.55\n0.55\n0.55\n0.55\n0.55\n0.55\n0.55\n0.55\n0.55\n-0.16\n-0.09\n-0.09\n\n\nT\n-0.19\n-0.19\n-0.19\n0.34\n1.00\n0.69\n0.69\n0.69\n0.69\n-0.07\n-0.07\n-0.07\n-0.07\n-0.07\n-0.07\n-0.07\n-0.07\n-0.07\n-0.07\n-0.13\n0.69\n0.69\n\n\nE\n-0.13\n-0.13\n-0.13\n0.55\n0.69\n1.00\n1.00\n1.00\n1.00\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.09\n-0.05\n-0.05\n\n\nF\n-0.13\n-0.13\n-0.13\n0.55\n0.69\n1.00\n1.00\n1.00\n1.00\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.09\n-0.05\n-0.05\n\n\nG\n-0.13\n-0.13\n-0.13\n0.55\n0.69\n1.00\n1.00\n1.00\n1.00\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.09\n-0.05\n-0.05\n\n\nH\n-0.13\n-0.13\n-0.13\n0.55\n0.69\n1.00\n1.00\n1.00\n1.00\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.09\n-0.05\n-0.05\n\n\nI\n-0.13\n-0.13\n-0.13\n0.55\n-0.07\n-0.05\n-0.05\n-0.05\n-0.05\n1.00\n1.00\n1.00\n1.00\n1.00\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.09\n-0.05\n-0.05\n\n\nJ\n-0.13\n-0.13\n-0.13\n0.55\n-0.07\n-0.05\n-0.05\n-0.05\n-0.05\n1.00\n1.00\n1.00\n1.00\n1.00\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.09\n-0.05\n-0.05\n\n\nK\n-0.13\n-0.13\n-0.13\n0.55\n-0.07\n-0.05\n-0.05\n-0.05\n-0.05\n1.00\n1.00\n1.00\n1.00\n1.00\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.09\n-0.05\n-0.05\n\n\nL\n-0.13\n-0.13\n-0.13\n0.55\n-0.07\n-0.05\n-0.05\n-0.05\n-0.05\n1.00\n1.00\n1.00\n1.00\n1.00\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.09\n-0.05\n-0.05\n\n\nM\n-0.13\n-0.13\n-0.13\n0.55\n-0.07\n-0.05\n-0.05\n-0.05\n-0.05\n1.00\n1.00\n1.00\n1.00\n1.00\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.09\n-0.05\n-0.05\n\n\nN\n-0.13\n-0.13\n-0.13\n0.55\n-0.07\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n1.00\n1.00\n1.00\n1.00\n1.00\n-0.09\n-0.05\n-0.05\n\n\nO\n-0.13\n-0.13\n-0.13\n0.55\n-0.07\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n1.00\n1.00\n1.00\n1.00\n1.00\n-0.09\n-0.05\n-0.05\n\n\nP\n-0.13\n-0.13\n-0.13\n0.55\n-0.07\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n1.00\n1.00\n1.00\n1.00\n1.00\n-0.09\n-0.05\n-0.05\n\n\nQ\n-0.13\n-0.13\n-0.13\n0.55\n-0.07\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n1.00\n1.00\n1.00\n1.00\n1.00\n-0.09\n-0.05\n-0.05\n\n\nR\n-0.13\n-0.13\n-0.13\n0.55\n-0.07\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n1.00\n1.00\n1.00\n1.00\n1.00\n-0.09\n-0.05\n-0.05\n\n\nS\n0.05\n-0.24\n-0.24\n-0.16\n-0.13\n-0.09\n-0.09\n-0.09\n-0.09\n-0.09\n-0.09\n-0.09\n-0.09\n-0.09\n-0.09\n-0.09\n-0.09\n-0.09\n-0.09\n1.00\n-0.09\n-0.09\n\n\nU\n-0.13\n-0.13\n-0.13\n-0.09\n0.69\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.09\n1.00\n1.00\n\n\nV\n-0.13\n-0.13\n-0.13\n-0.09\n0.69\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.05\n-0.09\n1.00\n1.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Original Correlation Distance Matrix After Ten Iterations.\n\n\n\n\n\n\n\nA\nB\nC\nD\nT\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nP\nQ\nR\nS\nU\nV\n\n\n\n\nA\n1\n1\n1\n-1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n\n\nB\n1\n1\n1\n-1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n\n\nC\n1\n1\n1\n-1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n\n\nD\n-1\n-1\n-1\n1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n\n\nT\n1\n1\n1\n-1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n\n\nE\n1\n1\n1\n-1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n\n\nF\n1\n1\n1\n-1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n\n\nG\n1\n1\n1\n-1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n\n\nH\n1\n1\n1\n-1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n\n\nI\n-1\n-1\n-1\n1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n\n\nJ\n-1\n-1\n-1\n1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n\n\nK\n-1\n-1\n-1\n1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n\n\nL\n-1\n-1\n-1\n1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n\n\nM\n-1\n-1\n-1\n1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n\n\nN\n-1\n-1\n-1\n1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n\n\nO\n-1\n-1\n-1\n1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n\n\nP\n-1\n-1\n-1\n1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n\n\nQ\n-1\n-1\n-1\n1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n\n\nR\n-1\n-1\n-1\n1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n\n\nS\n1\n1\n1\n-1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n\n\nU\n1\n1\n1\n-1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n\n\nV\n1\n1\n1\n-1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Correlation Distance Matrix in (a) with Rows and Columns Reshuffled to Show Hidden Pattern.\n\n\n\n\n\n\n\nV\nU\nS\nH\nG\nF\nE\nT\nC\nA\nB\nR\nQ\nP\nO\nN\nM\nL\nK\nJ\nD\nI\n\n\n\n\nV\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\nU\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\nS\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\nH\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\nG\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\nF\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\nE\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\nT\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\nC\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\nA\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\nB\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n\n\nR\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nQ\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nP\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nO\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nN\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nM\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nL\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nK\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nJ\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nD\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nI\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\n\n\n\n\nTable 1: Correlation Distance Matrices Corresponding to an Undirected Graph.\n\n\n\nRunning the below code to reshufffke nodes gives the result in Table 1 (c):\n\nrs &lt;- corrMatOrder(c, order = \"hclust\")\nd &lt;- c[rs, rs]\n\nSo it turns out that there is indeed a secret pattern! The reshuffling shows that the nodes in the network can be divided into two blocks such within blocks all nodes are structurally similar (and some structurally equivalent) and across blocks, all nodes are structurally dissimilar. Thus \\(V, U, S, H, G, F, E, T, C, A, B\\) are members of one structurally similar block (let’s called them “Block 1”), and nodes \\(R, Q, P, O, N, M, L, K, J, D, I\\) are members of another structurally similar block (let’s called them “Block 2”). Nodes in “Block 1” are structurally dissimilar from nodes in “Block 2,” but structurally similar to one another and vice versa. To illustrate, Figure 2 is the same as Figure 1, but this time nodes are colored by their memberships in two separate blocks.\n\n    node.color &lt;- c(rep(\"tan3\",3), \"steelblue\", \"tan3\", rep(\"tan3\", 4), rep(\"steelblue\", 10), \"tan3\", rep(\"tan3\", 2))\n    p &lt;- ggraph(gr, layout = 'kk') \n    p &lt;- p + geom_edge_link(color = \"black\", width = 1.15) \n    p &lt;- p + geom_node_point(aes(x = x, y = y), color = node.color, size = 22)\n    p &lt;- p + geom_node_text(aes(label = name), size = 10, color = \"white\")\n    p &lt;- p + theme_graph()\n    p\n\n\n\n\n\n\n\nFigure 2: An undirected graph with block membership indicated by node color.\n\n\n\n\n\nNote that we haven’t changed any of the information in Table 1 (b) to get Table 1 (c). If you check, the row and column entries for each node in both figures are identical. It’s just that we changed the way the rows ordered vertically and the way the columns are ordered horizontally. For instance, node \\(A\\)’s pattern of connections is negatively correlated with node \\(I\\)’s in Table 1 (b), and has the same negative correlation entry in Table 1 (c). The same goes for each one of node \\(A\\)’s other correlations, and the same for each node in the table. Table 1 (b) and Table 1 (c) contain the same information it’s just that Table 1 (c) makes it easier to see a hidden pattern.\nThis property of the method of iterated correlations is the basis of a strategy for uncovering blocks of structurally similar actors in a network developed by a team of sociologists, physicists, and mathematicians working at Harvard in the 1970s. The technique is called blockmodeling. Let’s see how it works.\n\nWe Need to go Deeper!\nOf course, as Leo says: “We need to go deeper.” And indeed we can. What happens if we do the same analysis as above, but this time in the two node-induced subgraphs defined by the set of structurally similar nodes in each of the two blocks we uncovered in the original graph?\nRunning the below codes gives you the results in Table 2 (a) and Table 2 (b):\n\n    b1 &lt;- b[rs[1:11], rs[1:11]]\n    b2 &lt;- b[rs[12:22], rs[12:22]]\n    \n    c1 &lt;- b1\n    while (mean(abs(c1)) != 1) {\n      c1 &lt;- corr.dist(c1)\n      }\n    \n    c2 &lt;- b2\n    while (mean(abs(c2[c2 != 0])) != 1) {\n      c2 &lt;- corr.dist(c2)\n      }\n    rs1 &lt;- corrMatOrder(c1, order = \"hclust\")\n    d1 &lt;- c1[rs1, rs1]\n    d1\n    rs2 &lt;- corrMatOrder(c2, order = \"hclust\")\n    d2 &lt;- c2[rs2, rs2]\n    d2\n\n\n\n\nTable 2: Subgraph Blockmodels\n\n\n\n\n\n\n\n(a) Blockmodel of a subgraph.\n\n\n\n\n\n\n\nB\nA\nC\nS\nV\nU\nT\nE\nF\nH\nG\n\n\n\n\nB\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n\n\nA\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n\n\nC\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n\n\nS\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n\n\nV\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n\n\nU\n1\n1\n1\n1\n1\n1\n-1\n-1\n-1\n-1\n-1\n\n\nT\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n\n\nE\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n\n\nF\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n\n\nH\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n\n\nG\n-1\n-1\n-1\n-1\n-1\n-1\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Another Blockmodel of another subgraph\n\n\n\n\n\n\n\nI\nJ\nK\nM\nL\nD\nN\nO\nP\nR\nQ\n\n\n\n\nI\n1\n1\n1\n1\n1\n0\n-1\n-1\n-1\n-1\n-1\n\n\nJ\n1\n1\n1\n1\n1\n0\n-1\n-1\n-1\n-1\n-1\n\n\nK\n1\n1\n1\n1\n1\n0\n-1\n-1\n-1\n-1\n-1\n\n\nM\n1\n1\n1\n1\n1\n0\n-1\n-1\n-1\n-1\n-1\n\n\nL\n1\n1\n1\n1\n1\n0\n-1\n-1\n-1\n-1\n-1\n\n\nD\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nN\n-1\n-1\n-1\n-1\n-1\n0\n1\n1\n1\n1\n1\n\n\nO\n-1\n-1\n-1\n-1\n-1\n0\n1\n1\n1\n1\n1\n\n\nP\n-1\n-1\n-1\n-1\n-1\n0\n1\n1\n1\n1\n1\n\n\nR\n-1\n-1\n-1\n-1\n-1\n0\n1\n1\n1\n1\n1\n\n\nQ\n-1\n-1\n-1\n-1\n-1\n0\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that Table 1 (b) separates our original Block 2 into two further sub-blocks. Let’s call them “Block 2a” and “Block 2b.” Block 2a is composed of nodes \\(A, B, C, S, U, V\\) and Block 2b is composed of nodes \\(E, F, G, H, T\\).\nLet’s separates our original Block 2 into three further sub-blocks, as shown in Table 1 (b). There’s the block composed of nodes \\(I, J, K, L, M\\). Let’s call this “Block 2a”, the block composed of nodes \\(N, O, P, Q, R\\). Let’s call this “Block 2b.” Then, there’s node \\(D\\). Note that this node is only structurally similar to itself and is neither similar nor dissimilar to the other nodes in the subgraph \\(d^{corr} = 0\\), so it occupies a position all by itself! Let’s call it “Block 2c.”\n\n    n &lt;- c(\"B\", \"A\", \"C\", \"S\", \"V\", \"U\")\n    b3 &lt;- b[n, n]\n\n    c3 &lt;- b3\n    while (mean(abs(c3)) != 1) {\n      c3 &lt;- corr.dist(c3)\n    }\n    rs3 &lt;- corrMatOrder(c3, order = \"hclust\")\n    d3 &lt;- c3[rs3, rs3]\n    d3\n    \n    n &lt;- c(\"B\", \"A\", \"C\", \"S\")\n    b4 &lt;- b[n, n]\n    c4 &lt;- b4\n    while (mean(abs(c4)) != 1) {\n      c4 &lt;- corr.dist(c4)\n    }\n    rs4 &lt;- corrMatOrder(c4, order = \"hclust\")\n    d4 &lt;- c4[rs4, rs4]\n    d4\n\n\n\n\nTable 3: Subgraph Blockmodels\n\n\n\n\n\n\n\n(a) Blockmodel of a subgraph.\n\n\n\n\n\n\n\nS\nC\nB\nA\nV\nU\n\n\n\n\nS\n1\n1\n1\n1\n-1\n-1\n\n\nC\n1\n1\n1\n1\n-1\n-1\n\n\nB\n1\n1\n1\n1\n-1\n-1\n\n\nA\n1\n1\n1\n1\n-1\n-1\n\n\nV\n-1\n-1\n-1\n-1\n1\n1\n\n\nU\n-1\n-1\n-1\n-1\n1\n1\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Another Blockmodel of another subgraph\n\n\n\n\n\n\n\nB\nC\nA\nS\n\n\n\n\nB\n1\n1\n-1\n-1\n\n\nC\n1\n1\n-1\n-1\n\n\nA\n-1\n-1\n1\n1\n\n\nS\n-1\n-1\n1\n1\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s do a couple of final splits of the subgraph composed of nodes \\(A, B, C, S, U, V\\). This is shown in Table 3. The first split separates nodes in block \\(A, B, C, S\\) from those in block \\(U, V\\) (Table 3 (a)). The second splits the nodes in subgraph \\(A, B, C, S\\) into two blocks composed of \\(A, S\\) and \\(B, C\\), respectively (Table 3 (b)).\n\nnode.color &lt;- c(\"firebrick\", rep(\"steelblue\", 2), \"purple\", rep(\"tan3\", 5), rep(\"darkgreen\", 5), rep(\"#CC79A7\", 5), \"firebrick\", rep(\"darkturquoise\", 2))\n    p &lt;- ggraph(gr, layout = 'kk') \n    p &lt;- p + geom_edge_link(color = \"black\", width = 1.15) \n    p &lt;- p + geom_node_point(aes(x = x, y = y), color = node.color, size = 22)\n    p &lt;- p + geom_node_text(aes(label = name), size = 10, color = \"white\")\n    p &lt;- p + theme_graph()\n    p\n\n\n\n\n\n\n\nFigure 3: An undirected graph with block membership indicated by node color.\n\n\n\n\n\nFigure 3 shows the nodes in Figure 1 colored according to our final block partition. It is clear that the blockmodeling approach captures patterns of structural similarity. For instance, all the single-connection nodes connected to more central nodes get assigned to their own position: Block 1b: \\(E, F, G, H, T\\), Block 2a: \\(I, J, K, L, M\\), and Block 2b: \\(N, O, P, Q, R\\). The most central node \\(D\\) (in terms of Eigenvector centrality) occupies a unique position in the graph. Two of the three central nodes (in terms of degree centrality) \\(B, C\\) get assigned to their own position. Meanwhile \\(A, S\\) form their own structurally similar block. Finally, \\(U, V\\) also form their own structurally similar block as both are structurally equivalent in the orignal graph.\n\n\nThe Blocked Adjacency Matrix\nWhat happens if we were to go back fo the adjacency matrix corresponding to Figure 1, and then reshuffle the rows and columns to correspond to all these wonderful blocks we have uncovered? Well, we would en up with something like Table 4. This is called the blocked adjacency matrix. In the blocked adjacency matrix, the division between the nodes corresponding to each block of structurally similar nodes in Table 2 and Table 3 is marked by thick black lines going across the rows and columns.\nEach diagonal rectangle in Table 4 corresponds to within-block connections. Each off-diagonal rectangle corresponds to between block connections. There are two kinds of rectangles in the blocked adjacency matrix. First, there are rectangles that only contains zero entries. These are called zero blocks. For instance the top-left rectangle in Table 4 is a zero block. Then there rectangles that have some non-zero entries in them (ones, since this is a binary adjacency matrix). These are called one blocks. For instance, the fourth rectangle going down the rows (corresponding to the block that nodes \\(B, C\\) belong to) is a one block.\n\n\n\n\n\n\n\n\n\n\nI\nJ\nK\nL\nM\nN\nO\nP\nQ\nR\nT\nE\nF\nG\nH\nB\nC\nA\nS\nU\nV\nD\n\n\n\n\nI\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nJ\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nK\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nL\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nM\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\nN\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nO\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nP\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nQ\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nR\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nT\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n\n\nE\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nF\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nG\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nH\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nB\n1\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\nC\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\nA\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n1\n\n\nS\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n\n\nU\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\nV\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\nD\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n\n\n\n\n\n\n\nTable 4: Blocked adjancency matrix.\n\n\n\n\nZero-blocks indicate that the members of the row block don’t have any connections with the members the column block (which can include themselves!). For instance, the zero-block in the top-left corner of the blocked adjacency matrix in Table 4 indicates that the members of this block are not connected to one another in the network (and we can verify from Figure 3 that this is indeed the case).\nOne blocks indicate that the members of the column block share some connections with the members of the column block (which can also include themselves!). For instance, the one-block in the fourth rectangle going down the rows (corresponding to the block that nodes \\(B, C\\) belong to) tells us that members of this block are connected to at least one member of the \\(I, J, K, L, M\\) block (and we can verify from Figure 3 that this is indeed the case, since \\(B\\) is connected to all of them).\n\n\nThe Image Matrix\nFrom this reshuffled adjacency matrix, we can get to a reduced image matrix containing the relations not between the nodes in the graph, but between the blocks in the graph. The way we proceed to construct the image matrix is as follows:\n\nFirst we create an empty matrix \\(\\mathbf{B}\\) of dimensions \\(b \\times b\\) where \\(B\\) is the number of blocks in the blockmodel. In our example, \\(b = 7\\) so the image matrix has seven rows and seven columns. The \\(ij^{th}\\) cell in the image matrix \\(\\mathbf{B}\\) records the relationship between row block i and column block j in the blockmodel.\nSecond, we put a zero in the image matrix if the rectangle corresponding to the relationship between blocks i and j in Table 4 is a zero-block.\nThird, we put a one in the image matrix if the rectangle corresponding to the relationship between blocks i and j in Table 4 is a one-block.\n\nThe result is Table 5:\n\nc &lt;- matrix(c(0, 0, 0, 1, 0, 0, 0,\n                0, 0, 0, 1, 0, 0, 0,\n                0, 0, 0, 0, 1, 0, 0,\n                1, 1, 0, 0, 0, 0, 1,\n                0, 0, 1, 0, 0, 1, 1,\n                0, 0, 0, 0, 1, 0, 0,\n                0, 0, 0, 1, 1, 0, 0), \n              nrow = 7)\nrownames(c) &lt;- c(\"I, J, K, L\", \"N, O, P, Q, R\", \"T, E, F, G, H\", \"B, C\", \"A, S\", \"U, V\", \"D\")\ncolnames(c) &lt;- c(\"I, J, K, L\", \"N, O, P, Q, R\", \"T, E, F, G, H\", \"B, C\", \"A, S\", \"U, V\", \"D\")\nc\n\n\n\n\n\n\n\n\n\n\n\nI, J, K, L\nN, O, P, Q, R\nT, E, F, G, H\nB, C\nA, S\nU, V\nD\n\n\n\n\nI, J, K, L\n0\n0\n0\n1\n0\n0\n0\n\n\nN, O, P, Q, R\n0\n0\n0\n1\n0\n0\n0\n\n\nT, E, F, G, H\n0\n0\n0\n0\n1\n0\n0\n\n\nB, C\n1\n1\n0\n0\n0\n0\n1\n\n\nA, S\n0\n0\n1\n0\n0\n1\n1\n\n\nU, V\n0\n0\n0\n0\n1\n0\n0\n\n\nD\n0\n0\n0\n1\n1\n0\n0\n\n\n\n\n\n\n\nTable 5: Image matrix corresponding to the blockmodel of an undirected graph.\n\n\n\n\nSo the big blocked adjacency matrix in Table 1 (c) can be reduced to the image matrix shown Table 5, summarizing the relations between the blocks in the graph. This matrix, can then even be represented as a graph, so that we can see the pattern of relations between blocks! This is shown in Figure 4\n\n    gr &lt;- graph_from_adjacency_matrix(c) %&gt;% \n      as_tbl_graph() %&gt;% \n      activate(nodes) %&gt;% \n      mutate(names =  c(\"I, J, K, L\", \"N, O, P, Q, R\", \"T, E, F, G, H\", \"B, C\", \"A, S\", \"U, V\", \"D\"))\n    p &lt;- ggraph(gr, layout = 'tree') + \n         geom_edge_link(color = \"black\", width = 1.15)  + \n         geom_node_label(aes(label = names), size = c(5, 5, 5, 7, 7, 5, 10))\n    p &lt;- p + theme_graph()\n    p\n\n\n\n\n\n\n\nFigure 4: Graph representation of reduced image matrix from a blockmodel.\n\n\n\n\n\nThis is how blockmodeling works!\nNote that the final plot is done using tidygraph. You can try yourself to plot it with ggraph instead."
  },
  {
    "objectID": "teaching/sna/material/04/04-descriptives-2-bm.html#footnotes",
    "href": "teaching/sna/material/04/04-descriptives-2-bm.html#footnotes",
    "title": "Social Network Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis worksheet is inspired and adapted from this source↩︎"
  },
  {
    "objectID": "teaching/sna/material/05/05-special-nets.html",
    "href": "teaching/sna/material/05/05-special-nets.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "Signed Network and Two-Mode Networks\n\nExercise 1: Correlates of War\nLoad the “Correlates of War” dataset from the signnet package. The “cowList” dataset contains a list of 52 signed networks of inter-state relations over time (1946-1999). Two countries are connected by a positive tie if they form an alliance or have a peace treaty. A negative tie exists between countries who are at war or in other kinds of conflicts.\n\nlibrary(signnet)\ndata(\"cowList\")\n\nThe dataset includes 51 networks of international relations between nations (aggregated on 3 year time windows). A positive tie indicates some form of alliance between countries and a negative tie conflict or war.\nExcept for the first and last task, you can either do the exercises for all networks or choose one specific time window\n\nDid the number of conflicts increase or decrease over time?\nBefore computing any balance scores: What would you expect in terms of structural balance?\nCalculate the triangle based balance score. Does it mach your intuition?\nVisualize a network using ggsigned() and decide whether it makes sense to estimate a regular blockmodel or if you need to specify a general one and compute the blockmodel\n\n\n\nExercise 2: Two Mode Projections\nThe file senate15.csv contains all bill cosponsorships of the US Senate from 2015 to 2017.\n\nread the file dat &lt;- read.csv(\"senate15.csv\")\nCreate a data frame of the senators\nsenators &lt;- unique(dat[,1:2])\nConstruct the two mode network of senators and bills with\ng &lt;- bipartite_from_data_frame(dat,\"bill\",\"name\")\nprint the igraph object and check how the two modes are distinguished\ncheck ?bipartite.projection and only create the weighted projection between senators. Call the network proj. What does the weight indicate?\nInspect the plot. Can you see any structure?\ndelete edges with a weight less than x, where you should try different values for x. What do you think are “good” values for x? Does a structure emerge for any x?\n\n\n\n\n\n\n\nTip\n\n\n\nFor the exploratory part of the exercise you can do the following:\n\nV(proj)$party &lt;- senators$party[match(V(proj)$name,senators$name)]\nV(proj)$color &lt;- ifelse(V(proj)$party==\"D\",\"blue\",\n                 ifelse(V(proj)$party==\"R\",\"red\",\"yellow\"))\n\nplot(proj,layout = layout_with_kk,vertex.label = NA,vertex.size = 3)\n\nto delete edges, use delete_edges()"
  },
  {
    "objectID": "teaching/sna/material/02/02-graph-theory.html",
    "href": "teaching/sna/material/02/02-graph-theory.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "We’ll use the igraph package to explore key graph-theoretical concepts.\n\nlibrary(igraph)"
  },
  {
    "objectID": "teaching/sna/material/02/02-graph-theory.html#graph-theory-in-r-with-igraph",
    "href": "teaching/sna/material/02/02-graph-theory.html#graph-theory-in-r-with-igraph",
    "title": "Social Network Analysis",
    "section": "",
    "text": "We’ll use the igraph package to explore key graph-theoretical concepts.\n\nlibrary(igraph)"
  },
  {
    "objectID": "teaching/sna/material/02/02-graph-theory.html#undirected-graph",
    "href": "teaching/sna/material/02/02-graph-theory.html#undirected-graph",
    "title": "Social Network Analysis",
    "section": "Undirected Graph",
    "text": "Undirected Graph\nWe’ll use an undirected graph with 8 nodes.\n\ng &lt;- graph_from_literal(\n  1 -- 2, \n  1 -- 3,\n  2 -- 4,\n  3 -- 5,\n  4 -- 5,\n  5 -- 6,\n  6 -- 7,\n  6 -- 8,\n  7 -- 8\n)\n\nplot(g, vertex.label.cex = 1.2, vertex.size = 20)\n\n\n\n\n\n\n\n\n\n\nDegree & Degree Distribution\n\ndeg &lt;- degree(g)\ndeg\n\n1 2 3 4 5 6 7 8 \n2 2 2 2 3 3 2 2 \n\n# Degree distribution\ndist &lt;- degree_distribution(g)\nplot(dist, type = \"h\", main = \"Degree Distribution\", xlab = \"Degree\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n\nIdentify nodes with the highest and lowest degree. How does the distribution look?\n\n\n\nGraph Diameter\n\ndiameter(g)\n\n[1] 4\n\n\nWhat is the longest shortest path in the graph?\n\n\n\nShortest Paths\nFind shortest paths from node 1 to all others:\n\nsp &lt;- distances(g, v = 1)\nsp\n\n  1 2 3 4 5 6 7 8\n1 0 1 1 2 2 3 4 4\n\n\nDo you understand the output? What is the shortest path from node 1 to node 6?\n\n\n\nAdjacency Matrix\n\nadj_matrix &lt;- as_adjacency_matrix(g, sparse = FALSE)\nadj_matrix\n\n  1 2 3 4 5 6 7 8\n1 0 1 1 0 0 0 0 0\n2 1 0 0 1 0 0 0 0\n3 1 0 0 0 1 0 0 0\n4 0 1 0 0 1 0 0 0\n5 0 0 1 1 0 1 0 0\n6 0 0 0 0 1 0 1 1\n7 0 0 0 0 0 1 0 1\n8 0 0 0 0 0 1 1 0\n\n\nUse the adjacency matrix to compute the degree of each node. Compare with degree(g).\n\n\n\nCutpoints (Articulation Points)\n\narticulation_points &lt;- articulation_points(g)\narticulation_points\n\n+ 2/8 vertices, named, from be810f5:\n[1] 6 5\n\n\n\n\nBridges (Critical Edges)\n\nbridge_edges &lt;- which(is.na(edge_connectivity(g)))\nE(g)[bridge_edges]\n\n+ 0/9 edges from be810f5 (vertex names):\n\n# Alternatively use:\nbridges(g)\n\n+ 1/9 edge from be810f5 (vertex names):\n[1] 5--6\n\n\n\n\nVisualize Cutpoints and Bridges\n\nV(g)$color &lt;- ifelse(V(g) %in% articulation_points, \"red\", \"skyblue\")\nE(g)$color &lt;- ifelse(E(g) %in% bridges(g), \"red\", \"black\")\n\nplot(g, vertex.size = 20, vertex.label.cex = 1.2)\n\n\n\n\n\n\n\n\nWhich nodes and edges are critical to keeping the graph connected?"
  },
  {
    "objectID": "teaching/sna/material/02/02-graph-theory.html#directed-graph",
    "href": "teaching/sna/material/02/02-graph-theory.html#directed-graph",
    "title": "Social Network Analysis",
    "section": "Directed Graph",
    "text": "Directed Graph\nNow let’s work with a directed graph.\n\ng_dir &lt;- graph_from_literal(\n  A -+ B, A -+ C,\n  B -+ D,\n  C -+ D,\n  D -+ E,\n  E -+ F,\n  F -+ C\n)\n\nplot(g_dir, vertex.label.cex = 1.2, vertex.color = \"lightcoral\", edge.arrow.size = 0.5)\n\n\n\n\n\n\n\n\n\nIn-Degree and Out-Degree\n\ndegree(g_dir, mode = \"in\")   # incoming links\n\nA B C D E F \n0 1 2 2 1 1 \n\ndegree(g_dir, mode = \"out\")  # outgoing links\n\nA B C D E F \n2 1 1 1 1 1 \n\n\n\n\nStrongly Connected Components\n\ncomponents(g_dir, mode = \"strong\")\n\n$membership\nA B C D E F \n1 2 3 3 3 3 \n\n$csize\n[1] 1 1 4\n\n$no\n[1] 3\n\n\n\n\nDirected Paths and Diameter\n\ndiameter(g_dir, directed = TRUE)\n\n[1] 4\n\n\nExplore how cycles and direction affect path lengths."
  },
  {
    "objectID": "teaching/sna/material/02/02-graph-theory.html#exercises",
    "href": "teaching/sna/material/02/02-graph-theory.html#exercises",
    "title": "Social Network Analysis",
    "section": "Exercises",
    "text": "Exercises\n\nCreate some other small unidrected and directed graphs and see how the above measure vary on them\nimport the Florentine marriage and business network from the networkdata package and compute the appropriate measures from above on it\n\n\nlibrary(networkdata)\ndata(\"flo_marriage\")\ndata(\"flo_business\")"
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms1.html",
    "href": "teaching/sna/material/10/10-ergms1.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "We’re going to follow the ERGM modelling outline:\n\nspecify and estimate model parameters that should govern evolution of network\nsimulate other random networks based on specified models\ncompare the goodness of fit of observed to model networks.\n\nThe following resource is useful for looking up different model terms: ERGM terms.\nNote that we now are performing stochastic simulation – in some of the cases, your output will differ slightly from mine and between different runs (you can however use set.seed() to get exactly the same results).\n\n\n\nlibrary(statnet) # also loads the ERGM package\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(intergraph)\nlibrary(patchwork)\nlibrary(networkdata)\n\n\n\n\nWe will be primarily be working with matrix, network and graph objects. Note that ergm primarily requires network and adjacency matrices, but since we will be using ggraph to visualize networks we also need graph objects. We try to keep it clear here by using suffix g, net and mat to clarify object assignment."
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms1.html#packages-needed",
    "href": "teaching/sna/material/10/10-ergms1.html#packages-needed",
    "title": "Social Network Analysis",
    "section": "",
    "text": "library(statnet) # also loads the ERGM package\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(intergraph)\nlibrary(patchwork)\nlibrary(networkdata)"
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms1.html#object-types",
    "href": "teaching/sna/material/10/10-ergms1.html#object-types",
    "title": "Social Network Analysis",
    "section": "",
    "text": "We will be primarily be working with matrix, network and graph objects. Note that ergm primarily requires network and adjacency matrices, but since we will be using ggraph to visualize networks we also need graph objects. We try to keep it clear here by using suffix g, net and mat to clarify object assignment."
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms1.html#model-1-dyadic-independencebernoulli-graph",
    "href": "teaching/sna/material/10/10-ergms1.html#model-1-dyadic-independencebernoulli-graph",
    "title": "Social Network Analysis",
    "section": "Model 1: Dyadic independence/Bernoulli graph",
    "text": "Model 1: Dyadic independence/Bernoulli graph\n\nEstimation\nWe begin by specifying a Bernoulli model using the ergm function. This is done by only including number of edges as a term in the model (recall from lecture that this implies dyadic independence). Run the model and print out summary of model fit using below code:\n\nflom_mod1 &lt;- ergm(flom_net ~ edges) # fit the model\nsummary(flom_mod1) # get a summary of model\n\nCall:\nergm(formula = flom_net ~ edges)\n\nMaximum Likelihood Results:\n\n      Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges  -1.6094     0.2449      0  -6.571   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 166.4  on 120  degrees of freedom\n Residual Deviance: 108.1  on 119  degrees of freedom\n \nAIC: 110.1  BIC: 112.9  (Smaller is better. MC Std. Err. = 0)\n\n\nYou can also just print the estimated coefficient using only flom_mod1.\nQ1. How can you interpret the parameter estimate?\nThe log-odds of any tie occurring is: \\[ -1.609 \\times \\textrm{change in the number of ties} = -1.609 \\times 1 \\] for all ties, since the addition of any tie to the network changes the number of ties by 1. Corresponding probability is: \\[\\frac{\\exp{(-1.609)}}{1+\\exp{(-1.609)}}=0.1667\\] which is what you would expect, since there are 20/120 ties."
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms1.html#model-2-transitivity-effect-added",
    "href": "teaching/sna/material/10/10-ergms1.html#model-2-transitivity-effect-added",
    "title": "Social Network Analysis",
    "section": "Model 2: Transitivity effect added",
    "text": "Model 2: Transitivity effect added\n\nEstimation\nNext, we add a term the number of completed triangles/triads (which would indicate transitivity).\n\nset.seed(1) #include if you want the same results shown here\nflom_mod2 &lt;- ergm(flom_net ~ edges + triangle)\nsummary(flom_mod2) \n\nCall:\nergm(formula = flom_net ~ edges + triangle)\n\nMonte Carlo Maximum Likelihood Results:\n\n         Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges     -1.6913     0.3219      0  -5.254   &lt;1e-04 ***\ntriangle   0.1808     0.5567      0   0.325    0.745    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 166.4  on 120  degrees of freedom\n Residual Deviance: 108.1  on 118  degrees of freedom\n \nAIC: 112.1  BIC: 117.6  (Smaller is better. MC Std. Err. = 0.01061)\n\n\nQ2 How can you interpret the parameter estimates?\nQ3 What do the parameter estimates tell us about the configurations specified in the model?\nConditional log-odds of two actors forming a tie is:\n\n\\(-1.644\\times\\) change in the number of ties + \\(0.134 \\times\\) change in number of triangles\nif the tie will not add any triangles to the network, its log-odds is: -1.644\nif it will add one triangle to the network, its log-odds is: -1.644 + 0.134\nif it will add two triangles to the network, its log-odds is: -1.644 + 0.134 \\(\\times\\) 2\n\n\n\nMCMC diagnostics\nYou can use mcmc.diagnostics(flom_mod2) to observe the behavior of the MCMC estimation algorithm and check for degeneracy. What you want to see in the MCMC diagnostics: the MCMC sample statistics varying randomly around the observed values at each step in the trace plots (which means the chain is mixing well) and the difference between the observed and simulated values of the sample statistics should have a roughly bell-shaped distribution, centered at 0 (which means no difference):\n\nmcmc.diagnostics(flom_mod2, center = TRUE)\n\n\n\n\n\n\n\n\nSample statistics summary:\n\nIterations = 14336:262144\nThinning interval = 1024 \nNumber of chains = 1 \nSample size per chain = 243 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n           Mean    SD Naive SE Time-series SE\nedges    0.2058 4.957   0.3180         0.3180\ntriangle 0.2222 2.866   0.1839         0.1839\n\n2. Quantiles for each variable:\n\n         2.5% 25% 50% 75% 97.5%\nedges      -9  -3   0   4  9.95\ntriangle   -3  -2   0   1  7.95\n\n\nAre sample statistics significantly different from observed?\n               edges  triangle    (Omni)\ndiff.      0.2057613 0.2222222        NA\ntest stat. 0.6471018 1.2086209 1.6694087\nP-val.     0.5175661 0.2268085 0.4367585\n\nSample statistics cross-correlations:\n             edges  triangle\nedges    1.0000000 0.7771561\ntriangle 0.7771561 1.0000000\n\nSample statistics auto-correlation:\nChain 1 \n                edges    triangle\nLag 0     1.000000000  1.00000000\nLag 1024  0.056425983 -0.04043396\nLag 2048  0.006273791  0.01940035\nLag 3072 -0.051649675 -0.02455474\nLag 4096 -0.034487041  0.02913158\nLag 5120 -0.023329356  0.01178056\n\nSample statistics burn-in diagnostic (Geweke):\nChain 1 \n\nFraction in 1st window = 0.1\nFraction in 2nd window = 0.5 \n\n   edges triangle \n1.458524 1.483872 \n\nIndividual P-values (lower = worse):\n    edges  triangle \n0.1446962 0.1378428 \nJoint P-value (lower = worse):  0.1011719 \n\nNote: MCMC diagnostics shown here are from the last round of\n  simulation, prior to computation of final parameter estimates.\n  Because the final estimates are refinements of those used for this\n  simulation run, these diagnostics may understate model performance.\n  To directly assess the performance of the final model on in-model\n  statistics, please use the GOF command: gof(ergmFitObject,\n  GOF=~model).\n\n\nQ4 How would you interpret these results?\n\n\nSimulation\nWhen we have estimated the coefficients of an ERGM, we have defined a probability distribution across all networks of the same size. If the model is a good fit to the observed data, networks drawn from this distribution resemble the observed data. To draw networks from this distribution we use the simulate() function. We draw ten networks from the specified model and use the below command to get a summary of what the network statistics edges and triangles are for each of the ten sampled networks.\n\nflom_mod2.sim &lt;- simulate(flom_mod2, nsim = 10)\nsummary(flom_mod2.sim)\n\nNumber of Networks: 10 \nModel: flom_net ~ edges + triangle \nReference: ~Bernoulli \nConstraints: ~. ~. - observed \nStored network statistics:\n      edges triangle\n [1,]    16        3\n [2,]    26        7\n [3,]    18        1\n [4,]    17        1\n [5,]    22        1\n [6,]    18        1\n [7,]    11        1\n [8,]    22        4\n [9,]    20        3\n[10,]    26        6\nattr(,\"monitored\")\n[1] FALSE FALSE\n\n\nNumber of Networks: 10 \nModel: flom_net ~ edges + triangle \nReference: ~Bernoulli \nConstraints: ~. ~. - observed \n\n\nThis should give you a list over the ten networks and columns representing how many edges and triangles are apparent in each simulated case. Since you have listed all the simulated networks, you can simply call each one of them individually. For example, in the below, we call simulated networks 1 and 2:\n\nflom_mod2.sim[[1]]\nflom_mod2.sim[[2]]\n\nYou can also choose one of the networks to visualize, below is an example for the tenth, i.e. last on the list of, simulated network:\n\nflom.sim_g &lt;-asIgraph(flom_mod2.sim[[10]])\nflom.sim_p &lt;- ggraph(flom.sim_g, layout = \"stress\") + \n  geom_edge_link0(edge_colour = \"#666060\", \n                  edge_width = 0.8, edge_alpha = 1) +\n  geom_node_point(fill = \"#808080\", colour = \"#808080\",  \n                  size = 7, shape = 21, stroke = 0.9) +\n  theme_graph() + \n  theme(legend.position = \"none\") +\n  ggtitle(\"Simulated network\")\nflom.sim_p\n\n\n\n\n\n\n\n\nThese simulations are crucial for examining the goodness of fit which we will do next.\n\n\n3. Goodness of Fit\nThe MCMC algorithm draws a dyad at random at each step, and evaluates the probability of a tie from the perspective of these two nodes. That probability is governed by the ergm-terms specified in the model, and the current estimates of the coefficients on these terms. Once the estimates converge, simulations from the model will produce networks that are centered on the observed model statistics i.e. those we control for (otherwise it is a sign that something has gone wrong in the estimation process). The networks will also have other emergent global properties that are not represented by explicit terms in the model. Thus, goodness of fit can be done in two ways, where the first is to be preferred:\n\nevaluate the fit to the specified terms in the model (done by default)\nevaluate the fit of terms not specified in the model to emergent global network properties\n\nIf the first does not indicate something off in the estimation process, you can use the second where three terms that can be used to evaluate the fit to emergent global network properties:\n\nthe node level (degree)\nthe edge level (esp: edgewise share partners)\nthe dyad level (geodesic distances)\n\nWe check now whether the specified model above fits the observed data and how well it reproduces it. We do this by choosing a network statistic (that is not specified in the model), and comparing the value of this statistic to the distribution of values we get in simulated networks from our model. We use the gof() function.\n\nflom_mod2.gof &lt;- gof(flom_mod2) # this will produce 4 plots\npar(mfrow=c(2,2)) # figure orientation with 2 rows and 2 columns\nplot(flom_mod2.gof) # gof plots\n\n\n\n\n\n\n\n\nTo get an output containing the summary of the gof:\n\nflom_mod2.gof # summary output of gof\n\nQ5 How would you interpret the goodness of fit here?"
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms1.html#model-1-homophily-and-clustering",
    "href": "teaching/sna/material/10/10-ergms1.html#model-1-homophily-and-clustering",
    "title": "Social Network Analysis",
    "section": "Model 1: homophily and clustering",
    "text": "Model 1: homophily and clustering\n\nEstimation\nWe are interested in running an ERGM with the following statistics (as done during lecture)\n\nnodecov(“practice”)\nmatch(“practice”)\ngwesp(decay = 0.693)\n\nQ6 Can you recall what these statistics represent? To run the ERGM:\n\nlaw_mod1 &lt;- ergm(law_net ~ edges\n  + nodecov(\"practice\") + match(\"practice\")\n  + gwesp(0.693, fixed = TRUE)\n)\nsummary(law_mod1)\n\nCall:\nergm(formula = law_net ~ edges + nodecov(\"practice\") + match(\"practice\") + \n    gwesp(0.693, fixed = TRUE))\n\nMonte Carlo Maximum Likelihood Results:\n\n                   Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges              -4.38591    0.32189      0 -13.626  &lt; 1e-04 ***\nnodecov.practice    0.18221    0.07121      0   2.559 0.010498 *  \nnodematch.practice  0.60364    0.16760      0   3.602 0.000316 ***\ngwesp.fixed.0.693   1.14125    0.16112      0   7.083  &lt; 1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 873.4  on 630  degrees of freedom\n Residual Deviance: 503.4  on 626  degrees of freedom\n \nAIC: 511.4  BIC: 529.2  (Smaller is better. MC Std. Err. = 0.3047)\n\n\nSee lecture slides for the interpretation of these coefficients.\n\n\nMCMC diagnostics\nCheck the model by running MCMC diagnostics to observe what is happening with the simulation algorithm:\n\nmcmc.diagnostics(law_mod1, center = TRUE)\n\n\n\n\n\n\n\n\nSample statistics summary:\n\nIterations = 122880:2424832\nThinning interval = 2048 \nNumber of chains = 1 \nSample size per chain = 1125 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n                    Mean    SD Naive SE Time-series SE\nedges              3.588 30.18   0.8997          2.050\nnodecov.practice   1.178 29.10   0.8677          1.771\nnodematch.practice 2.573 18.51   0.5520          1.187\ngwesp.fixed.0.693  6.930 59.05   1.7605          4.047\n\n2. Quantiles for each variable:\n\n                     2.5%    25%  50%   75% 97.5%\nedges               -70.0 -14.00  8.0 25.00  49.0\nnodecov.practice    -75.0 -12.00  6.0 21.00  43.9\nnodematch.practice  -41.8  -8.00  4.0 15.00  33.0\ngwesp.fixed.0.693  -132.5 -27.95 12.5 49.26 101.6\n\n\nAre sample statistics significantly different from observed?\n                edges nodecov.practice nodematch.practice gwesp.fixed.0.693\ndiff.      3.58755556        1.1777778         2.57333333        6.93039339\ntest stat. 1.74979703        0.6652089         2.16772023        1.71264367\nP-val.     0.08015334        0.5059169         0.03017998        0.08677811\n                 (Omni)\ndiff.                NA\ntest stat. 1.890437e+01\nP-val.     9.736672e-04\n\nSample statistics cross-correlations:\n                       edges nodecov.practice nodematch.practice\nedges              1.0000000        0.8720346          0.9463973\nnodecov.practice   0.8720346        1.0000000          0.8292965\nnodematch.practice 0.9463973        0.8292965          1.0000000\ngwesp.fixed.0.693  0.9942961        0.8753042          0.9426539\n                   gwesp.fixed.0.693\nedges                      0.9942961\nnodecov.practice           0.8753042\nnodematch.practice         0.9426539\ngwesp.fixed.0.693          1.0000000\n\nSample statistics auto-correlation:\nChain 1 \n              edges nodecov.practice nodematch.practice gwesp.fixed.0.693\nLag 0     1.0000000       1.00000000          1.0000000         1.0000000\nLag 2048  0.7091548       0.61237659          0.6442048         0.6814578\nLag 4096  0.4717707       0.37544290          0.4192158         0.4478894\nLag 6144  0.3156917       0.22673439          0.2935639         0.2997408\nLag 8192  0.2111325       0.13342547          0.1785633         0.2016442\nLag 10240 0.1487237       0.07132966          0.1299507         0.1454400\n\nSample statistics burn-in diagnostic (Geweke):\nChain 1 \n\nFraction in 1st window = 0.1\nFraction in 2nd window = 0.5 \n\n             edges   nodecov.practice nodematch.practice  gwesp.fixed.0.693 \n          1.493081           0.917114           1.500024           1.640398 \n\nIndividual P-values (lower = worse):\n             edges   nodecov.practice nodematch.practice  gwesp.fixed.0.693 \n         0.1354159          0.3590829          0.1336083          0.1009225 \nJoint P-value (lower = worse):  0.005343654 \n\nNote: MCMC diagnostics shown here are from the last round of\n  simulation, prior to computation of final parameter estimates.\n  Because the final estimates are refinements of those used for this\n  simulation run, these diagnostics may understate model performance.\n  To directly assess the performance of the final model on in-model\n  statistics, please use the GOF command: gof(ergmFitObject,\n  GOF=~model).\n\n\nQ6 Do you see any problems with model degeneracy here? Is the estimation process working as it should?\n\n\nGoodness of Fit\nGoodness of fit can be checked as done earlier:\n\nlaw_mod1.gof &lt;- gof(law_mod1) # this will produce 4 plots\npar(mfrow = c(2, 2)) # figure orientation with 2 rows and 2 columns\nplot(law_mod1.gof)\n\n\n\n\n\n\n\n\nNote that you should not use esp to assess goodness of fit since it was explicitly modeled via the gwesp term in the specified model."
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms1.html#model-1-reciprocity-effect",
    "href": "teaching/sna/material/10/10-ergms1.html#model-1-reciprocity-effect",
    "title": "Social Network Analysis",
    "section": "Model 1: Reciprocity effect",
    "text": "Model 1: Reciprocity effect\n\nEstimation\n\nknecht4_mod1 &lt;- ergm(knecht4_net ~ edges + mutual)\nsummary(knecht4_mod1) \n\nCall:\nergm(formula = knecht4_net ~ edges + mutual)\n\nMonte Carlo Maximum Likelihood Results:\n\n       Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges   -2.1889     0.1450      0 -15.100   &lt;1e-04 ***\nmutual   2.4115     0.3212      0   7.507   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 901.1  on 650  degrees of freedom\n Residual Deviance: 562.9  on 648  degrees of freedom\n \nAIC: 566.9  BIC: 575.8  (Smaller is better. MC Std. Err. = 0.7652)\n\n\nQ7 How do you interpret these results?\n\n\nMCMC diagnostics\n\nmcmc.diagnostics(knecht4_mod1)\n\n\n\n\n\n\n\n\nSample statistics summary:\n\nIterations = 14336:262144\nThinning interval = 1024 \nNumber of chains = 1 \nSample size per chain = 243 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean     SD Naive SE Time-series SE\nedges  -1.0988 11.869   0.7614         0.9010\nmutual -0.3333  5.357   0.3436         0.4182\n\n2. Quantiles for each variable:\n\n         2.5%  25% 50% 75% 97.5%\nedges  -26.00 -9.5   0   7    19\nmutual -10.95 -4.0   0   3    11\n\n\nAre sample statistics significantly different from observed?\n                edges     mutual    (Omni)\ndiff.      -1.0987654 -0.3333333        NA\ntest stat. -1.2194889 -0.7970729 1.8580854\nP-val.      0.2226587  0.4254087 0.3982094\n\nSample statistics cross-correlations:\n           edges    mutual\nedges  1.0000000 0.8121613\nmutual 0.8121613 1.0000000\n\nSample statistics auto-correlation:\nChain 1 \n               edges      mutual\nLag 0     1.00000000  1.00000000\nLag 1024  0.16476519  0.19190028\nLag 2048 -0.03275616 -0.03862647\nLag 3072 -0.04234861 -0.10047043\nLag 4096  0.03109986  0.06411610\nLag 5120  0.05887512  0.04907514\n\nSample statistics burn-in diagnostic (Geweke):\nChain 1 \n\nFraction in 1st window = 0.1\nFraction in 2nd window = 0.5 \n\n      edges      mutual \n-0.09833024 -0.92194343 \n\nIndividual P-values (lower = worse):\n    edges    mutual \n0.9216701 0.3565581 \nJoint P-value (lower = worse):  0.3677926 \n\nNote: MCMC diagnostics shown here are from the last round of\n  simulation, prior to computation of final parameter estimates.\n  Because the final estimates are refinements of those used for this\n  simulation run, these diagnostics may understate model performance.\n  To directly assess the performance of the final model on in-model\n  statistics, please use the GOF command: gof(ergmFitObject,\n  GOF=~model).\n\n\nQ8 How do you interpret these results?\n\n\nGoodness of fit\nNote that since we now are considering a directed network, we need to separate in- and out-degree when assessing the goodness of fit:\n\nknecht4_mod1.gof &lt;- gof(knecht4_mod1) # this will produce 4 plots\npar(mfrow = c(3,2)) # figure orientation with 2 rows and 2 columns\nplot(knecht4_mod1.gof)\n\n\n\n\n\n\n\n\nQ9 How do you interpret these results?"
  },
  {
    "objectID": "teaching/sna/material/10/10-ergms1.html#model-2-reciprocity-and-homophily-effect",
    "href": "teaching/sna/material/10/10-ergms1.html#model-2-reciprocity-and-homophily-effect",
    "title": "Social Network Analysis",
    "section": "Model 2: Reciprocity and homophily effect",
    "text": "Model 2: Reciprocity and homophily effect\nNow we also include a homophily effect, i.e. do students tend to befriend others of the same gender?\nQ10 Run the usual steps of fitting and ERGM, checking the estimation algorithm and assessing the goodness of fit. The ERGM syntax is shown below. What can you conclude?\n\nknecht4_mod2 &lt;- ergm(knecht4_net ~ edges +  nodecov(\"gender\") + \n                       nodematch(\"gender\") + mutual)\nsummary(knecht4_mod2) \n\nCall:\nergm(formula = knecht4_net ~ edges + nodecov(\"gender\") + nodematch(\"gender\") + \n    mutual)\n\nMonte Carlo Maximum Likelihood Results:\n\n                 Estimate Std. Error MCMC % z value Pr(&gt;|z|)    \nedges             -4.1981     0.3843      0 -10.925   &lt;1e-04 ***\nnodecov.gender     0.4966     0.1192      0   4.167   &lt;1e-04 ***\nnodematch.gender   1.2061     0.2257      0   5.344   &lt;1e-04 ***\nmutual             2.0843     0.3735      0   5.580   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 901.1  on 650  degrees of freedom\n Residual Deviance: 520.4  on 646  degrees of freedom\n \nAIC: 528.4  BIC: 546.3  (Smaller is better. MC Std. Err. = 0.8236)"
  },
  {
    "objectID": "teaching/sna/material/09/09-rgm.html",
    "href": "teaching/sna/material/09/09-rgm.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "In this session, we introduces a range of models used to represent and understand the structure of social and relational networks. We begin with the classic \\(G(n,p)\\) random graph model, in which each pair of nodes is connected independently with a fixed probability. While analytically tractable and conceptually simple, \\(G(n,p)\\) falls short in capturing the structural complexity of real-world networks; it fails to account for common features such as clustering, skewed degree distributions, or short average path lengths.\nTo address these shortcomings, we explore several alternative models that incorporate more realistic structural constraints. The small-world model captures the coexistence of high local clustering and global reachability observed in many social systems. The configuration model enables the generation of random graphs with a fixed degree sequence, offering more control over node-level connectivity patterns. For each model, we discuss its definition, how to simulate or estimate it, and its relevance for modeling real network data.\n\n\n\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(networkdata)\nlibrary(tidyverse)\n\n\n\n\nTo see why the \\(G(n, p)\\) model is often an inadequate representation of real-world networks, we can compare its properties to those of an actual empirical network. A typical social or informational network displays three features that are not captured well by \\(G(n, p)\\): a right-skewed degree distribution (with hubs), high clustering or triadic closure, and short average path lengths. While \\(G(n, p)\\) can match the density of a network, it assumes a binomial (or normal) degree distribution, minimal clustering, and does not account for structural heterogeneity.\nThe example below uses the igraph package in R and a real network dataset of moderate-to-large size to illustrate these differences. We load a real-world network; a network of co-appearances of characters in Victor Hugo’s novel “Les Miserables” which can be loaded from the networkdata package.\n\n\n\n\n\n\n\n\n\nWe compute this its key structural properties, then generate a random graph with the same number of nodes and expected density using sample_gnp(). We then compare the two in terms of degree distribution, transitivity (clustering), and average geodesic distance.\nThe code below summarizes key structural properties of the observed Les Misérables network and the corresponding \\(G(n, p)\\) random graph. These include the global clustering coefficient (measuring the tendency of nodes to form closed triads), the average geodesic distance (a measure of path efficiency), and the maximum degree (the highest number of connections any single node has).\nThe observed network shows substantially higher clustering, a slightly shorter average path length, and a much larger maximum degree. These results highlight that the empirical network is both more locally cohesive and more hierarchically structured than its random counterpart. The presence of hubs and local clusters—common in real-world networks—is not reproduced by the \\(G(n, p)\\) model, which assumes uniform and independent edge probabilities.\nTogether, these differences support the conclusion that random tie formation alone cannot explain the structure of this network.\n\nlibrary(knitr)\n# Load the Les Misérables network from networkdata\ndata(\"miserables\")\ng_obs &lt;- miserables\n\n# Basic stats of the observed network\nn &lt;- vcount(g_obs)\nm &lt;- ecount(g_obs)\ndensity_obs &lt;- edge_density(g_obs)\ndeg_obs &lt;- degree(g_obs)\nclustering_obs &lt;- transitivity(g_obs, type = \"global\")\ndist_obs &lt;- mean_distance(g_obs, directed = FALSE, unconnected = TRUE)\n\n# Generate a G(n, p) graph with the same density\nset.seed(123)\ng_gnp &lt;- sample_gnp(n = n, p = density_obs, directed = FALSE)\n\ndeg_gnp &lt;- degree(g_gnp)\nclustering_gnp &lt;- transitivity(g_gnp, type = \"global\")\ndist_gnp &lt;- mean_distance(g_gnp, directed = FALSE, unconnected = TRUE)\n\n# Combine comparison into a data frame\ncomparison &lt;- data.frame(\n  Model = c(\"Observed\", \"G(n, p)\"),\n  Clustering = c(clustering_obs, clustering_gnp),\n  AvgPathLength = c(dist_obs, dist_gnp),\n  MaxDegree = c(max(deg_obs), max(deg_gnp))\n)\n\n# Format with kable\nkable(comparison, caption = \"Comparison of structural features: Observed vs G(n, p)\",\n      digits = 3, align = \"c\")\n\n\nComparison of structural features: Observed vs G(n, p)\n\n\nModel\nClustering\nAvgPathLength\nMaxDegree\n\n\n\n\nObserved\n0.499\n4.861\n36\n\n\nG(n, p)\n0.086\n2.636\n12\n\n\n\n\n\nTo further illustrate the limitations of the \\(G(n, p)\\) model, we also examine the degree distributions of the observed network and the simulated random graph. Real-world networks often exhibit right-skewed degree distributions, with many nodes having few connections and a small number of hubs with very high degree. In contrast, the \\(G(n, p)\\) model produces a binomial (and approximately normal) degree distribution, where most nodes have degrees clustered around the mean. By comparing these two distributions side by side, we can observe how poorly the random model captures the heterogeneity present in the empirical network.\n\n# Plot the degree distributions\ndf_deg &lt;- data.frame(\n  Degree = c(deg_obs, deg_gnp),\n  Type = rep(c(\"Observed\", \"G(n, p)\"), times = c(length(deg_obs), length(deg_gnp)))\n)\n\nggplot(df_deg, aes(x = Degree, fill = Type)) +\n  geom_histogram(position = \"identity\", bins = 20, alpha = 0.6, color = \"white\") +\n  facet_wrap(~Type, scales = \"free_y\") +\n  labs(x = \"Node Degree\", y = \"Frequency\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"skyblue\", \"tomato\")) +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\nThis example underscores the need for more realistic network models that can capture multiple structural properties simultaneously. While the \\(G(n, p)\\) model offers a useful theoretical baseline, its assumptions of uniform edge probability and independent tie formation lead to networks with unrealistic degree distributions. In particular, it fails to capture the heterogeneity observed in many real-world systems, where some nodes act as hubs while others have very few connections. To address this limitation, we turn to the configuration model, which allows us to fix the degree sequence of the network and thereby preserve node-level connectivity patterns. This model represents a natural next step toward our second random graph model.\n\n\n\n\n\n\nNote: \\(G(n, p)\\) and CUG Given Density\n\n\n\nThe \\(G(n, p)\\) model is mathematically equivalent to a Conditional Uniform Graph (CUG) test given density. In both cases, edges are formed between node pairs independently with fixed probability \\(p\\), and the overall network density is preserved on average across simulations.\nHowever, there are key differences in interpretation and usage:\n\nThe \\(G(n, p)\\) model is a generative model used to define a probability distribution over the space of graphs with \\(n\\) nodes and tie probability \\(p\\). It is often used in theoretical network science as a baseline or null model.\nA CUG test given density is a hypothesis testing framework. It conditions on the observed number of nodes and the expected density, and tests whether an observed network statistic (e.g., mutual ties, clustering) deviates significantly from what would be expected by chance.\n\nIn practice, simulating random graphs under the \\(G(n, p)\\) model is functionally identical to conducting a CUG test with fixed density. The distinction lies in whether the model is used for generative modeling or for evaluating the statistical significance of observed network features.\n\n\n\n\n\nwe continue with the Les Misérables co-appearance network and compare it to a random network generated from the configuration model. The goal is to assess how well the configuration model replicates key structural features of the observed network when it exactly preserves the degree sequence but randomizes the specific tie configuration.\nWe use the igraph package to compute network properties and simulate the configuration model using sample_degseq(). The configuration model guarantees that each node retains its observed degree. We create a comparison table and visualize the degree distribution as before.\n\n# Get observed degree sequence\ndeg_seq &lt;- degree(g_obs)\n\n# Compute observed properties\nclustering_obs &lt;- transitivity(g_obs, type = \"global\")\ndist_obs &lt;- mean_distance(g_obs, directed = FALSE, unconnected = TRUE)\n\n# Simulate configuration model with the same degree sequence\nset.seed(123)\ng_conf &lt;- sample_degseq(deg_seq, method = \"fast.heur.simple\")\n\n# Compute simulated properties\nclustering_conf &lt;- transitivity(g_conf, type = \"global\")\ndist_conf &lt;- mean_distance(g_conf, directed = FALSE, unconnected = TRUE)\n\n# Degree distribution comparison\ndeg_conf &lt;- degree(g_conf)\ndeg_df &lt;- data.frame(\n  Degree = c(deg_seq, deg_conf),\n  Type = rep(c(\"Observed\", \"Configuration Model\"), times = c(length(deg_seq), length(deg_conf)))\n)\n\n# Summary table\nconf_comparison &lt;- data.frame(\n  Model = c(\"Observed\", \"Configuration Model\"),\n  Clustering = c(clustering_obs, clustering_conf),\n  AvgPathLength = c(dist_obs, dist_conf),\n  MaxDegree = c(max(deg_seq), max(deg_conf))\n)\n\n# Display comparison table with kable\nkable(conf_comparison, caption = \"Comparison of structural features: Observed vs Configuration Model\",\n      digits = 3, align = \"c\")\n\n\nComparison of structural features: Observed vs Configuration Model\n\n\nModel\nClustering\nAvgPathLength\nMaxDegree\n\n\n\n\nObserved\n0.499\n4.861\n36\n\n\nConfiguration Model\n0.239\n2.503\n36\n\n\n\n\n# degree distribution plot\nggplot(deg_df, aes(x = Degree, fill = Type)) +\n  geom_histogram(position = \"identity\", bins = 20, alpha = 0.6, color = \"white\") +\n  facet_wrap(~Type, scales = \"free_y\") +\n  labs(x = \"Node Degree\", y = \"Frequency\") +\n  scale_fill_manual(values = c(\"skyblue\", \"tomato\")) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\nAs expected, the degree distribution of the simulated network matches that of the original exactly. However, when we examine higher-order properties, such as the global clustering coefficient and average path length, we find notable differences. The observed network has significantly more clustering, suggesting the presence of structured triadic closure that is not reproduced by the configuration model’s randomized pairing process. The average path length may also differ, although it often remains in the same general range.\nThese results highlight an important distinction: while the configuration model controls for degree-based features, it does not account for clustering, community structure, or other forms of structural dependency. As such, it is useful as a baseline or null model for testing whether observed patterns can be explained by degree alone.\n\n\n\n\n\n\nNote: Configuration Model vs. CUG Given Degree\n\n\n\nThe configuration model and a Conditional Uniform Graph (CUG) test given degree both generate random networks that preserve the observed degree sequence. In this sense, they are conceptually aligned: both assume that node-level connectivity (i.e., degrees) is fixed and use this constraint to explore how other structural features might arise by chance.\nThe key distinction lies in how each is used. The configuration model is a generative model; it produces random graphs that exactly match a specified degree sequence, often for theoretical or simulation purposes. A CUG test given degree, on the other hand, is a hypothesis testing framework. It evaluates whether a particular network statistic (such as clustering or transitivity) in the observed network is unusually high or low compared to what would be expected under random tie arrangement, given the same degree sequence.\n\n\n\n\n\nTo evaluate how well the small-world model captures structural features of a real network, we simulate a small-world graph using the same number of nodes and approximate average degree as the Les Misérables co-appearance network. We then compare the simulated graph to the observed one in terms of degree distribution, clustering, and average path length.\nThe simulation uses igraph::sample_smallworld(), which generates a Watts–Strogatz small-world graph by starting from a regular ring lattice and randomly rewiring edges with a given probability \\(p\\). We set \\(p = 0.05\\) to introduce moderate randomness while maintaining local structure (we discuss the choice of \\(p\\) in more detail below).\n\navg_deg_obs &lt;- mean(deg_obs)\nk &lt;- round(avg_deg_obs / 2)  # average degree per side for ring lattice\n\n# Simulate small-world graph\nset.seed(123)\ng_sw &lt;- sample_smallworld(dim = 1, size = n, nei = k, p = 0.05)\n\n# Compute properties\nclustering_obs &lt;- transitivity(g_obs, type = \"global\")\nclustering_sw &lt;- transitivity(g_sw, type = \"global\")\n\ndist_obs &lt;- mean_distance(g_obs, directed = FALSE, unconnected = TRUE)\ndist_sw &lt;- mean_distance(g_sw, directed = FALSE, unconnected = TRUE)\n\ndeg_sw &lt;- degree(g_sw)\n\n# Comparison table\nsw_comparison &lt;- data.frame(\n  Model = c(\"Observed\", \"Small-World\"),\n  Clustering = c(clustering_obs, clustering_sw),\n  AvgPathLength = c(dist_obs, dist_sw),\n  MaxDegree = c(max(deg_obs), max(deg_sw))\n)\n\n# Print formatted table\nkable(sw_comparison, caption = \"Comparison of structural features: Observed vs Small-World Model\",\n      digits = 3, align = \"c\")\n\n\nComparison of structural features: Observed vs Small-World Model\n\n\nModel\nClustering\nAvgPathLength\nMaxDegree\n\n\n\n\nObserved\n0.499\n4.861\n36\n\n\nSmall-World\n0.498\n3.778\n7\n\n\n\n\n# Degree distribution\ndeg_df &lt;- data.frame(\n  Degree = c(deg_sw, deg_obs),\n  Type = rep(c(\"Small-World\",\"Observed\"), times = c(length(deg_obs), length(deg_sw)))\n)\n # Reverse the factor levels\ndeg_df$Type &lt;- factor(deg_df$Type, levels = c(\"Small-World\", \"Observed\"))\n\nggplot(deg_df, aes(x = Degree, fill = Type)) +\n  geom_histogram(position = \"identity\", bins = 20, alpha = 0.6, color = \"white\") +\n  facet_wrap(~Type, scales = \"free_y\") +\n  labs(x = \"Node Degree\", y = \"Frequency\") +\n  scale_fill_manual(values = c(\"skyblue\",\"tomato\")) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\nThe simulation demonstrates how the small-world model approximates certain properties of the observed network. As shown in the table, the simulated network achieves a relatively short average path length, similar to that of the Les Misérables network, due to the introduction of random long-range ties. The clustering coefficient remains substantial, reflecting the model’s ability to preserve local neighborhood structure.\nHowever, the degree distribution in the small-world model shown in remains relatively narrow, with most nodes having degrees close to the average. This limitation highlights that while the small-world model captures some global and local properties, it does not account for degree heterogeneity. The results show that the small-world model offers a useful structural middle ground between regular and fully random graphs but still lacks the full complexity observed in empirical networks.\nNote that the choice of the rewiring probability \\(p\\) in the small-world model is crucial, as it balances regularity and randomness. Small values of \\(p\\) (e.g., between 0.01 and 0.2) are typically chosen to introduce enough randomness to significantly reduce path lengths, while still preserving high clustering. If \\(p\\) is too low, the network remains overly regular; if \\(p\\) is too high, the network behaves like a random graph and loses its local structure. In practice, \\(p\\) is often selected empirically to achieve small-world characteristics (high clustering and short average path length) relative to the number of nodes and degree.\nTo illustrate how the small-world model transitions between regular and random structure, we simulate multiple networks with the same number of nodes as Les Misérables network with varying values of the rewiring probability \\(p\\) and track how two key properties (clustering and average path length) change. This helps identify a “sweet spot” for \\(p\\) where the network retains high clustering but achieves short global paths, capturing the essence of small-world structure. The results are shown in ?@fig-rewire.\n\n\n\n\n\n\n\n\n\nThe plot shows a sharp transition in network structure as \\(p\\) increases. At \\(p = 0\\), the network is a regular lattice: clustering is high, but average path length is long. As \\(p\\) increases slightly (e.g., \\(p \\approx 0.1\\)), the average path length drops rapidly due to the introduction of long-range shortcuts, while clustering remains relatively high. This intermediate range is where small-world characteristics emerge.\nAs \\(p\\) approaches 1, the network becomes increasingly random (think \\(G(n,p)\\)): clustering drops off, and path length stabilizes at a low level. This demonstrates the trade-off between local cohesion and global efficiency controlled by the rewiring parameter \\(p\\).\n\n\n\nChoose a network of yourself and analyze and compare results using the three introduced random graph models."
  },
  {
    "objectID": "teaching/sna/material/09/09-rgm.html#packages-needed",
    "href": "teaching/sna/material/09/09-rgm.html#packages-needed",
    "title": "Social Network Analysis",
    "section": "",
    "text": "library(igraph)\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(networkdata)\nlibrary(tidyverse)"
  },
  {
    "objectID": "teaching/sna/material/09/09-rgm.html#the-erdősrényi-model-gn-p",
    "href": "teaching/sna/material/09/09-rgm.html#the-erdősrényi-model-gn-p",
    "title": "Social Network Analysis",
    "section": "",
    "text": "To see why the \\(G(n, p)\\) model is often an inadequate representation of real-world networks, we can compare its properties to those of an actual empirical network. A typical social or informational network displays three features that are not captured well by \\(G(n, p)\\): a right-skewed degree distribution (with hubs), high clustering or triadic closure, and short average path lengths. While \\(G(n, p)\\) can match the density of a network, it assumes a binomial (or normal) degree distribution, minimal clustering, and does not account for structural heterogeneity.\nThe example below uses the igraph package in R and a real network dataset of moderate-to-large size to illustrate these differences. We load a real-world network; a network of co-appearances of characters in Victor Hugo’s novel “Les Miserables” which can be loaded from the networkdata package.\n\n\n\n\n\n\n\n\n\nWe compute this its key structural properties, then generate a random graph with the same number of nodes and expected density using sample_gnp(). We then compare the two in terms of degree distribution, transitivity (clustering), and average geodesic distance.\nThe code below summarizes key structural properties of the observed Les Misérables network and the corresponding \\(G(n, p)\\) random graph. These include the global clustering coefficient (measuring the tendency of nodes to form closed triads), the average geodesic distance (a measure of path efficiency), and the maximum degree (the highest number of connections any single node has).\nThe observed network shows substantially higher clustering, a slightly shorter average path length, and a much larger maximum degree. These results highlight that the empirical network is both more locally cohesive and more hierarchically structured than its random counterpart. The presence of hubs and local clusters—common in real-world networks—is not reproduced by the \\(G(n, p)\\) model, which assumes uniform and independent edge probabilities.\nTogether, these differences support the conclusion that random tie formation alone cannot explain the structure of this network.\n\nlibrary(knitr)\n# Load the Les Misérables network from networkdata\ndata(\"miserables\")\ng_obs &lt;- miserables\n\n# Basic stats of the observed network\nn &lt;- vcount(g_obs)\nm &lt;- ecount(g_obs)\ndensity_obs &lt;- edge_density(g_obs)\ndeg_obs &lt;- degree(g_obs)\nclustering_obs &lt;- transitivity(g_obs, type = \"global\")\ndist_obs &lt;- mean_distance(g_obs, directed = FALSE, unconnected = TRUE)\n\n# Generate a G(n, p) graph with the same density\nset.seed(123)\ng_gnp &lt;- sample_gnp(n = n, p = density_obs, directed = FALSE)\n\ndeg_gnp &lt;- degree(g_gnp)\nclustering_gnp &lt;- transitivity(g_gnp, type = \"global\")\ndist_gnp &lt;- mean_distance(g_gnp, directed = FALSE, unconnected = TRUE)\n\n# Combine comparison into a data frame\ncomparison &lt;- data.frame(\n  Model = c(\"Observed\", \"G(n, p)\"),\n  Clustering = c(clustering_obs, clustering_gnp),\n  AvgPathLength = c(dist_obs, dist_gnp),\n  MaxDegree = c(max(deg_obs), max(deg_gnp))\n)\n\n# Format with kable\nkable(comparison, caption = \"Comparison of structural features: Observed vs G(n, p)\",\n      digits = 3, align = \"c\")\n\n\nComparison of structural features: Observed vs G(n, p)\n\n\nModel\nClustering\nAvgPathLength\nMaxDegree\n\n\n\n\nObserved\n0.499\n4.861\n36\n\n\nG(n, p)\n0.086\n2.636\n12\n\n\n\n\n\nTo further illustrate the limitations of the \\(G(n, p)\\) model, we also examine the degree distributions of the observed network and the simulated random graph. Real-world networks often exhibit right-skewed degree distributions, with many nodes having few connections and a small number of hubs with very high degree. In contrast, the \\(G(n, p)\\) model produces a binomial (and approximately normal) degree distribution, where most nodes have degrees clustered around the mean. By comparing these two distributions side by side, we can observe how poorly the random model captures the heterogeneity present in the empirical network.\n\n# Plot the degree distributions\ndf_deg &lt;- data.frame(\n  Degree = c(deg_obs, deg_gnp),\n  Type = rep(c(\"Observed\", \"G(n, p)\"), times = c(length(deg_obs), length(deg_gnp)))\n)\n\nggplot(df_deg, aes(x = Degree, fill = Type)) +\n  geom_histogram(position = \"identity\", bins = 20, alpha = 0.6, color = \"white\") +\n  facet_wrap(~Type, scales = \"free_y\") +\n  labs(x = \"Node Degree\", y = \"Frequency\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"skyblue\", \"tomato\")) +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\nThis example underscores the need for more realistic network models that can capture multiple structural properties simultaneously. While the \\(G(n, p)\\) model offers a useful theoretical baseline, its assumptions of uniform edge probability and independent tie formation lead to networks with unrealistic degree distributions. In particular, it fails to capture the heterogeneity observed in many real-world systems, where some nodes act as hubs while others have very few connections. To address this limitation, we turn to the configuration model, which allows us to fix the degree sequence of the network and thereby preserve node-level connectivity patterns. This model represents a natural next step toward our second random graph model.\n\n\n\n\n\n\nNote: \\(G(n, p)\\) and CUG Given Density\n\n\n\nThe \\(G(n, p)\\) model is mathematically equivalent to a Conditional Uniform Graph (CUG) test given density. In both cases, edges are formed between node pairs independently with fixed probability \\(p\\), and the overall network density is preserved on average across simulations.\nHowever, there are key differences in interpretation and usage:\n\nThe \\(G(n, p)\\) model is a generative model used to define a probability distribution over the space of graphs with \\(n\\) nodes and tie probability \\(p\\). It is often used in theoretical network science as a baseline or null model.\nA CUG test given density is a hypothesis testing framework. It conditions on the observed number of nodes and the expected density, and tests whether an observed network statistic (e.g., mutual ties, clustering) deviates significantly from what would be expected by chance.\n\nIn practice, simulating random graphs under the \\(G(n, p)\\) model is functionally identical to conducting a CUG test with fixed density. The distinction lies in whether the model is used for generative modeling or for evaluating the statistical significance of observed network features."
  },
  {
    "objectID": "teaching/sna/material/09/09-rgm.html#the-configuration-model",
    "href": "teaching/sna/material/09/09-rgm.html#the-configuration-model",
    "title": "Social Network Analysis",
    "section": "",
    "text": "we continue with the Les Misérables co-appearance network and compare it to a random network generated from the configuration model. The goal is to assess how well the configuration model replicates key structural features of the observed network when it exactly preserves the degree sequence but randomizes the specific tie configuration.\nWe use the igraph package to compute network properties and simulate the configuration model using sample_degseq(). The configuration model guarantees that each node retains its observed degree. We create a comparison table and visualize the degree distribution as before.\n\n# Get observed degree sequence\ndeg_seq &lt;- degree(g_obs)\n\n# Compute observed properties\nclustering_obs &lt;- transitivity(g_obs, type = \"global\")\ndist_obs &lt;- mean_distance(g_obs, directed = FALSE, unconnected = TRUE)\n\n# Simulate configuration model with the same degree sequence\nset.seed(123)\ng_conf &lt;- sample_degseq(deg_seq, method = \"fast.heur.simple\")\n\n# Compute simulated properties\nclustering_conf &lt;- transitivity(g_conf, type = \"global\")\ndist_conf &lt;- mean_distance(g_conf, directed = FALSE, unconnected = TRUE)\n\n# Degree distribution comparison\ndeg_conf &lt;- degree(g_conf)\ndeg_df &lt;- data.frame(\n  Degree = c(deg_seq, deg_conf),\n  Type = rep(c(\"Observed\", \"Configuration Model\"), times = c(length(deg_seq), length(deg_conf)))\n)\n\n# Summary table\nconf_comparison &lt;- data.frame(\n  Model = c(\"Observed\", \"Configuration Model\"),\n  Clustering = c(clustering_obs, clustering_conf),\n  AvgPathLength = c(dist_obs, dist_conf),\n  MaxDegree = c(max(deg_seq), max(deg_conf))\n)\n\n# Display comparison table with kable\nkable(conf_comparison, caption = \"Comparison of structural features: Observed vs Configuration Model\",\n      digits = 3, align = \"c\")\n\n\nComparison of structural features: Observed vs Configuration Model\n\n\nModel\nClustering\nAvgPathLength\nMaxDegree\n\n\n\n\nObserved\n0.499\n4.861\n36\n\n\nConfiguration Model\n0.239\n2.503\n36\n\n\n\n\n# degree distribution plot\nggplot(deg_df, aes(x = Degree, fill = Type)) +\n  geom_histogram(position = \"identity\", bins = 20, alpha = 0.6, color = \"white\") +\n  facet_wrap(~Type, scales = \"free_y\") +\n  labs(x = \"Node Degree\", y = \"Frequency\") +\n  scale_fill_manual(values = c(\"skyblue\", \"tomato\")) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\nAs expected, the degree distribution of the simulated network matches that of the original exactly. However, when we examine higher-order properties, such as the global clustering coefficient and average path length, we find notable differences. The observed network has significantly more clustering, suggesting the presence of structured triadic closure that is not reproduced by the configuration model’s randomized pairing process. The average path length may also differ, although it often remains in the same general range.\nThese results highlight an important distinction: while the configuration model controls for degree-based features, it does not account for clustering, community structure, or other forms of structural dependency. As such, it is useful as a baseline or null model for testing whether observed patterns can be explained by degree alone.\n\n\n\n\n\n\nNote: Configuration Model vs. CUG Given Degree\n\n\n\nThe configuration model and a Conditional Uniform Graph (CUG) test given degree both generate random networks that preserve the observed degree sequence. In this sense, they are conceptually aligned: both assume that node-level connectivity (i.e., degrees) is fixed and use this constraint to explore how other structural features might arise by chance.\nThe key distinction lies in how each is used. The configuration model is a generative model; it produces random graphs that exactly match a specified degree sequence, often for theoretical or simulation purposes. A CUG test given degree, on the other hand, is a hypothesis testing framework. It evaluates whether a particular network statistic (such as clustering or transitivity) in the observed network is unusually high or low compared to what would be expected under random tie arrangement, given the same degree sequence."
  },
  {
    "objectID": "teaching/sna/material/09/09-rgm.html#the-small-world-model",
    "href": "teaching/sna/material/09/09-rgm.html#the-small-world-model",
    "title": "Social Network Analysis",
    "section": "",
    "text": "To evaluate how well the small-world model captures structural features of a real network, we simulate a small-world graph using the same number of nodes and approximate average degree as the Les Misérables co-appearance network. We then compare the simulated graph to the observed one in terms of degree distribution, clustering, and average path length.\nThe simulation uses igraph::sample_smallworld(), which generates a Watts–Strogatz small-world graph by starting from a regular ring lattice and randomly rewiring edges with a given probability \\(p\\). We set \\(p = 0.05\\) to introduce moderate randomness while maintaining local structure (we discuss the choice of \\(p\\) in more detail below).\n\navg_deg_obs &lt;- mean(deg_obs)\nk &lt;- round(avg_deg_obs / 2)  # average degree per side for ring lattice\n\n# Simulate small-world graph\nset.seed(123)\ng_sw &lt;- sample_smallworld(dim = 1, size = n, nei = k, p = 0.05)\n\n# Compute properties\nclustering_obs &lt;- transitivity(g_obs, type = \"global\")\nclustering_sw &lt;- transitivity(g_sw, type = \"global\")\n\ndist_obs &lt;- mean_distance(g_obs, directed = FALSE, unconnected = TRUE)\ndist_sw &lt;- mean_distance(g_sw, directed = FALSE, unconnected = TRUE)\n\ndeg_sw &lt;- degree(g_sw)\n\n# Comparison table\nsw_comparison &lt;- data.frame(\n  Model = c(\"Observed\", \"Small-World\"),\n  Clustering = c(clustering_obs, clustering_sw),\n  AvgPathLength = c(dist_obs, dist_sw),\n  MaxDegree = c(max(deg_obs), max(deg_sw))\n)\n\n# Print formatted table\nkable(sw_comparison, caption = \"Comparison of structural features: Observed vs Small-World Model\",\n      digits = 3, align = \"c\")\n\n\nComparison of structural features: Observed vs Small-World Model\n\n\nModel\nClustering\nAvgPathLength\nMaxDegree\n\n\n\n\nObserved\n0.499\n4.861\n36\n\n\nSmall-World\n0.498\n3.778\n7\n\n\n\n\n# Degree distribution\ndeg_df &lt;- data.frame(\n  Degree = c(deg_sw, deg_obs),\n  Type = rep(c(\"Small-World\",\"Observed\"), times = c(length(deg_obs), length(deg_sw)))\n)\n # Reverse the factor levels\ndeg_df$Type &lt;- factor(deg_df$Type, levels = c(\"Small-World\", \"Observed\"))\n\nggplot(deg_df, aes(x = Degree, fill = Type)) +\n  geom_histogram(position = \"identity\", bins = 20, alpha = 0.6, color = \"white\") +\n  facet_wrap(~Type, scales = \"free_y\") +\n  labs(x = \"Node Degree\", y = \"Frequency\") +\n  scale_fill_manual(values = c(\"skyblue\",\"tomato\")) +\n  theme_minimal() +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\nThe simulation demonstrates how the small-world model approximates certain properties of the observed network. As shown in the table, the simulated network achieves a relatively short average path length, similar to that of the Les Misérables network, due to the introduction of random long-range ties. The clustering coefficient remains substantial, reflecting the model’s ability to preserve local neighborhood structure.\nHowever, the degree distribution in the small-world model shown in remains relatively narrow, with most nodes having degrees close to the average. This limitation highlights that while the small-world model captures some global and local properties, it does not account for degree heterogeneity. The results show that the small-world model offers a useful structural middle ground between regular and fully random graphs but still lacks the full complexity observed in empirical networks.\nNote that the choice of the rewiring probability \\(p\\) in the small-world model is crucial, as it balances regularity and randomness. Small values of \\(p\\) (e.g., between 0.01 and 0.2) are typically chosen to introduce enough randomness to significantly reduce path lengths, while still preserving high clustering. If \\(p\\) is too low, the network remains overly regular; if \\(p\\) is too high, the network behaves like a random graph and loses its local structure. In practice, \\(p\\) is often selected empirically to achieve small-world characteristics (high clustering and short average path length) relative to the number of nodes and degree.\nTo illustrate how the small-world model transitions between regular and random structure, we simulate multiple networks with the same number of nodes as Les Misérables network with varying values of the rewiring probability \\(p\\) and track how two key properties (clustering and average path length) change. This helps identify a “sweet spot” for \\(p\\) where the network retains high clustering but achieves short global paths, capturing the essence of small-world structure. The results are shown in ?@fig-rewire.\n\n\n\n\n\n\n\n\n\nThe plot shows a sharp transition in network structure as \\(p\\) increases. At \\(p = 0\\), the network is a regular lattice: clustering is high, but average path length is long. As \\(p\\) increases slightly (e.g., \\(p \\approx 0.1\\)), the average path length drops rapidly due to the introduction of long-range shortcuts, while clustering remains relatively high. This intermediate range is where small-world characteristics emerge.\nAs \\(p\\) approaches 1, the network becomes increasingly random (think \\(G(n,p)\\)): clustering drops off, and path length stabilizes at a low level. This demonstrates the trade-off between local cohesion and global efficiency controlled by the rewiring parameter \\(p\\)."
  },
  {
    "objectID": "teaching/sna/material/09/09-rgm.html#exercise",
    "href": "teaching/sna/material/09/09-rgm.html#exercise",
    "title": "Social Network Analysis",
    "section": "",
    "text": "Choose a network of yourself and analyze and compare results using the three introduced random graph models."
  },
  {
    "objectID": "teaching/sna/material/01/01-intro-pkgs.html",
    "href": "teaching/sna/material/01/01-intro-pkgs.html",
    "title": "Social Network Analysis",
    "section": "",
    "text": "Throughout the course we will use a variety of different packages of doing network analysis, modeling and visualization. Make sure to install them all and have them ready to load when needed:\n\ninstall.packages(\"igraph\")   \ninstall.packages(\"statnet\")  #installs ergm, network, and sna\ninstall.packages(\"snahelper\")\ninstall.packages(\"netUtils\")\ninstall.packages(\"ggraph\")\ninstall.packages(\"backbone\")\ninstall.packages(\"netrankr\")\ninstall.packages(\"signnet\")\ninstall.packages(\"intergraph\")\ninstall.packages(\"graphlayouts\")\ninstall.packages(\"visNetwork\")\ninstall.packages(\"patchwork\")\ninstall.packages(\"edgebundle\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"gganimate\")\ninstall.packages(\"ggforce\")\ninstall.packages(\"rsiena\")\n\ninstall.packages(\"remotes\")\nremotes::install_github(\"schochastics/networkdata\")\n\n\n\nWe will strat with loading the following packages:\n\nlibrary(igraph)\nlibrary(networkdata)\nlibrary(netUtils)\n\n\n\n\n(Interactive Session)\n\n\n\nnever load sna and igraph at the same time\n\nlibrary(sna)\n\n\ndata(\"flo_marriage\")\ndegree(flo_marriage)\n\nError in FUN(X[[i]], ...): as.edgelist.sna input must be an adjacency matrix/array, edgelist matrix, network, or sparse matrix, or list thereof.\n\n\nIf for any reason you have to load both, you can circumvent the error by explicitly stating package first\n\nigraph::degree(flo_marriage)\n\n  Acciaiuoli      Albizzi    Barbadori     Bischeri   Castellani       Ginori \n           1            3            2            3            3            1 \n    Guadagni Lamberteschi       Medici        Pazzi      Peruzzi        Pucci \n           4            1            6            1            3            0 \n     Ridolfi     Salviati      Strozzi   Tornabuoni \n           3            2            4            3 \n\n\nThe package intergraph can be used to transform an igraph object into a network object and vice versa.\n\n#install.packages(\"intergraph\")\nlibrary(intergraph)\nasNetwork(flo_marriage)\n\n Network attributes:\n  vertices = 16 \n  directed = FALSE \n  hyper = FALSE \n  loops = FALSE \n  multiple = FALSE \n  bipartite = FALSE \n  total edges= 20 \n    missing edges= 0 \n    non-missing edges= 20 \n\n Vertex attribute names: \n    vertex.names wealth X.priors X.ties \n\nNo edge attributes\n\ndegree(asNetwork(flo_marriage))\n\n [1]  2  6  4  6  6  2  8  2 12  2  6  0  6  4  8  6\n\n\nYou can unload a package without restarting R/RStudio.\n\ndetach(\"package:sna\", unload = TRUE)"
  },
  {
    "objectID": "teaching/sna/material/01/01-intro-pkgs.html#r-packages-for-network-analysis",
    "href": "teaching/sna/material/01/01-intro-pkgs.html#r-packages-for-network-analysis",
    "title": "Social Network Analysis",
    "section": "",
    "text": "Throughout the course we will use a variety of different packages of doing network analysis, modeling and visualization. Make sure to install them all and have them ready to load when needed:\n\ninstall.packages(\"igraph\")   \ninstall.packages(\"statnet\")  #installs ergm, network, and sna\ninstall.packages(\"snahelper\")\ninstall.packages(\"netUtils\")\ninstall.packages(\"ggraph\")\ninstall.packages(\"backbone\")\ninstall.packages(\"netrankr\")\ninstall.packages(\"signnet\")\ninstall.packages(\"intergraph\")\ninstall.packages(\"graphlayouts\")\ninstall.packages(\"visNetwork\")\ninstall.packages(\"patchwork\")\ninstall.packages(\"edgebundle\")\ninstall.packages(\"ggplot2\")\ninstall.packages(\"gganimate\")\ninstall.packages(\"ggforce\")\ninstall.packages(\"rsiena\")\n\ninstall.packages(\"remotes\")\nremotes::install_github(\"schochastics/networkdata\")\n\n\n\nWe will strat with loading the following packages:\n\nlibrary(igraph)\nlibrary(networkdata)\nlibrary(netUtils)\n\n\n\n\n(Interactive Session)\n\n\n\nnever load sna and igraph at the same time\n\nlibrary(sna)\n\n\ndata(\"flo_marriage\")\ndegree(flo_marriage)\n\nError in FUN(X[[i]], ...): as.edgelist.sna input must be an adjacency matrix/array, edgelist matrix, network, or sparse matrix, or list thereof.\n\n\nIf for any reason you have to load both, you can circumvent the error by explicitly stating package first\n\nigraph::degree(flo_marriage)\n\n  Acciaiuoli      Albizzi    Barbadori     Bischeri   Castellani       Ginori \n           1            3            2            3            3            1 \n    Guadagni Lamberteschi       Medici        Pazzi      Peruzzi        Pucci \n           4            1            6            1            3            0 \n     Ridolfi     Salviati      Strozzi   Tornabuoni \n           3            2            4            3 \n\n\nThe package intergraph can be used to transform an igraph object into a network object and vice versa.\n\n#install.packages(\"intergraph\")\nlibrary(intergraph)\nasNetwork(flo_marriage)\n\n Network attributes:\n  vertices = 16 \n  directed = FALSE \n  hyper = FALSE \n  loops = FALSE \n  multiple = FALSE \n  bipartite = FALSE \n  total edges= 20 \n    missing edges= 0 \n    non-missing edges= 20 \n\n Vertex attribute names: \n    vertex.names wealth X.priors X.ties \n\nNo edge attributes\n\ndegree(asNetwork(flo_marriage))\n\n [1]  2  6  4  6  6  2  8  2 12  2  6  0  6  4  8  6\n\n\nYou can unload a package without restarting R/RStudio.\n\ndetach(\"package:sna\", unload = TRUE)"
  },
  {
    "objectID": "teaching/sna/material/01/01-intro-pkgs.html#handling-network-data",
    "href": "teaching/sna/material/01/01-intro-pkgs.html#handling-network-data",
    "title": "Social Network Analysis",
    "section": "Handling network data",
    "text": "Handling network data\n\nInbuilt network data\nThe networkdata package includes around 1000 datsets and more than 2000 networks. Throughout the course will use several examples using data from this package. Spend some time exploring datasets in this package (you will be asked to choose and work on one of them for you empirical study).\n\ndata(package = \"networkdata\")\n\n\nFreeman’s datasets from http://moreno.ss.uci.edu/data (not available anymore)\nMovie networks from https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/T4HBA3\nCovert networks from https://sites.google.com/site/ucinetsoftware/datasets/covert-networks\nAnimal networks from https://bansallab.github.io/asnr/\nShakespeare’s plays networks build with data from https://github.com/mallaham/Shakespeare-Plays\nSome networks from http://konect.uni-koblenz.de/\nTennis networks compiled from https://github.com/JeffSackmann (please give credit to him if you use this data)\nStar Wars Character Interactions (Episode 1-7) from https://github.com/evelinag/StarWars-social-network"
  },
  {
    "objectID": "teaching/intro-stats/index.html",
    "href": "teaching/intro-stats/index.html",
    "title": "Introductory Statistics",
    "section": "",
    "text": "The lecture notes can be found in the following bookdown:\nFoundational Statistics (work in progress)"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/bootstrap.html",
    "href": "teaching/tidyverse-II/material/case-studies/bootstrap.html",
    "title": "Bootstrap resampling and tidy regression models",
    "section": "",
    "text": "Bootstrapping consists of randomly sampling a data set with replacement, then performing the analysis individually on each bootstrapped replicate. The variation in the resulting estimate is then a reasonable approximation of the variance in our estimate.\nLet’s say we want to fit a nonlinear model to the weight/mileage relationship in the mtcars data set.\n\nlibrary(tidymodels)\n\nggplot(mtcars, aes(mpg, wt)) + \n    geom_point()\n\n\n\n\n\n\n\n\nWe might use the method of nonlinear least squares (via the nls() function) to fit a model.\n\nnlsfit &lt;- nls(mpg ~ k / wt + b, mtcars, start = list(k = 1, b = 0))\nsummary(nlsfit)\n#&gt; \n#&gt; Formula: mpg ~ k/wt + b\n#&gt; \n#&gt; Parameters:\n#&gt;   Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; k   45.829      4.249  10.786 7.64e-12 ***\n#&gt; b    4.386      1.536   2.855  0.00774 ** \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.774 on 30 degrees of freedom\n#&gt; \n#&gt; Number of iterations to convergence: 1 \n#&gt; Achieved convergence tolerance: 6.813e-09\n\nggplot(mtcars, aes(wt, mpg)) +\n    geom_point() +\n    geom_line(aes(y = predict(nlsfit)))\n\n\n\n\n\n\n\n\nWhile this does provide a p-value and confidence intervals for the parameters, these are based on model assumptions that may not hold in real data. Bootstrapping is a popular method for providing confidence intervals and predictions that are more robust to the nature of the data."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/bootstrap.html#introduction",
    "href": "teaching/tidyverse-II/material/case-studies/bootstrap.html#introduction",
    "title": "Bootstrap resampling and tidy regression models",
    "section": "",
    "text": "Bootstrapping consists of randomly sampling a data set with replacement, then performing the analysis individually on each bootstrapped replicate. The variation in the resulting estimate is then a reasonable approximation of the variance in our estimate.\nLet’s say we want to fit a nonlinear model to the weight/mileage relationship in the mtcars data set.\n\nlibrary(tidymodels)\n\nggplot(mtcars, aes(mpg, wt)) + \n    geom_point()\n\n\n\n\n\n\n\n\nWe might use the method of nonlinear least squares (via the nls() function) to fit a model.\n\nnlsfit &lt;- nls(mpg ~ k / wt + b, mtcars, start = list(k = 1, b = 0))\nsummary(nlsfit)\n#&gt; \n#&gt; Formula: mpg ~ k/wt + b\n#&gt; \n#&gt; Parameters:\n#&gt;   Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; k   45.829      4.249  10.786 7.64e-12 ***\n#&gt; b    4.386      1.536   2.855  0.00774 ** \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.774 on 30 degrees of freedom\n#&gt; \n#&gt; Number of iterations to convergence: 1 \n#&gt; Achieved convergence tolerance: 6.813e-09\n\nggplot(mtcars, aes(wt, mpg)) +\n    geom_point() +\n    geom_line(aes(y = predict(nlsfit)))\n\n\n\n\n\n\n\n\nWhile this does provide a p-value and confidence intervals for the parameters, these are based on model assumptions that may not hold in real data. Bootstrapping is a popular method for providing confidence intervals and predictions that are more robust to the nature of the data."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/bootstrap.html#bootstrapping-models",
    "href": "teaching/tidyverse-II/material/case-studies/bootstrap.html#bootstrapping-models",
    "title": "Bootstrap resampling and tidy regression models",
    "section": "Bootstrapping models",
    "text": "Bootstrapping models\nWe can use the bootstraps() function in the rsample package to sample bootstrap replications. First, we construct 2000 bootstrap replicates of the data, each of which has been randomly sampled with replacement. The resulting object is an rset, which is a data frame with a column of rsplit objects.\nAn rsplit object has two main components: an analysis data set and an assessment data set, accessible via analysis(rsplit) and assessment(rsplit) respectively. For bootstrap samples, the analysis data set is the bootstrap sample itself, and the assessment data set consists of all the out-of-bag samples.\n\nset.seed(27)\nboots &lt;- bootstraps(mtcars, times = 2000, apparent = TRUE)\nboots\n#&gt; # Bootstrap sampling with apparent sample \n#&gt; # A tibble: 2,001 × 2\n#&gt;    splits          id           \n#&gt;    &lt;list&gt;          &lt;chr&gt;        \n#&gt;  1 &lt;split [32/13]&gt; Bootstrap0001\n#&gt;  2 &lt;split [32/10]&gt; Bootstrap0002\n#&gt;  3 &lt;split [32/13]&gt; Bootstrap0003\n#&gt;  4 &lt;split [32/11]&gt; Bootstrap0004\n#&gt;  5 &lt;split [32/9]&gt;  Bootstrap0005\n#&gt;  6 &lt;split [32/10]&gt; Bootstrap0006\n#&gt;  7 &lt;split [32/11]&gt; Bootstrap0007\n#&gt;  8 &lt;split [32/13]&gt; Bootstrap0008\n#&gt;  9 &lt;split [32/11]&gt; Bootstrap0009\n#&gt; 10 &lt;split [32/11]&gt; Bootstrap0010\n#&gt; # ℹ 1,991 more rows\n\nLet’s create a helper function to fit an nls() model on each bootstrap sample, and then use purrr::map() to apply this function to all the bootstrap samples at once. Similarly, we create a column of tidy coefficient information by unnesting.\n\nfit_nls_on_bootstrap &lt;- function(split) {\n    nls(mpg ~ k / wt + b, analysis(split), start = list(k = 1, b = 0))\n}\n\nboot_models &lt;-\n  boots %&gt;% \n  mutate(model = map(splits, fit_nls_on_bootstrap),\n         coef_info = map(model, tidy))\n\nboot_coefs &lt;- \n  boot_models %&gt;% \n  unnest(coef_info)\n\nThe unnested coefficient information contains a summary of each replication combined in a single data frame:\n\nboot_coefs\n#&gt; # A tibble: 4,002 × 8\n#&gt;    splits          id          model term  estimate std.error statistic  p.value\n#&gt;    &lt;list&gt;          &lt;chr&gt;       &lt;lis&gt; &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1 &lt;split [32/13]&gt; Bootstrap0… &lt;nls&gt; k        42.1       4.05     10.4  1.91e-11\n#&gt;  2 &lt;split [32/13]&gt; Bootstrap0… &lt;nls&gt; b         5.39      1.43      3.78 6.93e- 4\n#&gt;  3 &lt;split [32/10]&gt; Bootstrap0… &lt;nls&gt; k        49.9       5.66      8.82 7.82e-10\n#&gt;  4 &lt;split [32/10]&gt; Bootstrap0… &lt;nls&gt; b         3.73      1.92      1.94 6.13e- 2\n#&gt;  5 &lt;split [32/13]&gt; Bootstrap0… &lt;nls&gt; k        37.8       2.68     14.1  9.01e-15\n#&gt;  6 &lt;split [32/13]&gt; Bootstrap0… &lt;nls&gt; b         6.73      1.17      5.75 2.78e- 6\n#&gt;  7 &lt;split [32/11]&gt; Bootstrap0… &lt;nls&gt; k        45.6       4.45     10.2  2.70e-11\n#&gt;  8 &lt;split [32/11]&gt; Bootstrap0… &lt;nls&gt; b         4.75      1.62      2.93 6.38e- 3\n#&gt;  9 &lt;split [32/9]&gt;  Bootstrap0… &lt;nls&gt; k        43.6       4.63      9.41 1.85e-10\n#&gt; 10 &lt;split [32/9]&gt;  Bootstrap0… &lt;nls&gt; b         5.89      1.68      3.51 1.44e- 3\n#&gt; # ℹ 3,992 more rows"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/bootstrap.html#confidence-intervals",
    "href": "teaching/tidyverse-II/material/case-studies/bootstrap.html#confidence-intervals",
    "title": "Bootstrap resampling and tidy regression models",
    "section": "Confidence intervals",
    "text": "Confidence intervals\nWe can then calculate confidence intervals (using what is called the percentile method):\n\npercentile_intervals &lt;- int_pctl(boot_models, coef_info)\npercentile_intervals\n#&gt; # A tibble: 2 × 6\n#&gt;   term   .lower .estimate .upper .alpha .method   \n#&gt;   &lt;chr&gt;   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;     \n#&gt; 1 b      0.0475      4.12   7.31   0.05 percentile\n#&gt; 2 k     37.6        46.7   59.8    0.05 percentile\n\nOr we can use histograms to get a more detailed idea of the uncertainty in each estimate:\n\nggplot(boot_coefs, aes(estimate)) +\n  geom_histogram(bins = 30) +\n  facet_wrap( ~ term, scales = \"free\") +\n  geom_vline(aes(xintercept = .lower), data = percentile_intervals, col = \"blue\") +\n  geom_vline(aes(xintercept = .upper), data = percentile_intervals, col = \"blue\")\n\n\n\n\n\n\n\n\nThe rsample package also has functions for other types of confidence intervals."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/bootstrap.html#possible-model-fits",
    "href": "teaching/tidyverse-II/material/case-studies/bootstrap.html#possible-model-fits",
    "title": "Bootstrap resampling and tidy regression models",
    "section": "Possible model fits",
    "text": "Possible model fits\nWe can use augment() to visualize the uncertainty in the fitted curve. Since there are so many bootstrap samples, we’ll only show a sample of the model fits in our visualization:\n\nboot_aug &lt;- \n  boot_models %&gt;% \n  sample_n(200) %&gt;% \n  mutate(augmented = map(model, augment)) %&gt;% \n  unnest(augmented)\n\nboot_aug\n#&gt; # A tibble: 6,400 × 8\n#&gt;    splits          id            model  coef_info   mpg    wt .fitted .resid\n#&gt;    &lt;list&gt;          &lt;chr&gt;         &lt;list&gt; &lt;list&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1 &lt;split [32/11]&gt; Bootstrap1644 &lt;nls&gt;  &lt;tibble&gt;   16.4  4.07    15.6  0.829\n#&gt;  2 &lt;split [32/11]&gt; Bootstrap1644 &lt;nls&gt;  &lt;tibble&gt;   19.7  2.77    21.9 -2.21 \n#&gt;  3 &lt;split [32/11]&gt; Bootstrap1644 &lt;nls&gt;  &lt;tibble&gt;   19.2  3.84    16.4  2.84 \n#&gt;  4 &lt;split [32/11]&gt; Bootstrap1644 &lt;nls&gt;  &lt;tibble&gt;   21.4  2.78    21.8 -0.437\n#&gt;  5 &lt;split [32/11]&gt; Bootstrap1644 &lt;nls&gt;  &lt;tibble&gt;   26    2.14    27.8 -1.75 \n#&gt;  6 &lt;split [32/11]&gt; Bootstrap1644 &lt;nls&gt;  &lt;tibble&gt;   33.9  1.84    32.0  1.88 \n#&gt;  7 &lt;split [32/11]&gt; Bootstrap1644 &lt;nls&gt;  &lt;tibble&gt;   32.4  2.2     27.0  5.35 \n#&gt;  8 &lt;split [32/11]&gt; Bootstrap1644 &lt;nls&gt;  &lt;tibble&gt;   30.4  1.62    36.1 -5.70 \n#&gt;  9 &lt;split [32/11]&gt; Bootstrap1644 &lt;nls&gt;  &lt;tibble&gt;   21.5  2.46    24.4 -2.86 \n#&gt; 10 &lt;split [32/11]&gt; Bootstrap1644 &lt;nls&gt;  &lt;tibble&gt;   26    2.14    27.8 -1.75 \n#&gt; # ℹ 6,390 more rows\n\n\nggplot(boot_aug, aes(wt, mpg)) +\n  geom_line(aes(y = .fitted, group = id), alpha = .2, col = \"blue\") +\n  geom_point()\n\n\n\n\n\n\n\n\nWith only a few small changes, we could easily perform bootstrapping with other kinds of predictive or hypothesis testing models, since the tidy() and augment() functions works for many statistical outputs. As another example, we could use smooth.spline(), which fits a cubic smoothing spline to data:\n\nfit_spline_on_bootstrap &lt;- function(split) {\n    data &lt;- analysis(split)\n    smooth.spline(data$wt, data$mpg, df = 4)\n}\n\nboot_splines &lt;- \n  boots %&gt;% \n  sample_n(200) %&gt;% \n  mutate(spline = map(splits, fit_spline_on_bootstrap),\n         aug_train = map(spline, augment))\n\nsplines_aug &lt;- \n  boot_splines %&gt;% \n  unnest(aug_train)\n\nggplot(splines_aug, aes(x, y)) +\n  geom_line(aes(y = .fitted, group = id), alpha = 0.2, col = \"blue\") +\n  geom_point()"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/bootstrap.html#session-info",
    "href": "teaching/tidyverse-II/material/case-studies/bootstrap.html#session-info",
    "title": "Bootstrap resampling and tidy regression models",
    "section": "Session information",
    "text": "Session information\n\n#&gt; ─ Session info ─────────────────────────────────────────────────────\n#&gt;  version  R version 4.4.1 (2024-06-14)\n#&gt;  language (EN)\n#&gt;  date     2025-05-05\n#&gt;  pandoc   3.2\n#&gt;  quarto   1.7.29\n#&gt; \n#&gt; ─ Packages ─────────────────────────────────────────────────────────\n#&gt;  package      version date (UTC) source\n#&gt;  broom        1.0.7   2024-09-26 CRAN (R 4.4.1)\n#&gt;  dials        1.4.0   2025-02-13 CRAN (R 4.4.1)\n#&gt;  dplyr        1.1.4   2023-11-17 CRAN (R 4.4.0)\n#&gt;  ggplot2      3.5.1   2024-04-23 CRAN (R 4.4.0)\n#&gt;  infer        1.0.7   2024-03-25 CRAN (R 4.4.0)\n#&gt;  parsnip      1.3.1   2025-03-12 CRAN (R 4.4.1)\n#&gt;  purrr        1.0.4   2025-02-05 CRAN (R 4.4.1)\n#&gt;  recipes      1.2.0   2025-03-17 CRAN (R 4.4.1)\n#&gt;  rlang        1.1.5   2025-01-17 CRAN (R 4.4.1)\n#&gt;  rsample      1.2.1   2024-03-25 CRAN (R 4.4.0)\n#&gt;  tibble       3.2.1   2023-03-20 CRAN (R 4.4.0)\n#&gt;  tidymodels   1.3.0   2025-02-21 CRAN (R 4.4.1)\n#&gt;  tune         1.3.0   2025-02-21 CRAN (R 4.4.1)\n#&gt;  workflows    1.2.0   2025-02-19 CRAN (R 4.4.1)\n#&gt;  yardstick    1.3.2   2025-01-22 CRAN (R 4.4.1)\n#&gt; \n#&gt; ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/coefficients.html",
    "href": "teaching/tidyverse-II/material/case-studies/coefficients.html",
    "title": "Working with model coefficients",
    "section": "",
    "text": "There are many types of statistical models with diverse kinds of structure. Some models have coefficients (a.k.a. weights) for each term in the model. Familiar examples of such models are linear or logistic regression, but more complex models (e.g. neural networks, MARS) can also have model coefficients. When we work with models that use weights or coefficients, we often want to examine the estimated coefficients.\nThis article describes how to retrieve the estimated coefficients from models fit using tidymodels. To use code in this article, you will need to install the following packages: glmnet and tidymodels."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/coefficients.html#introduction",
    "href": "teaching/tidyverse-II/material/case-studies/coefficients.html#introduction",
    "title": "Working with model coefficients",
    "section": "",
    "text": "There are many types of statistical models with diverse kinds of structure. Some models have coefficients (a.k.a. weights) for each term in the model. Familiar examples of such models are linear or logistic regression, but more complex models (e.g. neural networks, MARS) can also have model coefficients. When we work with models that use weights or coefficients, we often want to examine the estimated coefficients.\nThis article describes how to retrieve the estimated coefficients from models fit using tidymodels. To use code in this article, you will need to install the following packages: glmnet and tidymodels."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/coefficients.html#linear-regression",
    "href": "teaching/tidyverse-II/material/case-studies/coefficients.html#linear-regression",
    "title": "Working with model coefficients",
    "section": "Linear regression",
    "text": "Linear regression\nLet’s start with a linear regression model:\n\\[\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\ldots + \\hat{\\beta}_px_p\\]\nThe \\(\\beta\\) values are the coefficients and the \\(x_j\\) are model predictors, or features.\nLet’s use the Chicago train data where we predict the ridership at the Clark and Lake station (column name: ridership) with the previous ridership data 14 days prior at three of the stations.\nThe data are in the modeldata package:\n\nlibrary(tidymodels)\ntidymodels_prefer()\ntheme_set(theme_bw())\n\ndata(Chicago)\n\nChicago &lt;- Chicago %&gt;% select(ridership, Clark_Lake, Austin, Harlem)\n\n\nA single model\nLet’s start by fitting only a single parsnip model object. We’ll create a model specification using linear_reg().\n\n\n\n\n\n\nNote\n\n\n\nThe default engine is \"lm\" so no call to set_engine() is required.\n\n\nThe fit() function estimates the model coefficients, given a formula and data set.\n\nlm_spec &lt;- linear_reg()\nlm_fit &lt;- fit(lm_spec, ridership ~ ., data = Chicago)\nlm_fit\n#&gt; parsnip model object\n#&gt; \n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ridership ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)   Clark_Lake       Austin       Harlem  \n#&gt;      1.6778       0.9035       0.6123      -0.5550\n\nThe best way to retrieve the fitted parameters is to use the tidy() method. This function, in the broom package, returns the coefficients and their associated statistics in a data frame with standardized column names:\n\ntidy(lm_fit)\n#&gt; # A tibble: 4 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)    1.68     0.156      10.7  1.11e- 26\n#&gt; 2 Clark_Lake     0.904    0.0280     32.3  5.14e-210\n#&gt; 3 Austin         0.612    0.320       1.91 5.59e-  2\n#&gt; 4 Harlem        -0.555    0.165      -3.36 7.85e-  4\n\nWe’ll use this function in subsequent sections.\n\n\nResampled or tuned models\nThe tidymodels framework emphasizes the use of resampling methods to evaluate and characterize how well a model works. While time series resampling methods are appropriate for these data, we can also use the bootstrap to resample the data. This is a standard resampling approach when evaluating the uncertainty in statistical estimates.\nWe’ll use five bootstrap resamples of the data to simplify the plots and output (normally, we would use a larger number of resamples for more reliable estimates).\n\nset.seed(123)\nbt &lt;- bootstraps(Chicago, times = 5)\n\nWith resampling, we fit the same model to the different simulated versions of the data set produced by resampling. The tidymodels function fit_resamples() is the recommended approach for doing so.\n\n\n\n\n\n\nWarning\n\n\n\nThe fit_resamples() function does not automatically save the model objects for each resample since these can be quite large and its main purpose is estimating performance. However, we can pass a function to fit_resamples() that can save the model object or any other aspect of the fit.\n\n\nThis function takes a single argument that represents the fitted workflow object (even if you don’t give fit_resamples() a workflow).\nFrom this, we can extract the model fit. There are two “levels” of model objects that are available:\n\nThe parsnip model object, which wraps the underlying model object. We retrieve this using the extract_fit_parsnip() function.\nThe underlying model object (a.k.a. the engine fit) via the extract_fit_engine().\n\nWe’ll use the latter option and then tidy this model object as we did in the previous section. Let’s add this to the control function so that we can re-use it.\n\nget_lm_coefs &lt;- function(x) {\n  x %&gt;% \n    # get the lm model object\n    extract_fit_engine() %&gt;% \n    # transform its format\n    tidy()\n}\ntidy_ctrl &lt;- control_grid(extract = get_lm_coefs)\n\nThis argument is then passed to fit_resamples():\n\nlm_res &lt;- \n  lm_spec %&gt;% \n  fit_resamples(ridership ~ ., resamples = bt, control = tidy_ctrl)\nlm_res\n#&gt; # Resampling results\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 5 × 5\n#&gt;   splits              id         .metrics         .notes           .extracts\n#&gt;   &lt;list&gt;              &lt;chr&gt;      &lt;list&gt;           &lt;list&gt;           &lt;list&gt;   \n#&gt; 1 &lt;split [5698/2076]&gt; Bootstrap1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n#&gt; 2 &lt;split [5698/2098]&gt; Bootstrap2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n#&gt; 3 &lt;split [5698/2064]&gt; Bootstrap3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n#&gt; 4 &lt;split [5698/2082]&gt; Bootstrap4 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n#&gt; 5 &lt;split [5698/2088]&gt; Bootstrap5 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;\n\nNote that there is a .extracts column in our resampling results. This object contains the output of our get_lm_coefs() function for each resample. The structure of the elements of this column is a little complex. Let’s start by looking at the first element (which corresponds to the first resample):\n\nlm_res$.extracts[[1]]\n#&gt; # A tibble: 1 × 2\n#&gt;   .extracts        .config             \n#&gt;   &lt;list&gt;           &lt;chr&gt;               \n#&gt; 1 &lt;tibble [4 × 5]&gt; Preprocessor1_Model1\n\nThere is another column in this element called .extracts that has the results of the tidy() function call:\n\nlm_res$.extracts[[1]]$.extracts[[1]]\n#&gt; # A tibble: 4 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)    1.40     0.157       8.90 7.23e- 19\n#&gt; 2 Clark_Lake     0.842    0.0280     30.1  2.39e-184\n#&gt; 3 Austin         1.46     0.320       4.54 5.70e-  6\n#&gt; 4 Harlem        -0.637    0.163      -3.92 9.01e-  5\n\nThese nested columns can be flattened via the purrr unnest() function:\n\nlm_res %&gt;% \n  select(id, .extracts) %&gt;% \n  unnest(.extracts) \n#&gt; # A tibble: 5 × 3\n#&gt;   id         .extracts        .config             \n#&gt;   &lt;chr&gt;      &lt;list&gt;           &lt;chr&gt;               \n#&gt; 1 Bootstrap1 &lt;tibble [4 × 5]&gt; Preprocessor1_Model1\n#&gt; 2 Bootstrap2 &lt;tibble [4 × 5]&gt; Preprocessor1_Model1\n#&gt; 3 Bootstrap3 &lt;tibble [4 × 5]&gt; Preprocessor1_Model1\n#&gt; 4 Bootstrap4 &lt;tibble [4 × 5]&gt; Preprocessor1_Model1\n#&gt; 5 Bootstrap5 &lt;tibble [4 × 5]&gt; Preprocessor1_Model1\n\nWe still have a column of nested tibbles, so we can run the same command again to get the data into a more useful format:\n\nlm_coefs &lt;- \n  lm_res %&gt;% \n  select(id, .extracts) %&gt;% \n  unnest(.extracts) %&gt;% \n  unnest(.extracts)\n\nlm_coefs %&gt;% select(id, term, estimate, p.value)\n#&gt; # A tibble: 20 × 4\n#&gt;    id         term        estimate   p.value\n#&gt;    &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 Bootstrap1 (Intercept)    1.40  7.23e- 19\n#&gt;  2 Bootstrap1 Clark_Lake     0.842 2.39e-184\n#&gt;  3 Bootstrap1 Austin         1.46  5.70e-  6\n#&gt;  4 Bootstrap1 Harlem        -0.637 9.01e-  5\n#&gt;  5 Bootstrap2 (Intercept)    1.69  2.87e- 28\n#&gt;  6 Bootstrap2 Clark_Lake     0.911 1.06e-219\n#&gt;  7 Bootstrap2 Austin         0.595 5.93e-  2\n#&gt;  8 Bootstrap2 Harlem        -0.580 3.88e-  4\n#&gt;  9 Bootstrap3 (Intercept)    1.27  3.43e- 16\n#&gt; 10 Bootstrap3 Clark_Lake     0.859 5.03e-194\n#&gt; 11 Bootstrap3 Austin         1.09  6.77e-  4\n#&gt; 12 Bootstrap3 Harlem        -0.470 4.34e-  3\n#&gt; 13 Bootstrap4 (Intercept)    1.95  2.91e- 34\n#&gt; 14 Bootstrap4 Clark_Lake     0.974 1.47e-233\n#&gt; 15 Bootstrap4 Austin        -0.116 7.21e-  1\n#&gt; 16 Bootstrap4 Harlem        -0.620 2.11e-  4\n#&gt; 17 Bootstrap5 (Intercept)    1.87  1.98e- 33\n#&gt; 18 Bootstrap5 Clark_Lake     0.901 1.16e-210\n#&gt; 19 Bootstrap5 Austin         0.494 1.15e-  1\n#&gt; 20 Bootstrap5 Harlem        -0.512 1.73e-  3\n\nThat’s better! Now, let’s plot the model coefficients for each resample:\n\nlm_coefs %&gt;%\n  filter(term != \"(Intercept)\") %&gt;% \n  ggplot(aes(x = term, y = estimate, group = id, col = id)) +  \n  geom_hline(yintercept = 0, lty = 3) + \n  geom_line(alpha = 0.3, lwd = 1.2) + \n  labs(y = \"Coefficient\", x = NULL) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nThere seems to be a lot of uncertainty in the coefficient for the Austin station data, but less for the other two.\nLooking at the code for unnesting the results, you may find the double-nesting structure excessive or cumbersome. However, the extraction functionality is flexible, and a simpler structure would prevent many use cases."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/coefficients.html#more-complex-a-glmnet-model",
    "href": "teaching/tidyverse-II/material/case-studies/coefficients.html#more-complex-a-glmnet-model",
    "title": "Working with model coefficients",
    "section": "More complex: a glmnet model",
    "text": "More complex: a glmnet model\nThe glmnet model can fit the same linear regression model structure shown above. It uses regularization (a.k.a penalization) to estimate the model parameters. This has the benefit of shrinking the coefficients towards zero, important in situations where there are strong correlations between predictors or if some feature selection is required. Both of these cases are true for our Chicago train data set.\nThere are two types of penalization that this model uses:\n\nLasso (a.k.a. \\(L_1\\)) penalties can shrink the model terms so much that they are absolute zero (i.e. their effect is entirely removed from the model).\nWeight decay (a.k.a ridge regression or \\(L_2\\)) uses a different type of penalty that is most useful for highly correlated predictors.\n\nThe glmnet model has two primary tuning parameters, the total amount of penalization and the mixture of the two penalty types. For example, this specification:\n\nglmnet_spec &lt;- \n  linear_reg(penalty = 0.1, mixture = 0.95) %&gt;% \n  set_engine(\"glmnet\")\n\nhas a penalty that is 95% lasso and 5% weight decay. The total amount of these two penalties is 0.1 (which is fairly high).\n\n\n\n\n\n\nNote\n\n\n\nModels with regularization require that predictors are all on the same scale. The ridership at our three stations are very different, but glmnet automatically centers and scales the data. You can use recipes to center and scale your data yourself.\n\n\nLet’s combine the model specification with a formula in a model workflow() and then fit the model to the data:\n\nglmnet_wflow &lt;- \n  workflow() %&gt;% \n  add_model(glmnet_spec) %&gt;% \n  add_formula(ridership ~ .)\n\nglmnet_fit &lt;- fit(glmnet_wflow, Chicago)\nglmnet_fit\n#&gt; ══ Workflow [trained] ════════════════════════════════════════════════\n#&gt; Preprocessor: Formula\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ──────────────────────────────────────────────────────\n#&gt; ridership ~ .\n#&gt; \n#&gt; ── Model ─────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = \"gaussian\",      alpha = ~0.95) \n#&gt; \n#&gt;    Df  %Dev Lambda\n#&gt; 1   0  0.00 6.1040\n#&gt; 2   1 12.75 5.5620\n#&gt; 3   1 23.45 5.0680\n#&gt; 4   1 32.43 4.6180\n#&gt; 5   1 39.95 4.2070\n#&gt; 6   1 46.25 3.8340\n#&gt; 7   1 51.53 3.4930\n#&gt; 8   1 55.94 3.1830\n#&gt; 9   1 59.62 2.9000\n#&gt; 10  1 62.70 2.6420\n#&gt; 11  2 65.28 2.4080\n#&gt; 12  2 67.44 2.1940\n#&gt; 13  2 69.23 1.9990\n#&gt; 14  2 70.72 1.8210\n#&gt; 15  2 71.96 1.6600\n#&gt; 16  2 73.00 1.5120\n#&gt; 17  2 73.86 1.3780\n#&gt; 18  2 74.57 1.2550\n#&gt; 19  2 75.17 1.1440\n#&gt; 20  2 75.66 1.0420\n#&gt; 21  2 76.07 0.9496\n#&gt; 22  2 76.42 0.8653\n#&gt; 23  2 76.70 0.7884\n#&gt; 24  2 76.94 0.7184\n#&gt; 25  2 77.13 0.6545\n#&gt; 26  2 77.30 0.5964\n#&gt; 27  2 77.43 0.5434\n#&gt; 28  2 77.55 0.4951\n#&gt; 29  2 77.64 0.4512\n#&gt; 30  2 77.72 0.4111\n#&gt; 31  2 77.78 0.3746\n#&gt; 32  2 77.84 0.3413\n#&gt; 33  2 77.88 0.3110\n#&gt; 34  2 77.92 0.2833\n#&gt; 35  2 77.95 0.2582\n#&gt; 36  2 77.98 0.2352\n#&gt; 37  2 78.00 0.2143\n#&gt; 38  2 78.01 0.1953\n#&gt; 39  2 78.03 0.1779\n#&gt; 40  2 78.04 0.1621\n#&gt; 41  2 78.05 0.1477\n#&gt; 42  2 78.06 0.1346\n#&gt; 43  2 78.07 0.1226\n#&gt; 44  2 78.07 0.1118\n#&gt; 45  2 78.08 0.1018\n#&gt; 46  2 78.08 0.0928\n#&gt; \n#&gt; ...\n#&gt; and 9 more lines.\n\nIn this output, the term lambda is used to represent the penalty.\nNote that the output shows many values of the penalty despite our specification of penalty = 0.1. It turns out that this model fits a “path” of penalty values. Even though we are interested in a value of 0.1, we can get the model coefficients for many associated values of the penalty from the same model object.\nLet’s look at two different approaches to obtaining the coefficients. Both will use the tidy() method. One will tidy a glmnet object and the other will tidy a tidymodels object.\n\nUsing glmnet penalty values\nThis glmnet fit contains multiple penalty values which depend on the data set; changing the data (or the mixture amount) often produces a different set of values. For this data set, there are 55 penalties available. To get the set of penalties produced for this data set, we can extract the engine fit and tidy:\n\nglmnet_fit %&gt;% \n  extract_fit_engine() %&gt;% \n  tidy() %&gt;% \n  rename(penalty = lambda) %&gt;%   # &lt;- for consistent naming\n  filter(term != \"(Intercept)\")\n#&gt; # A tibble: 99 × 5\n#&gt;    term        step estimate penalty dev.ratio\n#&gt;    &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n#&gt;  1 Clark_Lake     2   0.0753    5.56     0.127\n#&gt;  2 Clark_Lake     3   0.145     5.07     0.234\n#&gt;  3 Clark_Lake     4   0.208     4.62     0.324\n#&gt;  4 Clark_Lake     5   0.266     4.21     0.400\n#&gt;  5 Clark_Lake     6   0.319     3.83     0.463\n#&gt;  6 Clark_Lake     7   0.368     3.49     0.515\n#&gt;  7 Clark_Lake     8   0.413     3.18     0.559\n#&gt;  8 Clark_Lake     9   0.454     2.90     0.596\n#&gt;  9 Clark_Lake    10   0.491     2.64     0.627\n#&gt; 10 Clark_Lake    11   0.526     2.41     0.653\n#&gt; # ℹ 89 more rows\n\nThis works well but, it turns out that our penalty value (0.1) is not in the list produced by the model! The underlying package has functions that use interpolation to produce coefficients for this specific value, but the tidy() method for glmnet objects does not use it.\n\n\nUsing specific penalty values\nIf we run the tidy() method on the workflow or parsnip object, a different function is used that returns the coefficients for the penalty value that we specified:\n\ntidy(glmnet_fit)\n#&gt; # A tibble: 4 × 3\n#&gt;   term        estimate penalty\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 (Intercept)    1.69      0.1\n#&gt; 2 Clark_Lake     0.846     0.1\n#&gt; 3 Austin         0.271     0.1\n#&gt; 4 Harlem         0         0.1\n\nFor any another (single) penalty, we can use an additional argument:\n\ntidy(glmnet_fit, penalty = 5.5620)  # A value from above\n#&gt; # A tibble: 4 × 3\n#&gt;   term        estimate penalty\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 (Intercept)  12.6       5.56\n#&gt; 2 Clark_Lake    0.0753    5.56\n#&gt; 3 Austin        0         5.56\n#&gt; 4 Harlem        0         5.56\n\nThe reason for having two tidy() methods is that, with tidymodels, the focus is on using a specific penalty value.\n\n\nTuning a glmnet model\nIf we know a priori acceptable values for penalty and mixture, we can use the fit_resamples() function as we did before with linear regression. Otherwise, we can tune those parameters with the tidymodels tune_*() functions.\nLet’s tune our glmnet model over both parameters with this grid:\n\npen_vals &lt;- 10^seq(-3, 0, length.out = 10)\ngrid &lt;- crossing(penalty = pen_vals, mixture = c(0.1, 1.0))\ngrid\n#&gt; # A tibble: 20 × 2\n#&gt;    penalty mixture\n#&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1 0.001       0.1\n#&gt;  2 0.001       1  \n#&gt;  3 0.00215     0.1\n#&gt;  4 0.00215     1  \n#&gt;  5 0.00464     0.1\n#&gt;  6 0.00464     1  \n#&gt;  7 0.01        0.1\n#&gt;  8 0.01        1  \n#&gt;  9 0.0215      0.1\n#&gt; 10 0.0215      1  \n#&gt; 11 0.0464      0.1\n#&gt; 12 0.0464      1  \n#&gt; 13 0.1         0.1\n#&gt; 14 0.1         1  \n#&gt; 15 0.215       0.1\n#&gt; 16 0.215       1  \n#&gt; 17 0.464       0.1\n#&gt; 18 0.464       1  \n#&gt; 19 1           0.1\n#&gt; 20 1           1\n\nHere is where more glmnet-related complexity comes in: we know that each resample and each value of mixture will probably produce a different set of penalty values contained in the model object. How can we look at the coefficients at the specific penalty values that we are using to tune?\nThe approach that we suggest is to use the special path_values option for glmnet. Details are described in the technical documentation about glmnet and tidymodels but in short, this parameter will assign the collection of penalty values used by each glmnet fit (regardless of the data or value of mixture).\nWe can pass these as an engine argument and then update our previous workflow object:\n\nglmnet_tune_spec &lt;- \n  linear_reg(penalty = tune(), mixture = tune()) %&gt;% \n  set_engine(\"glmnet\", path_values = pen_vals)\n\nglmnet_wflow &lt;- \n  glmnet_wflow %&gt;% \n  update_model(glmnet_tune_spec)\n\nNow we will use an extraction function similar to when we used ordinary least squares. We add an additional argument to retain coefficients that are shrunk to zero by the lasso penalty:\n\nget_glmnet_coefs &lt;- function(x) {\n  x %&gt;% \n    extract_fit_engine() %&gt;% \n    tidy(return_zeros = TRUE) %&gt;% \n    rename(penalty = lambda)\n}\nparsnip_ctrl &lt;- control_grid(extract = get_glmnet_coefs)\n\nglmnet_res &lt;- \n  glmnet_wflow %&gt;% \n  tune_grid(\n    resamples = bt,\n    grid = grid,\n    control = parsnip_ctrl\n  )\nglmnet_res\n#&gt; # Tuning results\n#&gt; # Bootstrap sampling \n#&gt; # A tibble: 5 × 5\n#&gt;   splits              id         .metrics          .notes           .extracts\n#&gt;   &lt;list&gt;              &lt;chr&gt;      &lt;list&gt;            &lt;list&gt;           &lt;list&gt;   \n#&gt; 1 &lt;split [5698/2076]&gt; Bootstrap1 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n#&gt; 2 &lt;split [5698/2098]&gt; Bootstrap2 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n#&gt; 3 &lt;split [5698/2064]&gt; Bootstrap3 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n#&gt; 4 &lt;split [5698/2082]&gt; Bootstrap4 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt; \n#&gt; 5 &lt;split [5698/2088]&gt; Bootstrap5 &lt;tibble [40 × 6]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;\n\nAs noted before, the elements of the main .extracts column have an embedded list column with the results of get_glmnet_coefs():\n\nglmnet_res$.extracts[[1]] %&gt;% head()\n#&gt; # A tibble: 6 × 4\n#&gt;   penalty mixture .extracts         .config              \n#&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;list&gt;            &lt;chr&gt;                \n#&gt; 1       1     0.1 &lt;tibble [40 × 5]&gt; Preprocessor1_Model01\n#&gt; 2       1     0.1 &lt;tibble [40 × 5]&gt; Preprocessor1_Model02\n#&gt; 3       1     0.1 &lt;tibble [40 × 5]&gt; Preprocessor1_Model03\n#&gt; 4       1     0.1 &lt;tibble [40 × 5]&gt; Preprocessor1_Model04\n#&gt; 5       1     0.1 &lt;tibble [40 × 5]&gt; Preprocessor1_Model05\n#&gt; 6       1     0.1 &lt;tibble [40 × 5]&gt; Preprocessor1_Model06\n\nglmnet_res$.extracts[[1]]$.extracts[[1]] %&gt;% head()\n#&gt; # A tibble: 6 × 5\n#&gt;   term         step estimate penalty dev.ratio\n#&gt;   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)     1    0.568  1          0.769\n#&gt; 2 (Intercept)     2    0.432  0.464      0.775\n#&gt; 3 (Intercept)     3    0.607  0.215      0.779\n#&gt; 4 (Intercept)     4    0.846  0.1        0.781\n#&gt; 5 (Intercept)     5    1.06   0.0464     0.782\n#&gt; 6 (Intercept)     6    1.22   0.0215     0.783\n\nAs before, we’ll have to use a double unnest(). Since the penalty value is in both the top-level and lower-level .extracts, we’ll use select() to get rid of the first version (but keep mixture):\n\nglmnet_res %&gt;% \n  select(id, .extracts) %&gt;% \n  unnest(.extracts) %&gt;% \n  select(id, mixture, .extracts) %&gt;%  # &lt;- removes the first penalty column\n  unnest(.extracts)\n\nBut wait! We know that each glmnet fit contains all of the coefficients. This means, for a specific resample and value of mixture, the results are the same:\n\nall.equal(\n  # First bootstrap, first `mixture`, first `penalty`\n  glmnet_res$.extracts[[1]]$.extracts[[1]],\n  # First bootstrap, first `mixture`, second `penalty`\n  glmnet_res$.extracts[[1]]$.extracts[[2]]\n)\n#&gt; [1] TRUE\n\nFor this reason, we’ll add a slice(1) when grouping by id and mixture. This will get rid of the replicated results.\n\nglmnet_coefs &lt;- \n  glmnet_res %&gt;% \n  select(id, .extracts) %&gt;% \n  unnest(.extracts) %&gt;% \n  select(id, mixture, .extracts) %&gt;% \n  group_by(id, mixture) %&gt;%          # ┐\n  slice(1) %&gt;%                       # │ Remove the redundant results\n  ungroup() %&gt;%                      # ┘\n  unnest(.extracts)\n\nglmnet_coefs %&gt;% \n  select(id, penalty, mixture, term, estimate) %&gt;% \n  filter(term != \"(Intercept)\")\n#&gt; # A tibble: 300 × 5\n#&gt;    id         penalty mixture term       estimate\n#&gt;    &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n#&gt;  1 Bootstrap1 1           0.1 Clark_Lake    0.391\n#&gt;  2 Bootstrap1 0.464       0.1 Clark_Lake    0.485\n#&gt;  3 Bootstrap1 0.215       0.1 Clark_Lake    0.590\n#&gt;  4 Bootstrap1 0.1         0.1 Clark_Lake    0.680\n#&gt;  5 Bootstrap1 0.0464      0.1 Clark_Lake    0.746\n#&gt;  6 Bootstrap1 0.0215      0.1 Clark_Lake    0.793\n#&gt;  7 Bootstrap1 0.01        0.1 Clark_Lake    0.817\n#&gt;  8 Bootstrap1 0.00464     0.1 Clark_Lake    0.828\n#&gt;  9 Bootstrap1 0.00215     0.1 Clark_Lake    0.834\n#&gt; 10 Bootstrap1 0.001       0.1 Clark_Lake    0.837\n#&gt; # ℹ 290 more rows\n\nNow we have the coefficients. Let’s look at how they behave as more regularization is used:\n\nglmnet_coefs %&gt;% \n  filter(term != \"(Intercept)\") %&gt;% \n  mutate(mixture = format(mixture)) %&gt;% \n  ggplot(aes(x = penalty, y = estimate, col = mixture, groups = id)) + \n  geom_hline(yintercept = 0, lty = 3) +\n  geom_line(alpha = 0.5, lwd = 1.2) + \n  facet_wrap(~ term) + \n  scale_x_log10() +\n  scale_color_brewer(palette = \"Accent\") +\n  labs(y = \"coefficient\") +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\nNotice a couple of things:\n\nWith a pure lasso model (i.e., mixture = 1), the Austin station predictor is selected out in each resample. With a mixture of both penalties, its influence increases. Also, as the penalty increases, the uncertainty in this coefficient decreases.\nThe Harlem predictor is either quickly selected out of the model or goes from negative to positive."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/coefficients.html#session-info",
    "href": "teaching/tidyverse-II/material/case-studies/coefficients.html#session-info",
    "title": "Working with model coefficients",
    "section": "Session information",
    "text": "Session information\n\n#&gt; ─ Session info ─────────────────────────────────────────────────────\n#&gt;  version  R version 4.5.0 (2025-04-11)\n#&gt;  language (EN)\n#&gt;  date     2025-06-30\n#&gt;  pandoc   3.2\n#&gt;  quarto   1.7.29\n#&gt; \n#&gt; ─ Packages ─────────────────────────────────────────────────────────\n#&gt;  package      version date (UTC) source\n#&gt;  broom        1.0.8   2025-03-28 CRAN (R 4.5.0)\n#&gt;  dials        1.4.0   2025-02-13 CRAN (R 4.5.0)\n#&gt;  dplyr        1.1.4   2023-11-17 CRAN (R 4.5.0)\n#&gt;  ggplot2      3.5.2   2025-04-09 CRAN (R 4.5.0)\n#&gt;  glmnet       4.1-8   2023-08-22 CRAN (R 4.5.0)\n#&gt;  infer        1.0.8   2025-04-14 CRAN (R 4.5.0)\n#&gt;  parsnip      1.3.1   2025-03-12 CRAN (R 4.5.0)\n#&gt;  purrr        1.0.4   2025-02-05 CRAN (R 4.5.0)\n#&gt;  recipes      1.3.0   2025-04-17 CRAN (R 4.5.0)\n#&gt;  rlang        1.1.6   2025-04-11 CRAN (R 4.5.0)\n#&gt;  rsample      1.3.0   2025-04-02 CRAN (R 4.5.0)\n#&gt;  tibble       3.2.1   2023-03-20 CRAN (R 4.5.0)\n#&gt;  tidymodels   1.3.0   2025-02-21 CRAN (R 4.5.0)\n#&gt;  tune         1.3.0   2025-02-21 CRAN (R 4.5.0)\n#&gt;  workflows    1.2.0   2025-02-19 CRAN (R 4.5.0)\n#&gt;  yardstick    1.3.2   2025-01-22 CRAN (R 4.5.0)\n#&gt; \n#&gt; ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/infer.html",
    "href": "teaching/tidyverse-II/material/case-studies/infer.html",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "",
    "text": "This article only requires the tidymodels package.\nThe tidymodels package infer implements an expressive grammar to perform statistical inference that coheres with the tidyverse design framework. Rather than providing methods for specific statistical tests, this package consolidates the principles that are shared among common hypothesis tests into a set of 4 main verbs (functions), supplemented with many utilities to visualize and extract information from their outputs.\nRegardless of which hypothesis test we’re using, we’re still asking the same kind of question:\n\nIs the effect or difference in our observed data real, or due to chance?\n\nTo answer this question, we start by assuming that the observed data came from some world where “nothing is going on” (i.e. the observed effect was simply due to random chance), and call this assumption our null hypothesis. (In reality, we might not believe in the null hypothesis at all; the null hypothesis is in opposition to the alternate hypothesis, which supposes that the effect present in the observed data is actually due to the fact that “something is going on.”) We then calculate a test statistic from our data that describes the observed effect. We can use this test statistic to calculate a p-value, giving the probability that our observed data could come about if the null hypothesis was true. If this probability is below some pre-defined significance level \\(\\alpha\\), then we can reject our null hypothesis.\nIf you are new to hypothesis testing, take a look at\n\nSection 9.2 of Statistical Inference via Data Science\nThe American Statistical Association’s recent statement on p-values\n\nThe workflow of this package is designed around these ideas. Starting from some data set,\n\nspecify() allows you to specify the variable, or relationship between variables, that you’re interested in,\nhypothesize() allows you to declare the null hypothesis,\ngenerate() allows you to generate data reflecting the null hypothesis, and\ncalculate() allows you to calculate a distribution of statistics from the generated data to form the null distribution.\n\nThroughout this vignette, we make use of gss, a data set available in infer containing a sample of 500 observations of 11 variables from the General Social Survey.\n\nlibrary(tidymodels) # Includes the infer package\n\n# Set seed\nset.seed(1234)\n\n# load in the data set\ndata(gss)\n\n# take a look at its structure\ndplyr::glimpse(gss)\n#&gt; Rows: 500\n#&gt; Columns: 11\n#&gt; $ year    &lt;dbl&gt; 2014, 1994, 1998, 1996, 1994, 1996, 1990, 2016, 2000, 1998, 20…\n#&gt; $ age     &lt;dbl&gt; 36, 34, 24, 42, 31, 32, 48, 36, 30, 33, 21, 30, 38, 49, 25, 56…\n#&gt; $ sex     &lt;fct&gt; male, female, male, male, male, female, female, female, female…\n#&gt; $ college &lt;fct&gt; degree, no degree, degree, no degree, degree, no degree, no de…\n#&gt; $ partyid &lt;fct&gt; ind, rep, ind, ind, rep, rep, dem, ind, rep, dem, dem, ind, de…\n#&gt; $ hompop  &lt;dbl&gt; 3, 4, 1, 4, 2, 4, 2, 1, 5, 2, 4, 3, 4, 4, 2, 2, 3, 2, 1, 2, 5,…\n#&gt; $ hours   &lt;dbl&gt; 50, 31, 40, 40, 40, 53, 32, 20, 40, 40, 23, 52, 38, 72, 48, 40…\n#&gt; $ income  &lt;ord&gt; $25000 or more, $20000 - 24999, $25000 or more, $25000 or more…\n#&gt; $ class   &lt;fct&gt; middle class, working class, working class, working class, mid…\n#&gt; $ finrela &lt;fct&gt; below average, below average, below average, above average, ab…\n#&gt; $ weight  &lt;dbl&gt; 0.8960034, 1.0825000, 0.5501000, 1.0864000, 1.0825000, 1.08640…\n\nEach row is an individual survey response, containing some basic demographic information on the respondent as well as some additional variables. See ?gss for more information on the variables included and their source. Note that this data (and our examples on it) are for demonstration purposes only, and will not necessarily provide accurate estimates unless weighted properly. For these examples, let’s suppose that this data set is a representative sample of a population we want to learn about: American adults."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/infer.html#introduction",
    "href": "teaching/tidyverse-II/material/case-studies/infer.html#introduction",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "",
    "text": "This article only requires the tidymodels package.\nThe tidymodels package infer implements an expressive grammar to perform statistical inference that coheres with the tidyverse design framework. Rather than providing methods for specific statistical tests, this package consolidates the principles that are shared among common hypothesis tests into a set of 4 main verbs (functions), supplemented with many utilities to visualize and extract information from their outputs.\nRegardless of which hypothesis test we’re using, we’re still asking the same kind of question:\n\nIs the effect or difference in our observed data real, or due to chance?\n\nTo answer this question, we start by assuming that the observed data came from some world where “nothing is going on” (i.e. the observed effect was simply due to random chance), and call this assumption our null hypothesis. (In reality, we might not believe in the null hypothesis at all; the null hypothesis is in opposition to the alternate hypothesis, which supposes that the effect present in the observed data is actually due to the fact that “something is going on.”) We then calculate a test statistic from our data that describes the observed effect. We can use this test statistic to calculate a p-value, giving the probability that our observed data could come about if the null hypothesis was true. If this probability is below some pre-defined significance level \\(\\alpha\\), then we can reject our null hypothesis.\nIf you are new to hypothesis testing, take a look at\n\nSection 9.2 of Statistical Inference via Data Science\nThe American Statistical Association’s recent statement on p-values\n\nThe workflow of this package is designed around these ideas. Starting from some data set,\n\nspecify() allows you to specify the variable, or relationship between variables, that you’re interested in,\nhypothesize() allows you to declare the null hypothesis,\ngenerate() allows you to generate data reflecting the null hypothesis, and\ncalculate() allows you to calculate a distribution of statistics from the generated data to form the null distribution.\n\nThroughout this vignette, we make use of gss, a data set available in infer containing a sample of 500 observations of 11 variables from the General Social Survey.\n\nlibrary(tidymodels) # Includes the infer package\n\n# Set seed\nset.seed(1234)\n\n# load in the data set\ndata(gss)\n\n# take a look at its structure\ndplyr::glimpse(gss)\n#&gt; Rows: 500\n#&gt; Columns: 11\n#&gt; $ year    &lt;dbl&gt; 2014, 1994, 1998, 1996, 1994, 1996, 1990, 2016, 2000, 1998, 20…\n#&gt; $ age     &lt;dbl&gt; 36, 34, 24, 42, 31, 32, 48, 36, 30, 33, 21, 30, 38, 49, 25, 56…\n#&gt; $ sex     &lt;fct&gt; male, female, male, male, male, female, female, female, female…\n#&gt; $ college &lt;fct&gt; degree, no degree, degree, no degree, degree, no degree, no de…\n#&gt; $ partyid &lt;fct&gt; ind, rep, ind, ind, rep, rep, dem, ind, rep, dem, dem, ind, de…\n#&gt; $ hompop  &lt;dbl&gt; 3, 4, 1, 4, 2, 4, 2, 1, 5, 2, 4, 3, 4, 4, 2, 2, 3, 2, 1, 2, 5,…\n#&gt; $ hours   &lt;dbl&gt; 50, 31, 40, 40, 40, 53, 32, 20, 40, 40, 23, 52, 38, 72, 48, 40…\n#&gt; $ income  &lt;ord&gt; $25000 or more, $20000 - 24999, $25000 or more, $25000 or more…\n#&gt; $ class   &lt;fct&gt; middle class, working class, working class, working class, mid…\n#&gt; $ finrela &lt;fct&gt; below average, below average, below average, above average, ab…\n#&gt; $ weight  &lt;dbl&gt; 0.8960034, 1.0825000, 0.5501000, 1.0864000, 1.0825000, 1.08640…\n\nEach row is an individual survey response, containing some basic demographic information on the respondent as well as some additional variables. See ?gss for more information on the variables included and their source. Note that this data (and our examples on it) are for demonstration purposes only, and will not necessarily provide accurate estimates unless weighted properly. For these examples, let’s suppose that this data set is a representative sample of a population we want to learn about: American adults."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/infer.html#specify-variables",
    "href": "teaching/tidyverse-II/material/case-studies/infer.html#specify-variables",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "Specify variables",
    "text": "Specify variables\nThe specify() function can be used to specify which of the variables in the data set you’re interested in. If you’re only interested in, say, the age of the respondents, you might write:\n\ngss %&gt;%\n  specify(response = age)\n#&gt; Response: age (numeric)\n#&gt; # A tibble: 500 × 1\n#&gt;      age\n#&gt;    &lt;dbl&gt;\n#&gt;  1    36\n#&gt;  2    34\n#&gt;  3    24\n#&gt;  4    42\n#&gt;  5    31\n#&gt;  6    32\n#&gt;  7    48\n#&gt;  8    36\n#&gt;  9    30\n#&gt; 10    33\n#&gt; # ℹ 490 more rows\n\nOn the front end, the output of specify() just looks like it selects off the columns in the dataframe that you’ve specified. What do we see if we check the class of this object, though?\n\ngss %&gt;%\n  specify(response = age) %&gt;%\n  class()\n#&gt; [1] \"infer\"      \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nWe can see that the infer class has been appended on top of the dataframe classes; this new class stores some extra metadata.\nIf you’re interested in two variables (age and partyid, for example) you can specify() their relationship in one of two (equivalent) ways:\n\n# as a formula\ngss %&gt;%\n  specify(age ~ partyid)\n#&gt; Response: age (numeric)\n#&gt; Explanatory: partyid (factor)\n#&gt; # A tibble: 500 × 2\n#&gt;      age partyid\n#&gt;    &lt;dbl&gt; &lt;fct&gt;  \n#&gt;  1    36 ind    \n#&gt;  2    34 rep    \n#&gt;  3    24 ind    \n#&gt;  4    42 ind    \n#&gt;  5    31 rep    \n#&gt;  6    32 rep    \n#&gt;  7    48 dem    \n#&gt;  8    36 ind    \n#&gt;  9    30 rep    \n#&gt; 10    33 dem    \n#&gt; # ℹ 490 more rows\n\n# with the named arguments\ngss %&gt;%\n  specify(response = age, explanatory = partyid)\n#&gt; Response: age (numeric)\n#&gt; Explanatory: partyid (factor)\n#&gt; # A tibble: 500 × 2\n#&gt;      age partyid\n#&gt;    &lt;dbl&gt; &lt;fct&gt;  \n#&gt;  1    36 ind    \n#&gt;  2    34 rep    \n#&gt;  3    24 ind    \n#&gt;  4    42 ind    \n#&gt;  5    31 rep    \n#&gt;  6    32 rep    \n#&gt;  7    48 dem    \n#&gt;  8    36 ind    \n#&gt;  9    30 rep    \n#&gt; 10    33 dem    \n#&gt; # ℹ 490 more rows\n\nIf you’re doing inference on one proportion or a difference in proportions, you will need to use the success argument to specify which level of your response variable is a success. For instance, if you’re interested in the proportion of the population with a college degree, you might use the following code:\n\n# specifying for inference on proportions\ngss %&gt;%\n  specify(response = college, success = \"degree\")\n#&gt; Response: college (factor)\n#&gt; # A tibble: 500 × 1\n#&gt;    college  \n#&gt;    &lt;fct&gt;    \n#&gt;  1 degree   \n#&gt;  2 no degree\n#&gt;  3 degree   \n#&gt;  4 no degree\n#&gt;  5 degree   \n#&gt;  6 no degree\n#&gt;  7 no degree\n#&gt;  8 degree   \n#&gt;  9 degree   \n#&gt; 10 no degree\n#&gt; # ℹ 490 more rows"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/infer.html#declare-the-hypothesis",
    "href": "teaching/tidyverse-II/material/case-studies/infer.html#declare-the-hypothesis",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "Declare the hypothesis",
    "text": "Declare the hypothesis\nThe next step in the infer pipeline is often to declare a null hypothesis using hypothesize(). The first step is to supply one of “independence” or “point” to the null argument. If your null hypothesis assumes independence between two variables, then this is all you need to supply to hypothesize():\n\ngss %&gt;%\n  specify(college ~ partyid, success = \"degree\") %&gt;%\n  hypothesize(null = \"independence\")\n#&gt; Response: college (factor)\n#&gt; Explanatory: partyid (factor)\n#&gt; Null Hypot...\n#&gt; # A tibble: 500 × 2\n#&gt;    college   partyid\n#&gt;    &lt;fct&gt;     &lt;fct&gt;  \n#&gt;  1 degree    ind    \n#&gt;  2 no degree rep    \n#&gt;  3 degree    ind    \n#&gt;  4 no degree ind    \n#&gt;  5 degree    rep    \n#&gt;  6 no degree rep    \n#&gt;  7 no degree dem    \n#&gt;  8 degree    ind    \n#&gt;  9 degree    rep    \n#&gt; 10 no degree dem    \n#&gt; # ℹ 490 more rows\n\nIf you’re doing inference on a point estimate, you will also need to provide one of p (the true proportion of successes, between 0 and 1), mu (the true mean), med (the true median), or sigma (the true standard deviation). For instance, if the null hypothesis is that the mean number of hours worked per week in our population is 40, we would write:\n\ngss %&gt;%\n  specify(response = hours) %&gt;%\n  hypothesize(null = \"point\", mu = 40)\n#&gt; Response: hours (numeric)\n#&gt; Null Hypothesis: point\n#&gt; # A tibble: 500 × 1\n#&gt;    hours\n#&gt;    &lt;dbl&gt;\n#&gt;  1    50\n#&gt;  2    31\n#&gt;  3    40\n#&gt;  4    40\n#&gt;  5    40\n#&gt;  6    53\n#&gt;  7    32\n#&gt;  8    20\n#&gt;  9    40\n#&gt; 10    40\n#&gt; # ℹ 490 more rows\n\nAgain, from the front-end, the dataframe outputted from hypothesize() looks almost exactly the same as it did when it came out of specify(), but infer now “knows” your null hypothesis."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/infer.html#generate-the-distribution",
    "href": "teaching/tidyverse-II/material/case-studies/infer.html#generate-the-distribution",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "Generate the distribution",
    "text": "Generate the distribution\nOnce we’ve asserted our null hypothesis using hypothesize(), we can construct a null distribution based on this hypothesis. We can do this using one of several methods, supplied in the type argument:\n\nbootstrap: A bootstrap sample will be drawn for each replicate, where a sample of size equal to the input sample size is drawn (with replacement) from the input sample data.\n\npermute: For each replicate, each input value will be randomly reassigned (without replacement) to a new output value in the sample.\n\nsimulate: A value will be sampled from a theoretical distribution with parameters specified in hypothesize() for each replicate. (This option is currently only applicable for testing point estimates.)\n\nContinuing on with our example above, about the average number of hours worked a week, we might write:\n\ngss %&gt;%\n  specify(response = hours) %&gt;%\n  hypothesize(null = \"point\", mu = 40) %&gt;%\n  generate(reps = 5000, type = \"bootstrap\")\n#&gt; Response: hours (numeric)\n#&gt; Null Hypothesis: point\n#&gt; # A tibble: 2,500,000 × 2\n#&gt; # Groups:   replicate [5,000]\n#&gt;    replicate hours\n#&gt;        &lt;int&gt; &lt;dbl&gt;\n#&gt;  1         1  58.6\n#&gt;  2         1  35.6\n#&gt;  3         1  28.6\n#&gt;  4         1  38.6\n#&gt;  5         1  28.6\n#&gt;  6         1  38.6\n#&gt;  7         1  38.6\n#&gt;  8         1  57.6\n#&gt;  9         1  58.6\n#&gt; 10         1  38.6\n#&gt; # ℹ 2,499,990 more rows\n\nIn the above example, we take 5000 bootstrap samples to form our null distribution.\nTo generate a null distribution for the independence of two variables, we could also randomly reshuffle the pairings of explanatory and response variables to break any existing association. For instance, to generate 5000 replicates that can be used to create a null distribution under the assumption that political party affiliation is not affected by age:\n\ngss %&gt;%\n  specify(partyid ~ age) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 5000, type = \"permute\")\n#&gt; Response: partyid (factor)\n#&gt; Explanatory: age (numeric)\n#&gt; Null Hypothes...\n#&gt; # A tibble: 2,500,000 × 3\n#&gt; # Groups:   replicate [5,000]\n#&gt;    partyid   age replicate\n#&gt;    &lt;fct&gt;   &lt;dbl&gt;     &lt;int&gt;\n#&gt;  1 ind        36         1\n#&gt;  2 ind        34         1\n#&gt;  3 ind        24         1\n#&gt;  4 rep        42         1\n#&gt;  5 dem        31         1\n#&gt;  6 dem        32         1\n#&gt;  7 dem        48         1\n#&gt;  8 rep        36         1\n#&gt;  9 ind        30         1\n#&gt; 10 dem        33         1\n#&gt; # ℹ 2,499,990 more rows"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/infer.html#calculate-statistics",
    "href": "teaching/tidyverse-II/material/case-studies/infer.html#calculate-statistics",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "Calculate statistics",
    "text": "Calculate statistics\nDepending on whether you’re carrying out computation-based inference or theory-based inference, you will either supply calculate() with the output of generate() or hypothesize(), respectively. The function, for one, takes in a stat argument, which is currently one of \"mean\", \"median\", \"sum\", \"sd\", \"prop\", \"count\", \"diff in means\", \"diff in medians\", \"diff in props\", \"Chisq\", \"F\", \"t\", \"z\", \"slope\", or \"correlation\". For example, continuing our example above to calculate the null distribution of mean hours worked per week:\n\ngss %&gt;%\n  specify(response = hours) %&gt;%\n  hypothesize(null = \"point\", mu = 40) %&gt;%\n  generate(reps = 5000, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"mean\")\n#&gt; Response: hours (numeric)\n#&gt; Null Hypothesis: point\n#&gt; # A tibble: 5,000 × 2\n#&gt;    replicate  stat\n#&gt;        &lt;int&gt; &lt;dbl&gt;\n#&gt;  1         1  39.8\n#&gt;  2         2  39.6\n#&gt;  3         3  39.8\n#&gt;  4         4  39.2\n#&gt;  5         5  39.0\n#&gt;  6         6  39.8\n#&gt;  7         7  40.6\n#&gt;  8         8  40.6\n#&gt;  9         9  40.4\n#&gt; 10        10  39.0\n#&gt; # ℹ 4,990 more rows\n\nThe output of calculate() here shows us the sample statistic (in this case, the mean) for each of our 1000 replicates. If you’re carrying out inference on differences in means, medians, or proportions, or \\(t\\) and \\(z\\) statistics, you will need to supply an order argument, giving the order in which the explanatory variables should be subtracted. For instance, to find the difference in mean age of those that have a college degree and those that don’t, we might write:\n\ngss %&gt;%\n  specify(age ~ college) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 5000, type = \"permute\") %&gt;%\n  calculate(\"diff in means\", order = c(\"degree\", \"no degree\"))\n#&gt; Response: age (numeric)\n#&gt; Explanatory: college (factor)\n#&gt; Null Hypothes...\n#&gt; # A tibble: 5,000 × 2\n#&gt;    replicate    stat\n#&gt;        &lt;int&gt;   &lt;dbl&gt;\n#&gt;  1         1 -0.0378\n#&gt;  2         2  1.55  \n#&gt;  3         3  0.465 \n#&gt;  4         4  1.39  \n#&gt;  5         5 -0.161 \n#&gt;  6         6 -0.179 \n#&gt;  7         7  0.0151\n#&gt;  8         8  0.914 \n#&gt;  9         9 -1.32  \n#&gt; 10        10 -0.426 \n#&gt; # ℹ 4,990 more rows"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/infer.html#other-utilities",
    "href": "teaching/tidyverse-II/material/case-studies/infer.html#other-utilities",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "Other utilities",
    "text": "Other utilities\nThe infer package also offers several utilities to extract meaning out of summary statistics and null distributions; the package provides functions to visualize where a statistic is relative to a distribution (with visualize()), calculate p-values (with get_p_value()), and calculate confidence intervals (with get_confidence_interval()).\nTo illustrate, we’ll go back to the example of determining whether the mean number of hours worked per week is 40 hours.\n\n# find the point estimate\npoint_estimate &lt;- gss %&gt;%\n  specify(response = hours) %&gt;%\n  calculate(stat = \"mean\")\n\n# generate a null distribution\nnull_dist &lt;- gss %&gt;%\n  specify(response = hours) %&gt;%\n  hypothesize(null = \"point\", mu = 40) %&gt;%\n  generate(reps = 5000, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"mean\")\n\nOur point estimate 41.382 seems pretty close to 40, but a little bit different. We might wonder if this difference is just due to random chance, or if the mean number of hours worked per week in the population really isn’t 40.\nWe could initially just visualize the null distribution.\n\nnull_dist %&gt;%\n  visualize()\n\n\n\n\n\n\n\n\nWhere does our sample’s observed statistic lie on this distribution? We can use the obs_stat argument to specify this.\n\nnull_dist %&gt;%\n  visualize() +\n  shade_p_value(obs_stat = point_estimate, direction = \"two_sided\")\n\n\n\n\n\n\n\n\nNotice that infer has also shaded the regions of the null distribution that are as (or more) extreme than our observed statistic. (Also, note that we now use the + operator to apply the shade_p_value() function. This is because visualize() outputs a plot object from ggplot2 instead of a dataframe, and the + operator is needed to add the p-value layer to the plot object.) The red bar looks like it’s slightly far out on the right tail of the null distribution, so observing a sample mean of 41.382 hours would be somewhat unlikely if the mean was actually 40 hours. How unlikely, though?\n\n# get a two-tailed p-value\np_value &lt;- null_dist %&gt;%\n  get_p_value(obs_stat = point_estimate, direction = \"two_sided\")\n\np_value\n#&gt; # A tibble: 1 × 1\n#&gt;   p_value\n#&gt;     &lt;dbl&gt;\n#&gt; 1   0.046\n\nIt looks like the p-value is 0.046, which is pretty small—if the true mean number of hours worked per week was actually 40, the probability of our sample mean being this far (1.382 hours) from 40 would be 0.046. This may or may not be statistically significantly different, depending on the significance level \\(\\alpha\\) you decided on before you ran this analysis. If you had set \\(\\alpha = .05\\), then this difference would be statistically significant, but if you had set \\(\\alpha = .01\\), then it would not be.\nTo get a confidence interval around our estimate, we can write:\n\n# start with the null distribution\nnull_dist %&gt;%\n  # calculate the confidence interval around the point estimate\n  get_confidence_interval(point_estimate = point_estimate,\n                          # at the 95% confidence level\n                          level = .95,\n                          # using the standard error\n                          type = \"se\")\n#&gt; # A tibble: 1 × 2\n#&gt;   lower_ci upper_ci\n#&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1     40.1     42.7\n\nAs you can see, 40 hours per week is not contained in this interval, which aligns with our previous conclusion that this finding is significant at the confidence level \\(\\alpha = .05\\)."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/infer.html#theoretical-methods",
    "href": "teaching/tidyverse-II/material/case-studies/infer.html#theoretical-methods",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "Theoretical methods",
    "text": "Theoretical methods\nThe infer package also provides functionality to use theoretical methods for \"Chisq\", \"F\" and \"t\" test statistics.\nGenerally, to find a null distribution using theory-based methods, use the same code that you would use to find the null distribution using randomization-based methods, but skip the generate() step. For example, if we wanted to find a null distribution for the relationship between age (age) and party identification (partyid) using randomization, we could write:\n\nnull_f_distn &lt;- gss %&gt;%\n   specify(age ~ partyid) %&gt;%\n   hypothesize(null = \"independence\") %&gt;%\n   generate(reps = 5000, type = \"permute\") %&gt;%\n   calculate(stat = \"F\")\n\nTo find the null distribution using theory-based methods, instead, skip the generate() step entirely:\n\nnull_f_distn_theoretical &lt;- gss %&gt;%\n   specify(age ~ partyid) %&gt;%\n   hypothesize(null = \"independence\") %&gt;%\n   calculate(stat = \"F\")\n\nWe’ll calculate the observed statistic to make use of in the following visualizations; this procedure is the same, regardless of the methods used to find the null distribution.\n\nF_hat &lt;- gss %&gt;% \n  specify(age ~ partyid) %&gt;%\n  calculate(stat = \"F\")\n\nNow, instead of just piping the null distribution into visualize(), as we would do if we wanted to visualize the randomization-based null distribution, we also need to provide method = \"theoretical\" to visualize().\n\nvisualize(null_f_distn_theoretical, method = \"theoretical\") +\n  shade_p_value(obs_stat = F_hat, direction = \"greater\")\n\n\n\n\n\n\n\n\nTo get a sense of how the theory-based and randomization-based null distributions relate, we can pipe the randomization-based null distribution into visualize() and also specify method = \"both\"\n\nvisualize(null_f_distn, method = \"both\") +\n  shade_p_value(obs_stat = F_hat, direction = \"greater\")\n\n\n\n\n\n\n\n\nThat’s it! See help(package = \"infer\") for a full list of functions and vignettes."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/infer.html#session-info",
    "href": "teaching/tidyverse-II/material/case-studies/infer.html#session-info",
    "title": "Hypothesis testing using resampling and tidy data",
    "section": "Session information",
    "text": "Session information\n\n#&gt; ─ Session info ─────────────────────────────────────────────────────\n#&gt;  version  R version 4.5.0 (2025-04-11)\n#&gt;  language (EN)\n#&gt;  date     2025-07-07\n#&gt;  pandoc   3.2\n#&gt;  quarto   1.7.29\n#&gt; \n#&gt; ─ Packages ─────────────────────────────────────────────────────────\n#&gt;  package      version date (UTC) source\n#&gt;  broom        1.0.8   2025-03-28 CRAN (R 4.5.0)\n#&gt;  dials        1.4.0   2025-02-13 CRAN (R 4.5.0)\n#&gt;  dplyr        1.1.4   2023-11-17 CRAN (R 4.5.0)\n#&gt;  ggplot2      3.5.2   2025-04-09 CRAN (R 4.5.0)\n#&gt;  infer        1.0.8   2025-04-14 CRAN (R 4.5.0)\n#&gt;  parsnip      1.3.1   2025-03-12 CRAN (R 4.5.0)\n#&gt;  purrr        1.0.4   2025-02-05 CRAN (R 4.5.0)\n#&gt;  recipes      1.3.0   2025-04-17 CRAN (R 4.5.0)\n#&gt;  rlang        1.1.6   2025-04-11 CRAN (R 4.5.0)\n#&gt;  rsample      1.3.0   2025-04-02 CRAN (R 4.5.0)\n#&gt;  tibble       3.2.1   2023-03-20 CRAN (R 4.5.0)\n#&gt;  tidymodels   1.3.0   2025-02-21 CRAN (R 4.5.0)\n#&gt;  tune         1.3.0   2025-02-21 CRAN (R 4.5.0)\n#&gt;  workflows    1.2.0   2025-02-19 CRAN (R 4.5.0)\n#&gt;  yardstick    1.3.2   2025-01-22 CRAN (R 4.5.0)\n#&gt; \n#&gt; ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/k-means.html",
    "href": "teaching/tidyverse-II/material/case-studies/k-means.html",
    "title": "K-means clustering with tidy data principles",
    "section": "",
    "text": "We only require the tidymodels package here.\nK-means clustering serves as a useful example of applying tidy data principles to statistical analysis, and especially the distinction between the three tidying functions:\n\ntidy()\naugment()\nglance()\n\nLet’s start by generating some random two-dimensional data with three clusters. Data in each cluster will come from a multivariate gaussian distribution, with different means for each cluster:\n\nlibrary(tidymodels)\n\nset.seed(77)\n\ncenters &lt;- tibble(\n  cluster = factor(1:3), \n  num_points = c(100, 150, 50),  # number points in each cluster\n  x1 = c(5, 0, -3),              # x1 coordinate of cluster center\n  x2 = c(-1, 1, -2)              # x2 coordinate of cluster center\n)\n\nlabelled_points &lt;- \n  centers %&gt;%\n  mutate(\n    x1 = map2(num_points, x1, rnorm),\n    x2 = map2(num_points, x2, rnorm)\n  ) %&gt;% \n  select(-num_points) %&gt;% \n  unnest(cols = c(x1, x2))\n\nggplot(labelled_points, aes(x1, x2, color = cluster)) +\n  geom_point(alpha = 0.3)\n\n\n\n\n\n\n\n\nThis is an ideal case for k-means clustering."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/k-means.html#introduction",
    "href": "teaching/tidyverse-II/material/case-studies/k-means.html#introduction",
    "title": "K-means clustering with tidy data principles",
    "section": "",
    "text": "We only require the tidymodels package here.\nK-means clustering serves as a useful example of applying tidy data principles to statistical analysis, and especially the distinction between the three tidying functions:\n\ntidy()\naugment()\nglance()\n\nLet’s start by generating some random two-dimensional data with three clusters. Data in each cluster will come from a multivariate gaussian distribution, with different means for each cluster:\n\nlibrary(tidymodels)\n\nset.seed(77)\n\ncenters &lt;- tibble(\n  cluster = factor(1:3), \n  num_points = c(100, 150, 50),  # number points in each cluster\n  x1 = c(5, 0, -3),              # x1 coordinate of cluster center\n  x2 = c(-1, 1, -2)              # x2 coordinate of cluster center\n)\n\nlabelled_points &lt;- \n  centers %&gt;%\n  mutate(\n    x1 = map2(num_points, x1, rnorm),\n    x2 = map2(num_points, x2, rnorm)\n  ) %&gt;% \n  select(-num_points) %&gt;% \n  unnest(cols = c(x1, x2))\n\nggplot(labelled_points, aes(x1, x2, color = cluster)) +\n  geom_point(alpha = 0.3)\n\n\n\n\n\n\n\n\nThis is an ideal case for k-means clustering."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/k-means.html#how-does-k-means-work",
    "href": "teaching/tidyverse-II/material/case-studies/k-means.html#how-does-k-means-work",
    "title": "K-means clustering with tidy data principles",
    "section": "How does K-means work?",
    "text": "How does K-means work?\nRather than using equations, this short animation using the artwork of Allison Horst explains the clustering process:"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/k-means.html#clustering-in-r",
    "href": "teaching/tidyverse-II/material/case-studies/k-means.html#clustering-in-r",
    "title": "K-means clustering with tidy data principles",
    "section": "Clustering in R",
    "text": "Clustering in R\nWe’ll use the built-in kmeans() function, which accepts a data frame with all numeric columns as it’s primary argument.\n\npoints &lt;- \n  labelled_points %&gt;% \n  select(-cluster)\n\nkclust &lt;- kmeans(points, centers = 3)\nkclust\n#&gt; K-means clustering with 3 clusters of sizes 49, 153, 98\n#&gt; \n#&gt; Cluster means:\n#&gt;           x1         x2\n#&gt; 1 -2.6692834 -1.8040871\n#&gt; 2 -0.1223998  1.1122543\n#&gt; 3  5.0697846 -0.9176465\n#&gt; \n#&gt; Clustering vector:\n#&gt;   [1] 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [38] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n#&gt;  [75] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [112] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [149] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [186] 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n#&gt; [223] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1\n#&gt; [260] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1\n#&gt; [297] 1 1 1 1\n#&gt; \n#&gt; Within cluster sum of squares by cluster:\n#&gt; [1]  92.23037 256.70331 209.30745\n#&gt;  (between_SS / total_SS =  83.9 %)\n#&gt; \n#&gt; Available components:\n#&gt; \n#&gt; [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n#&gt; [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\nsummary(kclust)\n#&gt;              Length Class  Mode   \n#&gt; cluster      300    -none- numeric\n#&gt; centers        6    -none- numeric\n#&gt; totss          1    -none- numeric\n#&gt; withinss       3    -none- numeric\n#&gt; tot.withinss   1    -none- numeric\n#&gt; betweenss      1    -none- numeric\n#&gt; size           3    -none- numeric\n#&gt; iter           1    -none- numeric\n#&gt; ifault         1    -none- numeric\n\nThe output is a list of vectors, where each component has a different length. There’s one of length 300, the same as our original data set. There are two elements of length 3 (withinss and tot.withinss) and centers is a matrix with 3 rows. And then there are the elements of length 1: totss, tot.withinss, betweenss, and iter. (The value ifault indicates possible algorithm problems.)\nThese differing lengths have important meaning when we want to tidy our data set; they signify that each type of component communicates a different kind of information.\n\ncluster (300 values) contains information about each point\ncenters, withinss, and size (3 values) contain information about each cluster\ntotss, tot.withinss, betweenss, and iter (1 value) contain information about the full clustering\n\nWhich of these do we want to extract? There is no right answer; each of them may be interesting to an analyst. Because they communicate entirely different information (not to mention there’s no straightforward way to combine them), they are extracted by separate functions. augment adds the point classifications to the original data set:\n\naugment(kclust, points)\n#&gt; # A tibble: 300 × 3\n#&gt;       x1      x2 .cluster\n#&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;   \n#&gt;  1  4.45 -1.69   3       \n#&gt;  2  6.09  0.764  3       \n#&gt;  3  5.64 -0.340  3       \n#&gt;  4  6.04  0.0481 3       \n#&gt;  5  5.17 -0.929  3       \n#&gt;  6  6.14 -2.09   3       \n#&gt;  7  4.03 -0.849  3       \n#&gt;  8  4.87 -0.0558 3       \n#&gt;  9  5.15 -1.38   3       \n#&gt; 10  6.44 -1.23   3       \n#&gt; # ℹ 290 more rows\n\nThe tidy() function summarizes on a per-cluster level:\n\ntidy(kclust)\n#&gt; # A tibble: 3 × 5\n#&gt;       x1     x2  size withinss cluster\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;fct&gt;  \n#&gt; 1 -2.67  -1.80     49     92.2 1      \n#&gt; 2 -0.122  1.11    153    257.  2      \n#&gt; 3  5.07  -0.918    98    209.  3\n\nAnd as it always does, the glance() function extracts a single-row summary:\n\nglance(kclust)\n#&gt; # A tibble: 1 × 4\n#&gt;   totss tot.withinss betweenss  iter\n#&gt;   &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n#&gt; 1 3457.         558.     2899.     2"
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/k-means.html#exploratory-clustering",
    "href": "teaching/tidyverse-II/material/case-studies/k-means.html#exploratory-clustering",
    "title": "K-means clustering with tidy data principles",
    "section": "Exploratory clustering",
    "text": "Exploratory clustering\nWhile these summaries are useful, they would not have been too difficult to extract out from the data set yourself. The real power comes from combining these analyses with other tools like dplyr.\nLet’s say we want to explore the effect of different choices of k, from 1 to 9, on this clustering. First cluster the data 9 times, each using a different value of k, then create columns containing the tidied, glanced and augmented data:\n\nkclusts &lt;- \n  tibble(k = 1:9) %&gt;%\n  mutate(\n    kclust = map(k, ~kmeans(points, .x)),\n    tidied = map(kclust, tidy),\n    glanced = map(kclust, glance),\n    augmented = map(kclust, augment, points)\n  )\n\nkclusts\n#&gt; # A tibble: 9 × 5\n#&gt;       k kclust   tidied           glanced          augmented         \n#&gt;   &lt;int&gt; &lt;list&gt;   &lt;list&gt;           &lt;list&gt;           &lt;list&gt;            \n#&gt; 1     1 &lt;kmeans&gt; &lt;tibble [1 × 5]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [300 × 3]&gt;\n#&gt; 2     2 &lt;kmeans&gt; &lt;tibble [2 × 5]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [300 × 3]&gt;\n#&gt; 3     3 &lt;kmeans&gt; &lt;tibble [3 × 5]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [300 × 3]&gt;\n#&gt; 4     4 &lt;kmeans&gt; &lt;tibble [4 × 5]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [300 × 3]&gt;\n#&gt; 5     5 &lt;kmeans&gt; &lt;tibble [5 × 5]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [300 × 3]&gt;\n#&gt; 6     6 &lt;kmeans&gt; &lt;tibble [6 × 5]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [300 × 3]&gt;\n#&gt; 7     7 &lt;kmeans&gt; &lt;tibble [7 × 5]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [300 × 3]&gt;\n#&gt; 8     8 &lt;kmeans&gt; &lt;tibble [8 × 5]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [300 × 3]&gt;\n#&gt; 9     9 &lt;kmeans&gt; &lt;tibble [9 × 5]&gt; &lt;tibble [1 × 4]&gt; &lt;tibble [300 × 3]&gt;\n\nWe can turn these into three separate data sets each representing a different type of data: using tidy(), using augment(), and using glance(). Each of these goes into a separate data set as they represent different types of data.\n\nclusters &lt;- \n  kclusts %&gt;%\n  unnest(cols = c(tidied))\n\nassignments &lt;- \n  kclusts %&gt;% \n  unnest(cols = c(augmented))\n\nclusterings &lt;- \n  kclusts %&gt;%\n  unnest(cols = c(glanced))\n\nNow we can plot the original points using the data from augment(), with each point colored according to the predicted cluster.\n\np1 &lt;- \n  ggplot(assignments, aes(x = x1, y = x2)) +\n  geom_point(aes(color = .cluster), alpha = 0.8) + \n  facet_wrap(~ k)\np1\n\n\n\n\n\n\n\n\nAlready we get a good sense of the proper number of clusters (3), and how the k-means algorithm functions when k is too high or too low. We can then add the centers of the cluster using the data from tidy():\n\np2 &lt;- p1 + geom_point(data = clusters, size = 10, shape = \"x\")\np2\n\n\n\n\n\n\n\n\nThe data from glance() fills a different but equally important purpose; it lets us view trends of some summary statistics across values of k. Of particular interest is the total within sum of squares, saved in the tot.withinss column.\n\nggplot(clusterings, aes(k, tot.withinss)) +\n  geom_line() +\n  geom_point()\n\n\n\n\n\n\n\n\nThis represents the variance within the clusters. It decreases as k increases, but notice a bend (or “elbow”) around k = 3. This bend indicates that additional clusters beyond the third have little value. (See here for a more mathematically rigorous interpretation and implementation of this method). Thus, all three methods of tidying data provided by broom are useful for summarizing clustering output."
  },
  {
    "objectID": "teaching/tidyverse-II/material/case-studies/k-means.html#session-info",
    "href": "teaching/tidyverse-II/material/case-studies/k-means.html#session-info",
    "title": "K-means clustering with tidy data principles",
    "section": "Session information",
    "text": "Session information\n\n#&gt; ─ Session info ─────────────────────────────────────────────────────\n#&gt;  version  R version 4.4.1 (2024-06-14)\n#&gt;  language (EN)\n#&gt;  date     2025-05-05\n#&gt;  pandoc   3.2\n#&gt;  quarto   1.7.29\n#&gt; \n#&gt; ─ Packages ─────────────────────────────────────────────────────────\n#&gt;  package      version date (UTC) source\n#&gt;  broom        1.0.7   2024-09-26 CRAN (R 4.4.1)\n#&gt;  dials        1.4.0   2025-02-13 CRAN (R 4.4.1)\n#&gt;  dplyr        1.1.4   2023-11-17 CRAN (R 4.4.0)\n#&gt;  ggplot2      3.5.1   2024-04-23 CRAN (R 4.4.0)\n#&gt;  infer        1.0.7   2024-03-25 CRAN (R 4.4.0)\n#&gt;  parsnip      1.3.1   2025-03-12 CRAN (R 4.4.1)\n#&gt;  purrr        1.0.4   2025-02-05 CRAN (R 4.4.1)\n#&gt;  recipes      1.2.0   2025-03-17 CRAN (R 4.4.1)\n#&gt;  rlang        1.1.5   2025-01-17 CRAN (R 4.4.1)\n#&gt;  rsample      1.2.1   2024-03-25 CRAN (R 4.4.0)\n#&gt;  tibble       3.2.1   2023-03-20 CRAN (R 4.4.0)\n#&gt;  tidymodels   1.3.0   2025-02-21 CRAN (R 4.4.1)\n#&gt;  tune         1.3.0   2025-02-21 CRAN (R 4.4.1)\n#&gt;  workflows    1.2.0   2025-02-19 CRAN (R 4.4.1)\n#&gt;  yardstick    1.3.2   2025-01-22 CRAN (R 4.4.1)\n#&gt; \n#&gt; ────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "teaching/stat-learn/index.html",
    "href": "teaching/stat-learn/index.html",
    "title": "Statistical Learning",
    "section": "",
    "text": "Schedule\n\n\n\n\nslides\npractical\ndata\nworksheet\n\n\n\n\n1: Introduction: What is Statistical Learning?\n\n\n.zip\n.qmd\n\n\n2: Linear Regression I\n\n\n\n\n\n\n3: Linear Regression II\n\n\n\n\n\n\n4: Classification I\n\n\n\n\n\n\n5: Classification II\n\n\n\n\n\n\n6: Model Validation\n\n\n\n\n\n\n7: Model Selection & Regularization\n\n\n\n\n\n\n8: “Non-linear” Linear Regression\n\n\n\n\n\n\n9: Tree Based Methods\n\n\n\n\n\n\n10: Support Vector Machines\n\n\n\n\n\n\n11: Neural Networks\n\n\n\n\n\n\n12: Clustering\n\n\n\n\n\n\n13: Principal Component Analysis\n\n\n\n\n\n\n14: Review"
  },
  {
    "objectID": "teaching/tidyverse-I/material/bechdel.html",
    "href": "teaching/tidyverse-I/material/bechdel.html",
    "title": "Bechdel",
    "section": "",
    "text": "In this mini analysis we work with the data used in the FiveThirtyEight story titled “The Dollar-And-Cents Case Against Hollywood’s Exclusion of Women”. We will together fill in the blanks denoted by ___."
  },
  {
    "objectID": "teaching/tidyverse-I/material/bechdel.html#data-and-packages",
    "href": "teaching/tidyverse-I/material/bechdel.html#data-and-packages",
    "title": "Bechdel",
    "section": "Data and packages",
    "text": "Data and packages\nWe start with loading the packages we’ll use.\n\nlibrary(fivethirtyeight)\nlibrary(tidyverse)\n\nThe dataset contains information on 1794 movies released between 1970 and 2013. However we’ll focus our analysis on movies released between 1990 and 2013.\n\nbechdel90_13 &lt;- bechdel %&gt;% \n  filter(between(year, 1990, 2013))\n\nThere are ___ such movies.\nThe financial variables we’ll focus on are the following:\n\nbudget_2013: Budget in 2013 inflation adjusted dollars\ndomgross_2013: Domestic gross (US) in 2013 inflation adjusted dollars\nintgross_2013: Total International (i.e., worldwide) gross in 2013 inflation adjusted dollars\n\nAnd we’ll also use the binary and clean_test variables for grouping."
  },
  {
    "objectID": "teaching/tidyverse-I/material/bechdel.html#analysis",
    "href": "teaching/tidyverse-I/material/bechdel.html#analysis",
    "title": "Bechdel",
    "section": "Analysis",
    "text": "Analysis\nLet’s take a look at how median budget and gross vary by whether the movie passed the Bechdel test, which is stored in the binary variable.\n\nbechdel90_13 %&gt;%\n  group_by(binary) %&gt;%\n  summarise(\n    med_budget = median(budget_2013),\n    med_domgross = median(domgross_2013, na.rm = TRUE),\n    med_intgross = median(intgross_2013, na.rm = TRUE)\n    )\n\n# A tibble: 2 × 4\n  binary med_budget med_domgross med_intgross\n  &lt;chr&gt;       &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 FAIL    48385984.    57318606.    104475669\n2 PASS    31070724     45330446.     80124349\n\n\nNext, let’s take a look at how median budget and gross vary by a more detailed indicator of the Bechdel test result. This information is stored in the clean_test variable, which takes on the following values:\n\nok = passes test\ndubious\nmen = women only talk about men\nnotalk = women don’t talk to each other\nnowomen = fewer than two women\n\n\nbechdel90_13 %&gt;%\n  #group_by(___) %&gt;%\n  summarise(\n    med_budget = median(budget_2013),\n    med_domgross = median(domgross_2013, na.rm = TRUE),\n    med_intgross = median(intgross_2013, na.rm = TRUE)\n    )\n\n# A tibble: 1 × 3\n  med_budget med_domgross med_intgross\n       &lt;int&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1   37878971     52270207     93523336\n\n\nIn order to evaluate how return on investment varies among movies that pass and fail the Bechdel test, we’ll first create a new variable called roi as the ratio of the gross to budget.\n\nbechdel90_13 &lt;- bechdel90_13 %&gt;%\n  mutate(roi = (intgross_2013 + domgross_2013) / budget_2013)\n\nLet’s see which movies have the highest return on investment.\n\nbechdel90_13 %&gt;%\n  arrange(desc(roi)) %&gt;% \n  select(title, roi, year)\n\n# A tibble: 1,615 × 3\n   title                     roi  year\n   &lt;chr&gt;                   &lt;dbl&gt; &lt;int&gt;\n 1 Paranormal Activity      671.  2007\n 2 The Blair Witch Project  648.  1999\n 3 El Mariachi              583.  1992\n 4 Clerks.                  258.  1994\n 5 In the Company of Men    231.  1997\n 6 Napoleon Dynamite        227.  2004\n 7 Once                     190.  2006\n 8 The Devil Inside         155.  2012\n 9 Primer                   142.  2004\n10 Fireproof                134.  2008\n# ℹ 1,605 more rows\n\n\nBelow is a visualization of the return on investment by test result, however it’s difficult to see the distributions due to a few extreme observations.\n\nggplot(data = bechdel90_13, \n       mapping = aes(x = clean_test, y = roi, color = binary)) +\n  geom_boxplot() +\n  labs(\n    title = \"Return on investment vs. Bechdel test result\",\n    x = \"Detailed Bechdel result\",\n    y = \"___\",\n    color = \"Binary Bechdel result\"\n    )\n\n\n\n\n\n\n\n\nWhat are those movies with very high returns on investment?\n\nbechdel90_13 %&gt;%\n  filter(roi &gt; 400) %&gt;%\n  select(title, budget_2013, domgross_2013, year)\n\n# A tibble: 3 × 4\n  title                   budget_2013 domgross_2013  year\n  &lt;chr&gt;                         &lt;int&gt;         &lt;dbl&gt; &lt;int&gt;\n1 Paranormal Activity          505595     121251476  2007\n2 The Blair Witch Project      839077     196538593  1999\n3 El Mariachi                   11622       3388636  1992\n\n\nZooming in on the movies with roi &lt; ___ provides a better view of how the medians across the categories compare:\n\nggplot(data = bechdel90_13, mapping = aes(x = clean_test, y = roi, color = binary)) +\n  geom_boxplot() +\n  labs(\n    title = \"Return on investment vs. Bechdel test result\",\n    subtitle = \"___\", # Something about zooming in to a certain level\n    x = \"Detailed Bechdel result\",\n    y = \"Return on investment\",\n    color = \"Binary Bechdel result\"\n    ) +\n  coord_cartesian(ylim = c(0, 15))"
  },
  {
    "objectID": "teaching/tidyverse-I/material/sales-excel.html",
    "href": "teaching/tidyverse-I/material/sales-excel.html",
    "title": "Sales",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl)\n\n\nRead in the Excel file called sales.xlsx from the data-raw/ folder such that it looks like the following.\n\n\n\n\n\n\n\n\n\n\n\nStretch goal: Manipulate the sales data such such that it looks like the following."
  },
  {
    "objectID": "teaching/tidyverse-I/material/starwars-ws.html",
    "href": "teaching/tidyverse-I/material/starwars-ws.html",
    "title": "Visualizing Starwars characters",
    "section": "",
    "text": "Glimpse at the starwars data frame.\n\n\nglimpse(starwars)\n\nRows: 87\nColumns: 14\n$ name       &lt;chr&gt; \"Luke Skywalker\", \"C-3PO\", \"R2-D2\", \"Darth Vader\", \"Leia Or…\n$ height     &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 2…\n$ mass       &lt;dbl&gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77.…\n$ hair_color &lt;chr&gt; \"blond\", NA, NA, \"none\", \"brown\", \"brown, grey\", \"brown\", N…\n$ skin_color &lt;chr&gt; \"fair\", \"gold\", \"white, blue\", \"white\", \"light\", \"light\", \"…\n$ eye_color  &lt;chr&gt; \"blue\", \"yellow\", \"red\", \"yellow\", \"brown\", \"blue\", \"blue\",…\n$ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57.0, …\n$ sex        &lt;chr&gt; \"male\", \"none\", \"none\", \"male\", \"female\", \"male\", \"female\",…\n$ gender     &lt;chr&gt; \"masculine\", \"masculine\", \"masculine\", \"masculine\", \"femini…\n$ homeworld  &lt;chr&gt; \"Tatooine\", \"Tatooine\", \"Naboo\", \"Tatooine\", \"Alderaan\", \"T…\n$ species    &lt;chr&gt; \"Human\", \"Droid\", \"Droid\", \"Human\", \"Human\", \"Human\", \"Huma…\n$ films      &lt;list&gt; &lt;\"A New Hope\", \"The Empire Strikes Back\", \"Return of the J…\n$ vehicles   &lt;list&gt; &lt;\"Snowspeeder\", \"Imperial Speeder Bike\"&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, \"Imp…\n$ starships  &lt;list&gt; &lt;\"X-wing\", \"Imperial shuttle\"&gt;, &lt;&gt;, &lt;&gt;, \"TIE Advanced x1\",…\n\n\n\nModify the following plot to change the color of all points to \"pink\".\n\n\nggplot(starwars, \n       aes(x = height, y = mass, color = gender, size = birth_year)) +\n  geom_point(color = \"pink\")\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nAdd labels for title, x and y axes, and size of points. Uncomment to see the effect.\n\n\nggplot(starwars, \n       aes(x = height, y = mass, color = gender, size = birth_year)) +\n  geom_point(color = \"#30509C\") +\n  labs(\n    #title = \"___\",\n    #x = \"___\", \n    #y = \"___\",\n    #___\n    )\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nPick a single numerical variable and make a histogram of it. Select a reasonable binwidth for it.\n\n(A little bit of starter code is provided below, and the code chunk is set to not be evaluated with eval: false because the current code in there is not valid code and hence the document wouldn’t knit. Once you replace the code with valid code, set the chunk option to eval: true, or remove the eval option altogether since it’s set to true by default.)\n\nggplot(starwars, aes(___)) +\n  geom___\n\n\nPick a numerical variable and a categorical variable and make a visualization (you pick the type!) to visualization the relationship between the two variables. Along with your code and output, provide an interpretation of the visualization.\n\nInterpretation goes here…\n\nPick a single categorical variable from the data set and make a bar plot of its distribution.\n\n\nPick two categorical variables and make a visualization to visualize the relationship between the two variables. Along with your code and output, provide an interpretation of the visualization.\n\nInterpretation goes here…\n\nPick two numerical variables and two categorical variables and make a visualization that incorporates all of them and provide an interpretation with your answer.\n\n(This time no starter code is provided, you’re on your own!)\nInterpretation goes here…"
  },
  {
    "objectID": "teaching/tidyverse-I/material/countries-of-the-world.html",
    "href": "teaching/tidyverse-I/material/countries-of-the-world.html",
    "title": "Countries of the world",
    "section": "",
    "text": "In order to complete this assignment you will need a Chrome browser with the Selector Gadget extension installed.\nThis website lists the names of 250 countries, as well as their flag, capital, population and size in square kilometres. Our goal could be to read this information into R for each country so that we can potentially analyse it further.\nBefore we start, we should load the required packages (we will also need the tidyverse package this time) and read the website with the function read_html() and assign it to an R object.\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(DT)\n\npage &lt;- read_html(\"https://scrapethissite.com/pages/simple/\")"
  },
  {
    "objectID": "teaching/tidyverse-I/material/countries-of-the-world.html#country-names",
    "href": "teaching/tidyverse-I/material/countries-of-the-world.html#country-names",
    "title": "Countries of the world",
    "section": "Country names",
    "text": "Country names\nUse the Selector Gadget to identify the CSS selectors needed to extract country names.\n\ncountry &lt;- page %&gt;%\n  html_elements(\".country-name\") %&gt;%\n  html_text(trim = TRUE) \n\nhead(country)\n\n[1] \"Andorra\"              \"United Arab Emirates\" \"Afghanistan\"         \n[4] \"Antigua and Barbuda\"  \"Anguilla\"             \"Albania\""
  },
  {
    "objectID": "teaching/tidyverse-I/material/countries-of-the-world.html#capitals-population-and-area",
    "href": "teaching/tidyverse-I/material/countries-of-the-world.html#capitals-population-and-area",
    "title": "Countries of the world",
    "section": "Capitals, population and area",
    "text": "Capitals, population and area\nLet us now turn to the further information for each country. Again use the selector gadget to identify the CSS selector needed which in this case is .country-info:\n\npage %&gt;%\n  html_elements(\".country-info\") %&gt;%\n  html_text(trim = TRUE) %&gt;% \n  head(n = 10)\n\n [1] \"Capital: Andorra la VellaPopulation: 84000Area (km2): 468.0\"   \n [2] \"Capital: Abu DhabiPopulation: 4975593Area (km2): 82880.0\"      \n [3] \"Capital: KabulPopulation: 29121286Area (km2): 647500.0\"        \n [4] \"Capital: St. John'sPopulation: 86754Area (km2): 443.0\"         \n [5] \"Capital: The ValleyPopulation: 13254Area (km2): 102.0\"         \n [6] \"Capital: TiranaPopulation: 2986952Area (km2): 28748.0\"         \n [7] \"Capital: YerevanPopulation: 2968000Area (km2): 29800.0\"        \n [8] \"Capital: LuandaPopulation: 13068161Area (km2): 1246700.0\"      \n [9] \"Capital: NonePopulation: 0Area (km2): 1.4E7\"                   \n[10] \"Capital: Buenos AiresPopulation: 41343201Area (km2): 2766890.0\"\n\n\nSo we get the names of the capitals, but also the population and the size of the country. The selector was not specific enough and we have to tell html_elements() more precisely which of these we are interested in. These CSS selectors differ between the three countries’ information:\n\nThe selector country-capital gives us the capital of the countries:\n\n\ncapital &lt;- page %&gt;%\n  html_elements(\".country-capital\") %&gt;%\n  html_text(trim = TRUE) \n\nhead(capital)\n\n[1] \"Andorra la Vella\" \"Abu Dhabi\"        \"Kabul\"            \"St. John's\"      \n[5] \"The Valley\"       \"Tirana\"          \n\n\n\nThe selector country-population gives us the population of the countries:\n\n\npopulation &lt;-  page %&gt;%\n  html_elements(\".country-population\") %&gt;%\n  html_text() %&gt;% \n  as.numeric()\nhead(population)\n\n[1]    84000  4975593 29121286    86754    13254  2986952\n\n\n\nThe selector country-area gives us the area of the countries:\n\n\narea &lt;-  page %&gt;%\n  html_elements(\".country-area\") %&gt;%\n  html_text() %&gt;% \n  as.numeric()\nhead(area)\n\n[1]    468  82880 647500    443    102  28748\n\n\nNote that we need to tell R to interpret the “text” read from the HTML code as numbers using the function as.numeric()."
  },
  {
    "objectID": "teaching/tidyverse-I/material/countries-of-the-world.html#merge-into-one-tibble",
    "href": "teaching/tidyverse-I/material/countries-of-the-world.html#merge-into-one-tibble",
    "title": "Countries of the world",
    "section": "Merge into one tibble",
    "text": "Merge into one tibble\nWe could already continue working with this, but for many applications it is more practical if we combine the data in a vertical form:\n\ncountries &lt;- tibble(\n  country = country,\n  capital = capital,\n  population = population,\n  area = area\n)\ncountries\n\n# A tibble: 250 × 4\n   country              capital          population     area\n   &lt;chr&gt;                &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;\n 1 Andorra              Andorra la Vella      84000      468\n 2 United Arab Emirates Abu Dhabi           4975593    82880\n 3 Afghanistan          Kabul              29121286   647500\n 4 Antigua and Barbuda  St. John's            86754      443\n 5 Anguilla             The Valley            13254      102\n 6 Albania              Tirana              2986952    28748\n 7 Armenia              Yerevan             2968000    29800\n 8 Angola               Luanda             13068161  1246700\n 9 Antarctica           None                      0 14000000\n10 Argentina            Buenos Aires       41343201  2766890\n# ℹ 240 more rows"
  },
  {
    "objectID": "teaching/tidyverse-I/material/countries-of-the-world.html#all-in-one-step",
    "href": "teaching/tidyverse-I/material/countries-of-the-world.html#all-in-one-step",
    "title": "Countries of the world",
    "section": "All in one step",
    "text": "All in one step\nIf we are sure that we do not need the individual vectors, we can also perform the reading of the data and the creation of the tibble in a single step. Below you can see how the complete scraping process can be completed in relatively few lines.\n\npage &lt;- \"https://scrapethissite.com/pages/simple/\" %&gt;%\n  read_html()\n\ncountries_2 &lt;- tibble(\n  Land = page %&gt;%\n    html_elements(css = \".country-name\") %&gt;% \n    html_text(trim = TRUE),\n  capital = page %&gt;% \n    html_elements(css = \".country-capital\") %&gt;% \n    html_text(),\n  population = page %&gt;% \n    html_elements(css = \".country-population\") %&gt;% \n    html_text() %&gt;% \n    as.numeric(),\n  area = page %&gt;% \n    html_elements(css = \".country-area\") %&gt;% \n    html_text() %&gt;% \n    as.numeric()\n)\n\ncountries_2\n\n# A tibble: 250 × 4\n   Land                 capital          population     area\n   &lt;chr&gt;                &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;\n 1 Andorra              Andorra la Vella      84000      468\n 2 United Arab Emirates Abu Dhabi           4975593    82880\n 3 Afghanistan          Kabul              29121286   647500\n 4 Antigua and Barbuda  St. John's            86754      443\n 5 Anguilla             The Valley            13254      102\n 6 Albania              Tirana              2986952    28748\n 7 Armenia              Yerevan             2968000    29800\n 8 Angola               Luanda             13068161  1246700\n 9 Antarctica           None                      0 14000000\n10 Argentina            Buenos Aires       41343201  2766890\n# ℹ 240 more rows"
  },
  {
    "objectID": "teaching/tidyverse-I/material/la-quinta-ws.html",
    "href": "teaching/tidyverse-I/material/la-quinta-ws.html",
    "title": "La Quinta is Spanish for next to Denny’s",
    "section": "",
    "text": "Have you ever taken a road trip in the US and thought to yourself “I wonder what La Quinta means”. Well, the late comedian Mitch Hedberg thinks it’s Spanish for next to Denny’s.\nIf you’re not familiar with these two establishments, Denny’s is a casual diner chain that is open 24 hours and La Quinta Inn and Suites is a hotel chain.\nThese two establishments tend to be clustered together, or at least this observation is a joke made famous by Mitch Hedberg. In this lab we explore the validity of this joke and along the way learn some more data wrangling and tips for visualizing spatial data.\nThe inspiration for this comes from a blog post by John Reiser on his new jersey geographer blog. You can read that analysis here. Reiser’s blog post focuses on scraping data from Denny’s and La Quinta Inn and Suites websites using Python. Here, we focus on visualization and analysis of these data."
  },
  {
    "objectID": "teaching/tidyverse-I/material/la-quinta-ws.html#packages",
    "href": "teaching/tidyverse-I/material/la-quinta-ws.html#packages",
    "title": "La Quinta is Spanish for next to Denny’s",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for much of the data wrangling and visualisation and the data lives in the dsbox package. These packages are already installed for you. You can load them by running the following in your Console:\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "teaching/tidyverse-I/material/la-quinta-ws.html#data",
    "href": "teaching/tidyverse-I/material/la-quinta-ws.html#data",
    "title": "La Quinta is Spanish for next to Denny’s",
    "section": "Data",
    "text": "Data\nThe data sets we’ll use are called dennys and laquinta and are available for download. Note that these data were scraped from here and here, respectively. You can find information about the data sets here and here. To help with our analysis we will also use a data set on US states.\n\nlaquinta &lt;- read_csv(\"data-4-wrangling-II/laquinta/laquinta.csv\")\ndennys &lt;- read_csv(\"data-4-wrangling-II/laquinta//dennys.csv\")\nstates &lt;- read_csv(\"data-4-wrangling-II/laquinta//states.csv\")\n\nEach observation in the states dataset represents a state, including DC. Along with the name of the state we have the two-letter abbreviation and we have the geographic area of the state (in square miles)."
  },
  {
    "objectID": "teaching/tidyverse-I/material/brexit.html",
    "href": "teaching/tidyverse-I/material/brexit.html",
    "title": "Brexit",
    "section": "",
    "text": "library(tidyverse)\n\nIn September 2019, YouGov survey asked 1,639 GB adults the following question:\n\nIn hindsight, do you think Britain was right/wrong to vote to leave EU?\n\nRight to leave\n\nWrong to leave\n\nDon’t know\n\n\nThe data from the survey is in data/brexit.csv.\n\nbrexit &lt;- read_csv(\"data-brexit/brexit.csv\")\n\nIn the course video we made the following visualisation.\n\nbrexit &lt;- brexit %&gt;%\n  mutate(\n    region = fct_relevel(region, \"london\", \"rest_of_south\", \"midlands_wales\", \"north\", \"scot\"),\n    region = fct_recode(region, London = \"london\", `Rest of South` = \"rest_of_south\", `Midlands / Wales` = \"midlands_wales\", North = \"north\", Scotland = \"scot\")\n  )\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  ) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\",\n    \"Right\" = \"#67a9cf\",\n    \"Don't know\" = \"gray\"\n  )) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn this application exercise we tell different stories with the same data.\n\nExercise 1 - Free scales\nAdd scales = \"free_x\" as an argument to the facet_wrap() function. How does the visualisation change? How is the story this visualisation telling different than the story the original plot tells?\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region,\n    nrow = 1, labeller = label_wrap_gen(width = 12),\n    # ___\n  ) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  ) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\",\n    \"Right\" = \"#67a9cf\",\n    \"Don't know\" = \"gray\"\n  )) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nExercise 2 - Comparing proportions across facets\nFirst, calculate the proportion of wrong, right, and don’t know answers in each category and then plot these proportions (rather than the counts) and then improve axis labeling. How is the story this visualisation telling different than the story the original plot tells? Hint: You’ll need the scales package to improve axis labeling, which means you’ll need to load it on top of the document as well.\n\n\nExercise 3 - Comparing proportions across bars\nRecreate the same visualisation from the previous exercise, this time dodging the bars for opinion proportions for each region, rather than faceting by region and then improve the legend. How is the story this visualisation telling different than the story the previous plot tells?"
  },
  {
    "objectID": "teaching/tidyverse-I/material/legos.html",
    "href": "teaching/tidyverse-I/material/legos.html",
    "title": "Legos",
    "section": "",
    "text": "Here, we work with (simulated) data from Lego sales in 2018 for a sample of customers who bought Legos in the US."
  },
  {
    "objectID": "teaching/tidyverse-I/material/legos.html#data-and-packages",
    "href": "teaching/tidyverse-I/material/legos.html#data-and-packages",
    "title": "Legos",
    "section": "Data and Packages",
    "text": "Data and Packages\nWe’ll use the tidyverse package for much of the data wrangling and visualisation and the data is given to import.\n\nlibrary(tidyverse)\n\nThe following variables are available in the data set:\n\nfirst_name: First name of customer\nlast_name: Last name of customer\nage: Age of customer\nphone_number: Phone number of customer\nset_id: Set ID of lego set purchased\nnumber: Item number of lego set purchased\ntheme: Theme of lego set purchased\nsubtheme: Sub theme of lego set purchased\nyear: Year of purchase\nname: Name of lego set purchased\npieces: Number of pieces of legos in set purchased\nus_price: Price of set purchase in US Dollars\nimage_url: Image URL of lego set purchased\nquantity: Quantity of lego set(s) purchased"
  },
  {
    "objectID": "teaching/tidyverse-I/material/starwars.html",
    "href": "teaching/tidyverse-I/material/starwars.html",
    "title": "Visualizing Starwars characters",
    "section": "",
    "text": "Glimpse at the starwars data frame.\n\n\nglimpse(starwars)\n\nRows: 87\nColumns: 14\n$ name       &lt;chr&gt; \"Luke Skywalker\", \"C-3PO\", \"R2-D2\", \"Darth Vader\", \"Leia Or…\n$ height     &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 2…\n$ mass       &lt;dbl&gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77.…\n$ hair_color &lt;chr&gt; \"blond\", NA, NA, \"none\", \"brown\", \"brown, grey\", \"brown\", N…\n$ skin_color &lt;chr&gt; \"fair\", \"gold\", \"white, blue\", \"white\", \"light\", \"light\", \"…\n$ eye_color  &lt;chr&gt; \"blue\", \"yellow\", \"red\", \"yellow\", \"brown\", \"blue\", \"blue\",…\n$ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57.0, …\n$ sex        &lt;chr&gt; \"male\", \"none\", \"none\", \"male\", \"female\", \"male\", \"female\",…\n$ gender     &lt;chr&gt; \"masculine\", \"masculine\", \"masculine\", \"masculine\", \"femini…\n$ homeworld  &lt;chr&gt; \"Tatooine\", \"Tatooine\", \"Naboo\", \"Tatooine\", \"Alderaan\", \"T…\n$ species    &lt;chr&gt; \"Human\", \"Droid\", \"Droid\", \"Human\", \"Human\", \"Human\", \"Huma…\n$ films      &lt;list&gt; &lt;\"A New Hope\", \"The Empire Strikes Back\", \"Return of the J…\n$ vehicles   &lt;list&gt; &lt;\"Snowspeeder\", \"Imperial Speeder Bike\"&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, \"Imp…\n$ starships  &lt;list&gt; &lt;\"X-wing\", \"Imperial shuttle\"&gt;, &lt;&gt;, &lt;&gt;, \"TIE Advanced x1\",…\n\n\n\nModify the following plot to change the color of all points to \"pink\".\n\n\nggplot(starwars, \n       aes(x = height, y = mass, color = gender, size = birth_year)) +\n  geom_point(color = \"pink\")\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nAdd labels for title, x and y axes, and size of points. Uncomment to see the effect.\n\n\nggplot(starwars, \n       aes(x = height, y = mass, color = gender, size = birth_year)) +\n  geom_point(color = \"#30509C\") +\n  labs(\n    #title = \"___\",\n    #x = \"___\", \n    #y = \"___\",\n    #___\n    )\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nPick a single numerical variable and make a histogram of it. Select a reasonable binwidth for it.\n\n(A little bit of starter code is provided below, and the code chunk is set to not be evaluated with eval: false because the current code in there is not valid code and hence the document wouldn’t knit. Once you replace the code with valid code, set the chunk option to eval: true, or remove the eval option altogether since it’s set to true by default.)\n\nggplot(starwars, aes(___)) +\n  geom___\n\n\nPick a numerical variable and a categorical variable and make a visualization (you pick the type!) to visualization the relationship between the two variables. Along with your code and output, provide an interpretation of the visualization.\n\nInterpretation goes here…\n\nPick a single categorical variable from the data set and make a bar plot of its distribution.\n\n\nPick two categorical variables and make a visualization to visualize the relationship between the two variables. Along with your code and output, provide an interpretation of the visualization.\n\nInterpretation goes here…\n\nPick two numerical variables and two categorical variables and make a visualization that incorporates all of them and provide an interpretation with your answer.\n\n(This time no starter code is provided, you’re on your own!)\nInterpretation goes here…"
  },
  {
    "objectID": "teaching/tidyverse-I/material/plastic-waste-ws.html",
    "href": "teaching/tidyverse-I/material/plastic-waste-ws.html",
    "title": "Global plastic waste",
    "section": "",
    "text": "Plastic pollution is a major and growing problem, negatively affecting oceans and wildlife health. Our World in Data has a lot of great data at various levels including globally, per country, and over time. For this lab we focus on data from 2010.\nAdditionally, National Geographic ran a data visualization communication contest on plastic waste as seen here."
  },
  {
    "objectID": "teaching/tidyverse-I/material/plastic-waste-ws.html#packages",
    "href": "teaching/tidyverse-I/material/plastic-waste-ws.html#packages",
    "title": "Global plastic waste",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for this analysis.\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "teaching/tidyverse-I/material/plastic-waste-ws.html#data",
    "href": "teaching/tidyverse-I/material/plastic-waste-ws.html#data",
    "title": "Global plastic waste",
    "section": "Data",
    "text": "Data\nThe dataset for this assignment can be found as a csv file. You can read it in using the following (make sure you save the data in your working directory).\n\nplastic_waste &lt;- read_csv(\"data-pw/plastic-waste.csv\")\n\nThe variable descriptions are as follows:\n\ncode: 3 Letter country code\nentity: Country name\ncontinent: Continent name\nyear: Year\ngdp_per_cap: GDP per capita constant 2011 international $, rate\nplastic_waste_per_cap: Amount of plastic waste per capita in kg/day\nmismanaged_plastic_waste_per_cap: Amount of mismanaged plastic waste per capita in kg/day\nmismanaged_plastic_waste: Tonnes of mismanaged plastic waste\ncoastal_pop: Number of individuals living on/near coast\ntotal_pop: Total population according to Gapminder"
  },
  {
    "objectID": "teaching/tidyverse-I/material/uoe-art.html",
    "href": "teaching/tidyverse-I/material/uoe-art.html",
    "title": "University of Edinburgh Art Collection",
    "section": "",
    "text": "The University of Edinburgh Art Collection “supports the world-leading research and teaching that happens within the University. Comprised of an astonishing range of objects and ideas spanning two millennia and a multitude of artistic forms, the collection reflects not only the long and rich trajectory of the University, but also major national and international shifts in art history.”\nIn this practical we’ll scrape data on all art pieces in the Edinburgh College of Art collection."
  },
  {
    "objectID": "teaching/tidyverse-I/material/uoe-art.html#r-scripts-vs.-quarto-documents",
    "href": "teaching/tidyverse-I/material/uoe-art.html#r-scripts-vs.-quarto-documents",
    "title": "University of Edinburgh Art Collection",
    "section": "R scripts vs. Quarto documents",
    "text": "R scripts vs. Quarto documents\nToday you’ll be using both R scripts and R Markdown documents:\n\nuse R scripts in the web scraping stage and ultimately save the scraped data as a csv.\nuse an Quarto document in the web analysis stage, where we start off by reading in the csv file we wrote out in the scraping stage."
  },
  {
    "objectID": "teaching/tidyverse-I/material/uoe-art.html#packages",
    "href": "teaching/tidyverse-I/material/uoe-art.html#packages",
    "title": "University of Edinburgh Art Collection",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for much of the data wrangling and visualisation, the robotstxt package to check if we’re allowed to scrape the data, the rvest package for data scraping.\n\nlibrary(tidyverse) \nlibrary(robotstxt)\nlibrary(rvest)"
  },
  {
    "objectID": "teaching/tidyverse-I/material/uoe-art.html#data",
    "href": "teaching/tidyverse-I/material/uoe-art.html#data",
    "title": "University of Edinburgh Art Collection",
    "section": "Data",
    "text": "Data\nThis assignment does not come with any prepared datasets. Instead you’ll be scraping the data! But before doing so, let’s check that a bot has permissions to access pages on this domain.\n\npaths_allowed(\"https://collections.ed.ac.uk/art)\")\n\n\n collections.ed.ac.uk                      \n\n\n[1] TRUE"
  },
  {
    "objectID": "teaching/tidyverse-I/material/uoe-art.html#scraping-a-single-page",
    "href": "teaching/tidyverse-I/material/uoe-art.html#scraping-a-single-page",
    "title": "University of Edinburgh Art Collection",
    "section": "Scraping a single page",
    "text": "Scraping a single page\nWe will start off by scraping data on the first 10 pieces in the collection from here.\nFirst, we define a new object called first_url, which is the link above. Then, we read the page at this url with the read_html() function from the rvest package. The code for this is already provided in 01-scrape-page-one.R.\n\n# set url\nfirst_url &lt;- \"https://collections.ed.ac.uk/art/search/*:*/Collection:%22edinburgh+college+of+art%7C%7C%7CEdinburgh+College+of+Art%22?offset=0\"\n\n# read html page\npage &lt;- read_html(first_url)\n\nFor the ten pieces on this page we will extract title, artist, and link information, and put these three variables in a data frame.\n\nTitles\nLet’s start with titles. We make use of the SelectorGadget to identify the tags for the relevant nodes:\n\n\n\n\n\n\n\n\n\n\npage %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%\n  html_node(\"h3 a\")\n\n{xml_nodeset (10)}\n [1] &lt;a href=\"./record/20696?highlight=*:*\"&gt;South Frieze of the Parthenon Fri ...\n [2] &lt;a href=\"./record/53701?highlight=*:*\"&gt;Espresso Cup                      ...\n [3] &lt;a href=\"./record/99347?highlight=*:*\"&gt;Untitled - Two Apes Sun Bathing   ...\n [4] &lt;a href=\"./record/21212?highlight=*:*\"&gt;Portrait of a Seated Woman        ...\n [5] &lt;a href=\"./record/21289?highlight=*:*\"&gt;Seated Male Nude                  ...\n [6] &lt;a href=\"./record/99370?highlight=*:*\"&gt;Nighttime Scene of the City and R ...\n [7] &lt;a href=\"./record/21178?highlight=*:*\"&gt;Portrait of Man in Red Jacket     ...\n [8] &lt;a href=\"./record/20743?highlight=*:*\"&gt;Harbour Scene 'KY16'              ...\n [9] &lt;a href=\"./record/21568?highlight=*:*\"&gt;Untitled                          ...\n[10] &lt;a href=\"./record/102688?highlight=*:*\"&gt;Machine stitched net             ...\n\n\nThen we extract the text with html_text():\n\npage %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%\n  html_node(\"h3 a\") %&gt;%\n  html_text()\n\n [1] \"South Frieze of the Parthenon Frieze                                                                            (1836-1837)\"\n [2] \"Espresso Cup                                    \"                                                                           \n [3] \"Untitled - Two Apes Sun Bathing                                                                            (1963)\"          \n [4] \"Portrait of a Seated Woman                                                                            (1954)\"               \n [5] \"Seated Male Nude                                                                            (1961)\"                         \n [6] \"Nighttime Scene of the City and River                                                                            (1962)\"    \n [7] \"Portrait of Man in Red Jacket                                                                            (1968)\"            \n [8] \"Harbour Scene 'KY16'                                                                            (1964)\"                     \n [9] \"Untitled                                                                            (May 1987)\"                             \n[10] \"Machine stitched net                                                                            (1946)\"                     \n\n\nAnd get rid of all the spurious white space in the text with str_squish(), which reduces repeated whitespace inside a string.\nTake a look at the help for str_squish() to find out more about how it works and how it’s different from str_trim().\n\npage %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%\n  html_node(\"h3 a\") %&gt;%\n  html_text() %&gt;% \n  str_squish()\n\n [1] \"South Frieze of the Parthenon Frieze (1836-1837)\"\n [2] \"Espresso Cup\"                                    \n [3] \"Untitled - Two Apes Sun Bathing (1963)\"          \n [4] \"Portrait of a Seated Woman (1954)\"               \n [5] \"Seated Male Nude (1961)\"                         \n [6] \"Nighttime Scene of the City and River (1962)\"    \n [7] \"Portrait of Man in Red Jacket (1968)\"            \n [8] \"Harbour Scene 'KY16' (1964)\"                     \n [9] \"Untitled (May 1987)\"                             \n[10] \"Machine stitched net (1946)\"                     \n\n\nAnd finally save the resulting data as a vector of length 10:\n\ntitles &lt;- page %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%\n  html_node(\"h3 a\") %&gt;%\n  html_text() %&gt;%\n  str_squish()\n\n\n\nLinks\nThe same nodes that contain the text for the titles also contains information on the links to individual art piece pages for each title. We can extract this information using a new function from the rvest package, html_attr(), which extracts attributes.\nA mini HTML lesson! The following is how we define hyperlinked text in HTML:\n&lt;a href=\"https://www.google.com\"&gt;Search on Google&lt;/a&gt;\nAnd this is how the text would look like on a webpage: Search on Google.\nHere the text is Search on Google and the href attribute contains the url of the website you’d go to if you click on the hyperlinked text: https://www.google.com.\nThe moral of the story is: the link is stored in the href attribute.\n\npage %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%   # same nodes\n  html_node(\"h3 a\") %&gt;%         # as before\n  html_attr(\"href\")             # but get href attribute instead of text\n\n [1] \"./record/20696?highlight=*:*\"  \"./record/53701?highlight=*:*\" \n [3] \"./record/99347?highlight=*:*\"  \"./record/21212?highlight=*:*\" \n [5] \"./record/21289?highlight=*:*\"  \"./record/99370?highlight=*:*\" \n [7] \"./record/21178?highlight=*:*\"  \"./record/20743?highlight=*:*\" \n [9] \"./record/21568?highlight=*:*\"  \"./record/102688?highlight=*:*\"\n\n\nThese don’t really look like URLs as we know then though. They’re relative links.\nSee the help for str_replace() to find out how it works. Remember that the first argument is passed in from the pipeline, so you just need to define the pattern and replacement arguments.\n\nClick on one of art piece titles in your browser and take note of the url of the webpage it takes you to. Think about how that url compares to what we scraped above? How is it different? Using str_replace(), fix the URLs. You’ll note something special happening in the pattern to replace. We want to replace the ., but we have it as \\\\.. This is because the period . is a special character and so we need to escape it first with backslashes, \\\\s.\n\n\n\nArtists\n\nFill in the blanks to scrape artist names.\n\n\n\nPut it altogether\n\nFill in the blanks to organize everything in a tibble.\n\n\n\nScrape the next page\n\nClick on the next page, and grab its url. Fill in the blank in to define a new object: second_url. Copy-paste code from top of the R script to scrape the new set of art pieces, and save the resulting data frame as second_ten."
  },
  {
    "objectID": "teaching/tidyverse-I/material/uoe-art.html#functions",
    "href": "teaching/tidyverse-I/material/uoe-art.html#functions",
    "title": "University of Edinburgh Art Collection",
    "section": "Functions",
    "text": "Functions\nYou’ve been using R functions, now it’s time to write your own!\nLet’s start simple. Here is a function that takes in an argument x, and adds 2 to it.\n\nadd_two &lt;- function(x){\n  x + 2\n}\n\nLet’s test it:\n\nadd_two(3)\n\n[1] 5\n\nadd_two(10)\n\n[1] 12\n\n\nThe skeleton for defining functions in R is as follows:\n\nfunction_name &lt;- function(input){\n  # do something with the input(s)\n  # return something\n}\n\nThen, a function for scraping a page should look something like:\nReminder: Function names should be short but evocative verbs.\n\nfunction_name &lt;- function(url){\n  # read page at url\n  # extract title, link, artist info for n pieces on page\n  # return a n x 3 tibble\n}\n\n\nFill in the blanks using code you already developed in the previous exercises. Name the function scrape_page.\n\nTest out your new function by running the following in the console. Does the output look right? Discuss with teammates whether you’re getting the same results as before.\n\nscrape_page(first_url)\nscrape_page(second_url)"
  },
  {
    "objectID": "teaching/tidyverse-I/material/uoe-art.html#iteration",
    "href": "teaching/tidyverse-I/material/uoe-art.html#iteration",
    "title": "University of Edinburgh Art Collection",
    "section": "Iteration",
    "text": "Iteration\nWe went from manually scraping individual pages to writing a function to do the same. Next, we will work on making our workflow a little more efficient by using R to iterate over all pages that contain information on the art collection.\nThat means we give develop a list of URLs (of pages that each have 10 art pieces), and write some code that applies the scrape_page() function to each page, and combines the resulting data frames from each page into a single data frame with 3289 rows and 3 columns.\n\nList of URLs\nClick through the first few of the pages in the art collection and observe their URLs to confirm the following pattern:\n[sometext]offset=0     # Pieces 1-10\n[sometext]offset=10    # Pieces 11-20\n[sometext]offset=20    # Pieces 21-30\n[sometext]offset=30    # Pieces 31-40\n...\n[sometext]offset=3280  # Pieces 3281-3289\nWe can construct these URLs in R by pasting together two pieces: (1) a common (root) text for the beginning of the URL, and (2) numbers starting at 0, increasing by 10, all the way up to 3289. Two new functions are helpful for accomplishing this: glue() for pasting two pieces of text and seq() for generating a sequence of numbers.\n\nFill in the blanks to construct the list of URLs.\n\n\n\nMapping\nFinally, we’re ready to iterate over the list of URLs we constructed. We will do this by mapping the function we developed over the list of URLs. There are a series of mapping functions in R and they each take the following form:\nmap([x], [function to apply to each element of x])\nIn our case x is the list of URLs we constructed and the function to apply to each element of x is the function we developed earlier, scrape_page. And as a result we want a data frame, so we use map_dfr function:\n\nmap_dfr(urls, scrape_page)\n\n\nFill in the blanks to scrape all pages, and to create a new data frame called uoe_art.\n\n\n\nWrite out data\n\nFinally write out the data frame you constructed into the data folder so that you can use it in the analysis section."
  },
  {
    "objectID": "teaching/tidyverse-I/material/uoe-art.html#analysis",
    "href": "teaching/tidyverse-I/material/uoe-art.html#analysis",
    "title": "University of Edinburgh Art Collection",
    "section": "Analysis",
    "text": "Analysis\nFor the rest of the exercises you can work in Quarto/R Markdown.\nNow that we have a tidy dataset that we can analyze, let’s do that!\nWe’ll start with some data cleaning, to clean up the dates that appear at the end of some title text in parentheses. Some of these are years, others are more specific dates, some art pieces have no date information whatsoever, and others have some non-date information in parentheses. This should be interesting to clean up!\nFirst thing we’ll try is to separate the title column into two: one for the actual title and the other for the date if it exists. In human speak, we need to\n“separate the title column at the first occurrence of ( and put the contents on one side of the ( into a column called title and the contents on the other side into a column called date”\nLuckily, there’s a function that does just this: separate()!\nAnd once we have completed separating the single title column into title and date, we need to do further clean-up in the date column to get rid of extraneous )s with str_remove(), capture year information, and save the data as a numeric variable.\n\nFill in the blanks in to implement the data wrangling we described above. Note that this will result in some warnings when you run the code, and that’s OK! Read the warnings, and explain what they mean, and why we are ok with leaving them in given that our objective is to just capture year where it’s convenient to do so.\nPrint out a summary of the data frame using the skim() function. How many pieces have artist info missing? How many have year info missing?\nMake a histogram of years. Use a reasonable binwidth. Do you see anything out of the ordinary?\nFind which piece has the out of the ordinary year and go to its page on the art collection website to find the correct year for it. Can you tell why our code didn’t capture the correct year information? Correct the error in the data frame and visualize the data again.\n\nHint: You’ll want to use mutate() and if_else() or case_when() to implement the correction.\n\nWho is the most commonly featured artist in the collection? Do you know them? Any guess as to why the university has so many pieces from them?\nFinal question! How many art pieces have the word “child” in their title? Try to figure it out, and ask for help if you’re stuck.\n\nHint: str_subset() can be helful here. You should consider how you might capture titles where the word appears as “child” and “Child”.\nSource: https://collections.ed.ac.uk/art/about"
  },
  {
    "objectID": "teaching/tidyverse-I/material/nobel-laureates.html",
    "href": "teaching/tidyverse-I/material/nobel-laureates.html",
    "title": "Nobel Laureates",
    "section": "",
    "text": "In January 2017, Buzzfeed published an article on why Nobel laureates show immigration is so important for American science. You can read the article here. In the article they show that while most living Nobel laureates in the sciences are based in the US, many of them were born in other countries. This is one reason why scientific leaders say that immigration is vital for progress. In this lab we will work with the data from this article to recreate some of their visualizations as well as explore new questions."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nobel-laureates.html#packages",
    "href": "teaching/tidyverse-I/material/nobel-laureates.html#packages",
    "title": "Nobel Laureates",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for much of the data wrangling. This package is already installed for you. You can load them by running the following in your Console:\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "teaching/tidyverse-I/material/nobel-laureates.html#data",
    "href": "teaching/tidyverse-I/material/nobel-laureates.html#data",
    "title": "Nobel Laureates",
    "section": "Data",
    "text": "Data\nThe dataset for this assignment can be found as a CSV (comma separated values) file. You can read it in using the following.\n\nnobel &lt;- read_csv(\"data-4-wrangling-I/nobel.csv\")\n\nThe variable descriptions are as follows:\n\nid: ID number\nfirstname: First name of laureate\nsurname: Surname\nyear: Year prize won\ncategory: Category of prize\naffiliation: Affiliation of laureate\ncity: City of laureate in prize year\ncountry: Country of laureate in prize year\nborn_date: Birth date of laureate\ndied_date: Death date of laureate\ngender: Gender of laureate\nborn_city: City where laureate was born\nborn_country: Country where laureate was born\nborn_country_code: Code of country where laureate was born\ndied_city: City where laureate died\ndied_country: Country where laureate died\ndied_country_code: Code of country where laureate died\noverall_motivation: Overall motivation for recognition\nshare: Number of other winners award is shared with\nmotivation: Motivation for recognition\n\nIn a few cases the name of the city/country changed after laureate was given (e.g. in 1975 Bosnia and Herzegovina was called the Socialist Federative Republic of Yugoslavia). In these cases the variables below reflect a different name than their counterparts without the suffix `_original`.\n\nborn_country_original: Original country where laureate was born\nborn_city_original: Original city where laureate was born\ndied_country_original: Original country where laureate died\ndied_city_original: Original city where laureate died\ncity_original: Original city where laureate lived at the time of winning the award\ncountry_original: Original country where laureate lived at the time of winning the award"
  },
  {
    "objectID": "teaching/tidyverse-I/material/nobel-laureates.html#get-to-know-your-data",
    "href": "teaching/tidyverse-I/material/nobel-laureates.html#get-to-know-your-data",
    "title": "Nobel Laureates",
    "section": "Get to know your data",
    "text": "Get to know your data\n\nHow many observations and how many variables are in the dataset? Use inline code to answer this question. What does each row represent?\n\nThere are some observations in this dataset that we will exclude from our analysis to match the Buzzfeed results.\n\nCreate a new data frame called nobel_living that filters for\n\n\nlaureates for whom country is available\nlaureates who are people as opposed to organizations (organizations are denoted with \"org\" as their gender)\nlaureates who are still alive (their died_date is NA)\n\nConfirm that once you have filtered for these characteristics you are left with a data frame with 228 observations, once again using inline code."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nobel-laureates.html#most-living-nobel-laureates-were-based-in-the-us-when-they-won-their-prizes",
    "href": "teaching/tidyverse-I/material/nobel-laureates.html#most-living-nobel-laureates-were-based-in-the-us-when-they-won-their-prizes",
    "title": "Nobel Laureates",
    "section": "Most living Nobel laureates were based in the US when they won their prizes",
    "text": "Most living Nobel laureates were based in the US when they won their prizes\n… says the Buzzfeed article. Let’s see if that’s true.\nFirst, we’ll create a new variable to identify whether the laureate was in the US when they won their prize. We’ll use the mutate() function for this. The following pipeline mutates the nobel_living data frame by adding a new variable called country_us. We use an if statement to create this variable. The first argument in the if_else() function we’re using to write this if statement is the condition we’re testing for. If country is equal to \"USA\", we set country_us to \"USA\". If not, we set the country_us to \"Other\".\nNote: we can achieve the same result using the fct_other() function we’ve seen before (i.e. with country_us = fct_other(country, \"USA\")). We decided to use the if_else() here to show you one example of an if statement in R.\n\nnobel_living &lt;- nobel_living %&gt;%\n  mutate(\n    country_us = if_else(country == \"USA\", \"USA\", \"Other\")\n  )\n\nNext, we will limit our analysis to only the following categories: Physics, Medicine, Chemistry, and Economics.\n\nnobel_living_science &lt;- nobel_living %&gt;%\n  filter(category %in% c(\"Physics\", \"Medicine\", \"Chemistry\", \"Economics\"))\n\nFor the next exercise work with the nobel_living_science data frame you created above.\n\nCreate a faceted bar plot visualizing the relationship between the category of prize and whether the laureate was in the US when they won the nobel prize. Interpret your visualization, and say a few words about whether the Buzzfeed headline is supported by the data.\n\nYour visualization should be faceted by category.\nFor each facet you should have two bars, one for winners in the US and one for Other.\nFlip the coordinates so the bars are horizontal, not vertical."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nobel-laureates.html#but-of-those-us-based-nobel-laureates-many-were-born-in-other-countries",
    "href": "teaching/tidyverse-I/material/nobel-laureates.html#but-of-those-us-based-nobel-laureates-many-were-born-in-other-countries",
    "title": "Nobel Laureates",
    "section": "But of those US-based Nobel laureates, many were born in other countries",
    "text": "But of those US-based Nobel laureates, many were born in other countries\nHint: You should be able to cheat borrow from code you used earlier to create the country_us variable.\n\nCreate a new variable called born_country_us that has the value \"USA\" if the laureate is born in the US, and \"Other\" otherwise. How many of the winners are born in the US?\nAdd a second variable to your visualization from Exercise 3 based on whether the laureate was born in the US or not. Based on your visualization, do the data appear to support Buzzfeed’s claim? Explain your reasoning in 1-2 sentences.\n\nYour final visualization should contain a facet for each category.\nWithin each facet, there should be a bar for whether the laureate won the award in the US or not.\nEach bar should have segments for whether the laureate was born in the US or not."
  },
  {
    "objectID": "teaching/tidyverse-I/material/nobel-laureates.html#heres-where-those-immigrant-nobelists-were-born",
    "href": "teaching/tidyverse-I/material/nobel-laureates.html#heres-where-those-immigrant-nobelists-were-born",
    "title": "Nobel Laureates",
    "section": "Here’s where those immigrant Nobelists were born",
    "text": "Here’s where those immigrant Nobelists were born\nNote: your bar plot won’t exactly match the one from the Buzzfeed article. This is likely because the data has been updated since the article was published.\n\nIn a single pipeline, filter for laureates who won their prize in the US, but were born outside of the US, and then create a frequency table (with the count() function) for their birth country (born_country) and arrange the resulting data frame in descending order of number of observations for each country. Which country is the most common?"
  },
  {
    "objectID": "teaching/tidyverse-I/material/college-majors.html",
    "href": "teaching/tidyverse-I/material/college-majors.html",
    "title": "What should I major in?",
    "section": "",
    "text": "The first step in the process of turning information into knowledge process is to summarize and describe the raw information - the data. In this assignment we explore data on college majors and earnings, specifically the data begin the FiveThirtyEight story “The Economic Guide To Picking A College Major”.\nThese data originally come from the American Community Survey (ACS) 2010-2012 Public Use Microdata Series. While this is outside the scope of this assignment, if you are curious about how raw data from the ACS were cleaned and prepared, see the code FiveThirtyEight authors used.\nWe should also note that there are many considerations that go into picking a major. Earnings potential and employment prospects are two of them, and they are important, but they don’t tell the whole story. Keep this in mind as you analyze the data."
  },
  {
    "objectID": "teaching/tidyverse-I/material/college-majors.html#packages",
    "href": "teaching/tidyverse-I/material/college-majors.html#packages",
    "title": "What should I major in?",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for much of the data wrangling and visualisation, the scales package for better formatting of labels on visualisations, and the data lives in the fivethirtyeight package. These packages are already installed for you. You can load them by running the following in your Console:\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(fivethirtyeight)"
  },
  {
    "objectID": "teaching/tidyverse-I/material/college-majors.html#data",
    "href": "teaching/tidyverse-I/material/college-majors.html#data",
    "title": "What should I major in?",
    "section": "Data",
    "text": "Data\nThe data can be found in the fivethirtyeight package, and it’s called college_recent_grads. Since the dataset is distributed with the package, we don’t need to load it separately; it becomes available to us when we load the package. You can find out more about the dataset by inspecting its documentation, which you can access by running ?college_recent_grads in the Console or using the Help menu in RStudio to search for college_recent_grads. You can also find this information here.\nYou can also take a quick peek at your data frame and view its dimensions with the glimpse function.\n\nglimpse(college_recent_grads)\n\nRows: 173\nColumns: 21\n$ rank                        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,…\n$ major_code                  &lt;int&gt; 2419, 2416, 2415, 2417, 2405, 2418, 6202, …\n$ major                       &lt;chr&gt; \"Petroleum Engineering\", \"Mining And Miner…\n$ major_category              &lt;chr&gt; \"Engineering\", \"Engineering\", \"Engineering…\n$ total                       &lt;int&gt; 2339, 756, 856, 1258, 32260, 2573, 3777, 1…\n$ sample_size                 &lt;int&gt; 36, 7, 3, 16, 289, 17, 51, 10, 1029, 631, …\n$ men                         &lt;int&gt; 2057, 679, 725, 1123, 21239, 2200, 2110, 8…\n$ women                       &lt;int&gt; 282, 77, 131, 135, 11021, 373, 1667, 960, …\n$ sharewomen                  &lt;dbl&gt; 0.1205643, 0.1018519, 0.1530374, 0.1073132…\n$ employed                    &lt;int&gt; 1976, 640, 648, 758, 25694, 1857, 2912, 15…\n$ employed_fulltime           &lt;int&gt; 1849, 556, 558, 1069, 23170, 2038, 2924, 1…\n$ employed_parttime           &lt;int&gt; 270, 170, 133, 150, 5180, 264, 296, 553, 1…\n$ employed_fulltime_yearround &lt;int&gt; 1207, 388, 340, 692, 16697, 1449, 2482, 82…\n$ unemployed                  &lt;int&gt; 37, 85, 16, 40, 1672, 400, 308, 33, 4650, …\n$ unemployment_rate           &lt;dbl&gt; 0.018380527, 0.117241379, 0.024096386, 0.0…\n$ p25th                       &lt;dbl&gt; 95000, 55000, 50000, 43000, 50000, 50000, …\n$ median                      &lt;dbl&gt; 110000, 75000, 73000, 70000, 65000, 65000,…\n$ p75th                       &lt;dbl&gt; 125000, 90000, 105000, 80000, 75000, 10200…\n$ college_jobs                &lt;int&gt; 1534, 350, 456, 529, 18314, 1142, 1768, 97…\n$ non_college_jobs            &lt;int&gt; 364, 257, 176, 102, 4440, 657, 314, 500, 1…\n$ low_wage_jobs               &lt;int&gt; 193, 50, 0, 0, 972, 244, 259, 220, 3253, 3…\n\n\nThe college_recent_grads data frame is a trove of information. Let’s think about some questions we might want to answer with these data:\n\nWhich major has the lowest unemployment rate?\nWhich major has the highest percentage of women?\nHow do the distributions of median income compare across major categories?\nDo women tend to choose majors with lower or higher earnings?\n\nIn the next section we aim to answer these questions."
  },
  {
    "objectID": "teaching/tidyverse-I/material/college-majors.html#which-major-has-the-lowest-unemployment-rate",
    "href": "teaching/tidyverse-I/material/college-majors.html#which-major-has-the-lowest-unemployment-rate",
    "title": "What should I major in?",
    "section": "Which major has the lowest unemployment rate?",
    "text": "Which major has the lowest unemployment rate?\nIn order to answer this question all we need to do is sort the data. We use the arrange function to do this, and sort it by the unemployment_rate variable. By default arrange sorts in ascending order, which is what we want here – we’re interested in the major with the lowest unemployment rate.\n\ncollege_recent_grads %&gt;%\n  arrange(unemployment_rate)\n\n# A tibble: 173 × 21\n    rank major_code major           major_category total sample_size   men women\n   &lt;int&gt;      &lt;int&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;int&gt;       &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1    53       4005 Mathematics An… Computers & M…   609           7   500   109\n 2    74       3801 Military Techn… Industrial Ar…   124           4   124     0\n 3    84       3602 Botany          Biology & Lif…  1329           9   626   703\n 4   113       1106 Soil Science    Agriculture &…   685           4   476   209\n 5   121       2301 Educational Ad… Education        804           5   280   524\n 6    15       2409 Engineering Me… Engineering     4321          30  3526   795\n 7    20       3201 Court Reporting Law & Public …  1148          14   877   271\n 8   120       2305 Mathematics Te… Education      14237         123  3872 10365\n 9     1       2419 Petroleum Engi… Engineering     2339          36  2057   282\n10    65       1100 General Agricu… Agriculture &… 10399         158  6053  4346\n# ℹ 163 more rows\n# ℹ 13 more variables: sharewomen &lt;dbl&gt;, employed &lt;int&gt;,\n#   employed_fulltime &lt;int&gt;, employed_parttime &lt;int&gt;,\n#   employed_fulltime_yearround &lt;int&gt;, unemployed &lt;int&gt;,\n#   unemployment_rate &lt;dbl&gt;, p25th &lt;dbl&gt;, median &lt;dbl&gt;, p75th &lt;dbl&gt;,\n#   college_jobs &lt;int&gt;, non_college_jobs &lt;int&gt;, low_wage_jobs &lt;int&gt;\n\n\nThis gives us what we wanted, but not in an ideal form. First, the name of the major barely fits on the page. Second, some of the variables are not that useful (e.g. major_code, major_category) and some we might want front and center are not easily viewed (e.g. unemployment_rate).\nWe can use the select function to choose which variables to display, and in which order:\n\ncollege_recent_grads %&gt;%\n  arrange(unemployment_rate) %&gt;%\n  select(rank, major, unemployment_rate)\n\n# A tibble: 173 × 3\n    rank major                                      unemployment_rate\n   &lt;int&gt; &lt;chr&gt;                                                  &lt;dbl&gt;\n 1    53 Mathematics And Computer Science                     0      \n 2    74 Military Technologies                                0      \n 3    84 Botany                                               0      \n 4   113 Soil Science                                         0      \n 5   121 Educational Administration And Supervision           0      \n 6    15 Engineering Mechanics Physics And Science            0.00633\n 7    20 Court Reporting                                      0.0117 \n 8   120 Mathematics Teacher Education                        0.0162 \n 9     1 Petroleum Engineering                                0.0184 \n10    65 General Agriculture                                  0.0196 \n# ℹ 163 more rows\n\n\nOk, this is looking better, but do we really need to display all those decimal places in the unemployment variable? Not really!\nWe can use the percent() function to clean up the display a bit.\n\ncollege_recent_grads %&gt;%\n  arrange(unemployment_rate) %&gt;%\n  select(rank, major, unemployment_rate) %&gt;%\n  mutate(unemployment_rate = percent(unemployment_rate))\n\n# A tibble: 173 × 3\n    rank major                                      unemployment_rate\n   &lt;int&gt; &lt;chr&gt;                                      &lt;chr&gt;            \n 1    53 Mathematics And Computer Science           0.00000%         \n 2    74 Military Technologies                      0.00000%         \n 3    84 Botany                                     0.00000%         \n 4   113 Soil Science                               0.00000%         \n 5   121 Educational Administration And Supervision 0.00000%         \n 6    15 Engineering Mechanics Physics And Science  0.63343%         \n 7    20 Court Reporting                            1.16897%         \n 8   120 Mathematics Teacher Education              1.62028%         \n 9     1 Petroleum Engineering                      1.83805%         \n10    65 General Agriculture                        1.96425%         \n# ℹ 163 more rows"
  },
  {
    "objectID": "teaching/tidyverse-I/material/college-majors.html#which-major-has-the-highest-percentage-of-women",
    "href": "teaching/tidyverse-I/material/college-majors.html#which-major-has-the-highest-percentage-of-women",
    "title": "What should I major in?",
    "section": "Which major has the highest percentage of women?",
    "text": "Which major has the highest percentage of women?\nTo answer such a question we need to arrange the data in descending order. For example, if earlier we were interested in the major with the highest unemployment rate, we would use the following:\n\nThe `desc` function specifies that we want `unemployment_rate` in descending order.\n\n\ncollege_recent_grads %&gt;%\n  arrange(desc(unemployment_rate)) %&gt;%\n  select(rank, major, unemployment_rate)\n\n# A tibble: 173 × 3\n    rank major                                      unemployment_rate\n   &lt;int&gt; &lt;chr&gt;                                                  &lt;dbl&gt;\n 1     6 Nuclear Engineering                                    0.177\n 2    90 Public Administration                                  0.159\n 3    85 Computer Networking And Telecommunications             0.152\n 4   171 Clinical Psychology                                    0.149\n 5    30 Public Policy                                          0.128\n 6   106 Communication Technologies                             0.120\n 7     2 Mining And Mineral Engineering                         0.117\n 8    54 Computer Programming And Data Processing               0.114\n 9    80 Geography                                              0.113\n10    59 Architecture                                           0.113\n# ℹ 163 more rows\n\n\n\nUsing what you’ve learned so far, arrange the data in descending order with respect to proportion of women in a major, and display only the major, the total number of people with major, and proportion of women. Show only the top 3 majors by adding top_n(3) at the end of the pipeline."
  },
  {
    "objectID": "teaching/tidyverse-I/material/college-majors.html#how-do-the-distributions-of-median-income-compare-across-major-categories",
    "href": "teaching/tidyverse-I/material/college-majors.html#how-do-the-distributions-of-median-income-compare-across-major-categories",
    "title": "What should I major in?",
    "section": "How do the distributions of median income compare across major categories?",
    "text": "How do the distributions of median income compare across major categories?\nNote: A percentile is a measure used in statistics indicating the value below which a given percentage of observations in a group of observations fall. For example, the 20th percentile is the value below which 20% of the observations may be found. (Source: Wikipedia\nThere are three types of incomes reported in this data frame: p25th, median, and p75th. These correspond to the 25th, 50th, and 75th percentiles of the income distribution of sampled individuals for a given major.\n\nWhy do we often choose the median, rather than the mean, to describe the typical income of a group of people?\n\nThe question we want to answer “How do the distributions of median income compare across major categories?”. We need to do a few things to answer this question: First, we need to group the data by major_category. Then, we need a way to summarize the distributions of median income within these groups. This decision will depend on the shapes of these distributions. So first, we need to visualize the data.\nWe use the ggplot() function to do this. The first argument is the data frame, and the next argument gives the mapping of the variables of the data to the aesthetic elements of the plot.\nLet’s start simple and take a look at the distribution of all median incomes, without considering the major categories.\n\nggplot(data = college_recent_grads, mapping = aes(x = median)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\nAlong with the plot, we get a message:\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nThis is telling us that we might want to reconsider the binwidth we chose for our histogram – or more accurately, the binwidth we didn’t specify. It’s good practice to always think in the context of the data and try out a few binwidths before settling on a binwidth. You might ask yourself: “What would be a meaningful difference in median incomes?” $1 is obviously too little, $10000 might be too high.\n\nTry binwidths of $1000 and $5000 and choose one. Explain your reasoning for your choice. Note that the binwidth is an argument for the geom_histogram function. So to specify a binwidth of $1000, you would use geom_histogram(binwidth = 1000).\n\nWe can also calculate summary statistics for this distribution using the summarise function:\n\ncollege_recent_grads %&gt;%\n  summarise(min = min(median), max = max(median),\n            mean = mean(median), med = median(median),\n            sd = sd(median), \n            q1 = quantile(median, probs = 0.25),\n            q3 = quantile(median, probs = 0.75))\n\n# A tibble: 1 × 7\n    min    max   mean   med     sd    q1    q3\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 22000 110000 40151. 36000 11470. 33000 45000\n\n\n\nBased on the shape of the histogram you created in the previous exercise, determine which of these summary statistics is useful for describing the distribution. Write up your description (remember shape, center, spread, any unusual observations) and include the summary statistic output as well.\nPlot the distribution of median income using a histogram, faceted by major_category. Use the binwidth you chose in the earlier exercise.\n\nNow that we’ve seen the shapes of the distributions of median incomes for each major category, we should have a better idea for which summary statistic to use to quantify the typical median income.\n\nWhich major category has the highest typical (you’ll need to decide what this means) median income? Use the partial code below, filling it in with the appropriate statistic and function. Also note that we are looking for the highest statistic, so make sure to arrange in the correct direction.\n\n\ncollege_recent_grads %&gt;%\n  group_by(major_category) %&gt;%\n  summarise(___ = ___(median)) %&gt;%\n  arrange(___)\n\n\nWhich major category is the least popular in this sample? To answer this question we use a new function called count, which first groups the data and then counts the number of observations in each category (see below). Add to the pipeline appropriately to arrange the results so that the major with the lowest observations is on top.\n\n\ncollege_recent_grads %&gt;%\n  count(major_category)\n\n# A tibble: 16 × 2\n   major_category                          n\n   &lt;chr&gt;                               &lt;int&gt;\n 1 Agriculture & Natural Resources        10\n 2 Arts                                    8\n 3 Biology & Life Science                 14\n 4 Business                               13\n 5 Communications & Journalism             4\n 6 Computers & Mathematics                11\n 7 Education                              16\n 8 Engineering                            29\n 9 Health                                 12\n10 Humanities & Liberal Arts              15\n11 Industrial Arts & Consumer Services     7\n12 Interdisciplinary                       1\n13 Law & Public Policy                     5\n14 Physical Sciences                      10\n15 Psychology & Social Work                9\n16 Social Science                          9"
  },
  {
    "objectID": "teaching/tidyverse-I/material/college-majors.html#all-stem-fields-arent-the-same",
    "href": "teaching/tidyverse-I/material/college-majors.html#all-stem-fields-arent-the-same",
    "title": "What should I major in?",
    "section": "All STEM fields aren’t the same",
    "text": "All STEM fields aren’t the same\nOne of the sections of the FiveThirtyEight story is “All STEM fields aren’t the same”. Let’s see if this is true.\nFirst, let’s create a new vector called stem_categories that lists the major categories that are considered STEM fields.\n\nstem_categories &lt;- c(\"Biology & Life Science\",\n                     \"Computers & Mathematics\",\n                     \"Engineering\",\n                     \"Physical Sciences\")\n\nThen, we can use this to create a new variable in our data frame indicating whether a major is STEM or not.\n\ncollege_recent_grads &lt;- college_recent_grads %&gt;%\n  mutate(major_type = ifelse(major_category %in% stem_categories, \"stem\", \"not stem\"))\n\nLet’s unpack this: with mutate we create a new variable called major_type, which is defined as \"stem\" if the major_category is in the vector called stem_categories we created earlier, and as \"not stem\" otherwise.\n%in% is a logical operator. Other logical operators that are commonly used are\n\n\n\nOperator\nOperation\n\n\n\n\nx &lt; y\nless than\n\n\nx &gt; y\ngreater than\n\n\nx &lt;= y\nless than or equal to\n\n\nx &gt;= y\ngreater than or equal to\n\n\nx != y\nnot equal to\n\n\nx == y\nequal to\n\n\nx %in% y\ncontains\n\n\nx | y\nor\n\n\nx & y\nand\n\n\n!x\nnot\n\n\n\nWe can use the logical operators to also filter our data for STEM majors whose median earnings is less than median for all majors’ median earnings, which we found to be $36,000 earlier.\n\ncollege_recent_grads %&gt;%\n  filter(\n    major_type == \"stem\",\n    median &lt; 36000\n  )\n\n# A tibble: 10 × 22\n    rank major_code major        major_category  total sample_size    men  women\n   &lt;int&gt;      &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;           &lt;int&gt;       &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n 1    93       1301 Environment… Biology & Lif…  25965         225  10787  15178\n 2    98       5098 Multi-Disci… Physical Scie…  62052         427  27015  35037\n 3   102       3608 Physiology   Biology & Lif…  22060          99   8422  13638\n 4   106       2001 Communicati… Computers & M…  18035         208  11431   6604\n 5   109       3611 Neuroscience Biology & Lif…  13663          53   4944   8719\n 6   111       5002 Atmospheric… Physical Scie…   4043          32   2744   1299\n 7   123       3699 Miscellaneo… Biology & Lif…  10706          63   4747   5959\n 8   124       3600 Biology      Biology & Lif… 280709        1370 111762 168947\n 9   133       3604 Ecology      Biology & Lif…   9154          86   3878   5276\n10   169       3609 Zoology      Biology & Lif…   8409          47   3050   5359\n# ℹ 14 more variables: sharewomen &lt;dbl&gt;, employed &lt;int&gt;,\n#   employed_fulltime &lt;int&gt;, employed_parttime &lt;int&gt;,\n#   employed_fulltime_yearround &lt;int&gt;, unemployed &lt;int&gt;,\n#   unemployment_rate &lt;dbl&gt;, p25th &lt;dbl&gt;, median &lt;dbl&gt;, p75th &lt;dbl&gt;,\n#   college_jobs &lt;int&gt;, non_college_jobs &lt;int&gt;, low_wage_jobs &lt;int&gt;,\n#   major_type &lt;chr&gt;\n\n\n\nWhich STEM majors have median salaries equal to or less than the median for all majors’ median earnings? Your output should only show the major name and median, 25th percentile, and 75th percentile earning for that major as and should be sorted such that the major with the highest median earning is on top."
  },
  {
    "objectID": "teaching/tidyverse-I/material/college-majors.html#what-types-of-majors-do-women-tend-to-major-in",
    "href": "teaching/tidyverse-I/material/college-majors.html#what-types-of-majors-do-women-tend-to-major-in",
    "title": "What should I major in?",
    "section": "What types of majors do women tend to major in?",
    "text": "What types of majors do women tend to major in?\n\nCreate a scatterplot of median income vs. proportion of women in that major, coloured by whether the major is in a STEM field or not. Describe the association between these three variables."
  },
  {
    "objectID": "teaching/tidyverse-I/material/college-majors.html#further-exploration",
    "href": "teaching/tidyverse-I/material/college-majors.html#further-exploration",
    "title": "What should I major in?",
    "section": "Further exploration",
    "text": "Further exploration\n\nAsk a question of interest to you, and answer it using summary statistic(s) and/or visualization(s)."
  },
  {
    "objectID": "rpackage/netropy/index.html",
    "href": "rpackage/netropy/index.html",
    "title": "netropy",
    "section": "",
    "text": "Termeh Shafie"
  },
  {
    "objectID": "teaching/stat-learn/material/01/01-R-you-ready.html",
    "href": "teaching/stat-learn/material/01/01-R-you-ready.html",
    "title": "R you ready?",
    "section": "",
    "text": "In this lab, we will introduce some simple R commands. The best way to learn a new language is to try out the commands. R can be downloaded from\nhttp://cran.r-project.org/\nWe recommend that you run R within an integrated development environment (IDE) such as RStudio, which can be freely downloaded from\nhttp://rstudio.com\nThe RStudio website also provides a cloud-based version of R, which does not require installing any software.\n\n\nR uses functions to perform operations. To run a function called funcname, we type funcname(input1, input2), where the inputs (or arguments) input1 and input2 tell R how to run the function. A function can have any number of inputs. For example, to create a vector of numbers, we use the function c() (for concatenate). Any numbers inside the parentheses are joined together. The following command instructs R to join together the numbers 1, 3, 2, and 5, and to save them as a vector named x. When we type x, it gives us back the vector.\n\nx &lt;- c(1, 3, 2, 5)\nx\n\n[1] 1 3 2 5\n\n\nNote that the &gt; is not part of the command; rather, it is printed by R to indicate that it is ready for another command to be entered. We can also save things using = rather than &lt;-:\n\nx = c(1, 6, 2)\nx\n\n[1] 1 6 2\n\ny = c(1, 4, 3)\n\nHitting the up arrow multiple times will display the previous commands, which can then be edited. This is useful since one often wishes to repeat a similar command. In addition, typing ?funcname will always cause R to open a new help file window with additional information about the function funcname().\nWe can tell R to add two sets of numbers together. It will then add the first number from x to the first number from y, and so on. However, x and y should be the same length. We can check their length using the length() function.\n\nlength(x)\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\nx + y\n\n[1]  2 10  5\n\n\nThe ls() function allows us to look at a list of all of the objects, such as data and functions, that we have saved so far. The rm() function can be used to delete any that we don’t want.\n\nls()\n\n[1] \"x\" \"y\"\n\nrm(x, y)\nls()\n\ncharacter(0)\n\n\nIt’s also possible to remove all objects at once:\n\nrm(list = ls())\n\nThe matrix() function can be used to create a matrix of numbers. Before we use the matrix() function, we can learn more about it:\n\n?matrix\n\nThe help file reveals that the matrix() function takes a number of inputs, but for now we focus on the first three: the data (the entries in the matrix), the number of rows, and the number of columns. First, we create a simple matrix.\n\nx &lt;- matrix(data = c(1, 2, 3, 4), nrow = 2, ncol = 2)\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nNote that we could just as well omit typing data=, nrow=, and ncol= in the matrix() command above: that is, we could just type\n\nx &lt;- matrix(c(1, 2, 3, 4), 2, 2)\n\nand this would have the same effect. However, it can sometimes be useful to specify the names of the arguments passed in, since otherwise R will assume that the function arguments are passed into the function in the same order that is given in the function’s help file. As this example illustrates, by default R creates matrices by successively filling in columns. Alternatively, the byrow = TRUE option can be used to populate the matrix in order of the rows.\n\nmatrix(c(1, 2, 3, 4), 2, 2, byrow = TRUE)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n\nNotice that in the above command we did not assign the matrix to a value such as x. In this case the matrix is printed to the screen but is not saved for future calculations. The sqrt() function returns the square root of each element of a vector or matrix. The command x^2 raises each element of x to the power 2; any powers are possible, including fractional or negative powers.\n\nsqrt(x)\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\n\nThe rnorm() function generates a vector of random normal variables, with first argument n the sample size. Each time we call this function, we will get a different answer. Here we create two correlated sets of numbers, x and y, and use the cor() function to compute the correlation between them.\n\nx &lt;- rnorm(50)\ny &lt;- x + rnorm(50, mean = 50, sd = .1)\ncor(x, y)\n\n[1] 0.9959562\n\n\nBy default, rnorm() creates standard normal random variables with a mean of \\(0\\) and a standard deviation of \\(1\\). However, the mean and standard deviation can be altered using the mean and sd arguments, as illustrated above. Sometimes we want our code to reproduce the exact same set of random numbers; we can use the set.seed() function to do this. The set.seed() function takes an (arbitrary) integer argument.\n\nset.seed(1303)\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\n\nWe use set.seed() throughout the labs whenever we perform calculations involving random quantities. In general this should allow the user to reproduce our results. However, as new versions of R become available, small discrepancies may arise between this book and the output from R.\nThe mean() and var() functions can be used to compute the mean and variance of a vector of numbers. Applying sqrt() to the output of var() will give the standard deviation. Or we can simply use the sd() function.\n\nset.seed(3)\ny &lt;- rnorm(100)\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768\n\n\n\n\n\nThe plot() function is the primary way to plot data in R. For instance, plot(x, y) produces a scatterplot of the numbers in x versus the numbers in y. There are many additional options that can be passed in to the plot() function. For example, passing in the argument xlab will result in a label on the \\(x\\)-axis. To find out more information about the plot() function, type ?plot.\n\nx &lt;- rnorm(100)\ny &lt;- rnorm(100)\nplot(x, y)\n\n\n\n\n\n\n\nplot(x, y, xlab = \"this is the x-axis\",\n    ylab = \"this is the y-axis\",\n    main = \"Plot of X vs Y\")\n\n\n\n\n\n\n\n\nWe will often want to save the output of an R plot. The command that we use to do this will depend on the file type that we would like to create. For instance, to create a pdf, we use the pdf() function, and to create a jpeg, we use the jpeg() function.\n\npdf(\"Figure.pdf\")\nplot(x, y, col = \"gray\")\ndev.off()\n\nquartz_off_screen \n                2 \n\n\nThe function dev.off() indicates to R that we are done creating the plot. Alternatively, we can simply copy the plot window and paste it into an appropriate file type, such as a Word document.\nThe function seq() can be used to create a sequence of numbers. For instance, seq(a, b) makes a vector of integers between a and b. There are many other options: for instance, seq(0, 1, length = 10) makes a sequence of 10 numbers that are equally spaced between 0 and 1. Typing 3:11 is a shorthand for seq(3, 11) for integer arguments.\n\nx &lt;- seq(1, 10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx &lt;- 1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx &lt;- seq(-pi, pi, length = 50)\n\nWe will now create some more sophisticated plots. The contour() function produces a contour plot in order to represent three-dimensional data; it is like a topographical map. It takes three arguments:\n\nA vector of the x values (the first dimension),\nA vector of the y values (the second dimension), and\nA matrix whose elements correspond to the z value (the third dimension) for each pair of (x, y) coordinates.\n\nAs with the plot() function, there are many other inputs that can be used to fine-tune the output of the contour() function. To learn more about these, take a look at the help file by typing ?contour.\n\ny &lt;- x\nf &lt;- outer(x, y, function(x, y) cos(y) / (1 + x^2))\ncontour(x, y, f)\ncontour(x, y, f, nlevels = 45, add = T)\n\n\n\n\n\n\n\nfa &lt;- (f - t(f)) / 2\ncontour(x, y, fa, nlevels = 15)\n\n\n\n\n\n\n\n\nThe image() function works the same way as contour(), except that it produces a color-coded plot whose colors depend on the z value. This is known as a heatmap, and is sometimes used to plot temperature in weather forecasts. Alternatively, persp() can be used to produce a three-dimensional plot. The arguments theta and phi control the angles at which the plot is viewed.\n\nimage(x, y, fa)\n\n\n\n\n\n\n\npersp(x, y, fa)\n\n\n\n\n\n\n\npersp(x, y, fa, theta = 30)\n\n\n\n\n\n\n\npersp(x, y, fa, theta = 30, phi = 20)\n\n\n\n\n\n\n\npersp(x, y, fa, theta = 30, phi = 70)\n\n\n\n\n\n\n\npersp(x, y, fa, theta = 30, phi = 40)\n\n\n\n\n\n\n\n\n\n\n\nWe often wish to examine part of a set of data. Suppose that our data is stored in the matrix A.\n\nA &lt;- matrix(1:16, 4, 4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\n\nThen, typing\n\nA[2, 3]\n\n[1] 10\n\n\nwill select the element corresponding to the second row and the third column. The first number after the open-bracket symbol [ always refers to the row, and the second number always refers to the column. We can also select multiple rows and columns at a time, by providing vectors as the indices.\n\nA[c(1, 3), c(2, 4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3, 2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2, ]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[, 1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\n\nThe last two examples include either no index for the columns or no index for the rows. These indicate that R should include all columns or all rows, respectively. R treats a single row or column of a matrix as a vector.\n\nA[1, ]\n\n[1]  1  5  9 13\n\n\nThe use of a negative sign - in the index tells R to keep all rows or columns except those indicated in the index.\n\nA[-c(1, 3), ]\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1, 3), -c(1, 3, 4)]\n\n[1] 6 8\n\n\nThe dim() function outputs the number of rows followed by the number of columns of a given matrix.\n\ndim(A)\n\n[1] 4 4\n\n\n\n\n\nFor most analyses, the first step involves importing a data set into R. The read.table() function is one of the primary ways to do this. The help file contains details about how to use this function. We can use the function write.table() to export data.\nBefore attempting to load a data set, we must make sure that R knows to search for the data in the proper directory. For example, on a Windows system one could select the directory using the Change dir ... option under the File menu. However, the details of how to do this depend on the operating system (e.g. Windows, Mac, Unix) that is being used, and so we do not give further details here.\nWe begin by loading in the Auto data set. This data is part of the ISLR2 library, discussed in Chapter 3. To illustrate the read.table() function, we load it now from a text file, Auto.data, which you can find on the textbook website. The following command will load the Auto.data file into R and store it as an object called Auto, in a format referred to as a data frame. Once the data has been loaded, the View() function can be used to view it in a spreadsheet-like window. (This function can sometimes be a bit finicky. If you have trouble using it, then try the head() function instead.) The head() function can also be used to view the first few rows of the data.\n\nAuto &lt;- read.table(\"01-data/Auto.data\")\nknitr::kable(head(Auto)) # note: you only write head(Auto) here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\n\n\n18.0\n8\n307.0\n130.0\n3504.\n12.0\n70\n1\nchevrolet chevelle malibu\n\n\n15.0\n8\n350.0\n165.0\n3693.\n11.5\n70\n1\nbuick skylark 320\n\n\n18.0\n8\n318.0\n150.0\n3436.\n11.0\n70\n1\nplymouth satellite\n\n\n16.0\n8\n304.0\n150.0\n3433.\n12.0\n70\n1\namc rebel sst\n\n\n17.0\n8\n302.0\n140.0\n3449.\n10.5\n70\n1\nford torino\n\n\n\n\n\nNote that Auto.data is simply a text file, which you could alternatively open on your computer using a standard text editor (you can even change the format to .txt). It is often a good idea to view a data set using a text editor or other software such as Excel before loading it into R.\nThis particular data set has not been loaded correctly, because R has assumed that the variable names are part of the data and so has included them in the first row. The data set also includes a number of missing observations, indicated by a question mark ?. Missing values are a common occurrence in real data sets. Using the option header = T (or header = TRUE) in the read.table() function tells R that the first line of the file contains the variable names, and using the option na.strings tells R that any time it sees a particular character or set of characters (such as a question mark), it should be treated as a missing element of the data matrix.\n\nAuto &lt;- read.table(\"01-data/Auto.data\", header = T, na.strings = \"?\", stringsAsFactors = T)\n# View(Auto)\n\nThe stringsAsFactors = T argument tells R that any variable containing character strings should be interpreted as a qualitative variable, and that each distinct character string represents a distinct level for that qualitative variable. An easy way to load data from Excel into R is to save it as a csv (comma-separated values) file, and then use the read.csv() function.\n\nAuto &lt;- read.csv(\"01-data/Auto.csv\", na.strings = \"?\", stringsAsFactors = T)\ndim(Auto)\n\n[1] 397   9\n\nknitr::kable(Auto[1:4, ])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\n\n\n\n\n18\n8\n307\n130\n3504\n12.0\n70\n1\nchevrolet chevelle malibu\n\n\n15\n8\n350\n165\n3693\n11.5\n70\n1\nbuick skylark 320\n\n\n18\n8\n318\n150\n3436\n11.0\n70\n1\nplymouth satellite\n\n\n16\n8\n304\n150\n3433\n12.0\n70\n1\namc rebel sst\n\n\n\n\n\nThe dim() function tells us that the data has \\(397\\) observations, or rows, and nine variables, or columns. There are various ways to deal with the missing data. In this case, only five of the rows contain missing observations, and so we choose to use the na.omit() function to simply remove these rows.\n\nAuto &lt;- na.omit(Auto)\ndim(Auto)\n\n[1] 392   9\n\n\nOnce the data are loaded correctly, we can use names() to check the variable names.\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\nWe can use the plot() function to produce scatterplots of the quantitative variables. However, simply typing the variable names will produce an error message, because R does not know to look in the Auto data set for those variables.\nTo refer to a variable, we must type the data set and the variable name joined with a $ symbol. Alternatively, we can use the attach() function in order to tell R to make the variables in this data frame available by name.\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\n\nThe cylinders variable is stored as a numeric vector, so R has treated it as quantitative. However, since there are only a small number of possible values for cylinders, one may prefer to treat it as a qualitative variable. The as.factor() function converts quantitative variables into qualitative variables.\n\ncylinders &lt;- as.factor(cylinders)\n\nIf the variable plotted on the \\(x\\)-axis is qualitative, then boxplots will automatically be produced by the plot() function. As usual, a number of options can be specified in order to customize the plots.\n\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col = \"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col = \"red\", varwidth = T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col = \"red\", varwidth = T,\n    horizontal = T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col = \"red\", varwidth = T,\n    xlab = \"cylinders\", ylab = \"MPG\")\n\n\n\n\n\n\n\n\nThe hist() function can be used to plot a histogram. Note that col = 2 has the same effect as col = \"red\".\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg, col = 2)\n\n\n\n\n\n\n\nhist(mpg, col = 2, breaks = 15)\n\n\n\n\n\n\n\n\nThe pairs() function creates a scatterplot matrix, i.e. a scatterplot for every pair of variables. We can also produce scatterplots for just a subset of the variables.\n\npairs(Auto)\n\n\n\n\n\n\n\npairs(\n    ~ mpg + displacement + horsepower + weight + acceleration,\n    data = Auto\n  )\n\n\n\n\n\n\n\n\nIn conjunction with the plot() function, identify() provides a useful interactive method for identifying the value of a particular variable for points on a plot. We pass in three arguments to identify(): the \\(x\\)-axis variable, the \\(y\\)-axis variable, and the variable whose values we would like to see printed for each point. Then clicking one or more points in the plot and hitting Escape will cause R to print the values of the variable of interest. The numbers printed under the identify() function correspond to the rows for the selected points.\n\nplot(horsepower, mpg)\nidentify(horsepower, mpg, name)\n\n\n\n\n\n\n\n\ninteger(0)\n\n\nThe summary() function produces a numerical summary of each variable in a particular data set.\n\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.00   1st Qu.:4.000   1st Qu.:105.0   1st Qu.: 75.0   1st Qu.:2225  \n Median :22.75   Median :4.000   Median :151.0   Median : 93.5   Median :2804  \n Mean   :23.45   Mean   :5.472   Mean   :194.4   Mean   :104.5   Mean   :2978  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:275.8   3rd Qu.:126.0   3rd Qu.:3615  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                                               \n  acceleration        year           origin                      name    \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   amc matador       :  5  \n 1st Qu.:13.78   1st Qu.:73.00   1st Qu.:1.000   ford pinto        :  5  \n Median :15.50   Median :76.00   Median :1.000   toyota corolla    :  5  \n Mean   :15.54   Mean   :75.98   Mean   :1.577   amc gremlin       :  4  \n 3rd Qu.:17.02   3rd Qu.:79.00   3rd Qu.:2.000   amc hornet        :  4  \n Max.   :24.80   Max.   :82.00   Max.   :3.000   chevrolet chevette:  4  \n                                                 (Other)           :365  \n\n\nFor qualitative variables such as name, R will list the number of observations that fall in each category. We can also produce a summary of just a single variable.\n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.00   22.75   23.45   29.00   46.60 \n\n\nOnce we have finished using R, we type q() in order to shut it down, or quit. When exiting R, we have the option to save the current workspace so that all objects (such as data sets) that we have created in this R session will be available next time. Before exiting R, we may want to save a record of all of the commands that we typed in the most recent session; this can be accomplished using the savehistory() function. Next time we enter R, we can load that history using the loadhistory() function, if we wish."
  },
  {
    "objectID": "teaching/stat-learn/material/01/01-R-you-ready.html#basic-commands",
    "href": "teaching/stat-learn/material/01/01-R-you-ready.html#basic-commands",
    "title": "R you ready?",
    "section": "",
    "text": "R uses functions to perform operations. To run a function called funcname, we type funcname(input1, input2), where the inputs (or arguments) input1 and input2 tell R how to run the function. A function can have any number of inputs. For example, to create a vector of numbers, we use the function c() (for concatenate). Any numbers inside the parentheses are joined together. The following command instructs R to join together the numbers 1, 3, 2, and 5, and to save them as a vector named x. When we type x, it gives us back the vector.\n\nx &lt;- c(1, 3, 2, 5)\nx\n\n[1] 1 3 2 5\n\n\nNote that the &gt; is not part of the command; rather, it is printed by R to indicate that it is ready for another command to be entered. We can also save things using = rather than &lt;-:\n\nx = c(1, 6, 2)\nx\n\n[1] 1 6 2\n\ny = c(1, 4, 3)\n\nHitting the up arrow multiple times will display the previous commands, which can then be edited. This is useful since one often wishes to repeat a similar command. In addition, typing ?funcname will always cause R to open a new help file window with additional information about the function funcname().\nWe can tell R to add two sets of numbers together. It will then add the first number from x to the first number from y, and so on. However, x and y should be the same length. We can check their length using the length() function.\n\nlength(x)\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\nx + y\n\n[1]  2 10  5\n\n\nThe ls() function allows us to look at a list of all of the objects, such as data and functions, that we have saved so far. The rm() function can be used to delete any that we don’t want.\n\nls()\n\n[1] \"x\" \"y\"\n\nrm(x, y)\nls()\n\ncharacter(0)\n\n\nIt’s also possible to remove all objects at once:\n\nrm(list = ls())\n\nThe matrix() function can be used to create a matrix of numbers. Before we use the matrix() function, we can learn more about it:\n\n?matrix\n\nThe help file reveals that the matrix() function takes a number of inputs, but for now we focus on the first three: the data (the entries in the matrix), the number of rows, and the number of columns. First, we create a simple matrix.\n\nx &lt;- matrix(data = c(1, 2, 3, 4), nrow = 2, ncol = 2)\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nNote that we could just as well omit typing data=, nrow=, and ncol= in the matrix() command above: that is, we could just type\n\nx &lt;- matrix(c(1, 2, 3, 4), 2, 2)\n\nand this would have the same effect. However, it can sometimes be useful to specify the names of the arguments passed in, since otherwise R will assume that the function arguments are passed into the function in the same order that is given in the function’s help file. As this example illustrates, by default R creates matrices by successively filling in columns. Alternatively, the byrow = TRUE option can be used to populate the matrix in order of the rows.\n\nmatrix(c(1, 2, 3, 4), 2, 2, byrow = TRUE)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\n\nNotice that in the above command we did not assign the matrix to a value such as x. In this case the matrix is printed to the screen but is not saved for future calculations. The sqrt() function returns the square root of each element of a vector or matrix. The command x^2 raises each element of x to the power 2; any powers are possible, including fractional or negative powers.\n\nsqrt(x)\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\n\nThe rnorm() function generates a vector of random normal variables, with first argument n the sample size. Each time we call this function, we will get a different answer. Here we create two correlated sets of numbers, x and y, and use the cor() function to compute the correlation between them.\n\nx &lt;- rnorm(50)\ny &lt;- x + rnorm(50, mean = 50, sd = .1)\ncor(x, y)\n\n[1] 0.9959562\n\n\nBy default, rnorm() creates standard normal random variables with a mean of \\(0\\) and a standard deviation of \\(1\\). However, the mean and standard deviation can be altered using the mean and sd arguments, as illustrated above. Sometimes we want our code to reproduce the exact same set of random numbers; we can use the set.seed() function to do this. The set.seed() function takes an (arbitrary) integer argument.\n\nset.seed(1303)\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\n\nWe use set.seed() throughout the labs whenever we perform calculations involving random quantities. In general this should allow the user to reproduce our results. However, as new versions of R become available, small discrepancies may arise between this book and the output from R.\nThe mean() and var() functions can be used to compute the mean and variance of a vector of numbers. Applying sqrt() to the output of var() will give the standard deviation. Or we can simply use the sd() function.\n\nset.seed(3)\ny &lt;- rnorm(100)\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768"
  },
  {
    "objectID": "teaching/stat-learn/material/01/01-R-you-ready.html#graphics",
    "href": "teaching/stat-learn/material/01/01-R-you-ready.html#graphics",
    "title": "R you ready?",
    "section": "",
    "text": "The plot() function is the primary way to plot data in R. For instance, plot(x, y) produces a scatterplot of the numbers in x versus the numbers in y. There are many additional options that can be passed in to the plot() function. For example, passing in the argument xlab will result in a label on the \\(x\\)-axis. To find out more information about the plot() function, type ?plot.\n\nx &lt;- rnorm(100)\ny &lt;- rnorm(100)\nplot(x, y)\n\n\n\n\n\n\n\nplot(x, y, xlab = \"this is the x-axis\",\n    ylab = \"this is the y-axis\",\n    main = \"Plot of X vs Y\")\n\n\n\n\n\n\n\n\nWe will often want to save the output of an R plot. The command that we use to do this will depend on the file type that we would like to create. For instance, to create a pdf, we use the pdf() function, and to create a jpeg, we use the jpeg() function.\n\npdf(\"Figure.pdf\")\nplot(x, y, col = \"gray\")\ndev.off()\n\nquartz_off_screen \n                2 \n\n\nThe function dev.off() indicates to R that we are done creating the plot. Alternatively, we can simply copy the plot window and paste it into an appropriate file type, such as a Word document.\nThe function seq() can be used to create a sequence of numbers. For instance, seq(a, b) makes a vector of integers between a and b. There are many other options: for instance, seq(0, 1, length = 10) makes a sequence of 10 numbers that are equally spaced between 0 and 1. Typing 3:11 is a shorthand for seq(3, 11) for integer arguments.\n\nx &lt;- seq(1, 10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx &lt;- 1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx &lt;- seq(-pi, pi, length = 50)\n\nWe will now create some more sophisticated plots. The contour() function produces a contour plot in order to represent three-dimensional data; it is like a topographical map. It takes three arguments:\n\nA vector of the x values (the first dimension),\nA vector of the y values (the second dimension), and\nA matrix whose elements correspond to the z value (the third dimension) for each pair of (x, y) coordinates.\n\nAs with the plot() function, there are many other inputs that can be used to fine-tune the output of the contour() function. To learn more about these, take a look at the help file by typing ?contour.\n\ny &lt;- x\nf &lt;- outer(x, y, function(x, y) cos(y) / (1 + x^2))\ncontour(x, y, f)\ncontour(x, y, f, nlevels = 45, add = T)\n\n\n\n\n\n\n\nfa &lt;- (f - t(f)) / 2\ncontour(x, y, fa, nlevels = 15)\n\n\n\n\n\n\n\n\nThe image() function works the same way as contour(), except that it produces a color-coded plot whose colors depend on the z value. This is known as a heatmap, and is sometimes used to plot temperature in weather forecasts. Alternatively, persp() can be used to produce a three-dimensional plot. The arguments theta and phi control the angles at which the plot is viewed.\n\nimage(x, y, fa)\n\n\n\n\n\n\n\npersp(x, y, fa)\n\n\n\n\n\n\n\npersp(x, y, fa, theta = 30)\n\n\n\n\n\n\n\npersp(x, y, fa, theta = 30, phi = 20)\n\n\n\n\n\n\n\npersp(x, y, fa, theta = 30, phi = 70)\n\n\n\n\n\n\n\npersp(x, y, fa, theta = 30, phi = 40)"
  },
  {
    "objectID": "teaching/stat-learn/material/01/01-R-you-ready.html#indexing-data",
    "href": "teaching/stat-learn/material/01/01-R-you-ready.html#indexing-data",
    "title": "R you ready?",
    "section": "",
    "text": "We often wish to examine part of a set of data. Suppose that our data is stored in the matrix A.\n\nA &lt;- matrix(1:16, 4, 4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\n\nThen, typing\n\nA[2, 3]\n\n[1] 10\n\n\nwill select the element corresponding to the second row and the third column. The first number after the open-bracket symbol [ always refers to the row, and the second number always refers to the column. We can also select multiple rows and columns at a time, by providing vectors as the indices.\n\nA[c(1, 3), c(2, 4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3, 2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2, ]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[, 1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\n\nThe last two examples include either no index for the columns or no index for the rows. These indicate that R should include all columns or all rows, respectively. R treats a single row or column of a matrix as a vector.\n\nA[1, ]\n\n[1]  1  5  9 13\n\n\nThe use of a negative sign - in the index tells R to keep all rows or columns except those indicated in the index.\n\nA[-c(1, 3), ]\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1, 3), -c(1, 3, 4)]\n\n[1] 6 8\n\n\nThe dim() function outputs the number of rows followed by the number of columns of a given matrix.\n\ndim(A)\n\n[1] 4 4"
  },
  {
    "objectID": "teaching/stat-learn/material/01/01-R-you-ready.html#loading-data",
    "href": "teaching/stat-learn/material/01/01-R-you-ready.html#loading-data",
    "title": "R you ready?",
    "section": "",
    "text": "For most analyses, the first step involves importing a data set into R. The read.table() function is one of the primary ways to do this. The help file contains details about how to use this function. We can use the function write.table() to export data.\nBefore attempting to load a data set, we must make sure that R knows to search for the data in the proper directory. For example, on a Windows system one could select the directory using the Change dir ... option under the File menu. However, the details of how to do this depend on the operating system (e.g. Windows, Mac, Unix) that is being used, and so we do not give further details here.\nWe begin by loading in the Auto data set. This data is part of the ISLR2 library, discussed in Chapter 3. To illustrate the read.table() function, we load it now from a text file, Auto.data, which you can find on the textbook website. The following command will load the Auto.data file into R and store it as an object called Auto, in a format referred to as a data frame. Once the data has been loaded, the View() function can be used to view it in a spreadsheet-like window. (This function can sometimes be a bit finicky. If you have trouble using it, then try the head() function instead.) The head() function can also be used to view the first few rows of the data.\n\nAuto &lt;- read.table(\"01-data/Auto.data\")\nknitr::kable(head(Auto)) # note: you only write head(Auto) here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\n\n\n18.0\n8\n307.0\n130.0\n3504.\n12.0\n70\n1\nchevrolet chevelle malibu\n\n\n15.0\n8\n350.0\n165.0\n3693.\n11.5\n70\n1\nbuick skylark 320\n\n\n18.0\n8\n318.0\n150.0\n3436.\n11.0\n70\n1\nplymouth satellite\n\n\n16.0\n8\n304.0\n150.0\n3433.\n12.0\n70\n1\namc rebel sst\n\n\n17.0\n8\n302.0\n140.0\n3449.\n10.5\n70\n1\nford torino\n\n\n\n\n\nNote that Auto.data is simply a text file, which you could alternatively open on your computer using a standard text editor (you can even change the format to .txt). It is often a good idea to view a data set using a text editor or other software such as Excel before loading it into R.\nThis particular data set has not been loaded correctly, because R has assumed that the variable names are part of the data and so has included them in the first row. The data set also includes a number of missing observations, indicated by a question mark ?. Missing values are a common occurrence in real data sets. Using the option header = T (or header = TRUE) in the read.table() function tells R that the first line of the file contains the variable names, and using the option na.strings tells R that any time it sees a particular character or set of characters (such as a question mark), it should be treated as a missing element of the data matrix.\n\nAuto &lt;- read.table(\"01-data/Auto.data\", header = T, na.strings = \"?\", stringsAsFactors = T)\n# View(Auto)\n\nThe stringsAsFactors = T argument tells R that any variable containing character strings should be interpreted as a qualitative variable, and that each distinct character string represents a distinct level for that qualitative variable. An easy way to load data from Excel into R is to save it as a csv (comma-separated values) file, and then use the read.csv() function.\n\nAuto &lt;- read.csv(\"01-data/Auto.csv\", na.strings = \"?\", stringsAsFactors = T)\ndim(Auto)\n\n[1] 397   9\n\nknitr::kable(Auto[1:4, ])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nyear\norigin\nname\n\n\n\n\n18\n8\n307\n130\n3504\n12.0\n70\n1\nchevrolet chevelle malibu\n\n\n15\n8\n350\n165\n3693\n11.5\n70\n1\nbuick skylark 320\n\n\n18\n8\n318\n150\n3436\n11.0\n70\n1\nplymouth satellite\n\n\n16\n8\n304\n150\n3433\n12.0\n70\n1\namc rebel sst\n\n\n\n\n\nThe dim() function tells us that the data has \\(397\\) observations, or rows, and nine variables, or columns. There are various ways to deal with the missing data. In this case, only five of the rows contain missing observations, and so we choose to use the na.omit() function to simply remove these rows.\n\nAuto &lt;- na.omit(Auto)\ndim(Auto)\n\n[1] 392   9\n\n\nOnce the data are loaded correctly, we can use names() to check the variable names.\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\""
  },
  {
    "objectID": "teaching/stat-learn/material/01/01-R-you-ready.html#additional-graphical-and-numerical-summaries",
    "href": "teaching/stat-learn/material/01/01-R-you-ready.html#additional-graphical-and-numerical-summaries",
    "title": "R you ready?",
    "section": "",
    "text": "We can use the plot() function to produce scatterplots of the quantitative variables. However, simply typing the variable names will produce an error message, because R does not know to look in the Auto data set for those variables.\nTo refer to a variable, we must type the data set and the variable name joined with a $ symbol. Alternatively, we can use the attach() function in order to tell R to make the variables in this data frame available by name.\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\n\nThe cylinders variable is stored as a numeric vector, so R has treated it as quantitative. However, since there are only a small number of possible values for cylinders, one may prefer to treat it as a qualitative variable. The as.factor() function converts quantitative variables into qualitative variables.\n\ncylinders &lt;- as.factor(cylinders)\n\nIf the variable plotted on the \\(x\\)-axis is qualitative, then boxplots will automatically be produced by the plot() function. As usual, a number of options can be specified in order to customize the plots.\n\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col = \"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col = \"red\", varwidth = T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col = \"red\", varwidth = T,\n    horizontal = T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col = \"red\", varwidth = T,\n    xlab = \"cylinders\", ylab = \"MPG\")\n\n\n\n\n\n\n\n\nThe hist() function can be used to plot a histogram. Note that col = 2 has the same effect as col = \"red\".\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg, col = 2)\n\n\n\n\n\n\n\nhist(mpg, col = 2, breaks = 15)\n\n\n\n\n\n\n\n\nThe pairs() function creates a scatterplot matrix, i.e. a scatterplot for every pair of variables. We can also produce scatterplots for just a subset of the variables.\n\npairs(Auto)\n\n\n\n\n\n\n\npairs(\n    ~ mpg + displacement + horsepower + weight + acceleration,\n    data = Auto\n  )\n\n\n\n\n\n\n\n\nIn conjunction with the plot() function, identify() provides a useful interactive method for identifying the value of a particular variable for points on a plot. We pass in three arguments to identify(): the \\(x\\)-axis variable, the \\(y\\)-axis variable, and the variable whose values we would like to see printed for each point. Then clicking one or more points in the plot and hitting Escape will cause R to print the values of the variable of interest. The numbers printed under the identify() function correspond to the rows for the selected points.\n\nplot(horsepower, mpg)\nidentify(horsepower, mpg, name)\n\n\n\n\n\n\n\n\ninteger(0)\n\n\nThe summary() function produces a numerical summary of each variable in a particular data set.\n\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.00   1st Qu.:4.000   1st Qu.:105.0   1st Qu.: 75.0   1st Qu.:2225  \n Median :22.75   Median :4.000   Median :151.0   Median : 93.5   Median :2804  \n Mean   :23.45   Mean   :5.472   Mean   :194.4   Mean   :104.5   Mean   :2978  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:275.8   3rd Qu.:126.0   3rd Qu.:3615  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                                               \n  acceleration        year           origin                      name    \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   amc matador       :  5  \n 1st Qu.:13.78   1st Qu.:73.00   1st Qu.:1.000   ford pinto        :  5  \n Median :15.50   Median :76.00   Median :1.000   toyota corolla    :  5  \n Mean   :15.54   Mean   :75.98   Mean   :1.577   amc gremlin       :  4  \n 3rd Qu.:17.02   3rd Qu.:79.00   3rd Qu.:2.000   amc hornet        :  4  \n Max.   :24.80   Max.   :82.00   Max.   :3.000   chevrolet chevette:  4  \n                                                 (Other)           :365  \n\n\nFor qualitative variables such as name, R will list the number of observations that fall in each category. We can also produce a summary of just a single variable.\n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.00   22.75   23.45   29.00   46.60 \n\n\nOnce we have finished using R, we type q() in order to shut it down, or quit. When exiting R, we have the option to save the current workspace so that all objects (such as data sets) that we have created in this R session will be available next time. Before exiting R, we may want to save a record of all of the commands that we typed in the most recent session; this can be accomplished using the savehistory() function. Next time we enter R, we can load that history using the loadhistory() function, if we wish."
  },
  {
    "objectID": "teaching/stat-learn/material/01/01-R-you-ready.html#installation",
    "href": "teaching/stat-learn/material/01/01-R-you-ready.html#installation",
    "title": "R you ready?",
    "section": "Installation",
    "text": "Installation\nBefore getting started, make sure you have both tidyverse and tidymodels installed on your system. You can do this by running the following code in R:\n\n# Install the tidyverse and tidymodels collections of packages\ninstall.packages(\"tidyverse\")\ninstall.packages(\"tidymodels\")\n\n# To load them into your R session\nlibrary(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "teaching/stat-learn/material/01/01-R-you-ready.html#references",
    "href": "teaching/stat-learn/material/01/01-R-you-ready.html#references",
    "title": "R you ready?",
    "section": "References",
    "text": "References\n\nWickham, H., Averick, M., Bryan, J., Chang, W., D’Agostino McGowan, L., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., Takahashi, K., Vaughan, D., Wilke, C. O., Woo, K., & Yutani, H. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686. https://doi.org/10.21105/joss.01686\nKuhn, M., & Wickham, H. (2020). Tidymodels: a collection of packages for modeling and machine learning using tidyverse principles. Retrieved from https://www.tidymodels.org\nWickham, H., & Grolemund, G. (2017). R for Data Science. O’Reilly Media. Retrieved from https://r4ds.had.co.nz\nThe tidyverse official documentation: https://www.tidyverse.org"
  }
]