---
title: "Statistical Learning Lab 13 - PCA"
editor: visual
format:
  html:
    embed-resources: true
---

```{r}
library(ggplot2)
library(tidyverse)
library(mclust)
library(randomForest)

```

## Redux - Young People Survey:

Below we find survey data of 1000 people in Slovakia aged 15 to 30. We will try to detect distinct features using PCA

First we load the dataset and this time we transform categorical data to binary dummies for each level.

```{r}
responses = read.csv('responses.csv')
df_youth = data.frame(model.matrix(~ . -1, data = responses))
df_youth_scaled = data.frame(scale(df_youth))
```



### PCA

We will continue with last weeks data by applying PCA to find some better way to visualize our data. 

Last time we clustered our data and then went on to visualize this clustering. Unfortunately we did not scale our data and thus we overvalued the contributions of the height and weight variables. In PCA this has similar repercussions as we can see by setting the scale argument.


```{r}
pr_out = prcomp(df_youth, scale=TRUE)
pr_out_unscaled = prcomp(df_youth, scale=FALSE)

l = length(pr_out$sdev)
pca_plot_big = data.frame(
  var_unscaled = pr_out_unscaled$sdev ^ 2 / sum(pr_out_unscaled$sdev ^ 2),
  var_scaled = pr_out$sdev ^ 2 / sum(pr_out$sdev ^ 2),
  PC = c(1:l))
                          
pca_plot_big %>%
  ggplot(aes(x = PC, y = var_unscaled, alpha = 0.8)) + geom_bar(stat="identity") 

pca_plot_big %>%
  ggplot(aes(x = PC, y = var_scaled, alpha = 0.8)) + geom_bar(stat="identity") 
```


As we see here, the unscaled version is dominated by one factor which is a combination of age/weight/height, while the scaled result shows a much flatter distribution of relevant features. By looking at the explained variance of each PC, we can see how much redundant information is in our data. This is once again an exercise in "feeling things out" - we want to keep as much information as possible for using as little variables as possible.
We could use the summary function but it is very unhandy with a large number of dimensions



```{r}
# summary(pr_out)
```




For demonstration purposes we are going to look at the unscaled version. 
 
 
By default, the prcomp() function centers the variables to have mean zero. By using the option scale = TRUE, we scale the variables to have standard deviation one. The output from prcomp() contains a number of useful quantities.



- rotation stores the factor loadings for each variable

- x stores the PC score vectors

- sdev stores the sd of each PC


We can use these to better visualize our previous results. Let's cluster the data by PC dimensions instead of "natural ones".


```{r}
df_pr = data.frame(pr_out_unscaled$x[, 1:20])
hc.complete = hclust(dist(df_pr), method="complete")
plot(hc.complete, hang=-1, ann=F, labels = F)
```



```{r}
df_pr$cluster = factor(cutree(hc.complete, 4))
df_pr %>%
  ggplot(aes(x = PC1, y = PC2, color = cluster)) + geom_point()
```


As we can see, now our clusters are clearly seperable across the first two PC dimensions. But we cannot really get substantive meaning out of this. Checking factor loadings can make things a bit clearer:

```{r}

top_k_loadings = function(pr_obj, PCnum, k) {
  loadings = data.frame(pr_obj$rotation)
  loadings$Variable = rownames(loadings)
  
  topPC = loadings[order(abs(loadings[[PCnum]]),
                         decreasing = TRUE), ][1:k, ]
  
  print(topPC[PCnum])
}

top_k_loadings(pr_out_unscaled, 1, 3)

top_k_loadings(pr_out_unscaled, 2, 3)

top_k_loadings(pr_out_unscaled, 3, 3)

```


As we can see the unscaled variables clearly dominate the first two PCs. Once we redo this for the correctly scaled data we can find some new info:


```{r}
top_k_loadings(pr_out, 1, 3)
top_k_loadings(pr_out, 2, 3)
top_k_loadings(pr_out, 3, 3)
top_k_loadings(pr_out, 4, 3)
top_k_loadings(pr_out, 5, 3)


```


Looks like (apart from PC1 for Gender), we get PC dimensions mostly along subcultural lines!
When we run a clustering algorithm, we see that the subgroups seem to be pretty visible across the "classical" and "hiphop" PC dimensions.


```{r}
df_pr_scaled = data.frame(pr_out$x[, 2:4])
hc.complete_scaled = hclust(dist(df_pr_scaled), method="complete")
df_pr_scaled$cluster = factor(cutree(hc.complete_scaled, 3))
df_pr_scaled %>%
  ggplot(aes(x = PC2, y = PC3, color = cluster)) + geom_point()
```













