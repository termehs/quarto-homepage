[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science with Tidyverse I",
    "section": "",
    "text": "Make sure to install and load Tidyverse:\n\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n\nSchedule\n\n\n\n\nslides\npractical\ndata\n\n\n\n\n1: Meet the toolkit\n\n\n\n\n\n2: Data visualization and ggplot\n\n\n\n\n\n3: Visualizing numerical and categorical data\n\n\n.zip\n\n\n4: Grammar of data wrangling I\n\n \n.zip\n\n\n5: Grammar of data wrangling II\n\n\n.zip\n\n\n6: Tidying Data\n\n\n.zip\n\n\n     More Practicals\n\n \n.zip\n\n\n7: Data Types and Data Classes\n\n  \n.zip\n\n\n8: Importing and Recoding Data\n\n  \n.zip\n\n\n9: Functions and Iteration\n\n\n\n\n\n10: Effective Visualization\n\n\n.zip"
  },
  {
    "objectID": "material/bechdel.html",
    "href": "material/bechdel.html",
    "title": "Bechdel",
    "section": "",
    "text": "In this mini analysis we work with the data used in the FiveThirtyEight story titled “The Dollar-And-Cents Case Against Hollywood’s Exclusion of Women”. We will together fill in the blanks denoted by ___."
  },
  {
    "objectID": "material/bechdel.html#data-and-packages",
    "href": "material/bechdel.html#data-and-packages",
    "title": "Bechdel",
    "section": "Data and packages",
    "text": "Data and packages\nWe start with loading the packages we’ll use.\n\nlibrary(fivethirtyeight)\nlibrary(tidyverse)\n\nThe dataset contains information on 1794 movies released between 1970 and 2013. However we’ll focus our analysis on movies released between 1990 and 2013.\n\nbechdel90_13 &lt;- bechdel %&gt;% \n  filter(between(year, 1990, 2013))\n\nThere are ___ such movies.\nThe financial variables we’ll focus on are the following:\n\nbudget_2013: Budget in 2013 inflation adjusted dollars\ndomgross_2013: Domestic gross (US) in 2013 inflation adjusted dollars\nintgross_2013: Total International (i.e., worldwide) gross in 2013 inflation adjusted dollars\n\nAnd we’ll also use the binary and clean_test variables for grouping."
  },
  {
    "objectID": "material/bechdel.html#analysis",
    "href": "material/bechdel.html#analysis",
    "title": "Bechdel",
    "section": "Analysis",
    "text": "Analysis\nLet’s take a look at how median budget and gross vary by whether the movie passed the Bechdel test, which is stored in the binary variable.\n\nbechdel90_13 %&gt;%\n  group_by(binary) %&gt;%\n  summarise(\n    med_budget = median(budget_2013),\n    med_domgross = median(domgross_2013, na.rm = TRUE),\n    med_intgross = median(intgross_2013, na.rm = TRUE)\n    )\n\n# A tibble: 2 × 4\n  binary med_budget med_domgross med_intgross\n  &lt;chr&gt;       &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 FAIL    48385984.    57318606.    104475669\n2 PASS    31070724     45330446.     80124349\n\n\nNext, let’s take a look at how median budget and gross vary by a more detailed indicator of the Bechdel test result. This information is stored in the clean_test variable, which takes on the following values:\n\nok = passes test\ndubious\nmen = women only talk about men\nnotalk = women don’t talk to each other\nnowomen = fewer than two women\n\n\nbechdel90_13 %&gt;%\n  #group_by(___) %&gt;%\n  summarise(\n    med_budget = median(budget_2013),\n    med_domgross = median(domgross_2013, na.rm = TRUE),\n    med_intgross = median(intgross_2013, na.rm = TRUE)\n    )\n\n# A tibble: 1 × 3\n  med_budget med_domgross med_intgross\n       &lt;int&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1   37878971     52270207     93523336\n\n\nIn order to evaluate how return on investment varies among movies that pass and fail the Bechdel test, we’ll first create a new variable called roi as the ratio of the gross to budget.\n\nbechdel90_13 &lt;- bechdel90_13 %&gt;%\n  mutate(roi = (intgross_2013 + domgross_2013) / budget_2013)\n\nLet’s see which movies have the highest return on investment.\n\nbechdel90_13 %&gt;%\n  arrange(desc(roi)) %&gt;% \n  select(title, roi, year)\n\n# A tibble: 1,615 × 3\n   title                     roi  year\n   &lt;chr&gt;                   &lt;dbl&gt; &lt;int&gt;\n 1 Paranormal Activity      671.  2007\n 2 The Blair Witch Project  648.  1999\n 3 El Mariachi              583.  1992\n 4 Clerks.                  258.  1994\n 5 In the Company of Men    231.  1997\n 6 Napoleon Dynamite        227.  2004\n 7 Once                     190.  2006\n 8 The Devil Inside         155.  2012\n 9 Primer                   142.  2004\n10 Fireproof                134.  2008\n# ℹ 1,605 more rows\n\n\nBelow is a visualization of the return on investment by test result, however it’s difficult to see the distributions due to a few extreme observations.\n\nggplot(data = bechdel90_13, \n       mapping = aes(x = clean_test, y = roi, color = binary)) +\n  geom_boxplot() +\n  labs(\n    title = \"Return on investment vs. Bechdel test result\",\n    x = \"Detailed Bechdel result\",\n    y = \"___\",\n    color = \"Binary Bechdel result\"\n    )\n\n\n\n\n\n\n\n\nWhat are those movies with very high returns on investment?\n\nbechdel90_13 %&gt;%\n  filter(roi &gt; 400) %&gt;%\n  select(title, budget_2013, domgross_2013, year)\n\n# A tibble: 3 × 4\n  title                   budget_2013 domgross_2013  year\n  &lt;chr&gt;                         &lt;int&gt;         &lt;dbl&gt; &lt;int&gt;\n1 Paranormal Activity          505595     121251476  2007\n2 The Blair Witch Project      839077     196538593  1999\n3 El Mariachi                   11622       3388636  1992\n\n\nZooming in on the movies with roi &lt; ___ provides a better view of how the medians across the categories compare:\n\nggplot(data = bechdel90_13, mapping = aes(x = clean_test, y = roi, color = binary)) +\n  geom_boxplot() +\n  labs(\n    title = \"Return on investment vs. Bechdel test result\",\n    subtitle = \"___\", # Something about zooming in to a certain level\n    x = \"Detailed Bechdel result\",\n    y = \"Return on investment\",\n    color = \"Binary Bechdel result\"\n    ) +\n  coord_cartesian(ylim = c(0, 15))"
  },
  {
    "objectID": "bechdel.html",
    "href": "bechdel.html",
    "title": "Bechdel",
    "section": "",
    "text": "In this mini analysis we work with the data used in the FiveThirtyEight story titled “The Dollar-And-Cents Case Against Hollywood’s Exclusion of Women”. We will together fill in the blanks denoted by ___."
  },
  {
    "objectID": "bechdel.html#data-and-packages",
    "href": "bechdel.html#data-and-packages",
    "title": "Bechdel",
    "section": "Data and packages",
    "text": "Data and packages\nWe start with loading the packages we’ll use.\n\nlibrary(fivethirtyeight)\nlibrary(tidyverse)\n\nThe dataset contains information on 1794 movies released between 1970 and 2013. However we’ll focus our analysis on movies released between 1990 and 2013.\n\nbechdel90_13 &lt;- bechdel %&gt;% \n  filter(between(year, 1990, 2013))\n\nThere are ___ such movies.\nThe financial variables we’ll focus on are the following:\n\nbudget_2013: Budget in 2013 inflation adjusted dollars\ndomgross_2013: Domestic gross (US) in 2013 inflation adjusted dollars\nintgross_2013: Total International (i.e., worldwide) gross in 2013 inflation adjusted dollars\n\nAnd we’ll also use the binary and clean_test variables for grouping."
  },
  {
    "objectID": "bechdel.html#analysis",
    "href": "bechdel.html#analysis",
    "title": "Bechdel",
    "section": "Analysis",
    "text": "Analysis\nLet’s take a look at how median budget and gross vary by whether the movie passed the Bechdel test, which is stored in the binary variable.\n\nbechdel90_13 %&gt;%\n  group_by(binary) %&gt;%\n  summarise(\n    med_budget = median(budget_2013),\n    med_domgross = median(domgross_2013, na.rm = TRUE),\n    med_intgross = median(intgross_2013, na.rm = TRUE)\n    )\n\n# A tibble: 2 × 4\n  binary med_budget med_domgross med_intgross\n  &lt;chr&gt;       &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 FAIL    48385984.    57318606.    104475669\n2 PASS    31070724     45330446.     80124349\n\n\nNext, let’s take a look at how median budget and gross vary by a more detailed indicator of the Bechdel test result. This information is stored in the clean_test variable, which takes on the following values:\n\nok = passes test\ndubious\nmen = women only talk about men\nnotalk = women don’t talk to each other\nnowomen = fewer than two women\n\n\nbechdel90_13 %&gt;%\n  #group_by(___) %&gt;%\n  summarise(\n    med_budget = median(budget_2013),\n    med_domgross = median(domgross_2013, na.rm = TRUE),\n    med_intgross = median(intgross_2013, na.rm = TRUE)\n    )\n\n# A tibble: 1 × 3\n  med_budget med_domgross med_intgross\n       &lt;int&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1   37878971     52270207     93523336\n\n\nIn order to evaluate how return on investment varies among movies that pass and fail the Bechdel test, we’ll first create a new variable called roi as the ratio of the gross to budget.\n\nbechdel90_13 &lt;- bechdel90_13 %&gt;%\n  mutate(roi = (intgross_2013 + domgross_2013) / budget_2013)\n\nLet’s see which movies have the highest return on investment.\n\nbechdel90_13 %&gt;%\n  arrange(desc(roi)) %&gt;% \n  select(title, roi, year)\n\n# A tibble: 1,615 × 3\n   title                     roi  year\n   &lt;chr&gt;                   &lt;dbl&gt; &lt;int&gt;\n 1 Paranormal Activity      671.  2007\n 2 The Blair Witch Project  648.  1999\n 3 El Mariachi              583.  1992\n 4 Clerks.                  258.  1994\n 5 In the Company of Men    231.  1997\n 6 Napoleon Dynamite        227.  2004\n 7 Once                     190.  2006\n 8 The Devil Inside         155.  2012\n 9 Primer                   142.  2004\n10 Fireproof                134.  2008\n# ℹ 1,605 more rows\n\n\nBelow is a visualization of the return on investment by test result, however it’s difficult to see the distributions due to a few extreme observations.\n\nggplot(data = bechdel90_13, \n       mapping = aes(x = clean_test, y = roi, color = binary)) +\n  geom_boxplot() +\n  labs(\n    title = \"Return on investment vs. Bechdel test result\",\n    x = \"Detailed Bechdel result\",\n    y = \"___\",\n    color = \"Binary Bechdel result\"\n    )\n\n\n\n\n\n\n\n\nWhat are those movies with very high returns on investment?\n\nbechdel90_13 %&gt;%\n  filter(roi &gt; 400) %&gt;%\n  select(title, budget_2013, domgross_2013, year)\n\n# A tibble: 3 × 4\n  title                   budget_2013 domgross_2013  year\n  &lt;chr&gt;                         &lt;int&gt;         &lt;dbl&gt; &lt;int&gt;\n1 Paranormal Activity          505595     121251476  2007\n2 The Blair Witch Project      839077     196538593  1999\n3 El Mariachi                   11622       3388636  1992\n\n\nZooming in on the movies with roi &lt; ___ provides a better view of how the medians across the categories compare:\n\nggplot(data = bechdel90_13, mapping = aes(x = clean_test, y = roi, color = binary)) +\n  geom_boxplot() +\n  labs(\n    title = \"Return on investment vs. Bechdel test result\",\n    subtitle = \"___\", # Something about zooming in to a certain level\n    x = \"Detailed Bechdel result\",\n    y = \"Return on investment\",\n    color = \"Binary Bechdel result\"\n    ) +\n  coord_cartesian(ylim = c(0, 15))"
  },
  {
    "objectID": "material/starwars.html",
    "href": "material/starwars.html",
    "title": "Visualizing Starwars characters",
    "section": "",
    "text": "Glimpse at the starwars data frame.\n\n\nglimpse(starwars)\n\nRows: 87\nColumns: 14\n$ name       &lt;chr&gt; \"Luke Skywalker\", \"C-3PO\", \"R2-D2\", \"Darth Vader\", \"Leia Or…\n$ height     &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 2…\n$ mass       &lt;dbl&gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77.…\n$ hair_color &lt;chr&gt; \"blond\", NA, NA, \"none\", \"brown\", \"brown, grey\", \"brown\", N…\n$ skin_color &lt;chr&gt; \"fair\", \"gold\", \"white, blue\", \"white\", \"light\", \"light\", \"…\n$ eye_color  &lt;chr&gt; \"blue\", \"yellow\", \"red\", \"yellow\", \"brown\", \"blue\", \"blue\",…\n$ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57.0, …\n$ sex        &lt;chr&gt; \"male\", \"none\", \"none\", \"male\", \"female\", \"male\", \"female\",…\n$ gender     &lt;chr&gt; \"masculine\", \"masculine\", \"masculine\", \"masculine\", \"femini…\n$ homeworld  &lt;chr&gt; \"Tatooine\", \"Tatooine\", \"Naboo\", \"Tatooine\", \"Alderaan\", \"T…\n$ species    &lt;chr&gt; \"Human\", \"Droid\", \"Droid\", \"Human\", \"Human\", \"Human\", \"Huma…\n$ films      &lt;list&gt; &lt;\"A New Hope\", \"The Empire Strikes Back\", \"Return of the J…\n$ vehicles   &lt;list&gt; &lt;\"Snowspeeder\", \"Imperial Speeder Bike\"&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, \"Imp…\n$ starships  &lt;list&gt; &lt;\"X-wing\", \"Imperial shuttle\"&gt;, &lt;&gt;, &lt;&gt;, \"TIE Advanced x1\",…\n\n\n\nModify the following plot to change the color of all points to \"pink\".\n\n\nggplot(starwars, \n       aes(x = height, y = mass, color = gender, size = birth_year)) +\n  geom_point(color = \"pink\")\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nAdd labels for title, x and y axes, and size of points. Uncomment to see the effect.\n\n\nggplot(starwars, \n       aes(x = height, y = mass, color = gender, size = birth_year)) +\n  geom_point(color = \"#30509C\") +\n  labs(\n    #title = \"___\",\n    #x = \"___\", \n    #y = \"___\",\n    #___\n    )\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nPick a single numerical variable and make a histogram of it. Select a reasonable binwidth for it.\n\n(A little bit of starter code is provided below, and the code chunk is set to not be evaluated with eval: false because the current code in there is not valid code and hence the document wouldn’t knit. Once you replace the code with valid code, set the chunk option to eval: true, or remove the eval option altogether since it’s set to true by default.)\n\nggplot(starwars, aes(___)) +\n  geom___\n\n\nPick a numerical variable and a categorical variable and make a visualization (you pick the type!) to visualization the relationship between the two variables. Along with your code and output, provide an interpretation of the visualization.\n\nInterpretation goes here…\n\nPick a single categorical variable from the data set and make a bar plot of its distribution.\n\n\nPick two categorical variables and make a visualization to visualize the relationship between the two variables. Along with your code and output, provide an interpretation of the visualization.\n\nInterpretation goes here…\n\nPick two numerical variables and two categorical variables and make a visualization that incorporates all of them and provide an interpretation with your answer.\n\n(This time no starter code is provided, you’re on your own!)\nInterpretation goes here…"
  },
  {
    "objectID": "material/plastic-waste.html",
    "href": "material/plastic-waste.html",
    "title": "Global plastic waste",
    "section": "",
    "text": "Plastic pollution is a major and growing problem, negatively affecting oceans and wildlife health. Our World in Data has a lot of great data at various levels including globally, per country, and over time. For this lab we focus on data from 2010.\nAdditionally, National Geographic ran a data visualization communication contest on plastic waste as seen here."
  },
  {
    "objectID": "material/plastic-waste.html#packages",
    "href": "material/plastic-waste.html#packages",
    "title": "Global plastic waste",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for this analysis.\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "material/plastic-waste.html#data",
    "href": "material/plastic-waste.html#data",
    "title": "Global plastic waste",
    "section": "Data",
    "text": "Data\nThe dataset for this assignment can be found as a csv file. You can read it in using the following (make sure you save the data in your working directory).\n\nplastic_waste &lt;- read_csv(\"data03/plastic-waste.csv\")\n\nThe variable descriptions are as follows:\n\ncode: 3 Letter country code\nentity: Country name\ncontinent: Continent name\nyear: Year\ngdp_per_cap: GDP per capita constant 2011 international $, rate\nplastic_waste_per_cap: Amount of plastic waste per capita in kg/day\nmismanaged_plastic_waste_per_cap: Amount of mismanaged plastic waste per capita in kg/day\nmismanaged_plastic_waste: Tonnes of mismanaged plastic waste\ncoastal_pop: Number of individuals living on/near coast\ntotal_pop: Total population according to Gapminder"
  },
  {
    "objectID": "material/lab-03-nobel-laureates.html",
    "href": "material/lab-03-nobel-laureates.html",
    "title": "Lab 03 - Nobel laureates",
    "section": "",
    "text": "In January 2017, Buzzfeed published an article on why Nobel laureates show immigration is so important for American science. You can read the article here. In the article they show that while most living Nobel laureates in the sciences are based in the US, many of them were born in other countries. This is one reason why scientific leaders say that immigration is vital for progress. In this lab we will work with the data from this article to recreate some of their visualizations as well as explore new questions."
  },
  {
    "objectID": "material/lab-03-nobel-laureates.html#merges-and-merge-conflicts",
    "href": "material/lab-03-nobel-laureates.html#merges-and-merge-conflicts",
    "title": "Lab 03 - Nobel laureates",
    "section": "Merges and merge conflicts",
    "text": "Merges and merge conflicts\nThis is the second week you’re working in teams, so we’re going to make things a little more interesting and let all of you make changes and push those changes to your team repository. Sometimes things will go swimmingly, and sometimes you’ll run into merge conflicts. So our first task today is to walk you through a merge conflict!\n\nPushing to a repo replaces the code on GitHub with the code you have on your computer.\nIf a collaborator has made a change to your repo on GitHub that you haven’t incorporated into your local work, GitHub will stop you from pushing to the repo because this could overwrite your collaborator’s work!\nSo you need to explicitly “merge” your collaborator’s work before you can push.\nIf your and your collaborator’s changes are in different files or in different parts of the same file, git merges the work for you automatically when you *pull*.\nIf you both changed the same part of a file, git will produce a **merge conflict** because it doesn’t know how which change you want to keep and which change you want to overwrite.\n\nGit will put conflict markers in your code that look like:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD \n\nSee also: [dplyr documentation](https://dplyr.tidyverse.org/)   \n\n======= \n\nSee also [ggplot2 documentation](https://ggplot2.tidyverse.org/)  \n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; some1alpha2numeric3string4\nThe ===s separate your changes (top) from their changes (bottom).\nNote that on top you see the word HEAD, which indicates that these are your changes.\nAnd at the bottom you see some1alpha2numeric3string4 (well, it probably looks more like 28e7b2ceb39972085a0860892062810fb812a08f).\nThis is the hash (a unique identifier) of the commit your collaborator made with the conflicting change.\nYour job is to reconcile the changes: edit the file so that it incorporates the best of both versions and delete the &lt;&lt;&lt;, ===, and &gt;&gt;&gt; lines. Then you can stage and commit the result."
  },
  {
    "objectID": "material/lab-03-nobel-laureates.html#setup",
    "href": "material/lab-03-nobel-laureates.html#setup",
    "title": "Lab 03 - Nobel laureates",
    "section": "Setup",
    "text": "Setup\n\nClone the repo and open the .Rmd file.\nAssign the numbers 1, 2, 3, and 4 to each of the team members. If your team has fewer than 4 people, some people will need to have multiple numbers. If your team has more than 4 people, some people will need to share some numbers."
  },
  {
    "objectID": "material/lab-03-nobel-laureates.html#lets-cause-a-merge-conflict",
    "href": "material/lab-03-nobel-laureates.html#lets-cause-a-merge-conflict",
    "title": "Lab 03 - Nobel laureates",
    "section": "Let’s cause a merge conflict!",
    "text": "Let’s cause a merge conflict!\nOur goal is to see two different types of merges: first we’ll see a type of merge that git can’t figure out on its own how to do on its own (a merge conflict) and requires human intervention, then another type of where that git can figure out how to do without human intervention.\nDoing this will require some tight choreography, so pay attention!\nTake turns in completing the exercise, only one member at a time. Others should just watch, not doing anything on their own projects (this includes not even pulling changes!) until they are instructed to. If you feel like you won’t be able to resist the urge to touch your computer when it’s not your turn, we recommend putting your hands in your pockets or sitting on them!\nBefore starting: everyone should have the repo cloned and know which role number(s) they are.\nRole 1:\n\nChange the team name to your actual team name.\nKnit, commit, push.\n\n🛑 Make sure the previous role has finished before moving on to the next step.\nRole 2:\n\nChange the team name to some other word.\nKnit, commit, push. You should get an error.\nPull. Take a look at the document with the merge conflict.\nClear the merge conflict by editing the document to choose the correct/preferred change.\nKnit.\nClick the Stage checkbox for all files in your Git tab. Make sure they all have check marks, not filled-in boxes.\nCommit and push.\n\n🛑 Make sure the previous role has finished before moving on to the next step.\nRole 3:\n\nChange the a label of the first code chunk\nKnit, commit, push. You should get an error.\nPull. No merge conflicts should occur, but you should see a message about merging.\nNow push.\n\n🛑 Make sure the previous role has finished before moving on to the next step.\nRole 4:\n\nChange the label of the first code chunk to something other than previous role did.\nKnit, commit, push. You should get an error.\nPull. Take a look at the document with the merge conflict. Clear the merge conflict by choosing the correct/preferred change. Commit, and push.\n\n🛑 Make sure the previous role has finished before moving on to the next step.\nEveryone: Pull, and observe the changes in your document."
  },
  {
    "objectID": "material/lab-03-nobel-laureates.html#tips-for-collaborating-via-github",
    "href": "material/lab-03-nobel-laureates.html#tips-for-collaborating-via-github",
    "title": "Lab 03 - Nobel laureates",
    "section": "Tips for collaborating via GitHub",
    "text": "Tips for collaborating via GitHub\n\nAlways pull first before you start working.\nResolve a merge conflict (commit and push) before continuing your work. Never do new work while resolving a merge conflict.\nKnit, commit, and push often to minimize merge conflicts and/or to make merge conflicts easier to resolve.\nIf you find yourself in a situation that is difficult to resolve, ask questions ASAP. Don’t let it linger and get bigger."
  },
  {
    "objectID": "material/lab-03-nobel-laureates.html#warm-up",
    "href": "material/lab-03-nobel-laureates.html#warm-up",
    "title": "Lab 03 - Nobel laureates",
    "section": "Warm up",
    "text": "Warm up\nBefore we introduce the data, let’s warm up with some simple exercises.\n\nUpdate the YAML, changing the author name to your name, and knit the document.\nCommit your changes with a meaningful commit message.\nPush your changes to GitHub.\nGo to your repo on GitHub and confirm that your changes are visible in your Rmd and md files. If anything is missing, commit and push again."
  },
  {
    "objectID": "material/lab-03-nobel-laureates.html#packages",
    "href": "material/lab-03-nobel-laureates.html#packages",
    "title": "Lab 03 - Nobel laureates",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for much of the data wrangling. This package is already installed for you. You can load them by running the following in your Console:\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "material/lab-03-nobel-laureates.html#data",
    "href": "material/lab-03-nobel-laureates.html#data",
    "title": "Lab 03 - Nobel laureates",
    "section": "Data",
    "text": "Data\nThe dataset for this assignment can be found as a CSv (comma separated values) file in the data folder of your repository. You can read it in using the following.\n\nnobel &lt;- read_csv(\"data/nobel.csv\")\n\nThe variable descriptions are as follows:\n\nid: ID number\nfirstname: First name of laureate\nsurname: Surname\nyear: Year prize won\ncategory: Category of prize\naffiliation: Affiliation of laureate\ncity: City of laureate in prize year\ncountry: Country of laureate in prize year\nborn_date: Birth date of laureate\ndied_date: Death date of laureate\ngender: Gender of laureate\nborn_city: City where laureate was born\nborn_country: Country where laureate was born\nborn_country_code: Code of country where laureate was born\ndied_city: City where laureate died\ndied_country: Country where laureate died\ndied_country_code: Code of country where laureate died\noverall_motivation: Overall motivation for recognition\nshare: Number of other winners award is shared with\nmotivation: Motivation for recognition\n\nIn a few cases the name of the city/country changed after laureate was given (e.g. in 1975 Bosnia and Herzegovina was called the Socialist Federative Republic of Yugoslavia). In these cases the variables below reflect a different name than their counterparts without the suffix `_original`.\n\nborn_country_original: Original country where laureate was born\nborn_city_original: Original city where laureate was born\ndied_country_original: Original country where laureate died\ndied_city_original: Original city where laureate died\ncity_original: Original city where laureate lived at the time of winning the award\ncountry_original: Original country where laureate lived at the time of winning the award"
  },
  {
    "objectID": "material/lab-03-nobel-laureates.html#get-to-know-your-data",
    "href": "material/lab-03-nobel-laureates.html#get-to-know-your-data",
    "title": "Lab 03 - Nobel laureates",
    "section": "Get to know your data",
    "text": "Get to know your data\n\nHow many observations and how many variables are in the dataset? Use inline code to answer this question. What does each row represent?\n\nThere are some observations in this dataset that we will exclude from our analysis to match the Buzzfeed results.\n\nCreate a new data frame called nobel_living that filters for\n\n\nlaureates for whom country is available\nlaureates who are people as opposed to organizations (organizations are denoted with \"org\" as their gender)\nlaureates who are still alive (their died_date is NA)\n\nConfirm that once you have filtered for these characteristics you are left with a data frame with 228 observations, once again using inline code.\n🧶 ✅ ⬆️ Knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards."
  },
  {
    "objectID": "material/lab-03-nobel-laureates.html#most-living-nobel-laureates-were-based-in-the-us-when-they-won-their-prizes",
    "href": "material/lab-03-nobel-laureates.html#most-living-nobel-laureates-were-based-in-the-us-when-they-won-their-prizes",
    "title": "Lab 03 - Nobel laureates",
    "section": "Most living Nobel laureates were based in the US when they won their prizes",
    "text": "Most living Nobel laureates were based in the US when they won their prizes\n… says the Buzzfeed article. Let’s see if that’s true.\nFirst, we’ll create a new variable to identify whether the laureate was in the US when they won their prize. We’ll use the mutate() function for this. The following pipeline mutates the nobel_living data frame by adding a new variable called country_us. We use an if statement to create this variable. The first argument in the if_else() function we’re using to write this if statement is the condition we’re testing for. If country is equal to \"USA\", we set country_us to \"USA\". If not, we set the country_us to \"Other\".\n\nNote that we can achieve the same result using the `fct_other()` function we've seen before (i.e. with `country_us = fct_other(country, \"USA\")`). We decided to use the `if_else()` here to show you one example of an if statement in R.\n\n\nnobel_living &lt;- nobel_living %&gt;%\n  mutate(\n    country_us = if_else(country == \"USA\", \"USA\", \"Other\")\n  )\n\nNext, we will limit our analysis to only the following categories: Physics, Medicine, Chemistry, and Economics.\n\nnobel_living_science &lt;- nobel_living %&gt;%\n  filter(category %in% c(\"Physics\", \"Medicine\", \"Chemistry\", \"Economics\"))\n\nFor the next exercise work with the nobel_living_science data frame you created above. This means you’ll need to define this data frame in your R Markdown document, even though the next exercise doesn’t explicitly ask you to do so.\n\nCreate a faceted bar plot visualizing the relationship between the category of prize and whether the laureate was in the US when they won the nobel prize. Interpret your visualization, and say a few words about whether the Buzzfeed headline is supported by the data.\n\nYour visualization should be faceted by category.\nFor each facet you should have two bars, one for winners in the US and one for Other.\nFlip the coordinates so the bars are horizontal, not vertical.\n\n\n🧶 ✅ ⬆️ Knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.d"
  },
  {
    "objectID": "material/lab-03-nobel-laureates.html#but-of-those-us-based-nobel-laureates-many-were-born-in-other-countries",
    "href": "material/lab-03-nobel-laureates.html#but-of-those-us-based-nobel-laureates-many-were-born-in-other-countries",
    "title": "Lab 03 - Nobel laureates",
    "section": "But of those US-based Nobel laureates, many were born in other countries",
    "text": "But of those US-based Nobel laureates, many were born in other countries\n\n**Hint:** You should be able to ~~cheat~~ borrow from code you used earlier to create the `country_us` variable.\n\n\nCreate a new variable called born_country_us that has the value \"USA\" if the laureate is born in the US, and \"Other\" otherwise. How many of the winners are born in the US?\nAdd a second variable to your visualization from Exercise 3 based on whether the laureate was born in the US or not. Based on your visualization, do the data appear to support Buzzfeed’s claim? Explain your reasoning in 1-2 sentences.\n\nYour final visualization should contain a facet for each category.\nWithin each facet, there should be a bar for whether the laureate won the award in the US or not.\nEach bar should have segments for whether the laureate was born in the US or not.\n\n\n🧶 ✅ ⬆️ Knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards."
  },
  {
    "objectID": "material/lab-03-nobel-laureates.html#heres-where-those-immigrant-nobelists-were-born",
    "href": "material/lab-03-nobel-laureates.html#heres-where-those-immigrant-nobelists-were-born",
    "title": "Lab 03 - Nobel laureates",
    "section": "Here’s where those immigrant Nobelists were born",
    "text": "Here’s where those immigrant Nobelists were born\n\nNote that your bar plot won't exactly match the one from the Buzzfeed article. This is likely because the data has been updated since the article was published.\n\n\nIn a single pipeline, filter for laureates who won their prize in the US, but were born outside of the US, and then create a frequency table (with the count() function) for their birth country (born_country) and arrange the resulting data frame in descending order of number of observations for each country. Which country is the most common?\n\n🧶 ✅ ⬆️ Knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards and review the md document on GitHub to make sure you’re happy with the final state of your work.\nNow go back through your write up to make sure you’ve answered all questions and all of your R chunks are properly labelled. Once you decide as a team that you’re done with this lab, all members of the team should pull the changes and knit the R Markdown document to confirm that they can reproduce the report."
  },
  {
    "objectID": "material/hotels-datawrangling.html",
    "href": "material/hotels-datawrangling.html",
    "title": "Hotel bookings - data wrangling",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)\n# From TidyTuesday: https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-02-11/readme.md\nhotels &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-11/hotels.csv\")\nThe data is also available as a csv file which you can import directly."
  },
  {
    "objectID": "material/hotels-datawrangling.html#exercises",
    "href": "material/hotels-datawrangling.html#exercises",
    "title": "Hotel bookings - data wrangling",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.\nWarm up! Take a look at an overview of the data with the skim() function.\nNote: I already gave you the answer to this exercise. You just need to knit the document and view the output. A definition of all variables is given in the Data dictionary section at the end, though you don’t need to familiarize yourself with all variables in order to work through these exercises.\n\nskim(hotels)\n\n\nData summary\n\n\nName\nhotels\n\n\nNumber of rows\n119390\n\n\nNumber of columns\n32\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n13\n\n\nDate\n1\n\n\nnumeric\n18\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nhotel\n0\n1\n10\n12\n0\n2\n0\n\n\narrival_date_month\n0\n1\n3\n9\n0\n12\n0\n\n\nmeal\n0\n1\n2\n9\n0\n5\n0\n\n\ncountry\n0\n1\n2\n4\n0\n178\n0\n\n\nmarket_segment\n0\n1\n6\n13\n0\n8\n0\n\n\ndistribution_channel\n0\n1\n3\n9\n0\n5\n0\n\n\nreserved_room_type\n0\n1\n1\n1\n0\n10\n0\n\n\nassigned_room_type\n0\n1\n1\n1\n0\n12\n0\n\n\ndeposit_type\n0\n1\n10\n10\n0\n3\n0\n\n\nagent\n0\n1\n1\n4\n0\n334\n0\n\n\ncompany\n0\n1\n1\n4\n0\n353\n0\n\n\ncustomer_type\n0\n1\n5\n15\n0\n4\n0\n\n\nreservation_status\n0\n1\n7\n9\n0\n3\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nreservation_status_date\n0\n1\n2014-10-17\n2017-09-14\n2016-08-07\n926\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nis_canceled\n0\n1\n0.37\n0.48\n0.00\n0.00\n0.00\n1\n1\n▇▁▁▁▅\n\n\nlead_time\n0\n1\n104.01\n106.86\n0.00\n18.00\n69.00\n160\n737\n▇▂▁▁▁\n\n\narrival_date_year\n0\n1\n2016.16\n0.71\n2015.00\n2016.00\n2016.00\n2017\n2017\n▃▁▇▁▆\n\n\narrival_date_week_number\n0\n1\n27.17\n13.61\n1.00\n16.00\n28.00\n38\n53\n▅▇▇▇▅\n\n\narrival_date_day_of_month\n0\n1\n15.80\n8.78\n1.00\n8.00\n16.00\n23\n31\n▇▇▇▇▆\n\n\nstays_in_weekend_nights\n0\n1\n0.93\n1.00\n0.00\n0.00\n1.00\n2\n19\n▇▁▁▁▁\n\n\nstays_in_week_nights\n0\n1\n2.50\n1.91\n0.00\n1.00\n2.00\n3\n50\n▇▁▁▁▁\n\n\nadults\n0\n1\n1.86\n0.58\n0.00\n2.00\n2.00\n2\n55\n▇▁▁▁▁\n\n\nchildren\n4\n1\n0.10\n0.40\n0.00\n0.00\n0.00\n0\n10\n▇▁▁▁▁\n\n\nbabies\n0\n1\n0.01\n0.10\n0.00\n0.00\n0.00\n0\n10\n▇▁▁▁▁\n\n\nis_repeated_guest\n0\n1\n0.03\n0.18\n0.00\n0.00\n0.00\n0\n1\n▇▁▁▁▁\n\n\nprevious_cancellations\n0\n1\n0.09\n0.84\n0.00\n0.00\n0.00\n0\n26\n▇▁▁▁▁\n\n\nprevious_bookings_not_canceled\n0\n1\n0.14\n1.50\n0.00\n0.00\n0.00\n0\n72\n▇▁▁▁▁\n\n\nbooking_changes\n0\n1\n0.22\n0.65\n0.00\n0.00\n0.00\n0\n21\n▇▁▁▁▁\n\n\ndays_in_waiting_list\n0\n1\n2.32\n17.59\n0.00\n0.00\n0.00\n0\n391\n▇▁▁▁▁\n\n\nadr\n0\n1\n101.83\n50.54\n-6.38\n69.29\n94.58\n126\n5400\n▇▁▁▁▁\n\n\nrequired_car_parking_spaces\n0\n1\n0.06\n0.25\n0.00\n0.00\n0.00\n0\n8\n▇▁▁▁▁\n\n\ntotal_of_special_requests\n0\n1\n0.57\n0.79\n0.00\n0.00\n0.00\n1\n5\n▇▁▁▁▁\n\n\n\n\n\n\n\nExercise 2.\nAre people traveling on a whim? Let’s see…\nFill in the blanks for filtering for hotel bookings where the guest is not from the US (country code \"USA\") and the lead_time is less than 1 day.\n\nhotels %&gt;%\n  filter(\n    country ____ \"USA\", \n    lead_time ____ ____\n    )\n\n\n\nExercise 3.\nHow many bookings involve at least 1 child or baby?\nIn the following chunk, replace\n\n[AT LEAST] with the logical operator for “at least” (in two places)\n[OR] with the logical operator for “or”\n\nNote: You will need to set eval=TRUE when you have an answer you want to try out in the qmd file.\n\nhotels %&gt;%\n  filter(\n    children [AT LEAST] 1 [OR] babies [AT LEAST] 1\n    )\n\n\n\nExercise 4.\nDo you think it’s more likely to find bookings with children or babies in city hotels or resort hotels? Test your intuition. Using filter() determine the number of bookings in resort hotels that have more than 1 child or baby in the room? Then, do the same for city hotels, and compare the numbers of rows in the resulting filtered data frames.\n\n# add code here\n# pay attention to correctness and code style\n\n\n# add code here\n# pay attention to correctness and code style\n\n\n\nExercise 5.\nCreate a frequency table of the number of adults in a booking. Display the results in descending order so the most common observation is on top. What is the most common number of adults in bookings in this dataset? Are there any surprising results?\n\n# add code here\n# pay attention to correctness and code style\n\n\n\nExercise 6.\nRepeat Exercise 5, once for canceled bookings (is_canceled coded as 1) and once for not canceled bookings (is_canceled coded as 0). What does this reveal about the surprising results you spotted in the previous exercise?\n\n# add code here\n# pay attention to correctness and code style\n\n\n\nExercise 7.\nCalculate minimum, mean, median, and maximum average daily rate (adr) grouped by hotel type so that you can get these statistics separately for resort and city hotels. Which type of hotel is higher, on average?\n\n# add code here\n# pay attention to correctness and code style\n\n\n\nExercise 8.\nWe observe two unusual values in the summary statistics above – a negative minimum, and a very high maximum). What types of hotels are these? Locate these observations in the dataset and find out the arrival date (year and month) as well as how many people (adults, children, and babies) stayed in the room. You can investigate the data in the viewer to locate these values, but preferably you should identify them in a reproducible way with some code.\nHint: For example, you can filter for the given adr amounts and select the relevant columns.\n\n# add code here\n# pay attention to correctness and code style"
  },
  {
    "objectID": "material/hotels-datawrangling.html#data-dictionary",
    "href": "material/hotels-datawrangling.html#data-dictionary",
    "title": "Hotel bookings - data wrangling",
    "section": "Data dictionary",
    "text": "Data dictionary\nBelow is the full data dictionary. Note that it is long (there are lots of variables in the data), but we will be using a limited set of the variables for our analysis.\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nhotel\ncharacter\nHotel (H1 = Resort Hotel or H2 = City Hotel)\n\n\nis_canceled\ndouble\nValue indicating if the booking was canceled (1) or not (0)\n\n\nlead_time\ndouble\nNumber of days that elapsed between the entering date of the booking into the PMS and the arrival date\n\n\narrival_date_year\ndouble\nYear of arrival date\n\n\narrival_date_month\ncharacter\nMonth of arrival date\n\n\narrival_date_week_number\ndouble\nWeek number of year for arrival date\n\n\narrival_date_day_of_month\ndouble\nDay of arrival date\n\n\nstays_in_weekend_nights\ndouble\nNumber of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel\n\n\nstays_in_week_nights\ndouble\nNumber of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel\n\n\nadults\ndouble\nNumber of adults\n\n\nchildren\ndouble\nNumber of children\n\n\nbabies\ndouble\nNumber of babies\n\n\nmeal\ncharacter\nType of meal booked. Categories are presented in standard hospitality meal packages:  Undefined/SC – no meal package;BB – Bed & Breakfast;  HB – Half board (breakfast and one other meal – usually dinner);  FB – Full board (breakfast, lunch and dinner)\n\n\ncountry\ncharacter\nCountry of origin. Categories are represented in the ISO 3155–3:2013 format\n\n\nmarket_segment\ncharacter\nMarket segment designation. In categories, the term “TA” means “Travel Agents” and “TO” means “Tour Operators”\n\n\ndistribution_channel\ncharacter\nBooking distribution channel. The term “TA” means “Travel Agents” and “TO” means “Tour Operators”\n\n\nis_repeated_guest\ndouble\nValue indicating if the booking name was from a repeated guest (1) or not (0)\n\n\nprevious_cancellations\ndouble\nNumber of previous bookings that were cancelled by the customer prior to the current booking\n\n\nprevious_bookings_not_canceled\ndouble\nNumber of previous bookings not cancelled by the customer prior to the current booking\n\n\nreserved_room_type\ncharacter\nCode of room type reserved. Code is presented instead of designation for anonymity reasons\n\n\nassigned_room_type\ncharacter\nCode for the type of room assigned to the booking. Sometimes the assigned room type differs from the reserved room type due to hotel operation reasons (e.g. overbooking) or by customer request. Code is presented instead of designation for anonymity reasons\n\n\nbooking_changes\ndouble\nNumber of changes/amendments made to the booking from the moment the booking was entered on the PMS until the moment of check-in or cancellation\n\n\ndeposit_type\ncharacter\nIndication on if the customer made a deposit to guarantee the booking. This variable can assume three categories:No Deposit – no deposit was made;Non Refund – a deposit was made in the value of the total stay cost;Refundable – a deposit was made with a value under the total cost of stay.\n\n\nagent\ncharacter\nID of the travel agency that made the booking\n\n\ncompany\ncharacter\nID of the company/entity that made the booking or responsible for paying the booking. ID is presented instead of designation for anonymity reasons\n\n\ndays_in_waiting_list\ndouble\nNumber of days the booking was in the waiting list before it was confirmed to the customer\n\n\ncustomer_type\ncharacter\nType of booking, assuming one of four categories:Contract - when the booking has an allotment or other type of contract associated to it;Group – when the booking is associated to a group;Transient – when the booking is not part of a group or contract, and is not associated to other transient booking;Transient-party – when the booking is transient, but is associated to at least other transient booking\n\n\nadr\ndouble\nAverage Daily Rate as defined by dividing the sum of all lodging transactions by the total number of staying nights\n\n\nrequired_car_parking_spaces\ndouble\nNumber of car parking spaces required by the customer\n\n\ntotal_of_special_requests\ndouble\nNumber of special requests made by the customer (e.g. twin bed or high floor)\n\n\nreservation_status\ncharacter\nReservation last status, assuming one of three categories:Canceled – booking was canceled by the customer;Check-Out – customer has checked in but already departed;No-Show – customer did not check-in and did inform the hotel of the reason why\n\n\nreservation_status_date\ndouble\nDate at which the last status was set. This variable can be used in conjunction with the ReservationStatus to understand when was the booking canceled or when did the customer checked-out of the hotel"
  },
  {
    "objectID": "material/nobel-laureates.html",
    "href": "material/nobel-laureates.html",
    "title": "Nobel Laureates",
    "section": "",
    "text": "In January 2017, Buzzfeed published an article on why Nobel laureates show immigration is so important for American science. You can read the article here. In the article they show that while most living Nobel laureates in the sciences are based in the US, many of them were born in other countries. This is one reason why scientific leaders say that immigration is vital for progress. In this lab we will work with the data from this article to recreate some of their visualizations as well as explore new questions."
  },
  {
    "objectID": "material/nobel-laureates.html#packages",
    "href": "material/nobel-laureates.html#packages",
    "title": "Nobel Laureates",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for much of the data wrangling. This package is already installed for you. You can load them by running the following in your Console:\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "material/nobel-laureates.html#data",
    "href": "material/nobel-laureates.html#data",
    "title": "Nobel Laureates",
    "section": "Data",
    "text": "Data\nThe dataset for this assignment can be found as a CSV (comma separated values) file. You can read it in using the following.\n\nnobel &lt;- read_csv(\"data04/nobel.csv\")\n\nThe variable descriptions are as follows:\n\nid: ID number\nfirstname: First name of laureate\nsurname: Surname\nyear: Year prize won\ncategory: Category of prize\naffiliation: Affiliation of laureate\ncity: City of laureate in prize year\ncountry: Country of laureate in prize year\nborn_date: Birth date of laureate\ndied_date: Death date of laureate\ngender: Gender of laureate\nborn_city: City where laureate was born\nborn_country: Country where laureate was born\nborn_country_code: Code of country where laureate was born\ndied_city: City where laureate died\ndied_country: Country where laureate died\ndied_country_code: Code of country where laureate died\noverall_motivation: Overall motivation for recognition\nshare: Number of other winners award is shared with\nmotivation: Motivation for recognition\n\nIn a few cases the name of the city/country changed after laureate was given (e.g. in 1975 Bosnia and Herzegovina was called the Socialist Federative Republic of Yugoslavia). In these cases the variables below reflect a different name than their counterparts without the suffix `_original`.\n\nborn_country_original: Original country where laureate was born\nborn_city_original: Original city where laureate was born\ndied_country_original: Original country where laureate died\ndied_city_original: Original city where laureate died\ncity_original: Original city where laureate lived at the time of winning the award\ncountry_original: Original country where laureate lived at the time of winning the award"
  },
  {
    "objectID": "material/nobel-laureates.html#get-to-know-your-data",
    "href": "material/nobel-laureates.html#get-to-know-your-data",
    "title": "Nobel Laureates",
    "section": "Get to know your data",
    "text": "Get to know your data\n\nHow many observations and how many variables are in the dataset? Use inline code to answer this question. What does each row represent?\n\nThere are some observations in this dataset that we will exclude from our analysis to match the Buzzfeed results.\n\nCreate a new data frame called nobel_living that filters for\n\n\nlaureates for whom country is available\nlaureates who are people as opposed to organizations (organizations are denoted with \"org\" as their gender)\nlaureates who are still alive (their died_date is NA)\n\nConfirm that once you have filtered for these characteristics you are left with a data frame with 228 observations, once again using inline code."
  },
  {
    "objectID": "material/nobel-laureates.html#most-living-nobel-laureates-were-based-in-the-us-when-they-won-their-prizes",
    "href": "material/nobel-laureates.html#most-living-nobel-laureates-were-based-in-the-us-when-they-won-their-prizes",
    "title": "Nobel Laureates",
    "section": "Most living Nobel laureates were based in the US when they won their prizes",
    "text": "Most living Nobel laureates were based in the US when they won their prizes\n… says the Buzzfeed article. Let’s see if that’s true.\nFirst, we’ll create a new variable to identify whether the laureate was in the US when they won their prize. We’ll use the mutate() function for this. The following pipeline mutates the nobel_living data frame by adding a new variable called country_us. We use an if statement to create this variable. The first argument in the if_else() function we’re using to write this if statement is the condition we’re testing for. If country is equal to \"USA\", we set country_us to \"USA\". If not, we set the country_us to \"Other\".\nNote: we can achieve the same result using the fct_other() function we’ve seen before (i.e. with country_us = fct_other(country, \"USA\")). We decided to use the if_else() here to show you one example of an if statement in R.\n\nnobel_living &lt;- nobel_living %&gt;%\n  mutate(\n    country_us = if_else(country == \"USA\", \"USA\", \"Other\")\n  )\n\nNext, we will limit our analysis to only the following categories: Physics, Medicine, Chemistry, and Economics.\n\nnobel_living_science &lt;- nobel_living %&gt;%\n  filter(category %in% c(\"Physics\", \"Medicine\", \"Chemistry\", \"Economics\"))\n\nFor the next exercise work with the nobel_living_science data frame you created above.\n\nCreate a faceted bar plot visualizing the relationship between the category of prize and whether the laureate was in the US when they won the nobel prize. Interpret your visualization, and say a few words about whether the Buzzfeed headline is supported by the data.\n\nYour visualization should be faceted by category.\nFor each facet you should have two bars, one for winners in the US and one for Other.\nFlip the coordinates so the bars are horizontal, not vertical."
  },
  {
    "objectID": "material/nobel-laureates.html#but-of-those-us-based-nobel-laureates-many-were-born-in-other-countries",
    "href": "material/nobel-laureates.html#but-of-those-us-based-nobel-laureates-many-were-born-in-other-countries",
    "title": "Nobel Laureates",
    "section": "But of those US-based Nobel laureates, many were born in other countries",
    "text": "But of those US-based Nobel laureates, many were born in other countries\nHint: You should be able to cheat borrow from code you used earlier to create the country_us variable.\n\nCreate a new variable called born_country_us that has the value \"USA\" if the laureate is born in the US, and \"Other\" otherwise. How many of the winners are born in the US?\nAdd a second variable to your visualization from Exercise 3 based on whether the laureate was born in the US or not. Based on your visualization, do the data appear to support Buzzfeed’s claim? Explain your reasoning in 1-2 sentences.\n\nYour final visualization should contain a facet for each category.\nWithin each facet, there should be a bar for whether the laureate won the award in the US or not.\nEach bar should have segments for whether the laureate was born in the US or not."
  },
  {
    "objectID": "material/nobel-laureates.html#heres-where-those-immigrant-nobelists-were-born",
    "href": "material/nobel-laureates.html#heres-where-those-immigrant-nobelists-were-born",
    "title": "Nobel Laureates",
    "section": "Here’s where those immigrant Nobelists were born",
    "text": "Here’s where those immigrant Nobelists were born\nNote: your bar plot won’t exactly match the one from the Buzzfeed article. This is likely because the data has been updated since the article was published.\n\nIn a single pipeline, filter for laureates who won their prize in the US, but were born outside of the US, and then create a frequency table (with the count() function) for their birth country (born_country) and arrange the resulting data frame in descending order of number of observations for each country. Which country is the most common?"
  },
  {
    "objectID": "material/la-quinta.html",
    "href": "material/la-quinta.html",
    "title": "La Quinta is Spanish for next to Denny’s",
    "section": "",
    "text": "Have you ever taken a road trip in the US and thought to yourself “I wonder what La Quinta means”. Well, the late comedian Mitch Hedberg thinks it’s Spanish for next to Denny’s.\nIf you’re not familiar with these two establishments, Denny’s is a casual diner chain that is open 24 hours and La Quinta Inn and Suites is a hotel chain.\nThese two establishments tend to be clustered together, or at least this observation is a joke made famous by Mitch Hedberg. In this lab we explore the validity of this joke and along the way learn some more data wrangling and tips for visualizing spatial data.\nThe inspiration for this comes from a blog post by John Reiser on his new jersey geographer blog. You can read that analysis here. Reiser’s blog post focuses on scraping data from Denny’s and La Quinta Inn and Suites websites using Python. Here, we focus on visualization and analysis of these data."
  },
  {
    "objectID": "material/la-quinta.html#packages",
    "href": "material/la-quinta.html#packages",
    "title": "La Quinta is Spanish for next to Denny’s",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for much of the data wrangling and visualisation and the data lives in the dsbox package. These packages are already installed for you. You can load them by running the following in your Console:\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "material/la-quinta.html#data",
    "href": "material/la-quinta.html#data",
    "title": "La Quinta is Spanish for next to Denny’s",
    "section": "Data",
    "text": "Data\nThe data sets we’ll use are called dennys and laquinta and are available for download. Note that these data were scraped from here and here, respectively. You can find information about the data sets here and here. To help with our analysis we will also use a data set on US states.\n\nlaquinta &lt;- read_csv(\"data/laquinta.csv\")\ndennys &lt;- read_csv(\"data/dennys.csv\")\nstates &lt;- read_csv(\"data/states.csv\")\n\nEach observation in the states dataset represents a state, including DC. Along with the name of the state we have the two-letter abbreviation and we have the geographic area of the state (in square miles)."
  },
  {
    "objectID": "material/college-majors.html",
    "href": "material/college-majors.html",
    "title": "What should I major in?",
    "section": "",
    "text": "The first step in the process of turning information into knowledge process is to summarize and describe the raw information - the data. In this assignment we explore data on college majors and earnings, specifically the data begin the FiveThirtyEight story “The Economic Guide To Picking A College Major”.\nThese data originally come from the American Community Survey (ACS) 2010-2012 Public Use Microdata Series. While this is outside the scope of this assignment, if you are curious about how raw data from the ACS were cleaned and prepared, see the code FiveThirtyEight authors used.\nWe should also note that there are many considerations that go into picking a major. Earnings potential and employment prospects are two of them, and they are important, but they don’t tell the whole story. Keep this in mind as you analyze the data."
  },
  {
    "objectID": "material/college-majors.html#packages",
    "href": "material/college-majors.html#packages",
    "title": "What should I major in?",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for much of the data wrangling and visualisation, the scales package for better formatting of labels on visualisations, and the data lives in the fivethirtyeight package. These packages are already installed for you. You can load them by running the following in your Console:\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(fivethirtyeight)"
  },
  {
    "objectID": "material/college-majors.html#data",
    "href": "material/college-majors.html#data",
    "title": "What should I major in?",
    "section": "Data",
    "text": "Data\nThe data can be found in the fivethirtyeight package, and it’s called college_recent_grads. Since the dataset is distributed with the package, we don’t need to load it separately; it becomes available to us when we load the package. You can find out more about the dataset by inspecting its documentation, which you can access by running ?college_recent_grads in the Console or using the Help menu in RStudio to search for college_recent_grads. You can also find this information here.\nYou can also take a quick peek at your data frame and view its dimensions with the glimpse function.\n\nglimpse(college_recent_grads)\n\nRows: 173\nColumns: 21\n$ rank                        &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,…\n$ major_code                  &lt;int&gt; 2419, 2416, 2415, 2417, 2405, 2418, 6202, …\n$ major                       &lt;chr&gt; \"Petroleum Engineering\", \"Mining And Miner…\n$ major_category              &lt;chr&gt; \"Engineering\", \"Engineering\", \"Engineering…\n$ total                       &lt;int&gt; 2339, 756, 856, 1258, 32260, 2573, 3777, 1…\n$ sample_size                 &lt;int&gt; 36, 7, 3, 16, 289, 17, 51, 10, 1029, 631, …\n$ men                         &lt;int&gt; 2057, 679, 725, 1123, 21239, 2200, 2110, 8…\n$ women                       &lt;int&gt; 282, 77, 131, 135, 11021, 373, 1667, 960, …\n$ sharewomen                  &lt;dbl&gt; 0.1205643, 0.1018519, 0.1530374, 0.1073132…\n$ employed                    &lt;int&gt; 1976, 640, 648, 758, 25694, 1857, 2912, 15…\n$ employed_fulltime           &lt;int&gt; 1849, 556, 558, 1069, 23170, 2038, 2924, 1…\n$ employed_parttime           &lt;int&gt; 270, 170, 133, 150, 5180, 264, 296, 553, 1…\n$ employed_fulltime_yearround &lt;int&gt; 1207, 388, 340, 692, 16697, 1449, 2482, 82…\n$ unemployed                  &lt;int&gt; 37, 85, 16, 40, 1672, 400, 308, 33, 4650, …\n$ unemployment_rate           &lt;dbl&gt; 0.018380527, 0.117241379, 0.024096386, 0.0…\n$ p25th                       &lt;dbl&gt; 95000, 55000, 50000, 43000, 50000, 50000, …\n$ median                      &lt;dbl&gt; 110000, 75000, 73000, 70000, 65000, 65000,…\n$ p75th                       &lt;dbl&gt; 125000, 90000, 105000, 80000, 75000, 10200…\n$ college_jobs                &lt;int&gt; 1534, 350, 456, 529, 18314, 1142, 1768, 97…\n$ non_college_jobs            &lt;int&gt; 364, 257, 176, 102, 4440, 657, 314, 500, 1…\n$ low_wage_jobs               &lt;int&gt; 193, 50, 0, 0, 972, 244, 259, 220, 3253, 3…\n\n\nThe college_recent_grads data frame is a trove of information. Let’s think about some questions we might want to answer with these data:\n\nWhich major has the lowest unemployment rate?\nWhich major has the highest percentage of women?\nHow do the distributions of median income compare across major categories?\nDo women tend to choose majors with lower or higher earnings?\n\nIn the next section we aim to answer these questions."
  },
  {
    "objectID": "material/college-majors.html#which-major-has-the-lowest-unemployment-rate",
    "href": "material/college-majors.html#which-major-has-the-lowest-unemployment-rate",
    "title": "What should I major in?",
    "section": "Which major has the lowest unemployment rate?",
    "text": "Which major has the lowest unemployment rate?\nIn order to answer this question all we need to do is sort the data. We use the arrange function to do this, and sort it by the unemployment_rate variable. By default arrange sorts in ascending order, which is what we want here – we’re interested in the major with the lowest unemployment rate.\n\ncollege_recent_grads %&gt;%\n  arrange(unemployment_rate)\n\n# A tibble: 173 × 21\n    rank major_code major           major_category total sample_size   men women\n   &lt;int&gt;      &lt;int&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;int&gt;       &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1    53       4005 Mathematics An… Computers & M…   609           7   500   109\n 2    74       3801 Military Techn… Industrial Ar…   124           4   124     0\n 3    84       3602 Botany          Biology & Lif…  1329           9   626   703\n 4   113       1106 Soil Science    Agriculture &…   685           4   476   209\n 5   121       2301 Educational Ad… Education        804           5   280   524\n 6    15       2409 Engineering Me… Engineering     4321          30  3526   795\n 7    20       3201 Court Reporting Law & Public …  1148          14   877   271\n 8   120       2305 Mathematics Te… Education      14237         123  3872 10365\n 9     1       2419 Petroleum Engi… Engineering     2339          36  2057   282\n10    65       1100 General Agricu… Agriculture &… 10399         158  6053  4346\n# ℹ 163 more rows\n# ℹ 13 more variables: sharewomen &lt;dbl&gt;, employed &lt;int&gt;,\n#   employed_fulltime &lt;int&gt;, employed_parttime &lt;int&gt;,\n#   employed_fulltime_yearround &lt;int&gt;, unemployed &lt;int&gt;,\n#   unemployment_rate &lt;dbl&gt;, p25th &lt;dbl&gt;, median &lt;dbl&gt;, p75th &lt;dbl&gt;,\n#   college_jobs &lt;int&gt;, non_college_jobs &lt;int&gt;, low_wage_jobs &lt;int&gt;\n\n\nThis gives us what we wanted, but not in an ideal form. First, the name of the major barely fits on the page. Second, some of the variables are not that useful (e.g. major_code, major_category) and some we might want front and center are not easily viewed (e.g. unemployment_rate).\nWe can use the select function to choose which variables to display, and in which order:\n\ncollege_recent_grads %&gt;%\n  arrange(unemployment_rate) %&gt;%\n  select(rank, major, unemployment_rate)\n\n# A tibble: 173 × 3\n    rank major                                      unemployment_rate\n   &lt;int&gt; &lt;chr&gt;                                                  &lt;dbl&gt;\n 1    53 Mathematics And Computer Science                     0      \n 2    74 Military Technologies                                0      \n 3    84 Botany                                               0      \n 4   113 Soil Science                                         0      \n 5   121 Educational Administration And Supervision           0      \n 6    15 Engineering Mechanics Physics And Science            0.00633\n 7    20 Court Reporting                                      0.0117 \n 8   120 Mathematics Teacher Education                        0.0162 \n 9     1 Petroleum Engineering                                0.0184 \n10    65 General Agriculture                                  0.0196 \n# ℹ 163 more rows\n\n\nOk, this is looking better, but do we really need to display all those decimal places in the unemployment variable? Not really!\nWe can use the percent() function to clean up the display a bit.\n\ncollege_recent_grads %&gt;%\n  arrange(unemployment_rate) %&gt;%\n  select(rank, major, unemployment_rate) %&gt;%\n  mutate(unemployment_rate = percent(unemployment_rate))\n\n# A tibble: 173 × 3\n    rank major                                      unemployment_rate\n   &lt;int&gt; &lt;chr&gt;                                      &lt;chr&gt;            \n 1    53 Mathematics And Computer Science           0.00000%         \n 2    74 Military Technologies                      0.00000%         \n 3    84 Botany                                     0.00000%         \n 4   113 Soil Science                               0.00000%         \n 5   121 Educational Administration And Supervision 0.00000%         \n 6    15 Engineering Mechanics Physics And Science  0.63343%         \n 7    20 Court Reporting                            1.16897%         \n 8   120 Mathematics Teacher Education              1.62028%         \n 9     1 Petroleum Engineering                      1.83805%         \n10    65 General Agriculture                        1.96425%         \n# ℹ 163 more rows"
  },
  {
    "objectID": "material/college-majors.html#which-major-has-the-highest-percentage-of-women",
    "href": "material/college-majors.html#which-major-has-the-highest-percentage-of-women",
    "title": "What should I major in?",
    "section": "Which major has the highest percentage of women?",
    "text": "Which major has the highest percentage of women?\nTo answer such a question we need to arrange the data in descending order. For example, if earlier we were interested in the major with the highest unemployment rate, we would use the following:\n\nThe `desc` function specifies that we want `unemployment_rate` in descending order.\n\n\ncollege_recent_grads %&gt;%\n  arrange(desc(unemployment_rate)) %&gt;%\n  select(rank, major, unemployment_rate)\n\n# A tibble: 173 × 3\n    rank major                                      unemployment_rate\n   &lt;int&gt; &lt;chr&gt;                                                  &lt;dbl&gt;\n 1     6 Nuclear Engineering                                    0.177\n 2    90 Public Administration                                  0.159\n 3    85 Computer Networking And Telecommunications             0.152\n 4   171 Clinical Psychology                                    0.149\n 5    30 Public Policy                                          0.128\n 6   106 Communication Technologies                             0.120\n 7     2 Mining And Mineral Engineering                         0.117\n 8    54 Computer Programming And Data Processing               0.114\n 9    80 Geography                                              0.113\n10    59 Architecture                                           0.113\n# ℹ 163 more rows\n\n\n\nUsing what you’ve learned so far, arrange the data in descending order with respect to proportion of women in a major, and display only the major, the total number of people with major, and proportion of women. Show only the top 3 majors by adding top_n(3) at the end of the pipeline."
  },
  {
    "objectID": "material/college-majors.html#how-do-the-distributions-of-median-income-compare-across-major-categories",
    "href": "material/college-majors.html#how-do-the-distributions-of-median-income-compare-across-major-categories",
    "title": "What should I major in?",
    "section": "How do the distributions of median income compare across major categories?",
    "text": "How do the distributions of median income compare across major categories?\nNote: A percentile is a measure used in statistics indicating the value below which a given percentage of observations in a group of observations fall. For example, the 20th percentile is the value below which 20% of the observations may be found. (Source: Wikipedia\nThere are three types of incomes reported in this data frame: p25th, median, and p75th. These correspond to the 25th, 50th, and 75th percentiles of the income distribution of sampled individuals for a given major.\n\nWhy do we often choose the median, rather than the mean, to describe the typical income of a group of people?\n\nThe question we want to answer “How do the distributions of median income compare across major categories?”. We need to do a few things to answer this question: First, we need to group the data by major_category. Then, we need a way to summarize the distributions of median income within these groups. This decision will depend on the shapes of these distributions. So first, we need to visualize the data.\nWe use the ggplot() function to do this. The first argument is the data frame, and the next argument gives the mapping of the variables of the data to the aesthetic elements of the plot.\nLet’s start simple and take a look at the distribution of all median incomes, without considering the major categories.\n\nggplot(data = college_recent_grads, mapping = aes(x = median)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\nAlong with the plot, we get a message:\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nThis is telling us that we might want to reconsider the binwidth we chose for our histogram – or more accurately, the binwidth we didn’t specify. It’s good practice to always think in the context of the data and try out a few binwidths before settling on a binwidth. You might ask yourself: “What would be a meaningful difference in median incomes?” $1 is obviously too little, $10000 might be too high.\n\nTry binwidths of $1000 and $5000 and choose one. Explain your reasoning for your choice. Note that the binwidth is an argument for the geom_histogram function. So to specify a binwidth of $1000, you would use geom_histogram(binwidth = 1000).\n\nWe can also calculate summary statistics for this distribution using the summarise function:\n\ncollege_recent_grads %&gt;%\n  summarise(min = min(median), max = max(median),\n            mean = mean(median), med = median(median),\n            sd = sd(median), \n            q1 = quantile(median, probs = 0.25),\n            q3 = quantile(median, probs = 0.75))\n\n# A tibble: 1 × 7\n    min    max   mean   med     sd    q1    q3\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 22000 110000 40151. 36000 11470. 33000 45000\n\n\n\nBased on the shape of the histogram you created in the previous exercise, determine which of these summary statistics is useful for describing the distribution. Write up your description (remember shape, center, spread, any unusual observations) and include the summary statistic output as well.\nPlot the distribution of median income using a histogram, faceted by major_category. Use the binwidth you chose in the earlier exercise.\n\nNow that we’ve seen the shapes of the distributions of median incomes for each major category, we should have a better idea for which summary statistic to use to quantify the typical median income.\n\nWhich major category has the highest typical (you’ll need to decide what this means) median income? Use the partial code below, filling it in with the appropriate statistic and function. Also note that we are looking for the highest statistic, so make sure to arrange in the correct direction.\n\n\ncollege_recent_grads %&gt;%\n  group_by(major_category) %&gt;%\n  summarise(___ = ___(median)) %&gt;%\n  arrange(___)\n\n\nWhich major category is the least popular in this sample? To answer this question we use a new function called count, which first groups the data and then counts the number of observations in each category (see below). Add to the pipeline appropriately to arrange the results so that the major with the lowest observations is on top.\n\n\ncollege_recent_grads %&gt;%\n  count(major_category)\n\n# A tibble: 16 × 2\n   major_category                          n\n   &lt;chr&gt;                               &lt;int&gt;\n 1 Agriculture & Natural Resources        10\n 2 Arts                                    8\n 3 Biology & Life Science                 14\n 4 Business                               13\n 5 Communications & Journalism             4\n 6 Computers & Mathematics                11\n 7 Education                              16\n 8 Engineering                            29\n 9 Health                                 12\n10 Humanities & Liberal Arts              15\n11 Industrial Arts & Consumer Services     7\n12 Interdisciplinary                       1\n13 Law & Public Policy                     5\n14 Physical Sciences                      10\n15 Psychology & Social Work                9\n16 Social Science                          9"
  },
  {
    "objectID": "material/college-majors.html#all-stem-fields-arent-the-same",
    "href": "material/college-majors.html#all-stem-fields-arent-the-same",
    "title": "What should I major in?",
    "section": "All STEM fields aren’t the same",
    "text": "All STEM fields aren’t the same\nOne of the sections of the FiveThirtyEight story is “All STEM fields aren’t the same”. Let’s see if this is true.\nFirst, let’s create a new vector called stem_categories that lists the major categories that are considered STEM fields.\n\nstem_categories &lt;- c(\"Biology & Life Science\",\n                     \"Computers & Mathematics\",\n                     \"Engineering\",\n                     \"Physical Sciences\")\n\nThen, we can use this to create a new variable in our data frame indicating whether a major is STEM or not.\n\ncollege_recent_grads &lt;- college_recent_grads %&gt;%\n  mutate(major_type = ifelse(major_category %in% stem_categories, \"stem\", \"not stem\"))\n\nLet’s unpack this: with mutate we create a new variable called major_type, which is defined as \"stem\" if the major_category is in the vector called stem_categories we created earlier, and as \"not stem\" otherwise.\n%in% is a logical operator. Other logical operators that are commonly used are\n\n\n\nOperator\nOperation\n\n\n\n\nx &lt; y\nless than\n\n\nx &gt; y\ngreater than\n\n\nx &lt;= y\nless than or equal to\n\n\nx &gt;= y\ngreater than or equal to\n\n\nx != y\nnot equal to\n\n\nx == y\nequal to\n\n\nx %in% y\ncontains\n\n\nx | y\nor\n\n\nx & y\nand\n\n\n!x\nnot\n\n\n\nWe can use the logical operators to also filter our data for STEM majors whose median earnings is less than median for all majors’ median earnings, which we found to be $36,000 earlier.\n\ncollege_recent_grads %&gt;%\n  filter(\n    major_type == \"stem\",\n    median &lt; 36000\n  )\n\n# A tibble: 10 × 22\n    rank major_code major        major_category  total sample_size    men  women\n   &lt;int&gt;      &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;           &lt;int&gt;       &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n 1    93       1301 Environment… Biology & Lif…  25965         225  10787  15178\n 2    98       5098 Multi-Disci… Physical Scie…  62052         427  27015  35037\n 3   102       3608 Physiology   Biology & Lif…  22060          99   8422  13638\n 4   106       2001 Communicati… Computers & M…  18035         208  11431   6604\n 5   109       3611 Neuroscience Biology & Lif…  13663          53   4944   8719\n 6   111       5002 Atmospheric… Physical Scie…   4043          32   2744   1299\n 7   123       3699 Miscellaneo… Biology & Lif…  10706          63   4747   5959\n 8   124       3600 Biology      Biology & Lif… 280709        1370 111762 168947\n 9   133       3604 Ecology      Biology & Lif…   9154          86   3878   5276\n10   169       3609 Zoology      Biology & Lif…   8409          47   3050   5359\n# ℹ 14 more variables: sharewomen &lt;dbl&gt;, employed &lt;int&gt;,\n#   employed_fulltime &lt;int&gt;, employed_parttime &lt;int&gt;,\n#   employed_fulltime_yearround &lt;int&gt;, unemployed &lt;int&gt;,\n#   unemployment_rate &lt;dbl&gt;, p25th &lt;dbl&gt;, median &lt;dbl&gt;, p75th &lt;dbl&gt;,\n#   college_jobs &lt;int&gt;, non_college_jobs &lt;int&gt;, low_wage_jobs &lt;int&gt;,\n#   major_type &lt;chr&gt;\n\n\n\nWhich STEM majors have median salaries equal to or less than the median for all majors’ median earnings? Your output should only show the major name and median, 25th percentile, and 75th percentile earning for that major as and should be sorted such that the major with the highest median earning is on top."
  },
  {
    "objectID": "material/college-majors.html#what-types-of-majors-do-women-tend-to-major-in",
    "href": "material/college-majors.html#what-types-of-majors-do-women-tend-to-major-in",
    "title": "What should I major in?",
    "section": "What types of majors do women tend to major in?",
    "text": "What types of majors do women tend to major in?\n\nCreate a scatterplot of median income vs. proportion of women in that major, coloured by whether the major is in a STEM field or not. Describe the association between these three variables."
  },
  {
    "objectID": "material/college-majors.html#further-exploration",
    "href": "material/college-majors.html#further-exploration",
    "title": "What should I major in?",
    "section": "Further exploration",
    "text": "Further exploration\n\nAsk a question of interest to you, and answer it using summary statistic(s) and/or visualization(s)."
  },
  {
    "objectID": "material/legos.html",
    "href": "material/legos.html",
    "title": "Legos",
    "section": "",
    "text": "Here, we work with (simulated) data from Lego sales in 2018 for a sample of customers who bought Legos in the US."
  },
  {
    "objectID": "material/legos.html#data-and-packages",
    "href": "material/legos.html#data-and-packages",
    "title": "Legos",
    "section": "Data and Packages",
    "text": "Data and Packages\nWe’ll use the tidyverse package for much of the data wrangling and visualisation and the data is given to import.\n\nlibrary(tidyverse)\n\nThe following variables are available in the data set:\n\nfirst_name: First name of customer\nlast_name: Last name of customer\nage: Age of customer\nphone_number: Phone number of customer\nset_id: Set ID of lego set purchased\nnumber: Item number of lego set purchased\ntheme: Theme of lego set purchased\nsubtheme: Sub theme of lego set purchased\nyear: Year of purchase\nname: Name of lego set purchased\npieces: Number of pieces of legos in set purchased\nus_price: Price of set purchase in US Dollars\nimage_url: Image URL of lego set purchased\nquantity: Quantity of lego set(s) purchased"
  },
  {
    "objectID": "material/type-coercion.html",
    "href": "material/type-coercion.html",
    "title": "Type coercion",
    "section": "",
    "text": "c(1, 1L, \"C\")\n\n\nc(1, 1L, \"C\")\n\n[1] \"1\" \"1\" \"C\"\n\n\n\n1\n\n[1] 1\n\n1L\n\n[1] 1\n\n\"C\"\n\n[1] \"C\"\n\n\n\n#typeof(c(1, 1L, \"C\"))\n\n\nc(1L / 0, \"A\")\n\n\nc(1L / 0, \"A\")\n\n[1] \"Inf\" \"A\"  \n\n\n\ntypeof(1L)\n\n[1] \"integer\"\n\ntypeof(0)\n\n[1] \"double\"\n\ntypeof(1L/0)\n\n[1] \"double\"\n\ntypeof(\"A\")\n\n[1] \"character\"\n\n\n\n#typeof(c(1L / 0, \"A\"))\n\n\nc(1:3, 5)\n\n\nc(1:3, 5)\n\n[1] 1 2 3 5\n\n\n\ntypeof(1:3)\n\n[1] \"integer\"\n\ntypeof(5)\n\n[1] \"double\"\n\n\n\n#typeof(c(1:3, 5))\n\n\nc(3, \"3+\")\n\n\nc(3, \"3+\")\n\n[1] \"3\"  \"3+\"\n\n\n\ntypeof(3)\n\n[1] \"double\"\n\ntypeof(\"3+\")\n\n[1] \"character\"\n\n\n\n#typeof(c(3, \"3+\"))\n\n\nc(NA, TRUE)\n\n\nc(NA, TRUE)\n\n[1]   NA TRUE\n\n\n\ntypeof(NA)\n\n[1] \"logical\"\n\ntypeof(TRUE)\n\n[1] \"logical\"\n\n\n\n#typeof(c(NA, TRUE))"
  },
  {
    "objectID": "material/type-coercion.html#exercise",
    "href": "material/type-coercion.html#exercise",
    "title": "Type coercion",
    "section": "Exercise",
    "text": "Exercise\nDouble check that you do not have stored objects in your current session with the following command. This will list all objects that you have in your current R session.\n\nls()\n\ncharacter(0)\n\n\nIn case you have objects that you want to remove from the current session you can do so with the rm() function.  This command will remove all objects available in your current environment.\n\nrm(list = ls())\n\nThis command uses commands that we have not talked about yet. If you do not understand how it works now, you will do so after tomorrows lectures and exercises.\n\nCreate variables var1 and var2 and initialize them with two integers of choice.\nAdd the two variables and save them as a new variable named var3 and print the result.\nCheck the class, mode, and type for var1, var2, var3 and π (is found under the variable name pi in R)\n\n\nmode(pi)\n\n[1] \"numeric\"\n\nclass(pi)\n\n[1] \"numeric\"\n\ntypeof(pi)\n\n[1] \"double\"\n\n\n\nCreate two character variables containing a text of choice. Check the mode, class, and type of the first one.\n\n\ntext1 &lt;- \"test1\"\ntext2 &lt;- \"test2\"\n\nmode(text1)\n\n[1] \"character\"\n\nclass(text1)\n\n[1] \"character\"\n\ntypeof(text1)\n\n[1] \"character\"\n\n\nAdd var1 to it. What is the result and why?\n\ntext1+var1\n\nError in text1 + var1: non-numeric argument to binary operator\n\n\n\nConvert var3 to an integer, cast an integer variable to double, cast a string to a double.\n\n\nas.integer(var3)\n\n[1] 45\n\ni &lt;- 175\nas.double(i)\n\n[1] 175\n\nas.double(text1)\n\n[1] NA\n\n\n\nReport floor and ceiling of π and round π to 3 decimal places.\n\n\nfloor(pi)\n\n[1] 3\n\nceiling(pi)\n\n[1] 4\n\nround(pi, digits=3)\n\n[1] 3.142\n\n\n\nIs floor of π an integer?\n\n\nis.integer(floor(pi))\n\n[1] FALSE\n\n\n\nTreat \"3.56437\" string as number.\n\n\nas.numeric('3.56437')\n\n[1] 3.56437\n\n\n\nDivide ∞ by - ∞\n\n\nInf/-Inf\n\n[1] NaN\n\n\n\nCreate two freely chosen complex numbers.\n\n\nCheck that they are complex indeed.\nAdd, multiply and divide one by another.\nAdd an integer to their sum.\n\n\nc1 &lt;- 23 + 4i\nc2 &lt;- -15 - 7i\nis.complex(c1)\n\n[1] TRUE\n\nis.complex(c2)\n\n[1] TRUE\n\nc1 + c2\n\n[1] 8-3i\n\nc1 / c2\n\n[1] -1.361314+0.368613i\n\nc1 + c2 + 7\n\n[1] 15-3i\n\n\n\nPrint a truth table for OR (for three distinct logical values). Read about truth tables here.\n\n\nx &lt;- c(NA, FALSE, TRUE)\nnames(x) &lt;- as.character(x)\nouter(x, x, \"|\")\n\n      &lt;NA&gt; FALSE TRUE\n&lt;NA&gt;    NA    NA TRUE\nFALSE   NA FALSE TRUE\nTRUE  TRUE  TRUE TRUE\n\n\n\nMultiply a logical TRUE by a logical FALSE. Rise the logical true to the 7-th power.\n\n\nTRUE * FALSE\n\n[1] 0\n\nT^7\n\n[1] 1\n\n\n\nCreate two character variables containing two verses of your favorite song.\n\n\nConcatenate the two variables,\n\nPaste the variables with ‘*’ as separator.\n\nFind if ‘and’ occurs in the second line,\n\nSubstitute a word for another,\n\nExtract substring starting at the 5th character and 5 characters long.\n\n\nline1 &lt;- \"Hello darkness my old friend\"\nline2 &lt;- \"I've come to talk to you again\"\npaste(line1, line2, sep = \"\")\n\n[1] \"Hello darkness my old friendI've come to talk to you again\"\n\npaste(line1, line2, sep = \"*\")\n\n[1] \"Hello darkness my old friend*I've come to talk to you again\"\n\ngrep('and', line2)\n\ninteger(0)\n\nsub('Hello', 'Goodbye', line1)\n\n[1] \"Goodbye darkness my old friend\"\n\nsubstr(line1, 5, 5 + 5)\n\n[1] \"o dark\""
  },
  {
    "objectID": "material/type-coercion.html#r-environment",
    "href": "material/type-coercion.html#r-environment",
    "title": "Type coercion",
    "section": "R Environment",
    "text": "R Environment\n\nGet help for the t.test, table, locator and identify functions,\nCheck for all occurences of fisher.test in the docs,\nWhich package contains the plot.ecdf function. What does it do?\nFind package ‘reshape’-related questions on StackOverflow,\nSearch on the internet on how to load an XML file into R,\nInstall the ‘cgmisc’ package from GitHub,\nLook up the ‘cgmisc’ vignette,\nSee all the demos available for you and run one you like,\nRun examples for the fisher.test,\nCheck out CRANs view for genetics,\nInstall a CRAN package of choice,\nInstall the R-Forge package ’bigRR’title: “Untitled” author: “Termeh Shafie” date: “2024-06-10” output: html_document"
  },
  {
    "objectID": "material/hotels-forcats.html",
    "href": "material/hotels-forcats.html",
    "title": "Hotel bookings - factors",
    "section": "",
    "text": "library(tidyverse)\nlibrary(skimr)\n\nLoad the hotels data set we used in a previous practical. Render and view the following visualisation. How are the months ordered? What would be a better order? Then, reorder the months on the x-axis (levels of arrival_date_month) in a way that makes more sense. You will want to use a function from the forcats package, see https://forcats.tidyverse.org/reference/index.html for inspiration and help.\nStretch goal: If you finish the above task before time is up, change the y-axis label so the values are shown with dollar signs, e.g. $80 instead of 80. You will want to use a function from the scales package, see https://scales.r-lib.org/reference/index.html for inspiration and help.\n\nhotels %&gt;%\n  group_by(hotel, arrival_date_month) %&gt;%   # group by hotel type and arrival month\n  summarise(mean_adr = mean(adr)) %&gt;%       # calculate mean adr for each group\n  ggplot(aes(\n    x = arrival_date_month,                 # x-axis = arrival_date_month\n    y = mean_adr,                           # y-axis = mean_adr calculated above\n    group = hotel,                          # group lines by hotel type\n    color = hotel)                          # and color by hotel type\n    ) +\n  geom_line() +                             # use lines to represent data\n  theme_minimal() +                         # use a minimal theme\n  labs(\n    x = \"Arrival month\",                 # customize labels\n    y = \"Mean ADR (average daily rate)\",\n    title = \"Comparison of resort and city hotel prices across months\",\n    subtitle = \"Resort hotel prices soar in the summer while ciry hotel prices remain relatively constant throughout the year\",\n    color = \"Hotel type\"\n    )"
  },
  {
    "objectID": "material/data-type-class-exercises.html",
    "href": "material/data-type-class-exercises.html",
    "title": "Data Type and Data Classes: Exercises",
    "section": "",
    "text": "Double check that you do not have stored objects in your current session with the following command. This will list all objects that you have in your current R session.\n\nls()\n\ncharacter(0)\n\n\nIn case you have objects that you want to remove from the current session you can do so with the rm() function.  This command will remove all objects available in your current environment.\n\nrm(list = ls())\n\nThis command uses commands that we have not talked about yet. If you do not understand how it works now, you will do so after tomorrows lectures and exercises.\n\nCreate variables var1 and var2 and initialize them with two integers of choice.\nAdd the two variables and save them as a new variable named var3 and print the result.\nCheck the class, mode, and type for var1, var2, var3 and π (is found under the variable name pi in R)\nCreate two character variables containing a text of choice. Check the mode, class, and type of the first one.\n\nAdd var1 to it. What is the result and why?\n\n\n\n\nConvert var3 to an integer, cast an integer variable to double, cast a string to a double.\nReport floor and ceiling of π and round π to 3 decimal places.\nIs floor of π an integer?\nTreat \"3.56437\" string as number.\nDivide ∞ by - ∞\nPrint a truth table for OR (for three distinct logical values). Read about truth tables here.\nMultiply a logical TRUE by a logical FALSE. Rise the logical true to the 7-th power.\nCreate two character variables containing two verses of a chosen song.\n\n\nConcatenate the two variables,\n\nPaste the variables with ‘*’ as separator.\n\nFind if ‘and’ occurs in the second line,\n\nSubstitute a word for another,\n\nExtract substring starting at the 5th character and 5 characters long."
  },
  {
    "objectID": "material/data-type-class-exercises.html#exercise",
    "href": "material/data-type-class-exercises.html#exercise",
    "title": "Data Type and Data Classes: Exercises",
    "section": "",
    "text": "Double check that you do not have stored objects in your current session with the following command. This will list all objects that you have in your current R session.\n\nls()\n\ncharacter(0)\n\n\nIn case you have objects that you want to remove from the current session you can do so with the rm() function.  This command will remove all objects available in your current environment.\n\nrm(list = ls())\n\nThis command uses commands that we have not talked about yet. If you do not understand how it works now, you will do so after tomorrows lectures and exercises.\n\nCreate variables var1 and var2 and initialize them with two integers of choice.\nAdd the two variables and save them as a new variable named var3 and print the result.\nCheck the class, mode, and type for var1, var2, var3 and π (is found under the variable name pi in R)\nCreate two character variables containing a text of choice. Check the mode, class, and type of the first one.\n\nAdd var1 to it. What is the result and why?\n\n\n\n\nConvert var3 to an integer, cast an integer variable to double, cast a string to a double.\nReport floor and ceiling of π and round π to 3 decimal places.\nIs floor of π an integer?\nTreat \"3.56437\" string as number.\nDivide ∞ by - ∞\nPrint a truth table for OR (for three distinct logical values). Read about truth tables here.\nMultiply a logical TRUE by a logical FALSE. Rise the logical true to the 7-th power.\nCreate two character variables containing two verses of a chosen song.\n\n\nConcatenate the two variables,\n\nPaste the variables with ‘*’ as separator.\n\nFind if ‘and’ occurs in the second line,\n\nSubstitute a word for another,\n\nExtract substring starting at the 5th character and 5 characters long."
  },
  {
    "objectID": "material/nobels-csv.html",
    "href": "material/nobels-csv.html",
    "title": "Nobel winners",
    "section": "",
    "text": "library(tidyverse)\n\nLet’s first load the data:\n\nnobel &lt;- ___(___)\n\nThen let’s split the data into two:\n\n# stem laureates\n___ &lt;- nobel %&gt;%\n  filter(___)\n\n# non-steam laureates\n___ &lt;- nobel %&gt;%\n  filter(___)\n\nAnd finally write out the data:\n\n# add code for writing out the two data frames here"
  },
  {
    "objectID": "material/sales-excel.html",
    "href": "material/sales-excel.html",
    "title": "Sales",
    "section": "",
    "text": "library(tidyverse)\nlibrary(readxl)\n\n\nRead in the Excel file called sales.xlsx from the data-raw/ folder such that it looks like the following.\n\n\n\n\n\n\n\n\n\n\n\nStretch goal: Manipulate the sales data such such that it looks like the following."
  },
  {
    "objectID": "material/nyc-flights.html",
    "href": "material/nyc-flights.html",
    "title": "NYC flights",
    "section": "",
    "text": "For the below exercies we use the tidyverse package for much of the data wrangling and visualisation and the data lives in the nycflights13 package. This is data a dataset of flights departing from New York City (NYC) airports in the year 2013.\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\nThe data sets available in the package can be viewed using the following syntax:\nYou will need one or combinations of these to solve the following exercises. These tables are organized as the figure below shows."
  },
  {
    "objectID": "material/nyc-flights.html#packages",
    "href": "material/nyc-flights.html#packages",
    "title": "NYC flights",
    "section": "",
    "text": "For the below exercies we use the tidyverse package for much of the data wrangling and visualisation and the data lives in the nycflights13 package.\n\nlibrary(tidyverse)\nlibrary(nycflights13)"
  },
  {
    "objectID": "material/nyc-flights.html#data",
    "href": "material/nyc-flights.html#data",
    "title": "NYC flights",
    "section": "Data",
    "text": "Data\nThe data sets available in the package can be viewed using the following syntax:\nYou will need one or combinations of these to solve the following exercises."
  },
  {
    "objectID": "material/nyc-flights.html#exercise-1",
    "href": "material/nyc-flights.html#exercise-1",
    "title": "NYC flights",
    "section": "Exercise 1",
    "text": "Exercise 1\nImagine you wanted to draw (approximately) the route each plane flies from its origin to its destination. What variables would you need? What tables would you need to combine?"
  },
  {
    "objectID": "material/nyc-flights.html#exercise-2",
    "href": "material/nyc-flights.html#exercise-2",
    "title": "NYC flights",
    "section": "Exercise 2",
    "text": "Exercise 2\nThis plots the approximate flight paths of the first 100 flights in the flights dataset. Try reproducing it. Hint: you can create a layer of map borders using borders(state)."
  },
  {
    "objectID": "material/nyc-flights.html#exercise-3",
    "href": "material/nyc-flights.html#exercise-3",
    "title": "NYC flights",
    "section": "Exercise 3",
    "text": "Exercise 3\nWe know that some days of the year are “special”, and fewer people than usual fly on them. Since it is US data for 2013 we will consider: New Years Day, Independence Day, Thanksgiving Day, Christmas Day.\nHow might you represent that data as a data frame? What would be the primary keys of that table? How would it connect to the existing tables?"
  },
  {
    "objectID": "material/nyc-flights.html#exercise-4",
    "href": "material/nyc-flights.html#exercise-4",
    "title": "NYC flights",
    "section": "Exercise 4",
    "text": "Exercise 4\nCreate a visualization fo your own to illustrate if indeed fewer people than usual fly on the above special days."
  },
  {
    "objectID": "material/nyc-flights.html#packages-and-data",
    "href": "material/nyc-flights.html#packages-and-data",
    "title": "NYC flights",
    "section": "",
    "text": "For the below exercies we use the tidyverse package for much of the data wrangling and visualisation and the data lives in the nycflights13 package. This is data a dataset of flights departing from New York City (NYC) airports in the year 2013.\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\nThe data sets available in the package can be viewed using the following syntax:\nYou will need one or combinations of these to solve the following exercises. These tables are organized as the figure below shows."
  },
  {
    "objectID": "material/nyc-flights.html#familiarizing-ourselves-with-the-dataset",
    "href": "material/nyc-flights.html#familiarizing-ourselves-with-the-dataset",
    "title": "NYC flights",
    "section": "Familiarizing ourselves with the dataset",
    "text": "Familiarizing ourselves with the dataset\n\nWhat variables are included in the flights dataset? How many rows are there?\nWhat variables are included in the airports dataset? How many rows are there?\nWhich variables are included in the airlines dataset? How many rows are there?"
  },
  {
    "objectID": "material/nyc-flights.html#focusing-on-atlanta",
    "href": "material/nyc-flights.html#focusing-on-atlanta",
    "title": "NYC flights",
    "section": "Focusing on Atlanta",
    "text": "Focusing on Atlanta\n\nLet’s focus on flights from NYC area airports to Atlanta GA (FAA code ATL). Create a new object atlanta that includes only these flights. Hint: use filter()). How many flights to Atlanta were there in 2013?"
  },
  {
    "objectID": "material/nyc-flights.html#seasonality",
    "href": "material/nyc-flights.html#seasonality",
    "title": "NYC flights",
    "section": "Seasonality",
    "text": "Seasonality\n\nIs there a difference in the number of flights per month?\nSummarize the number of flights for each month and provide a sorted list with the months with the most flights first. Hint: use group_by() in combination with summarize())."
  },
  {
    "objectID": "material/nyc-flights.html#use-filter",
    "href": "material/nyc-flights.html#use-filter",
    "title": "NYC flights",
    "section": "Use filter()",
    "text": "Use filter()\n\nFind all flights that\n\n\nHad an arrival delay of two or more hours.\nFlew to Houston (IAH or HOU)\nWere operated by United, American, or Delta. Hint: In the flights dataset, the column carrier indicates the airline, but it uses two-character carrier codes. You can find the carrier codes for the airlines in the airlines dataset. Since the carrier code dataset only has 16 rows, and the names of the airlines in that dataset are not exactly “United”, “American”, or “Delta”, it is easiest to manually look up their carrier codes in that data.\nDeparted in summer (July, August, and September). Hint: the summer flights are those that departed in months 7 (July), 8 (August), and 9 (September).\nArrived more than two hours late, but didn’t leave late. Hint: Flights that arrived more than two hours late, but didn’t leave late will have an arrival delay of more than 120 minutes (arr_delay &gt; 120) and a non-positive departure delay (dep_delay &lt;=0)\nWere delayed by at least an hour, but made up over 30 minutes in flight. Hint: If a flight was delayed by at least an hour, then dep_delay &gt;= 60. If the flight didn’t make up any time in the air, then its arrival would be delayed by the same amount as its departure, meaning dep_delay == arr_delay, or alternatively, dep_delay - arr_delay == 0. If it makes up over 30 minutes in the air, then the arrival delay must be at least 30 minutes less than the departure delay, which is stated as dep_delay - arr_delay &gt; 30.\nDeparted between midnight and 6 am (inclusive). Hint: In dep_time, midnight is represented by 2400, not 0. You can verify this by checking the minimum and maximum of dep_time."
  },
  {
    "objectID": "material/nyc-flights.html#arrange-rows-with-arrange",
    "href": "material/nyc-flights.html#arrange-rows-with-arrange",
    "title": "NYC flights",
    "section": "Arrange rows with arrange()",
    "text": "Arrange rows with arrange()\n\nHow could you use arrange() to sort all missing values to the start? Hint: use is.na()) and add an indicator of whether the column has a missing value, the flights will first be sorted by desc(is.na(dep_time)). Since desc(is.na(dep_time)) is either TRUE when dep_time is missing, or FALSE, when it is not, the rows with missing values of dep_time will come first, since TRUE &gt; FALSE.\nSort flights to find the most delayed flights. Find the flights that left earliest.\nSort flights to find the fastest flights."
  },
  {
    "objectID": "material/nyc-flights.html#seelct-variables-with-select",
    "href": "material/nyc-flights.html#seelct-variables-with-select",
    "title": "NYC flights",
    "section": "Seelct variables with select()",
    "text": "Seelct variables with select()\n\nWhat does the one_of() function do? Why might it be helpful in conjunction with this vector?"
  },
  {
    "objectID": "material/nyc-flights.html#add-new-variables-with-mutate",
    "href": "material/nyc-flights.html#add-new-variables-with-mutate",
    "title": "NYC flights",
    "section": "Add new variables with mutate()",
    "text": "Add new variables with mutate()\n\nCompare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it?\nCome up with another approach that will give you the same output as not_cancelled %&gt;% count(dest) and not_cancelled %&gt;% count(tailnum, wt = distance) (without using count()).\nLook at the number of cancelled flights per day. Is there a pattern? Create a plot to visualize your answers.\nFor each destination, compute the total minutes of delay. For each flight, compute the proportion of the total delay for its destination.\nDelays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using lag() to explore how the delay of a flight is related to the delay of the immediately preceding flight. Use a plot to visualize this."
  },
  {
    "objectID": "material/nyc-flights.html#more-viz",
    "href": "material/nyc-flights.html#more-viz",
    "title": "NYC flights",
    "section": "More Viz",
    "text": "More Viz\n\nVisualize the distribution of on time departure rate across the three airports using a segmented bar plot. Hint: Remove NA’s and suppose that a flight that is delayed for less than 5 minutes is basically “on time”."
  },
  {
    "objectID": "material/nyc-flights.html#advanced-exercises",
    "href": "material/nyc-flights.html#advanced-exercises",
    "title": "NYC flights",
    "section": "Advanced Exercises:",
    "text": "Advanced Exercises:\n\nImagine you wanted to draw (approximately) the route each plane flies from its origin to its destination. What variables would you need? What tables from the package you loaded would you need to combine?\nThis plots the approximate flight paths of the first 100 flights in the flights dataset. Try reproducing it. Hint: you can create a layer of map borders using borders(state).\n\n\n\n\n\n\n\n\n\n\n\nWe know that some days of the year are “special”, and fewer people than usual fly on them. Since it is US data for 2013 we will consider: New Years Day, Independence Day, Thanksgiving Day, Christmas Day.\n\nHow might you represent that data as a data frame? What would be the primary keys of that table? How would it connect to the existing tables?\nWe can add a table of special dates, similar to the following table.\n\nspecial_days &lt;- tribble(\n  ~year, ~month, ~day, ~holiday,\n  2013, 01, 01, \"New Years Day\",\n  2013, 07, 04, \"Independence Day\",\n  2013, 11, 29, \"Thanksgiving Day\",\n  2013, 12, 25, \"Christmas Day\"\n)\n\nThe primary key of the table would be the (year, month, day) columns. The (year, month, day) columns could be used to join special_days with other tables.\n\nCreate a visualization fo your own to illustrate if indeed fewer people than usual fly on the above special days.\nCompute the average delay by destination, then join on the airports data frame so you can show the spatial distribution of delays. Here’s an easy way to draw a map of the United States (can you understand why we choose semi-join?):\n\n\nairports %&gt;%\n  semi_join(flights, c(\"faa\" = \"dest\")) %&gt;%\n  ggplot(aes(lon, lat)) +\n  borders(\"state\") +\n  geom_point() +\n  coord_quickmap() + \n  theme_void()\n\n\n\n\n\n\n\n\nHint: You might want to use the size or color of the points to display the average delay for each airport.\n\nWhat weather conditions make it more likely to see a delay? Use the variable precip (precipitation) from the weather dataset to answer this.\nWhat happened on June 13, 2013? Reproduce the following plot which displays the spatial pattern of delays, and then use Google to cross-reference with the weather. Hint: use library(viridis) to get the same colors."
  },
  {
    "objectID": "material/countries-of-the-world.html",
    "href": "material/countries-of-the-world.html",
    "title": "Countries of the world",
    "section": "",
    "text": "In order to complete this assignment you will need a Chrome browser with the Selector Gadget extension installed.\nThis website lists the names of 250 countries, as well as their flag, capital, population and size in square kilometres. Our goal could be to read this information into R for each country so that we can potentially analyse it further.\nBefore we start, we should load the required packages (we will also need the tidyverse package this time) and read the website with the function read_html() and assign it to an R object.\nlibrary(tidyverse)\nlibrary(rvest)\nlibrary(DT)\n\npage &lt;- read_html(\"https://scrapethissite.com/pages/simple/\")"
  },
  {
    "objectID": "material/countries-of-the-world.html#country-names",
    "href": "material/countries-of-the-world.html#country-names",
    "title": "Countries of the world",
    "section": "Country names",
    "text": "Country names\nUse the Selector Gadget to identify the CSS selectors needed to extract country names.\n\ncountry &lt;- page %&gt;%\n  html_elements(\".country-name\") %&gt;%\n  html_text(trim = TRUE) \n\nhead(country)\n\n[1] \"Andorra\"              \"United Arab Emirates\" \"Afghanistan\"         \n[4] \"Antigua and Barbuda\"  \"Anguilla\"             \"Albania\""
  },
  {
    "objectID": "material/countries-of-the-world.html#capitals-population-and-area",
    "href": "material/countries-of-the-world.html#capitals-population-and-area",
    "title": "Countries of the world",
    "section": "Capitals, population and area",
    "text": "Capitals, population and area\nLet us now turn to the further information for each country. Again use the selector gadget to identify the CSS selector needed which in this case is .country-info:\n\npage %&gt;%\n  html_elements(\".country-info\") %&gt;%\n  html_text(trim = TRUE) %&gt;% \n  head(n = 10)\n\n [1] \"Capital: Andorra la VellaPopulation: 84000Area (km2): 468.0\"   \n [2] \"Capital: Abu DhabiPopulation: 4975593Area (km2): 82880.0\"      \n [3] \"Capital: KabulPopulation: 29121286Area (km2): 647500.0\"        \n [4] \"Capital: St. John'sPopulation: 86754Area (km2): 443.0\"         \n [5] \"Capital: The ValleyPopulation: 13254Area (km2): 102.0\"         \n [6] \"Capital: TiranaPopulation: 2986952Area (km2): 28748.0\"         \n [7] \"Capital: YerevanPopulation: 2968000Area (km2): 29800.0\"        \n [8] \"Capital: LuandaPopulation: 13068161Area (km2): 1246700.0\"      \n [9] \"Capital: NonePopulation: 0Area (km2): 1.4E7\"                   \n[10] \"Capital: Buenos AiresPopulation: 41343201Area (km2): 2766890.0\"\n\n\nSo we get the names of the capitals, but also the population and the size of the country. The selector was not specific enough and we have to tell html_elements() more precisely which of these we are interested in. These CSS selectors differ between the three countries’ information:\n\nThe selector country-capital gives us the capital of the countries:\n\n\ncapital &lt;- page %&gt;%\n  html_elements(\".country-capital\") %&gt;%\n  html_text(trim = TRUE) \n\nhead(capital)\n\n[1] \"Andorra la Vella\" \"Abu Dhabi\"        \"Kabul\"            \"St. John's\"      \n[5] \"The Valley\"       \"Tirana\"          \n\n\n\nThe selector country-population gives us the population of the countries:\n\n\npopulation &lt;-  page %&gt;%\n  html_elements(\".country-population\") %&gt;%\n  html_text() %&gt;% \n  as.numeric()\nhead(population)\n\n[1]    84000  4975593 29121286    86754    13254  2986952\n\n\n\nThe selector country-area gives us the area of the countries:\n\n\narea &lt;-  page %&gt;%\n  html_elements(\".country-area\") %&gt;%\n  html_text() %&gt;% \n  as.numeric()\nhead(area)\n\n[1]    468  82880 647500    443    102  28748\n\n\nNote that we need to tell R to interpret the “text” read from the HTML code as numbers using the function as.numeric()."
  },
  {
    "objectID": "material/countries-of-the-world.html#merge-into-one-tibble",
    "href": "material/countries-of-the-world.html#merge-into-one-tibble",
    "title": "Countries of the world",
    "section": "Merge into one tibble",
    "text": "Merge into one tibble\nWe could already continue working with this, but for many applications it is more practical if we combine the data in a vertical form:\n\ncountries &lt;- tibble(\n  country = country,\n  capital = capital,\n  population = population,\n  area = area\n)\ncountries\n\n# A tibble: 250 × 4\n   country              capital          population     area\n   &lt;chr&gt;                &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;\n 1 Andorra              Andorra la Vella      84000      468\n 2 United Arab Emirates Abu Dhabi           4975593    82880\n 3 Afghanistan          Kabul              29121286   647500\n 4 Antigua and Barbuda  St. John's            86754      443\n 5 Anguilla             The Valley            13254      102\n 6 Albania              Tirana              2986952    28748\n 7 Armenia              Yerevan             2968000    29800\n 8 Angola               Luanda             13068161  1246700\n 9 Antarctica           None                      0 14000000\n10 Argentina            Buenos Aires       41343201  2766890\n# ℹ 240 more rows"
  },
  {
    "objectID": "material/countries-of-the-world.html#all-in-one-step",
    "href": "material/countries-of-the-world.html#all-in-one-step",
    "title": "Countries of the world",
    "section": "All in one step",
    "text": "All in one step\nIf we are sure that we do not need the individual vectors, we can also perform the reading of the data and the creation of the tibble in a single step. Below you can see how the complete scraping process can be completed in relatively few lines.\n\npage &lt;- \"https://scrapethissite.com/pages/simple/\" %&gt;%\n  read_html()\n\ncountries_2 &lt;- tibble(\n  Land = page %&gt;%\n    html_elements(css = \".country-name\") %&gt;% \n    html_text(trim = TRUE),\n  capital = page %&gt;% \n    html_elements(css = \".country-capital\") %&gt;% \n    html_text(),\n  population = page %&gt;% \n    html_elements(css = \".country-population\") %&gt;% \n    html_text() %&gt;% \n    as.numeric(),\n  area = page %&gt;% \n    html_elements(css = \".country-area\") %&gt;% \n    html_text() %&gt;% \n    as.numeric()\n)\n\ncountries_2\n\n# A tibble: 250 × 4\n   Land                 capital          population     area\n   &lt;chr&gt;                &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;\n 1 Andorra              Andorra la Vella      84000      468\n 2 United Arab Emirates Abu Dhabi           4975593    82880\n 3 Afghanistan          Kabul              29121286   647500\n 4 Antigua and Barbuda  St. John's            86754      443\n 5 Anguilla             The Valley            13254      102\n 6 Albania              Tirana              2986952    28748\n 7 Armenia              Yerevan             2968000    29800\n 8 Angola               Luanda             13068161  1246700\n 9 Antarctica           None                      0 14000000\n10 Argentina            Buenos Aires       41343201  2766890\n# ℹ 240 more rows"
  },
  {
    "objectID": "material/uoe-art.html",
    "href": "material/uoe-art.html",
    "title": "University of Edinburgh Art Collection",
    "section": "",
    "text": "The University of Edinburgh Art Collection “supports the world-leading research and teaching that happens within the University. Comprised of an astonishing range of objects and ideas spanning two millennia and a multitude of artistic forms, the collection reflects not only the long and rich trajectory of the University, but also major national and international shifts in art history.”\nIn this practical we’ll scrape data on all art pieces in the Edinburgh College of Art collection."
  },
  {
    "objectID": "material/uoe-art.html#r-scripts-vs.-quarto-documents",
    "href": "material/uoe-art.html#r-scripts-vs.-quarto-documents",
    "title": "University of Edinburgh Art Collection",
    "section": "R scripts vs. Quarto documents",
    "text": "R scripts vs. Quarto documents\nToday you’ll be using both R scripts and R Markdown documents:\n\nuse R scripts in the web scraping stage and ultimately save the scraped data as a csv.\nuse an Quarto document in the web analysis stage, where we start off by reading in the csv file we wrote out in the scraping stage."
  },
  {
    "objectID": "material/uoe-art.html#packages",
    "href": "material/uoe-art.html#packages",
    "title": "University of Edinburgh Art Collection",
    "section": "Packages",
    "text": "Packages\nWe’ll use the tidyverse package for much of the data wrangling and visualisation, the robotstxt package to check if we’re allowed to scrape the data, the rvest package for data scraping.\n\nlibrary(tidyverse) \nlibrary(robotstxt)\nlibrary(rvest)"
  },
  {
    "objectID": "material/uoe-art.html#data",
    "href": "material/uoe-art.html#data",
    "title": "University of Edinburgh Art Collection",
    "section": "Data",
    "text": "Data\nThis assignment does not come with any prepared datasets. Instead you’ll be scraping the data! But before doing so, let’s check that a bot has permissions to access pages on this domain.\n\npaths_allowed(\"https://collections.ed.ac.uk/art)\")\n\n\n collections.ed.ac.uk                      \n\n\n[1] TRUE"
  },
  {
    "objectID": "material/uoe-art.html#scraping-a-single-page",
    "href": "material/uoe-art.html#scraping-a-single-page",
    "title": "University of Edinburgh Art Collection",
    "section": "Scraping a single page",
    "text": "Scraping a single page\nWe will start off by scraping data on the first 10 pieces in the collection from here.\nFirst, we define a new object called first_url, which is the link above. Then, we read the page at this url with the read_html() function from the rvest package. The code for this is already provided in 01-scrape-page-one.R.\n\n# set url\nfirst_url &lt;- \"https://collections.ed.ac.uk/art/search/*:*/Collection:%22edinburgh+college+of+art%7C%7C%7CEdinburgh+College+of+Art%22?offset=0\"\n\n# read html page\npage &lt;- read_html(first_url)\n\nFor the ten pieces on this page we will extract title, artist, and link information, and put these three variables in a data frame.\n\nTitles\nLet’s start with titles. We make use of the SelectorGadget to identify the tags for the relevant nodes:\n\n\n\n\n\n\n\n\n\n\npage %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%\n  html_node(\"h3 a\")\n\n{xml_nodeset (10)}\n [1] &lt;a href=\"./record/20567?highlight=*:*\"&gt;Tomb of Ferry de Gros, Lord of Dy ...\n [2] &lt;a href=\"./record/22534?highlight=*:*\"&gt;Tomb                              ...\n [3] &lt;a href=\"./record/21218?highlight=*:*\"&gt;Portrait of a Woman               ...\n [4] &lt;a href=\"./record/21881?highlight=*:*\"&gt;Untitled                          ...\n [5] &lt;a href=\"./record/20907?highlight=*:*\"&gt;Standing Male Nude                ...\n [6] &lt;a href=\"./record/20972?highlight=*:*\"&gt;Seated Male Nude with Blanket     ...\n [7] &lt;a href=\"./record/99439?highlight=*:*\"&gt;Untitled - Woman and Man          ...\n [8] &lt;a href=\"./record/50508?highlight=*:*\"&gt;Unknown                           ...\n [9] &lt;a href=\"./record/21054?highlight=*:*\"&gt;Striding Male Nude                ...\n[10] &lt;a href=\"./record/21038?highlight=*:*\"&gt;Standing Male Nude                ...\n\n\nThen we extract the text with html_text():\n\npage %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%\n  html_node(\"h3 a\") %&gt;%\n  html_text()\n\n [1] \"Tomb of Ferry de Gros, Lord of Dyeghem Nieunland                                    \"                           \n [2] \"Tomb                                    \"                                                                       \n [3] \"Portrait of a Woman                                                                            (1955)\"          \n [4] \"Untitled                                                                            (Unknown)\"                  \n [5] \"Standing Male Nude                                                                            (JAN 1958)\"       \n [6] \"Seated Male Nude with Blanket                                                                            (1959)\"\n [7] \"Untitled - Woman and Man                                                                            (1981)\"     \n [8] \"Unknown                                                                            (1957)\"                      \n [9] \"Striding Male Nude                                                                            (1950)\"           \n[10] \"Standing Male Nude                                    \"                                                         \n\n\nAnd get rid of all the spurious white space in the text with str_squish(), which reduces repeated whitespace inside a string.\nTake a look at the help for str_squish() to find out more about how it works and how it’s different from str_trim().\n\npage %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%\n  html_node(\"h3 a\") %&gt;%\n  html_text() %&gt;% \n  str_squish()\n\n [1] \"Tomb of Ferry de Gros, Lord of Dyeghem Nieunland\"\n [2] \"Tomb\"                                            \n [3] \"Portrait of a Woman (1955)\"                      \n [4] \"Untitled (Unknown)\"                              \n [5] \"Standing Male Nude (JAN 1958)\"                   \n [6] \"Seated Male Nude with Blanket (1959)\"            \n [7] \"Untitled - Woman and Man (1981)\"                 \n [8] \"Unknown (1957)\"                                  \n [9] \"Striding Male Nude (1950)\"                       \n[10] \"Standing Male Nude\"                              \n\n\nAnd finally save the resulting data as a vector of length 10:\n\ntitles &lt;- page %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%\n  html_node(\"h3 a\") %&gt;%\n  html_text() %&gt;%\n  str_squish()\n\n\n\nLinks\nThe same nodes that contain the text for the titles also contains information on the links to individual art piece pages for each title. We can extract this information using a new function from the rvest package, html_attr(), which extracts attributes.\nA mini HTML lesson! The following is how we define hyperlinked text in HTML:\n&lt;a href=\"https://www.google.com\"&gt;Search on Google&lt;/a&gt;\nAnd this is how the text would look like on a webpage: Search on Google.\nHere the text is Search on Google and the href attribute contains the url of the website you’d go to if you click on the hyperlinked text: https://www.google.com.\nThe moral of the story is: the link is stored in the href attribute.\n\npage %&gt;%\n  html_nodes(\".iteminfo\") %&gt;%   # same nodes\n  html_node(\"h3 a\") %&gt;%         # as before\n  html_attr(\"href\")             # but get href attribute instead of text\n\n [1] \"./record/20567?highlight=*:*\" \"./record/22534?highlight=*:*\"\n [3] \"./record/21218?highlight=*:*\" \"./record/21881?highlight=*:*\"\n [5] \"./record/20907?highlight=*:*\" \"./record/20972?highlight=*:*\"\n [7] \"./record/99439?highlight=*:*\" \"./record/50508?highlight=*:*\"\n [9] \"./record/21054?highlight=*:*\" \"./record/21038?highlight=*:*\"\n\n\nThese don’t really look like URLs as we know then though. They’re relative links.\nSee the help for str_replace() to find out how it works. Remember that the first argument is passed in from the pipeline, so you just need to define the pattern and replacement arguments.\n\nClick on one of art piece titles in your browser and take note of the url of the webpage it takes you to. Think about how that url compares to what we scraped above? How is it different? Using str_replace(), fix the URLs. You’ll note something special happening in the pattern to replace. We want to replace the ., but we have it as \\\\.. This is because the period . is a special character and so we need to escape it first with backslashes, \\\\s.\n\n\n\nArtists\n\nFill in the blanks to scrape artist names.\n\n\n\nPut it altogether\n\nFill in the blanks to organize everything in a tibble.\n\n\n\nScrape the next page\n\nClick on the next page, and grab its url. Fill in the blank in to define a new object: second_url. Copy-paste code from top of the R script to scrape the new set of art pieces, and save the resulting data frame as second_ten."
  },
  {
    "objectID": "material/uoe-art.html#functions",
    "href": "material/uoe-art.html#functions",
    "title": "University of Edinburgh Art Collection",
    "section": "Functions",
    "text": "Functions\nYou’ve been using R functions, now it’s time to write your own!\nLet’s start simple. Here is a function that takes in an argument x, and adds 2 to it.\n\nadd_two &lt;- function(x){\n  x + 2\n}\n\nLet’s test it:\n\nadd_two(3)\n\n[1] 5\n\nadd_two(10)\n\n[1] 12\n\n\nThe skeleton for defining functions in R is as follows:\n\nfunction_name &lt;- function(input){\n  # do something with the input(s)\n  # return something\n}\n\nThen, a function for scraping a page should look something like:\nReminder: Function names should be short but evocative verbs.\n\nfunction_name &lt;- function(url){\n  # read page at url\n  # extract title, link, artist info for n pieces on page\n  # return a n x 3 tibble\n}\n\n\nFill in the blanks using code you already developed in the previous exercises. Name the function scrape_page.\n\nTest out your new function by running the following in the console. Does the output look right? Discuss with teammates whether you’re getting the same results as before.\n\nscrape_page(first_url)\nscrape_page(second_url)"
  },
  {
    "objectID": "material/uoe-art.html#iteration",
    "href": "material/uoe-art.html#iteration",
    "title": "University of Edinburgh Art Collection",
    "section": "Iteration",
    "text": "Iteration\nWe went from manually scraping individual pages to writing a function to do the same. Next, we will work on making our workflow a little more efficient by using R to iterate over all pages that contain information on the art collection.\nThat means we give develop a list of URLs (of pages that each have 10 art pieces), and write some code that applies the scrape_page() function to each page, and combines the resulting data frames from each page into a single data frame with 3289 rows and 3 columns.\n\nList of URLs\nClick through the first few of the pages in the art collection and observe their URLs to confirm the following pattern:\n[sometext]offset=0     # Pieces 1-10\n[sometext]offset=10    # Pieces 11-20\n[sometext]offset=20    # Pieces 21-30\n[sometext]offset=30    # Pieces 31-40\n...\n[sometext]offset=3280  # Pieces 3281-3289\nWe can construct these URLs in R by pasting together two pieces: (1) a common (root) text for the beginning of the URL, and (2) numbers starting at 0, increasing by 10, all the way up to 3289. Two new functions are helpful for accomplishing this: glue() for pasting two pieces of text and seq() for generating a sequence of numbers.\n\nFill in the blanks to construct the list of URLs.\n\n\n\nMapping\nFinally, we’re ready to iterate over the list of URLs we constructed. We will do this by mapping the function we developed over the list of URLs. There are a series of mapping functions in R and they each take the following form:\nmap([x], [function to apply to each element of x])\nIn our case x is the list of URLs we constructed and the function to apply to each element of x is the function we developed earlier, scrape_page. And as a result we want a data frame, so we use map_dfr function:\n\nmap_dfr(urls, scrape_page)\n\n\nFill in the blanks to scrape all pages, and to create a new data frame called uoe_art.\n\n\n\nWrite out data\n\nFinally write out the data frame you constructed into the data folder so that you can use it in the analysis section."
  },
  {
    "objectID": "material/uoe-art.html#analysis",
    "href": "material/uoe-art.html#analysis",
    "title": "University of Edinburgh Art Collection",
    "section": "Analysis",
    "text": "Analysis\nFor the rest of the exercises you can work in Quarto/R Markdown.\nNow that we have a tidy dataset that we can analyze, let’s do that!\nWe’ll start with some data cleaning, to clean up the dates that appear at the end of some title text in parentheses. Some of these are years, others are more specific dates, some art pieces have no date information whatsoever, and others have some non-date information in parentheses. This should be interesting to clean up!\nFirst thing we’ll try is to separate the title column into two: one for the actual title and the other for the date if it exists. In human speak, we need to\n“separate the title column at the first occurrence of ( and put the contents on one side of the ( into a column called title and the contents on the other side into a column called date”\nLuckily, there’s a function that does just this: separate()!\nAnd once we have completed separating the single title column into title and date, we need to do further clean-up in the date column to get rid of extraneous )s with str_remove(), capture year information, and save the data as a numeric variable.\n\nFill in the blanks in to implement the data wrangling we described above. Note that this will result in some warnings when you run the code, and that’s OK! Read the warnings, and explain what they mean, and why we are ok with leaving them in given that our objective is to just capture year where it’s convenient to do so.\nPrint out a summary of the data frame using the skim() function. How many pieces have artist info missing? How many have year info missing?\nMake a histogram of years. Use a reasonable binwidth. Do you see anything out of the ordinary?\nFind which piece has the out of the ordinary year and go to its page on the art collection website to find the correct year for it. Can you tell why our code didn’t capture the correct year information? Correct the error in the data frame and visualize the data again.\n\nHint: You’ll want to use mutate() and if_else() or case_when() to implement the correction.\n\nWho is the most commonly featured artist in the collection? Do you know them? Any guess as to why the university has so many pieces from them?\nFinal question! How many art pieces have the word “child” in their title? Try to figure it out, and ask for help if you’re stuck.\n\nHint: str_subset() can be helful here. You should consider how you might capture titles where the word appears as “child” and “Child”.\nSource: https://collections.ed.ac.uk/art/about"
  },
  {
    "objectID": "material/brexit.html",
    "href": "material/brexit.html",
    "title": "Brexit",
    "section": "",
    "text": "library(tidyverse)\n\nIn September 2019, YouGov survey asked 1,639 GB adults the following question:\n\nIn hindsight, do you think Britain was right/wrong to vote to leave EU?\n\nRight to leave\n\nWrong to leave\n\nDon’t know\n\n\nThe data from the survey is in data/brexit.csv.\n\nbrexit &lt;- read_csv(\"data11/brexit.csv\")\n\nIn the course video we made the following visualisation.\n\nbrexit &lt;- brexit %&gt;%\n  mutate(\n    region = fct_relevel(region, \"london\", \"rest_of_south\", \"midlands_wales\", \"north\", \"scot\"),\n    region = fct_recode(region, London = \"london\", `Rest of South` = \"rest_of_south\", `Midlands / Wales` = \"midlands_wales\", North = \"north\", Scotland = \"scot\")\n  )\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region, nrow = 1, labeller = label_wrap_gen(width = 12)) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  ) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\",\n    \"Right\" = \"#67a9cf\",\n    \"Don't know\" = \"gray\"\n  )) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIn this application exercise we tell different stories with the same data.\n\nExercise 1 - Free scales\nAdd scales = \"free_x\" as an argument to the facet_wrap() function. How does the visualisation change? How is the story this visualisation telling different than the story the original plot tells?\n\nggplot(brexit, aes(y = opinion, fill = opinion)) +\n  geom_bar() +\n  facet_wrap(~region,\n    nrow = 1, labeller = label_wrap_gen(width = 12),\n    # ___\n  ) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Was Britain right/wrong to vote to leave EU?\",\n    subtitle = \"YouGov Survey Results, 2-3 September 2019\",\n    caption = \"Source: bit.ly/2lCJZVg\",\n    x = NULL, y = NULL\n  ) +\n  scale_fill_manual(values = c(\n    \"Wrong\" = \"#ef8a62\",\n    \"Right\" = \"#67a9cf\",\n    \"Don't know\" = \"gray\"\n  )) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nExercise 2 - Comparing proportions across facets\nFirst, calculate the proportion of wrong, right, and don’t know answers in each category and then plot these proportions (rather than the counts) and then improve axis labeling. How is the story this visualisation telling different than the story the original plot tells? Hint: You’ll need the scales package to improve axis labeling, which means you’ll need to load it on top of the document as well.\n\n\nExercise 3 - Comparing proportions across bars\nRecreate the same visualisation from the previous exercise, this time dodging the bars for opinion proportions for each region, rather than faceting by region and then improve the legend. How is the story this visualisation telling different than the story the previous plot tells?"
  }
]