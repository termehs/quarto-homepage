{
  "hash": "7cdf442666ceb0640a79fff7219d6ef8",
  "result": {
    "engine": "knitr",
    "markdown": "---\nauthor: Termeh Shafie\ntitle: Statistical Entropy Analysis of Network Data\nimage: featured.png\ntitle-block-style: none\ntoc: true\npriority: 1\ndescription: In this project, a general framework for using statistical entropies to capture interdependencies among node and tie variables in multivariate networks is developed. \ncategories:\n  - entropy\n---\n\n\n## Project summary\nIn multivariate statistics, there is an abundance of different measures of centrality and spread, many of which cannot be applied on variables measured on nominal or ordinal scale. Since network data in majority comprises such variables, alternative measures for analyzing spread, flatness and association is needed. This is also of particular relevance given the special feature of interdependent observations in networks. \n\nMultivariate entropy analysis is a general statistical method for analyzing and finding dependence structure in data consisting of repeated observations of variables with a common domain and with discrete finite range spaces. Only nominal scale is required for each variable, so only the size of the variable's range space is important but not its actual values. Variables on ordinal or numerical scales, even continuous numerical scales, can be used, but they should be aggregated so that their ranges match the number of available repeated observations. By investigating the frequencies of occurrences of joint variable outcomes, complicated dependence structures, partial independence and conditional independence as well as redundancies and functional dependence can be found.\n\nSince 2015, I am working with [Ove Frank](https://scholar.google.com/citations?user=9hTp2rsAAAAJ&hl=en) and [Krzysztof Nowicki](https://portal.research.lu.se/en/persons/krzysztof-nowicki) on a project in which we build a systematic framework for using statistical entropy tools to analyze network data.\n\nThe proposed framework is implemented in the R package 'netropy' and a description of various functions implemented in the package are given in the following. More details are provided in the package\n[vignettes](https://cran.r-project.org/web/packages/netropy/)  and the references listed.\n\n## R package `netropy` \n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n\n\n\n\n\n\n### Package overview \n\n<img src=\"hex_netropy.png\" align=\"right\" width=\"200px\"/>\n[![CRAN status](https://www.r-pkg.org/badges/version/netropy)](https://cran.r-project.org/package=netropy)\n[![CRAN Downloads](http://cranlogs.r-pkg.org/badges/netropy)](https://CRAN.R-project.org/package=netropy) \n\nThis package introduces these entropy tools in the context of network data. Brief description of various functions implemented in the package are given in the following but more details are provided in the package vignettes and the references listed.\n\n### Installation\n\nYou can install the released version of netropy from [CRAN](https://CRAN.R-project.org) with:\n\n``` r\ninstall.packages(\"netropy\")\n```\n\nThe development version from [GitHub](https://github.com/) with:\n\n``` r\n# install.packages(\"devtools\")\ndevtools::install_github(\"termehs/netropy\")\n```\n\nTo load the package:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary('netropy')\n```\n:::\n\n\n### Loading internal data\nThe different entropy tools are explained and illustrated by exploring data from a network study of a corporate law firm, which has previously been analysed by several authors \n([link](https://www.stats.ox.ac.uk/~snijders/siena/Lazega_lawyers_data.htm)).\nThe data set is included in the package as a list with objects representing adjacency matrices for each of the three networks advice (directed), friendship (directed) and co-work (undirected), together with a data frame comprising 8 attributes on each of the 71 lawyers.\n\nTo load the data, extract each object and assign the correct names to them:\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(lawdata) \nadj.advice <- lawdata[[1]]\nadj.friend <- lawdata[[2]]\nadj.cowork <-lawdata[[3]]\ndf.att <- lawdata[[4]]\n```\n:::\n\n\n## Variable domains and data editing\nA requirement for the applicability of these entropy tools is the specification of discrete variables with finite range spaces on the same domain: either node attributes/vertex variables, edges/dyad variables or triad variables. These can be either observed or transformed as shown in the following using the above example data set.\n\nWe have 8 vertex variables with 71 observations, two of which (`years` and `age`) are numerical and needs categorization based on their cumulative distributions. This categorization is in details described in the vignette  \"variable domains and data editing\". Here we just show the new dataframe created (note that variable `senior` is omitted as it only comprises unique values and that we edit all variable to start from 0):\n\n::: {.cell}\n\n```{.r .cell-code}\natt.var <-\n  data.frame(\n    status   = df.att$status-1,\n    gender   = df.att$gender,\n    office   = df.att$office-1,\n    years    = ifelse(df.att$years <= 3,0,\n                      ifelse(df.att$years <= 13,1,2)),\n    age      = ifelse(df.att$age <= 35,0,\n                      ifelse(df.att$age <= 45,1,2)),\n    practice = df.att$practice,\n    lawschool= df.att$lawschool-1\n    )\nhead(att.var)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  status gender office years age practice lawschool\n1      0      1      0     2   2        1         0\n2      0      1      0     2   2        0         0\n3      0      1      1     1   2        1         0\n4      0      1      0     2   2        0         2\n5      0      1      1     2   2        1         1\n6      0      1      1     2   2        1         0\n```\n\n\n:::\n:::\n\nThese vertex variables can be transformed into dyad variables by using the function `get_dyad_var()`. Observed node attributes in the dataframe `att_var` are then transformed into pairs of individual attributes. For example,  `status` with binary outcomes  is transformed into dyads having 4 possible outcomes (0,0), (0,1), (1,0), (1,1):\n\n::: {.cell}\n\n```{.r .cell-code}\ndyad.status    <- get_dyad_var(att.var$status, type = 'att')\ndyad.gender    <- get_dyad_var(att.var$gender, type = 'att')\ndyad.office    <- get_dyad_var(att.var$office, type = 'att')\ndyad.years     <- get_dyad_var(att.var$years, type = 'att')\ndyad.age       <- get_dyad_var(att.var$age, type = 'att')\ndyad.practice  <- get_dyad_var(att.var$practice, type = 'att')\ndyad.lawschool <- get_dyad_var(att.var$lawschool, type = 'att')\n```\n:::\n\nSimilarly, dyad variables can be created based on observed ties.\nFor the undirected edges, we use indicator variables read directly from the adjacency matrix for the dyad in question, while for the directed ones (`advice` and `friendship`) we have pairs of indicators representing sending and receiving ties with 4 possible outcomes :\n\n::: {.cell}\n\n```{.r .cell-code}\ndyad.cwk    <- get_dyad_var(adj.cowork, type = 'tie')\ndyad.adv    <- get_dyad_var(adj.advice, type = 'tie')\ndyad.frn    <- get_dyad_var(adj.friend, type = 'tie')\n```\n:::\n\nAll 10 dyad variables are merged into one data frame for subsequent entropy analysis:\n\n::: {.cell}\n\n```{.r .cell-code}\ndyad.var <-\n  data.frame(cbind(status   = dyad.status$var,\n                  gender    = dyad.gender$var,\n                  office    = dyad.office$var,\n                  years     = dyad.years$var,\n                  age       = dyad.age$var,\n                  practice  = dyad.practice$var,\n                  lawschool = dyad.lawschool$var,\n                  cowork    = dyad.cwk$var,\n                  advice    = dyad.adv$var,\n                  friend    = dyad.frn$var)\n                  )\nhead(dyad.var)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  status gender office years age practice lawschool cowork advice friend\n1      3      3      0     8   8        1         0      0      3      2\n2      3      3      3     5   8        3         0      0      0      0\n3      3      3      3     5   8        2         0      0      1      0\n4      3      3      0     8   8        1         6      0      1      2\n5      3      3      0     8   8        0         6      0      1      1\n6      3      3      1     7   8        1         6      0      1      1\n```\n\n\n:::\n:::\n\n\nA similar function  `get_triad_var()` is implemented for transforming vertex variables and different relation types into triad variables. This is described in more detail in the vignette  \"variable domains and data editing\".\n\n## Univariate, bivariate and trivariate entropies\nThe function `entropy_bivar()` computes the bivariate entropies of all pairs of variables in the dataframe. The output is given as an upper triangular matrix with cells giving the bivariate entropies of row and column variables. The diagonal thus gives the univariate entropies for each variable in the dataframe:\n\n::: {.cell}\n\n```{.r .cell-code}\nH2 <- entropy_bivar(dyad.var)\nH2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          status gender office years   age practice lawschool cowork advice\nstatus     1.493  2.868  3.640 3.370 3.912    3.453     4.363  2.092  2.687\ngender        NA  1.547  3.758 3.939 4.274    3.506     4.439  2.158  2.785\noffice        NA     NA  2.239 4.828 4.901    4.154     5.058  2.792  3.388\nyears         NA     NA     NA 2.671 4.857    4.582     5.422  3.268  3.868\nage           NA     NA     NA    NA 2.801    4.743     5.347  3.411  4.028\npractice      NA     NA     NA    NA    NA    1.962     4.880  2.530  3.127\nlawschool     NA     NA     NA    NA    NA       NA     2.953  3.567  4.186\ncowork        NA     NA     NA    NA    NA       NA        NA  0.615  1.687\nadvice        NA     NA     NA    NA    NA       NA        NA     NA  1.248\nfriend        NA     NA     NA    NA    NA       NA        NA     NA     NA\n          friend\nstatus     2.324\ngender     2.415\noffice     3.044\nyears      3.483\nage        3.637\npractice   2.831\nlawschool  3.812\ncowork     1.456\nadvice     1.953\nfriend     0.881\n```\n\n\n:::\n:::\n\nBivariate entropies can be used to detect redundant variables that should be omitted from the dataframe for further analysis. This occurs when the univariate entropy for a variable is equal to the bivariate entropies for pairs including that variable. \nAs seen above, the dataframe `dyad.var`  has no redundant variables. This can also be checked using the function `redundancy()` which yields  a binary matrix as output indicating which row and column variables are hold the same information:\n\n::: {.cell}\n\n```{.r .cell-code}\nredundancy(dyad.var)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nNULL\n```\n\n\n:::\n:::\n\nMore examples of using the function `redundancy()` is given in the vignette  \"univariate bivariate and trivariate entropies\".\n\nTrivariate entropies can be computed using the function `entropy_trivar()` which returns a dataframe with the first three columns representing possible triples of variables `V1`,`V2`, and `V3` from the dataframe in question, and their entropies `H(V1,V2,V3)` as the fourth column. We illustrated this on the dataframe `dyad.var`:\n\n::: {.cell}\n\n```{.r .cell-code}\nH3 <- entropy_trivar(dyad.var)\nhead(H3, 10) # view first 10 rows of dataframe\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       V1     V2        V3 H(V1,V2,V3)\n1  status gender    office       4.938\n2  status gender     years       4.609\n3  status gender       age       5.129\n4  status gender  practice       4.810\n5  status gender lawschool       5.664\n6  status gender    cowork       3.464\n7  status gender    advice       4.048\n8  status gender    friend       3.685\n9  status office     years       5.321\n10 status office       age       5.721\n```\n\n\n:::\n:::\n\n\n## Joint entropy and association graphs\nJoint entropies is a non-negative measure of association among pairs of variables. It is equal to 0 if and only if two variables are completely independent of each other.\n\nThe function `joint_entropy()` computes the joint entropies between all pairs of variables in a given dataframe and returns a list consisting of the upper triangular joint entropy matrix (univariate entropies in the diagonal) and a dataframe giving the frequency distributions of unique joint entropy values. A function argument specifies the precision given in number of decimals for which the frequency distribution of unique entropy values is created (default is 3). Applying the function on the  dataframe `dyad.var` with two decimals:\n\n::: {.cell}\n\n```{.r .cell-code}\nJ <- joint_entropy(dyad.var, 2)\nJ$matrix\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          status gender office years  age practice lawschool cowork advice\nstatus      1.49   0.17   0.09  0.79 0.38     0.00      0.08   0.02   0.05\ngender        NA   1.55   0.03  0.28 0.07     0.00      0.06   0.00   0.01\noffice        NA     NA   2.24  0.08 0.14     0.05      0.13   0.06   0.10\nyears         NA     NA     NA  2.67 0.61     0.05      0.20   0.02   0.05\nage           NA     NA     NA    NA 2.80     0.02      0.41   0.01   0.02\npractice      NA     NA     NA    NA   NA     1.96      0.04   0.05   0.08\nlawschool     NA     NA     NA    NA   NA       NA      2.95   0.00   0.01\ncowork        NA     NA     NA    NA   NA       NA        NA   0.62   0.18\nadvice        NA     NA     NA    NA   NA       NA        NA     NA   1.25\nfriend        NA     NA     NA    NA   NA       NA        NA     NA     NA\n          friend\nstatus      0.05\ngender      0.01\noffice      0.08\nyears       0.07\nage         0.05\npractice    0.01\nlawschool   0.02\ncowork      0.04\nadvice      0.18\nfriend      0.88\n```\n\n\n:::\n\n```{.r .cell-code}\nJ$freq\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      j  #(J = j) #(J >= j)\n1  0.79         1         1\n2  0.61         1         2\n3  0.41         1         3\n4  0.38         1         4\n5  0.28         1         5\n6   0.2         1         6\n7  0.18         2         8\n8  0.17         1         9\n9  0.14         1        10\n10 0.13         1        11\n11  0.1         1        12\n12 0.09         1        13\n13 0.08         4        17\n14 0.07         2        19\n15 0.06         2        21\n16 0.05         7        28\n17 0.04         2        30\n18 0.03         1        31\n19 0.02         5        36\n20 0.01         5        41\n21    0         4        45\n```\n\n\n:::\n:::\n\nAs seen, the strongest association is between the variables `status` and `years` with joint entropy values of 0.79. We have independence (joint entropy value of 0) between two pairs of variables: (`status`,`practice`), (`practise`,`gender`), (`cowork`,`gender`),and  (`cowork`,`lawschool`).\n\nThese results can be illustrated in a association graph using the function `assoc_graph()` which returns a `ggraph` object in which nodes represent variables and links represent strength of association (thicker links indicate stronger dependence). To use the function we need to load the `ggraph` library and to determine a threshold which the graph drawn is based on. We set it to 0.15 so that we only visualize the strongest associations\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggraph)\nassoc_graph(dyad.var, 0.15)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/assoc_g-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\nGiven this threshold, we see isolated and disconnected nodes representing independent variables. \nWe note strong dependence between the three dyadic variables `status`,`years` and `age`, but also a somewhat strong dependence among the three variables  `lawschool`, `years` and `age`, and the three variables `status`, `years` and `gender`. The association graph can also be interpreted as a tendency for  relations `cowork` and `friend` to be independent conditionally on relation `advice`, that is, any dependence between dyad variables `cowork` and `friend` is explained by `advice`.\n\nA threshold that gives a graph with reasonably many small independent or conditionally independent subsets of variables can be considered to represent a multivariate model for further testing.\n\nMore details and examples of joint entropies and association graphs are given in the vignette \"joint entropies and association graphs\". \n\n## Prediction power based on expected conditional entropies\nThe function `prediction_power()` computes prediction power when pairs of variables in a given dataframe are used to predict a third variable from the same dataframe. The variable to be predicted and the dataframe in which this variable also is part of is given as input arguments, and the output is an upper triangular matrix giving the expected conditional entropies of pairs of row and column variables (denoted $X$ and $Y$) of the matrix, i.e. *EH(Z|X,Y)* where $Z$ is the variable to be predicted. The diagonal gives *EH(Z|X)* , that is when only one variable as a predictor. Note that `NA`'s are in the row and column representing the variable being predicted.\n\nAssume we are interested in predicting variable `status` (that is whether a lawyer in the data set is an associate or partner). This is done by running the following syntax\n\n::: {.cell}\n\n```{.r .cell-code}\nprediction_power('status', dyad.var)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          status gender office years   age practice lawschool cowork advice\nstatus        NA     NA     NA    NA    NA       NA        NA     NA     NA\ngender        NA  1.375  1.180 0.670 0.855    1.304     1.225  1.306  1.263\noffice        NA     NA  2.147 0.493 0.820    1.374     1.245  1.373  1.325\nyears         NA     NA     NA 2.265 0.573    0.682     0.554  0.691  0.667\nage           NA     NA     NA    NA 1.877    1.089     0.958  1.087  1.052\npractice      NA     NA     NA    NA    NA    2.446     1.388  1.459  1.410\nlawschool     NA     NA     NA    NA    NA       NA     3.335  1.390  1.337\ncowork        NA     NA     NA    NA    NA       NA        NA  2.419  1.400\nadvice        NA     NA     NA    NA    NA       NA        NA     NA  2.781\nfriend        NA     NA     NA    NA    NA       NA        NA     NA     NA\n          friend\nstatus        NA\ngender     1.270\noffice     1.334\nyears      0.684\nage        1.058\npractice   1.427\nlawschool  1.350\ncowork     1.411\nadvice     1.407\nfriend     3.408\n```\n\n\n:::\n:::\n\nFor better readability, the powers of different predictors can be conveniently compared by using prediction plots that display a color matrix with rows for $X$ and columns for $Y$ with darker colors in the cells when we have higher prediction power for $Z$. \n\nMore details and examples of expected conditional entropies and prediction power  are given in the package [vignette](https://cran.r-project.org/web/packages/netropy/vignettes/prediction_power.html). \n\n## Divergence Tests of Goodness of Fit\nOccurring cliques in association graphs represent connected components of dependent variables, and by comparing the graphs for different thresholds, specific structural models of multivariate dependence can be suggested and tested.  The function `div_gof()` allows such hypothesis tests for pairwise independence of $X$ and $Y$: $X \\bot Y$, and pairwise independence conditional a third variable $Z$: $X\\bot Y|Z$.\n\nTo test `friend`$\\bot$ `cowork`$|$`advice`, that is whether dyad variable `friend` is independent of `cowork` given `advice` we use the function as shown below:\n\n::: {.cell}\n\n```{.r .cell-code}\ndiv_gof(dat = dyad.var, var1 = \"friend\", var2 = \"cowork\", var_cond = \"advice\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     D df(D)\n1 0.94    12\n```\n\n\n:::\n:::\n\nNot specifying argument `var_cond` would instead test `friend`$\\bot$`cowork` without any conditioning.\n\n## References\n\nParts of the theoretical background is provided in the package vignettes, but for more details, consult the following literature:\n\n* Frank, O., & Shafie, T. (2016). Multivariate entropy analysis of network data.\n*Bulletin of Sociological Methodology/Bulletin de MÃ©thodologie Sociologique*, 129(1), 45-63. [link](/publications/multivariate_entropy_analysis/index.html)\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}