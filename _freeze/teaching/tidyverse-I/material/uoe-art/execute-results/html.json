{
  "hash": "d7396f64ab30c52797a8d67c71651e1b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"University of Edinburgh Art Collection\"\nauthor: \"Termeh Shafie <br> (adapted from original script by Mine Ã‡etinkaya-Rundel)\"\nformat:\n  html:\n    embed-resources: true\neditor: visual\nexecute:\n  cache:  false\n---\n\n\nThe University of Edinburgh Art Collection *\"supports the world-leading research and teaching that happens within the University. Comprised of an astonishing range of objects and ideas spanning two millennia and a multitude of artistic forms, the collection reflects not only the long and rich trajectory of the University, but also major national and international shifts in art history.\"*\n\n\n\nIn this practical we'll scrape data on all art pieces in the [Edinburgh College of Art collection](https://collections.ed.ac.uk/art/search/*:*/Collection:%22edinburgh+college+of+art%7C%7C%7CEdinburgh+College+of+Art%22).\n\n# Learning goals\n\n-   Working with R scripts\n-   Web scraping from a single page\n-   Writing functions\n-   Iteration by mapping functions\n-   Writing data out\n\n\n\n\n![](img/selectorgadget.png)\n\nIn order to complete this assignment you will need a Chrome browser with the [Selector Gadget extension](http://selectorgadget.com/) installed.\n\n\n\n## R scripts vs. Quarto documents\n\nToday you'll be using both R scripts and R Markdown documents:\n\n-  use R scripts in the web scraping stage and ultimately save the scraped data as a csv.\n\n-  use an Quarto document in the web analysis stage, where we start off by reading in the csv file we wrote out in the scraping stage.\n\n\n\n## Packages\n\nWe'll use the **tidyverse** package for much of the data wrangling and visualisation, the **robotstxt** package to check if we're allowed to scrape the data, the **rvest** package for data scraping.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) \nlibrary(robotstxt)\nlibrary(rvest)\n```\n:::\n\n\n## Data\n\nThis assignment does not come with any prepared datasets.\nInstead you'll be scraping the data!\nBut before doing so, let's check that a bot has permissions to access pages on this domain.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npaths_allowed(\"https://collections.ed.ac.uk/art)\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n collections.ed.ac.uk                      \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n# Exercises\n\n## Scraping a single page\n\n\nWe will start off by scraping data on the first 10 pieces in the collection from [here](https://collections.ed.ac.uk/art/search/*:*/Collection:%22edinburgh+college+of+art%7C%7C%7CEdinburgh+College+of+Art%22?offset=0).\n\nFirst, we define a new object called `first_url`, which is the link above.\nThen, we read the page at this url with the `read_html()` function from the **rvest** package.\nThe code for this is already provided in `01-scrape-page-one.R`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set url\nfirst_url <- \"https://collections.ed.ac.uk/art/search/*:*/Collection:%22edinburgh+college+of+art%7C%7C%7CEdinburgh+College+of+Art%22?offset=0\"\n\n# read html page\npage <- read_html(first_url)\n```\n:::\n\n\nFor the ten pieces on this page we will extract `title`, `artist`, and `link` information, and put these three variables in a data frame.\n\n### Titles\n\nLet's start with titles.\nWe make use of the SelectorGadget to identify the tags for the relevant nodes:\n\n\n::: {.cell fig.margin='true'}\n::: {.cell-output-display}\n![](img/iteminfo-h3a.gif)\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npage %>%\n  html_nodes(\".iteminfo\") %>%\n  html_node(\"h3 a\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n{xml_nodeset (10)}\n [1] <a href=\"./record/20696?highlight=*:*\">South Frieze of the Parthenon Fri ...\n [2] <a href=\"./record/53701?highlight=*:*\">Espresso Cup                      ...\n [3] <a href=\"./record/99347?highlight=*:*\">Untitled - Two Apes Sun Bathing   ...\n [4] <a href=\"./record/21212?highlight=*:*\">Portrait of a Seated Woman        ...\n [5] <a href=\"./record/21289?highlight=*:*\">Seated Male Nude                  ...\n [6] <a href=\"./record/99370?highlight=*:*\">Nighttime Scene of the City and R ...\n [7] <a href=\"./record/21178?highlight=*:*\">Portrait of Man in Red Jacket     ...\n [8] <a href=\"./record/20743?highlight=*:*\">Harbour Scene 'KY16'              ...\n [9] <a href=\"./record/21568?highlight=*:*\">Untitled                          ...\n[10] <a href=\"./record/102688?highlight=*:*\">Machine stitched net             ...\n```\n\n\n:::\n:::\n\n\nThen we extract the text with `html_text()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npage %>%\n  html_nodes(\".iteminfo\") %>%\n  html_node(\"h3 a\") %>%\n  html_text()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"South Frieze of the Parthenon Frieze                                                                            (1836-1837)\"\n [2] \"Espresso Cup                                    \"                                                                           \n [3] \"Untitled - Two Apes Sun Bathing                                                                            (1963)\"          \n [4] \"Portrait of a Seated Woman                                                                            (1954)\"               \n [5] \"Seated Male Nude                                                                            (1961)\"                         \n [6] \"Nighttime Scene of the City and River                                                                            (1962)\"    \n [7] \"Portrait of Man in Red Jacket                                                                            (1968)\"            \n [8] \"Harbour Scene 'KY16'                                                                            (1964)\"                     \n [9] \"Untitled                                                                            (May 1987)\"                             \n[10] \"Machine stitched net                                                                            (1946)\"                     \n```\n\n\n:::\n:::\n\n\nAnd get rid of all the spurious white space in the text with `str_squish()`, which reduces repeated whitespace inside a string.\n\nTake a look at the help for `str_squish()` to find out more about how it works and how it's different from `str_trim()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npage %>%\n  html_nodes(\".iteminfo\") %>%\n  html_node(\"h3 a\") %>%\n  html_text() %>% \n  str_squish()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"South Frieze of the Parthenon Frieze (1836-1837)\"\n [2] \"Espresso Cup\"                                    \n [3] \"Untitled - Two Apes Sun Bathing (1963)\"          \n [4] \"Portrait of a Seated Woman (1954)\"               \n [5] \"Seated Male Nude (1961)\"                         \n [6] \"Nighttime Scene of the City and River (1962)\"    \n [7] \"Portrait of Man in Red Jacket (1968)\"            \n [8] \"Harbour Scene 'KY16' (1964)\"                     \n [9] \"Untitled (May 1987)\"                             \n[10] \"Machine stitched net (1946)\"                     \n```\n\n\n:::\n:::\n\n\nAnd finally save the resulting data as a vector of length 10:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitles <- page %>%\n  html_nodes(\".iteminfo\") %>%\n  html_node(\"h3 a\") %>%\n  html_text() %>%\n  str_squish()\n```\n:::\n\n\n### Links\n\nThe same nodes that contain the text for the titles also contains information on the links to individual art piece pages for each title.\nWe can extract this information using a new function from the rvest package, `html_attr()`, which extracts attributes.\n\nA mini HTML lesson!\nThe following is how we define hyperlinked text in HTML:\n\n    <a href=\"https://www.google.com\">Search on Google</a>\n\nAnd this is how the text would look like on a webpage: [Search on Google](https://www.google.com).\n\nHere the text is `Search on Google` and the `href` attribute contains the url of the website you'd go to if you click on the hyperlinked text: `https://www.google.com`.\n\nThe moral of the story is: the link is stored in the `href` attribute.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npage %>%\n  html_nodes(\".iteminfo\") %>%   # same nodes\n  html_node(\"h3 a\") %>%         # as before\n  html_attr(\"href\")             # but get href attribute instead of text\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"./record/20696?highlight=*:*\"  \"./record/53701?highlight=*:*\" \n [3] \"./record/99347?highlight=*:*\"  \"./record/21212?highlight=*:*\" \n [5] \"./record/21289?highlight=*:*\"  \"./record/99370?highlight=*:*\" \n [7] \"./record/21178?highlight=*:*\"  \"./record/20743?highlight=*:*\" \n [9] \"./record/21568?highlight=*:*\"  \"./record/102688?highlight=*:*\"\n```\n\n\n:::\n:::\n\n\nThese don't really look like URLs as we know then though.\nThey're relative links.\n\n\nSee the help for `str_replace()` to find out how it works. Remember that the first argument is passed in from the pipeline, so you just need to define the `pattern` and `replacement` arguments.\n\n\n1.  Click on one of art piece titles in your browser and take note of the url of the webpage it takes you to. Think about how that url compares to what we scraped above? How is it different? Using `str_replace()`, fix the URLs. You'll note something special happening in the `pattern` to replace. We want to replace the `.`, but we have it as `\\\\.`. This is because the period `.` is a special character and so we need to escape it first with backslashes, `\\\\`s.\n\n### Artists\n\n2.  Fill in the blanks to scrape artist names.\n\n### Put it altogether\n\n3.  Fill in the blanks to organize everything in a tibble.\n\n### Scrape the next page\n\n4.  Click on the next page, and grab its url. Fill in the blank in to define a new object: `second_url`. Copy-paste code from top of the R script to scrape the new set of art pieces, and save the resulting data frame as `second_ten`.\n\n\n## Functions\n\n\nYou've been using R functions, now it's time to write your own!\n\nLet's start simple.\nHere is a function that takes in an argument `x`, and adds 2 to it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadd_two <- function(x){\n  x + 2\n}\n```\n:::\n\n\nLet's test it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadd_two(3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5\n```\n\n\n:::\n\n```{.r .cell-code}\nadd_two(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 12\n```\n\n\n:::\n:::\n\n\nThe skeleton for defining functions in R is as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfunction_name <- function(input){\n  # do something with the input(s)\n  # return something\n}\n```\n:::\n\n\nThen, a function for scraping a page should look something like:\n\n**Reminder:** Function names should be short but evocative verbs.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfunction_name <- function(url){\n  # read page at url\n  # extract title, link, artist info for n pieces on page\n  # return a n x 3 tibble\n}\n```\n:::\n\n\n5.  Fill in the blanks using code you already developed in the previous exercises. Name the function `scrape_page`.\n\nTest out your new function by running the following in the console.\nDoes the output look right?\nDiscuss with teammates whether you're getting the same results as before.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscrape_page(first_url)\nscrape_page(second_url)\n```\n:::\n\n\n\n## Iteration\n\nWe went from manually scraping individual pages to writing a function to do the same.\nNext, we will work on making our workflow a little more efficient by using R to iterate over all pages that contain information on the art collection.\n\n\nThat means we give develop a list of URLs (of pages that each have 10 art pieces), and write some code that applies the `scrape_page()` function to each page, and combines the resulting data frames from each page into a single data frame with 3289 rows and 3 columns.\n\n### List of URLs\n\nClick through the first few of the pages in the art collection and observe their URLs to confirm the following pattern:\n\n    [sometext]offset=0     # Pieces 1-10\n    [sometext]offset=10    # Pieces 11-20\n    [sometext]offset=20    # Pieces 21-30\n    [sometext]offset=30    # Pieces 31-40\n    ...\n    [sometext]offset=3280  # Pieces 3281-3289\n\nWe can construct these URLs in R by pasting together two pieces: (1) a common (`root`) text for the beginning of the URL, and (2) numbers starting at 0, increasing by 10, all the way up to 3289.\nTwo new functions are helpful for accomplishing this: `glue()` for pasting two pieces of text and `seq()` for generating a sequence of numbers.\n\n6.  Fill in the blanks to construct the list of URLs.\n\n### Mapping\n\nFinally, we're ready to iterate over the list of URLs we constructed.\nWe will do this by **map**ping the function we developed over the list of URLs.\nThere are a series of mapping functions in R and they each take the following form:\n\n    map([x], [function to apply to each element of x])\n\nIn our case `x` is the list of URLs we constructed and the function to apply to each element of `x` is the function we developed earlier, `scrape_page`.\nAnd as a result we want a data frame, so we use `map_dfr` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmap_dfr(urls, scrape_page)\n```\n:::\n\n\n7.  Fill in the blanks to scrape all pages, and to create a new data frame called `uoe_art`.\n\n### Write out data\n\n8.  Finally write out the data frame you constructed into the `data` folder so that you can use it in the analysis section.\n\n\n## Analysis \nFor the rest of the exercises you can work in Quarto/R Markdown.\n\nNow that we have a tidy dataset that we can analyze, let's do that!\n\nWe'll start with some data cleaning, to clean up the dates that appear at the end of some title text in parentheses.\nSome of these are years, others are more specific dates, some art pieces have no date information whatsoever, and others have some non-date information in parentheses.\nThis should be interesting to clean up!\n\nFirst thing we'll try is to separate the `title` column into two: one for the actual `title` and the other for the `date` if it exists.\nIn human speak, we need to\n\n\"separate the `title` column at the first occurrence of `(` and put the contents on one side of the `(` into a column called `title` and the contents on the other side into a column called `date`\"\n\nLuckily, there's a function that does just this: `separate()`!\n\nAnd once we have completed separating the single `title` column into `title` and `date`, we need to do further clean-up in the `date` column to get rid of extraneous `)`s with `str_remove()`, capture year information, and save the data as a numeric variable.\n\n\n\n9.  Fill in the blanks in to implement the data wrangling we described above. Note that this will result in some warnings when you run the code, and that's OK! Read the warnings, and explain what they mean, and why we are ok with leaving them in given that our objective is to just capture `year` where it's convenient to do so.\n\n\n10. Print out a summary of the data frame using the `skim()` function. How many pieces have artist info missing? How many have year info missing?\n11. Make a histogram of years. Use a reasonable binwidth. Do you see anything out of the ordinary?\n\n\n\n12. Find which piece has the out of the ordinary year and go to its page on the art collection website to find the correct year for it. Can you tell why our code didn't capture the correct year information? Correct the error in the data frame and visualize the data again.\n\n\n**Hint:** You'll want to use `mutate()` and `if_else()` or `case_when()` to implement the correction.\n\n13. Who is the most commonly featured artist in the collection? Do you know them? Any guess as to why the university has so many pieces from them?\n\n\n14. Final question! How many art pieces have the word \"child\" in their title? Try to figure it out, and ask for help if you're stuck.\n\n\n**Hint:** `str_subset()` can be helful here. You should consider how you might capture titles where the word appears as \"child\" and \"Child\".\n\n\nSource: <https://collections.ed.ac.uk/art/about>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}