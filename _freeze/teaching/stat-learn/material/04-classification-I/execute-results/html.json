{
  "hash": "3d85ea1a314ca80fd5cd31c320795d8b",
  "result": {
    "engine": "knitr",
    "markdown": "---\neditor: \n  markdown: \n    wrap: sentence\n---\n\n::: {.cell}\n\n:::\n\n\n# Classification Methods\n\n## The Stock Market Data\n\nWe will begin by examining some numerical and graphical summaries of the `Smarket` data, which is part of the `ISLR2` library.\nThis data set consists of percentage returns for the S&P 500 stock index over $1,250$ days, from the beginning of 2001 until the end of 2005.\nFor each date, we have recorded the percentage returns for each of the five previous trading days, `lagone` through `lagfive`.\nWe have also recorded `volume` (the number of shares traded on the previous day, in billions), `Today` (the percentage return on the date in question) and `direction` (whether the market was `Up` or `Down` on this date).\nOur goal is to predict `direction` (a qualitative response) using the other features.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ISLR2)\nnames(Smarket)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n[7] \"Volume\"    \"Today\"     \"Direction\"\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(Smarket)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1250    9\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(Smarket)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Year           Lag1                Lag2                Lag3          \n Min.   :2001   Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.922000  \n 1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500   1st Qu.:-0.640000  \n Median :2003   Median : 0.039000   Median : 0.039000   Median : 0.038500  \n Mean   :2003   Mean   : 0.003834   Mean   : 0.003919   Mean   : 0.001716  \n 3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.596750  \n Max.   :2005   Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.733000  \n      Lag4                Lag5              Volume           Today          \n Min.   :-4.922000   Min.   :-4.92200   Min.   :0.3561   Min.   :-4.922000  \n 1st Qu.:-0.640000   1st Qu.:-0.64000   1st Qu.:1.2574   1st Qu.:-0.639500  \n Median : 0.038500   Median : 0.03850   Median :1.4229   Median : 0.038500  \n Mean   : 0.001636   Mean   : 0.00561   Mean   :1.4783   Mean   : 0.003138  \n 3rd Qu.: 0.596750   3rd Qu.: 0.59700   3rd Qu.:1.6417   3rd Qu.: 0.596750  \n Max.   : 5.733000   Max.   : 5.73300   Max.   :3.1525   Max.   : 5.733000  \n Direction \n Down:602  \n Up  :648  \n           \n           \n           \n           \n```\n\n\n:::\n\n```{.r .cell-code}\npairs(Smarket)\n```\n\n::: {.cell-output-display}\n![](04-classification-I_files/figure-html/chunk1-1.png){width=672}\n:::\n:::\n\n\nThe `cor()` function produces a matrix that contains all of the pairwise correlations among the predictors in a data set.\nThe first command below gives an error message because the `direction` variable is qualitative.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(Smarket)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in cor(Smarket): 'x' must be numeric\n```\n\n\n:::\n\n```{.r .cell-code}\ncor(Smarket[, -9])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Year         Lag1         Lag2         Lag3         Lag4\nYear   1.00000000  0.029699649  0.030596422  0.033194581  0.035688718\nLag1   0.02969965  1.000000000 -0.026294328 -0.010803402 -0.002985911\nLag2   0.03059642 -0.026294328  1.000000000 -0.025896670 -0.010853533\nLag3   0.03319458 -0.010803402 -0.025896670  1.000000000 -0.024051036\nLag4   0.03568872 -0.002985911 -0.010853533 -0.024051036  1.000000000\nLag5   0.02978799 -0.005674606 -0.003557949 -0.018808338 -0.027083641\nVolume 0.53900647  0.040909908 -0.043383215 -0.041823686 -0.048414246\nToday  0.03009523 -0.026155045 -0.010250033 -0.002447647 -0.006899527\n               Lag5      Volume        Today\nYear    0.029787995  0.53900647  0.030095229\nLag1   -0.005674606  0.04090991 -0.026155045\nLag2   -0.003557949 -0.04338321 -0.010250033\nLag3   -0.018808338 -0.04182369 -0.002447647\nLag4   -0.027083641 -0.04841425 -0.006899527\nLag5    1.000000000 -0.02200231 -0.034860083\nVolume -0.022002315  1.00000000  0.014591823\nToday  -0.034860083  0.01459182  1.000000000\n```\n\n\n:::\n:::\n\n\nAs one would expect, the correlations between the lag variables and today's returns are close to zero.\nIn other words, there appears to be little correlation between today's returns and previous days' returns.\nThe only substantial correlation is between `Year` and `volume`.\nBy plotting the data, which is ordered chronologically, we see that `volume` is increasing over time.\nIn other words, the average number of shares traded daily increased from 2001 to 2005.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nattach(Smarket)\nplot(Volume)\n```\n\n::: {.cell-output-display}\n![](04-classification-I_files/figure-html/chunk3-1.png){width=672}\n:::\n:::\n\n\n## Logistic Regression\n\nNext, we will fit a logistic regression model in order to predict `direction` using `lagone` through `lagfive` and `volume`.\nThe `glm()` function can be used to fit many types of generalized linear models, including logistic regression.\nThe syntax of the `glm()` function is similar to that of `lm()`, except that we must pass in the argument `family = binomial` in order to tell `R` to run a logistic regression rather than some other type of generalized linear model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm.fits <- glm(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket, family = binomial\n  )\nsummary(glm.fits)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n    Volume, family = binomial, data = Smarket)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)\n(Intercept) -0.126000   0.240736  -0.523    0.601\nLag1        -0.073074   0.050167  -1.457    0.145\nLag2        -0.042301   0.050086  -0.845    0.398\nLag3         0.011085   0.049939   0.222    0.824\nLag4         0.009359   0.049974   0.187    0.851\nLag5         0.010313   0.049511   0.208    0.835\nVolume       0.135441   0.158360   0.855    0.392\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1731.2  on 1249  degrees of freedom\nResidual deviance: 1727.6  on 1243  degrees of freedom\nAIC: 1741.6\n\nNumber of Fisher Scoring iterations: 3\n```\n\n\n:::\n:::\n\n\nThe smallest $p$-value here is associated with `lagone`.\nThe negative coefficient for this predictor suggests that if the market had a positive return yesterday, then it is less likely to go up today.\nHowever, at a value of $0.15$, the $p$-value is still relatively large, and so there is no clear evidence of a real association between `lagone` and `direction`.\n\n\nThe `predict()` function can be used to predict the probability that the market will go up, given values of the predictors.\nThe `type = \"response\"` option tells `R` to output probabilities of the form $P(Y=1|X)$, as opposed to other information such as the logit.\nIf no data set is supplied to the `predict()` function, then the probabilities are computed for the training data that was used to fit the logistic regression model.\nHere we have printed only the first ten probabilities.\nWe know that these values correspond to the probability of the market going up, rather than down, because the `contrasts()` function indicates that `R` has created a dummy variable with a 1 for `Up`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm.probs <- predict(glm.fits, type = \"response\")\nglm.probs[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        1         2         3         4         5         6         7         8 \n0.5070841 0.4814679 0.4811388 0.5152224 0.5107812 0.5069565 0.4926509 0.5092292 \n        9        10 \n0.5176135 0.4888378 \n```\n\n\n:::\n\n```{.r .cell-code}\ncontrasts(Direction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Up\nDown  0\nUp    1\n```\n\n\n:::\n:::\n\n\nIn order to make a prediction as to whether the market will go up or down on a particular day, we must convert these predicted probabilities into class labels, `Up` or `Down`.\nThe following two commands create a vector of class predictions based on whether the predicted probability of a market increase is greater than or less than $0.5$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm.pred <- rep(\"Down\", 1250)\nglm.pred[glm.probs > .5] = \"Up\"\n```\n:::\n\n\nThe first command creates a vector of 1,250 `Down` elements.\nThe second line transforms to `Up` all of the elements for which the predicted probability of a market increase exceeds $0.5$.\nGiven these predictions, the `table()` function can be used to produce a confusion matrix in order to determine how many observations were correctly or incorrectly classified.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(glm.pred, Direction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        Direction\nglm.pred Down  Up\n    Down  145 141\n    Up    457 507\n```\n\n\n:::\n\n```{.r .cell-code}\n(507 + 145) / 1250\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5216\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(glm.pred == Direction)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5216\n```\n\n\n:::\n:::\n\n\nThe diagonal elements of the confusion matrix indicate correct predictions, while the off-diagonals represent incorrect predictions.\nHence our model correctly predicted that the market would go up on $507$ days and that it would go down on $145$ days, for a total of $507+145 = 652$ correct predictions.\nThe `mean()` function can be used to compute the fraction of days for which the prediction was correct.\nIn this case, logistic regression correctly predicted the movement of the market $52.2$ % of the time.\n\nAt first glance, it appears that the logistic regression model is working a little better than random guessing.\nHowever, this result is misleading because we trained and tested the model on the same set of $1,250$ observations.\nIn other words, $100\\%-52.2\\%=47.8\\%$, is the *training* error rate.\nAs we have seen previously, the training error rate is often overly optimistic---it tends to underestimate the test error rate.\nIn order to better assess the accuracy of the logistic regression model in this setting, we can fit the model using part of the data, and then examine how well it predicts the *held out* data.\n\nTo implement this strategy, we will first create a vector corresponding to the observations from 2001 through 2004.\nWe will then use this vector to create a held out data set of observations from 2005.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain <- (Year < 2005)\nSmarket.2005 <- Smarket[!train, ]\ndim(Smarket.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 252   9\n```\n\n\n:::\n\n```{.r .cell-code}\nDirection.2005 <- Direction[!train]\n```\n:::\n\n\nThe object `train` is a vector of $1{,}250$ elements, corresponding to the observations in our data set.\nThe elements of the vector that correspond to observations that occurred before 2005 are set to `TRUE`, whereas those that correspond to observations in 2005 are set to `FALSE`.\nThe object `train` is a *Boolean* vector, since its elements are `TRUE` and `FALSE`.\nBoolean vectors can be used to obtain a subset of the rows or columns of a matrix.\nFor instance, the command `Smarket[train, ]` would pick out a submatrix of the stock market data set, corresponding only to the dates before 2005, since those are the ones for which the elements of `train` are `TRUE`.\nThe `!` symbol can be used to reverse all of the elements of a Boolean vector.\nThat is, `!train` is a vector similar to `train`, except that the elements that are `TRUE` in `train` get swapped to `FALSE` in `!train`, and the elements that are `FALSE` in `train` get swapped to `TRUE` in `!train`.\nTherefore, `Smarket[!train, ]` yields a submatrix of the stock market data containing only the observations for which `train` is `FALSE`---that is, the observations with dates in 2005.\nThe output above indicates that there are 252 such observations.\n\nWe now fit a logistic regression model using only the subset of the observations that correspond to dates before 2005, using the `subset` argument.\nWe then obtain predicted probabilities of the stock market going up for each of the days in our test set---that is, for the days in 2005.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm.fits <- glm(\n    Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n    data = Smarket, family = binomial, subset = train\n  )\nglm.probs <- predict(glm.fits, Smarket.2005,\n    type = \"response\")\n```\n:::\n\n\nNotice that we have trained and tested our model on two completely separate data sets: training was performed using only the dates before 2005, and testing was performed using only the dates in 2005.\nFinally, we compute the predictions for 2005 and compare them to the actual movements of the market over that time period.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm.pred <- rep(\"Down\", 252)\nglm.pred[glm.probs > .5] <- \"Up\"\ntable(glm.pred, Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        Direction.2005\nglm.pred Down Up\n    Down   77 97\n    Up     34 44\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(glm.pred == Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4801587\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(glm.pred != Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5198413\n```\n\n\n:::\n:::\n\n\nThe `!=` notation means *not equal to*, and so the last command computes the test set error rate.\nThe results are rather disappointing: the test error rate is $52$ %, which is worse than random guessing!\nOf course this result is not all that surprising, given that one would not generally expect to be able to use previous days' returns to predict future market performance.\n(After all, if it were possible to do so, then the authors of this book would be out striking it rich rather than writing a statistics textbook.)\n\nWe recall that the logistic regression model had very underwhelming $p$-values associated with all of the predictors, and that the smallest $p$-value, though not very small, corresponded to `lagone`.\nPerhaps by removing the variables that appear not to be helpful in predicting `direction`, we can obtain a more effective model.\nAfter all, using predictors that have no relationship with the response tends to cause a deterioration in the test error rate (since such predictors cause an increase in variance without a corresponding decrease in bias), and so removing such predictors may in turn yield an improvement.\nBelow we have refit the logistic regression using just `lagone` and `lagtwo`, which seemed to have the highest predictive power in the original logistic regression model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm.fits <- glm(Direction ~ Lag1 + Lag2, data = Smarket,\n    family = binomial, subset = train)\nglm.probs <- predict(glm.fits, Smarket.2005,\n    type = \"response\")\nglm.pred <- rep(\"Down\", 252)\nglm.pred[glm.probs > .5] <- \"Up\"\ntable(glm.pred, Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        Direction.2005\nglm.pred Down  Up\n    Down   35  35\n    Up     76 106\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(glm.pred == Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5595238\n```\n\n\n:::\n\n```{.r .cell-code}\n106 / (106 + 76)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5824176\n```\n\n\n:::\n:::\n\n\nNow the results appear to be a little better: $56\\%$ of the daily movements have been correctly predicted.\nIt is worth noting that in this case, a much simpler strategy of predicting that the market will increase every day will also be correct $56\\%$ of the time!\nHence, in terms of overall error rate, the logistic regression method is no better than the naive approach.\nHowever, the confusion matrix shows that on days when logistic regression predicts an increase in the market, it has a $58\\%$ accuracy rate.\nThis suggests a possible trading strategy of buying on days when the model predicts an increasing market, and avoiding trades on days when a decrease is predicted.\nOf course one would need to investigate more carefully whether this small improvement was real or just due to random chance.\n\nSuppose that we want to predict the returns associated with particular values of `lagone` and `lagtwo`.\nIn particular, we want to predict `direction` on a day when `lagone` and `lagtwo` equal 1.2 and\\~1.1, respectively, and on a day when they equal 1.5 and \\$-\\$0.8.\nWe do this using the `predict()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(glm.fits,\n    newdata =\n      data.frame(Lag1 = c(1.2, 1.5),  Lag2 = c(1.1, -0.8)),\n    type = \"response\"\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        1         2 \n0.4791462 0.4960939 \n```\n\n\n:::\n:::\n\n\n\n### Exercise\n\n- Create a new `DirectionNum` column to store your variable as numeric (0/1)\n\n- Fit a linear model (Y ~ lag1 + lag2) using that new column and compare the estimates with the logistic model.\n\n\n## ROC curves\n\n\nROC curves are a great tool to assess our classification performance. The `pROC` package gives us an easy way to create one. Please make sure you have it installed!\n\n\n\nA ROC curve plots the sensitivity and specificity of our classifier.\n\nRemember:\n\nsensitivity = TP / TP + FN\n\nspecificity = TN / TN + FP\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages('pROC')\nlibrary(pROC)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nType 'citation(\"pROC\")' for a citation.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'pROC'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n```\n\n\n:::\n:::\n\n\n\nThis is a more explicit way to subset, again dividing via the `train` object.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSmarket_train = subset(Smarket, train ) # pre 2005\nSmarket_test = subset(Smarket, !train ) # post 2005\n```\n:::\n\n\n\n\nFor the pROC function `roc()` we need our data stored in numeric form, so let's fit our model and assign new columns to our df.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm.fits = glm(Direction ~ Lag1 + Lag2,\n               data = Smarket_train,\n               family = binomial(link = \"logit\"))\n\nglm.probs = predict(glm.fits, Smarket_test, type = \"response\")\nglm.pred <- rep(0, 252) \nglm.pred[glm.probs > .5] <- 1 # 0/1 instead of up/down\n\nSmarket_test$Preds = glm.pred\n\n# $Direction uses named factors, stored as 1/2, so we take -1 \n\nSmarket_test$DirectionNum = as.numeric(Smarket_test$Direction) - 1 \n```\n:::\n\n\n\nWe create a roc_object by using the true values and predictions of our test set\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nroc_obj = roc(Smarket_test$DirectionNum, Smarket_test$Preds)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting levels: control = 0, case = 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting direction: controls < cases\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(roc_obj)\n```\n\n::: {.cell-output-display}\n![](04-classification-I_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\nThis looks terrible! Both visually and performance wise. As we can see our curve is extremely close to the diagonal which indicates a completely random guess (50:50). This is in line with our previous assessment which gave us $56\\%$. \n\n\nWe can spice up the curve a bit by adding CIs, or add some colour, but pROC is a very base-R friendly package so feel free to shop around for some more \"modern\" implementations.\n \n \n\n::: {.cell}\n\n```{.r .cell-code}\nroc_obj = roc(Smarket_test$DirectionNum, Smarket_test$Preds)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting levels: control = 0, case = 1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSetting direction: controls < cases\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(roc_obj, print.auc =TRUE)\nplot(ci.se(roc_obj), type=\"shape\", col=\"lightblue\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in plot.ci.se(ci.se(roc_obj), type = \"shape\", col = \"lightblue\"): Low\ndefinition shape.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](04-classification-I_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(sens.ci, type=\"bars\")\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'sens.ci' not found\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n## Poisson Regression\n\nFinally, we fit a Poisson regression model to the `Bikeshare` data set, which measures the number of bike rentals (`bikers`) per hour in Washington, DC. The data can be found in the `ISLR2` library.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nattach(Bikeshare)\ndim(Bikeshare)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 8645   15\n```\n\n\n:::\n\n```{.r .cell-code}\nnames(Bikeshare)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"season\"     \"mnth\"       \"day\"        \"hr\"         \"holiday\"   \n [6] \"weekday\"    \"workingday\" \"weathersit\" \"temp\"       \"atemp\"     \n[11] \"hum\"        \"windspeed\"  \"casual\"     \"registered\" \"bikers\"    \n```\n\n\n:::\n:::\n\n\nWe begin by fitting a least squares linear regression model to the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod.lm <- lm(\n    bikers ~ mnth + hr + workingday + temp + weathersit,\n    data = Bikeshare\n  )\nsummary(mod.lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, \n    data = Bikeshare)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-299.00  -45.70   -6.23   41.08  425.29 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                -68.632      5.307 -12.932  < 2e-16 ***\nmnthFeb                      6.845      4.287   1.597 0.110398    \nmnthMarch                   16.551      4.301   3.848 0.000120 ***\nmnthApril                   41.425      4.972   8.331  < 2e-16 ***\nmnthMay                     72.557      5.641  12.862  < 2e-16 ***\nmnthJune                    67.819      6.544  10.364  < 2e-16 ***\nmnthJuly                    45.324      7.081   6.401 1.63e-10 ***\nmnthAug                     53.243      6.640   8.019 1.21e-15 ***\nmnthSept                    66.678      5.925  11.254  < 2e-16 ***\nmnthOct                     75.834      4.950  15.319  < 2e-16 ***\nmnthNov                     60.310      4.610  13.083  < 2e-16 ***\nmnthDec                     46.458      4.271  10.878  < 2e-16 ***\nhr1                        -14.579      5.699  -2.558 0.010536 *  \nhr2                        -21.579      5.733  -3.764 0.000168 ***\nhr3                        -31.141      5.778  -5.389 7.26e-08 ***\nhr4                        -36.908      5.802  -6.361 2.11e-10 ***\nhr5                        -24.135      5.737  -4.207 2.61e-05 ***\nhr6                         20.600      5.704   3.612 0.000306 ***\nhr7                        120.093      5.693  21.095  < 2e-16 ***\nhr8                        223.662      5.690  39.310  < 2e-16 ***\nhr9                        120.582      5.693  21.182  < 2e-16 ***\nhr10                        83.801      5.705  14.689  < 2e-16 ***\nhr11                       105.423      5.722  18.424  < 2e-16 ***\nhr12                       137.284      5.740  23.916  < 2e-16 ***\nhr13                       136.036      5.760  23.617  < 2e-16 ***\nhr14                       126.636      5.776  21.923  < 2e-16 ***\nhr15                       132.087      5.780  22.852  < 2e-16 ***\nhr16                       178.521      5.772  30.927  < 2e-16 ***\nhr17                       296.267      5.749  51.537  < 2e-16 ***\nhr18                       269.441      5.736  46.976  < 2e-16 ***\nhr19                       186.256      5.714  32.596  < 2e-16 ***\nhr20                       125.549      5.704  22.012  < 2e-16 ***\nhr21                        87.554      5.693  15.378  < 2e-16 ***\nhr22                        59.123      5.689  10.392  < 2e-16 ***\nhr23                        26.838      5.688   4.719 2.41e-06 ***\nworkingday                   1.270      1.784   0.711 0.476810    \ntemp                       157.209     10.261  15.321  < 2e-16 ***\nweathersitcloudy/misty     -12.890      1.964  -6.562 5.60e-11 ***\nweathersitlight rain/snow  -66.494      2.965 -22.425  < 2e-16 ***\nweathersitheavy rain/snow -109.745     76.667  -1.431 0.152341    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 76.5 on 8605 degrees of freedom\nMultiple R-squared:  0.6745,\tAdjusted R-squared:  0.6731 \nF-statistic: 457.3 on 39 and 8605 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nDue to space constraints, we truncate the output of `summary(mod.lm)`.\nIn `mod.lm`, the first level of `hr` (0) and `mnth` (Jan) are treated as the baseline values, and so no coefficient estimates are provided for them: implicitly, their coefficient estimates are zero, and all other levels are measured relative to these baselines.\nFor example, the Feb coefficient of $6.845$ signifies that, holding all other variables constant, there are on average about 7 more riders in February than in January.\nSimilarly there are about 16.5 more riders in March than in January.\n\nThe results seen in Section 4.6.1 used a slightly different coding of the variables `hr` and `mnth`, as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncontrasts(Bikeshare$hr) = contr.sum(24)\ncontrasts(Bikeshare$mnth) = contr.sum(12)\nmod.lm2 <- lm(\n    bikers ~ mnth + hr + workingday + temp + weathersit,\n    data = Bikeshare\n  )\nsummary(mod.lm2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, \n    data = Bikeshare)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-299.00  -45.70   -6.23   41.08  425.29 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                 73.5974     5.1322  14.340  < 2e-16 ***\nmnth1                      -46.0871     4.0855 -11.281  < 2e-16 ***\nmnth2                      -39.2419     3.5391 -11.088  < 2e-16 ***\nmnth3                      -29.5357     3.1552  -9.361  < 2e-16 ***\nmnth4                       -4.6622     2.7406  -1.701  0.08895 .  \nmnth5                       26.4700     2.8508   9.285  < 2e-16 ***\nmnth6                       21.7317     3.4651   6.272 3.75e-10 ***\nmnth7                       -0.7626     3.9084  -0.195  0.84530    \nmnth8                        7.1560     3.5347   2.024  0.04295 *  \nmnth9                       20.5912     3.0456   6.761 1.46e-11 ***\nmnth10                      29.7472     2.6995  11.019  < 2e-16 ***\nmnth11                      14.2229     2.8604   4.972 6.74e-07 ***\nhr1                        -96.1420     3.9554 -24.307  < 2e-16 ***\nhr2                       -110.7213     3.9662 -27.916  < 2e-16 ***\nhr3                       -117.7212     4.0165 -29.310  < 2e-16 ***\nhr4                       -127.2828     4.0808 -31.191  < 2e-16 ***\nhr5                       -133.0495     4.1168 -32.319  < 2e-16 ***\nhr6                       -120.2775     4.0370 -29.794  < 2e-16 ***\nhr7                        -75.5424     3.9916 -18.925  < 2e-16 ***\nhr8                         23.9511     3.9686   6.035 1.65e-09 ***\nhr9                        127.5199     3.9500  32.284  < 2e-16 ***\nhr10                        24.4399     3.9360   6.209 5.57e-10 ***\nhr11                       -12.3407     3.9361  -3.135  0.00172 ** \nhr12                         9.2814     3.9447   2.353  0.01865 *  \nhr13                        41.1417     3.9571  10.397  < 2e-16 ***\nhr14                        39.8939     3.9750  10.036  < 2e-16 ***\nhr15                        30.4940     3.9910   7.641 2.39e-14 ***\nhr16                        35.9445     3.9949   8.998  < 2e-16 ***\nhr17                        82.3786     3.9883  20.655  < 2e-16 ***\nhr18                       200.1249     3.9638  50.488  < 2e-16 ***\nhr19                       173.2989     3.9561  43.806  < 2e-16 ***\nhr20                        90.1138     3.9400  22.872  < 2e-16 ***\nhr21                        29.4071     3.9362   7.471 8.74e-14 ***\nhr22                        -8.5883     3.9332  -2.184  0.02902 *  \nhr23                       -37.0194     3.9344  -9.409  < 2e-16 ***\nworkingday                   1.2696     1.7845   0.711  0.47681    \ntemp                       157.2094    10.2612  15.321  < 2e-16 ***\nweathersitcloudy/misty     -12.8903     1.9643  -6.562 5.60e-11 ***\nweathersitlight rain/snow  -66.4944     2.9652 -22.425  < 2e-16 ***\nweathersitheavy rain/snow -109.7446    76.6674  -1.431  0.15234    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 76.5 on 8605 degrees of freedom\nMultiple R-squared:  0.6745,\tAdjusted R-squared:  0.6731 \nF-statistic: 457.3 on 39 and 8605 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nWhat is the difference between the two codings?\nIn `mod.lm2`, a coefficient estimate is reported for all but the last level of `hr` and `mnth`.\nImportantly, in `mod.lm2`, the coefficient estimate for the last level of `mnth` is not zero: instead, it equals the *negative of the sum of the coefficient estimates for all of the other levels*.\nSimilarly, in `mod.lm2`, the coefficient estimate for the last level of `hr` is the negative of the sum of the coefficient estimates for all of the other levels.\nThis means that the coefficients of `hr` and `mnth` in `mod.lm2` will always sum to zero, and can be interpreted as the difference from the mean level.\nFor example, the coefficient for January of $-46.087$ indicates that, holding all other variables constant, there are typically 46 fewer riders in January relative to the yearly average.\n\nIt is important to realize that the choice of coding really does not matter, provided that we interpret the model output correctly in light of the coding used.\nFor example, we see that the predictions from the linear model are the same regardless of coding:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum((predict(mod.lm) - predict(mod.lm2))^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.573305e-18\n```\n\n\n:::\n:::\n\n\nThe sum of squared differences is zero.\nWe can also see this using the `all.equal()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall.equal(predict(mod.lm), predict(mod.lm2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\nTo reproduce the left-hand side of Figure 4.13, we must first obtain the coefficient estimates associated with `mnth`.\nThe coefficients for January through November can be obtained directly from the `mod.lm2` object.\nThe coefficient for December must be explicitly computed as the negative sum of all the other months.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef.months <- c(coef(mod.lm2)[2:12],\n    -sum(coef(mod.lm2)[2:12]))\n```\n:::\n\n\nTo make the plot, we manually label the $x$-axis with the names of the months.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(coef.months, xlab = \"Month\", ylab = \"Coefficient\",\n    xaxt = \"n\", col = \"blue\", pch = 19, type = \"o\")\naxis(side = 1, at = 1:12, labels = c(\"J\", \"F\", \"M\", \"A\",\n    \"M\", \"J\", \"J\", \"A\", \"S\", \"O\", \"N\", \"D\"))\n```\n\n::: {.cell-output-display}\n![](04-classification-I_files/figure-html/chunk41-1.png){width=672}\n:::\n:::\n\n\nReproducing the right-hand side of Figure 4.13 follows a similar process.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef.hours <- c(coef(mod.lm2)[13:35],\n    -sum(coef(mod.lm2)[13:35]))\nplot(coef.hours, xlab = \"Hour\", ylab = \"Coefficient\",\n    col = \"blue\", pch = 19, type = \"o\")\n```\n\n::: {.cell-output-display}\n![](04-classification-I_files/figure-html/chunk42-1.png){width=672}\n:::\n:::\n\n\nNow, we consider instead fitting a Poisson regression model to the `Bikeshare` data.\nVery little changes, except that we now use the function `glm()` with the argument `family = poisson` to specify that we wish to fit a Poisson regression model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod.pois <- glm(\n    bikers ~ mnth + hr + workingday + temp + weathersit,\n    data = Bikeshare, family = poisson\n  )\nsummary(mod.pois)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, \n    family = poisson, data = Bikeshare)\n\nCoefficients:\n                           Estimate Std. Error  z value Pr(>|z|)    \n(Intercept)                4.118245   0.006021  683.964  < 2e-16 ***\nmnth1                     -0.670170   0.005907 -113.445  < 2e-16 ***\nmnth2                     -0.444124   0.004860  -91.379  < 2e-16 ***\nmnth3                     -0.293733   0.004144  -70.886  < 2e-16 ***\nmnth4                      0.021523   0.003125    6.888 5.66e-12 ***\nmnth5                      0.240471   0.002916   82.462  < 2e-16 ***\nmnth6                      0.223235   0.003554   62.818  < 2e-16 ***\nmnth7                      0.103617   0.004125   25.121  < 2e-16 ***\nmnth8                      0.151171   0.003662   41.281  < 2e-16 ***\nmnth9                      0.233493   0.003102   75.281  < 2e-16 ***\nmnth10                     0.267573   0.002785   96.091  < 2e-16 ***\nmnth11                     0.150264   0.003180   47.248  < 2e-16 ***\nhr1                       -0.754386   0.007879  -95.744  < 2e-16 ***\nhr2                       -1.225979   0.009953 -123.173  < 2e-16 ***\nhr3                       -1.563147   0.011869 -131.702  < 2e-16 ***\nhr4                       -2.198304   0.016424 -133.846  < 2e-16 ***\nhr5                       -2.830484   0.022538 -125.586  < 2e-16 ***\nhr6                       -1.814657   0.013464 -134.775  < 2e-16 ***\nhr7                       -0.429888   0.006896  -62.341  < 2e-16 ***\nhr8                        0.575181   0.004406  130.544  < 2e-16 ***\nhr9                        1.076927   0.003563  302.220  < 2e-16 ***\nhr10                       0.581769   0.004286  135.727  < 2e-16 ***\nhr11                       0.336852   0.004720   71.372  < 2e-16 ***\nhr12                       0.494121   0.004392  112.494  < 2e-16 ***\nhr13                       0.679642   0.004069  167.040  < 2e-16 ***\nhr14                       0.673565   0.004089  164.722  < 2e-16 ***\nhr15                       0.624910   0.004178  149.570  < 2e-16 ***\nhr16                       0.653763   0.004132  158.205  < 2e-16 ***\nhr17                       0.874301   0.003784  231.040  < 2e-16 ***\nhr18                       1.294635   0.003254  397.848  < 2e-16 ***\nhr19                       1.212281   0.003321  365.084  < 2e-16 ***\nhr20                       0.914022   0.003700  247.065  < 2e-16 ***\nhr21                       0.616201   0.004191  147.045  < 2e-16 ***\nhr22                       0.364181   0.004659   78.173  < 2e-16 ***\nhr23                       0.117493   0.005225   22.488  < 2e-16 ***\nworkingday                 0.014665   0.001955    7.502 6.27e-14 ***\ntemp                       0.785292   0.011475   68.434  < 2e-16 ***\nweathersitcloudy/misty    -0.075231   0.002179  -34.528  < 2e-16 ***\nweathersitlight rain/snow -0.575800   0.004058 -141.905  < 2e-16 ***\nweathersitheavy rain/snow -0.926287   0.166782   -5.554 2.79e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1052921  on 8644  degrees of freedom\nResidual deviance:  228041  on 8605  degrees of freedom\nAIC: 281159\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n:::\n\n\nWe can plot the coefficients associated with `mnth` and `hr`, in order to reproduce Figure 4.15:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef.mnth <- c(coef(mod.pois)[2:12],\n    -sum(coef(mod.pois)[2:12]))\nplot(coef.mnth, xlab = \"Month\", ylab = \"Coefficient\",\n     xaxt = \"n\", col = \"blue\", pch = 19, type = \"o\")\naxis(side = 1, at = 1:12, labels = c(\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \"J\", \"A\", \"S\", \"O\", \"N\", \"D\"))\n```\n\n::: {.cell-output-display}\n![](04-classification-I_files/figure-html/chunk44-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncoef.hours <- c(coef(mod.pois)[13:35],\n     -sum(coef(mod.pois)[13:35]))\nplot(coef.hours, xlab = \"Hour\", ylab = \"Coefficient\",\n    col = \"blue\", pch = 19, type = \"o\")\n```\n\n::: {.cell-output-display}\n![](04-classification-I_files/figure-html/chunk44-2.png){width=672}\n:::\n:::\n\n\nWe can once again use the `predict()` function to obtain the fitted values (predictions) from this Poisson regression model.\nHowever, we must use the argument `type = \"response\"` to specify that we want `R` to output $\\exp(\\hat\\beta_0 + \\hat\\beta_1 X_1 + \\ldots +\\hat\\beta_p X_p)$ rather than $\\hat\\beta_0 + \\hat\\beta_1 X_1 + \\ldots + \\hat\\beta_p X_p$, which it will output by default.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(predict(mod.lm2), predict(mod.pois, type = \"response\"))\nabline(0, 1, col = 2, lwd = 3)\n```\n\n::: {.cell-output-display}\n![](04-classification-I_files/figure-html/chunk45-1.png){width=672}\n:::\n:::\n\n\nThe predictions from the Poisson regression model are correlated with those from the linear model; however, the former are non-negative.\nAs a result the Poisson regression predictions tend to be larger than those from the linear model for either very low or very high levels of ridership.\n\nIn this section, we used the `glm()` function with the argument `family = poisson` in order to perform Poisson regression.\nEarlier in this lab we used the `glm()` function with `family = binomial` to perform logistic regression.\nOther choices for the `family` argument can be used to fit other types of GLMs.\nFor instance, `family = Gamma` fits a gamma regression model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "04-classification-I_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}