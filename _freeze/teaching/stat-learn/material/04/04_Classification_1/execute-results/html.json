{
  "hash": "3a046104fc183316e99fa068b8752f0c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Classification I\"\nauthor: \"Termeh Shafie\"\nformat:\n  html:\n    embed-resources: true\nnumber-sections: true\ntoc: true         \neditor: visual\nexecute:\n  cache:  false\n---\n\n\n\n\n# Logistic Regression\nIn our Linear Regression lectures, we talked about adding non-linearity through **Feature Engineering**, but that's not the only way! We can also use **link functions** to add non-linearity.\n\nLink functions are just algebra we do to the linear prediction ($\\mathbf{X}\\beta$) in order to get the predicted value we *actually* want (e.g a probability). \n\n$$\\underbrace{y = \\mathbf{X}\\beta}_\\text{Linear Model}$$\n$$\\underbrace{y = g^{-1}(\\mathbf{X}\\beta)}_\\text{Generalized Linear Model}$$\n\nOddly, we often specify our link function using it's *inverse*, hence the $g^{-1}()$ instead of $g()$. $g^{-1}()$ takes the linear prediction and transforms it into our desired predicted value. $g()$ takes our desired predicted value and transforms it back into our linear prediction.\n\n\nIn logistic regression, our goal is to predict a *probability* that a data point is in group `1`. We talked about using:\n\n- Linear Probability Models $g^{-1}: y = x$\n- Odds Models $g^{-1}: y = e^x$\n- Logistic Regression: $g^{-1}: y = \\frac{e^x}{1 + e^x}$\n\nLogistic Regression using the link function $g(x) = log{\\frac{x}{1-x}}$ and inverse link $g^{-1}: y = \\frac{e^x}{1 + e^x}$ gave us a great sigmoid shape that takes linear predictions ($y = \\mathbf{X}\\beta$) and turns them into predicted probabilities ($p = \\frac{e^{\\mathbf{X}\\beta}}{1 + e^{\\mathbf{X}\\beta}}$).\n\n## Maximum Likelihood Estimation\n\nJust like with Linear Regression, we can use **Maximum Likelihood Estimation** to choose the parameters (intercept and coefficients) of the model. But we have a different likelihood.\n\nIn a linear regression, we assumed that our *errors* are *normally* distributed around the regression line. For logistic regression, we assume that our *errors* are *Bernoulli* distributed. The Bernoulli distribution is a discrete distribution (since our outcome is *discrete*, a.k.a categorical) that tells you the proability of being `0` or `1`.\n\n## Bernoulli Likelihood\nThe formula for a Bernoulli distribution for a single data point $x$ is:\n\n$$ f(y;p(x)) = p(x)^{y} * (1-p(x))^{1-y}$$\n\nwhere $y$ is the group the data point belongs to (either `0` or `1`), and $p(x)$ is the predicted probability of that data point being a `1`.\n\nFor example, let's say we're looking at the probability that it's sunny tomorrow. The predicted probability, according to the weather channel is $p(x) = 0.8$. The likelihood of it being sunny ($k = 1$) is:\n\n\n\n$$ f(1;0.8) = 0.8^1 * (1-0.8)^{1-1} = 0.8$$\n\nThe likelihood of it not being sunny ($k = 0$) is: \n$$ f(0;0.8) = 0.8^0 * (1-0.8)^{1-0} = 0.2$$\n\n## Likelihood Function\nBut we don't just have a SINGLE data point when fitting a logistic regression, we have MANY. So, we multiply the likelihood of each data point together to get the likelihood of the dataset: \n\n$$\\prod_{i = 1}^n p(x_i)^{y_i} * (1-p(x_i))^{1-y_i}$$\n\nWe want to choose parameters (e.g. $\\beta_0$, or $\\beta_1$) that *maximize* this likelihood function. And how to we maximize it? We take it's (partial) derivatives and set them equal to zero!\n\nHowever, it turns out that its much easier to work with the *log* of this likelihood function, so we're often working with the *log* likelihood and taking it's derivatives (this will still find the optimal parameters for the model as the values that maximize the *log* likelihood will also maximize the likelihood):\n\n$$\\sum_{i = 1}^n y_i * log(p(x_i)) + (1-y_i) * log(1-p(x_i))$$\n\n## Loss Function\nNow it turns out, if we multiply the log loss by $-\\frac{1}{N}$, this log-loss is a really great **loss function** for logistic regression. Loss functions are metrics that\n\n1. measure the performance of your model, and\n2. have lower scores indicate better performing models\n\n$$-\\frac{1}{N} \\sum_{i = 1}^n y_i * log(p(x_i)) + (1-y_i) * log(1-p(x_i))$$\n\nLog-Loss (also called **Binary Cross Entropy**) does just that! Thus we often use it as a loss function for Logistic Regression. \n\n\n## Logistic Regression in R\n\nLet's build a Logistic Regression model in **R**. We’ll follow a similar workflow to what we used for linear models:\n\n1. Separate your data into predictors (`X`) and outcome (`y`), and optionally set up a train/test split.\n2. Create a model formula and initialize the logistic regression model using `glm()` with `family = binomial`.\n3. Fit the model to the training data.\n4. Use `predict()` on new data to obtain predicted probabilities or class predictions.\n5. Assess the model’s performance (e.g., accuracy, confusion matrix, ROC curve).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split data (example dataset)\nset.seed(123)\nidx <- sample(seq_len(nrow(df)), size = 0.8*nrow(df))\ntrain <- df[idx, ]\ntest  <- df[-idx, ]\n\n# 1. Define model formula\nformula <- outcome ~ predictor1 + predictor2\n\n# 2 & 3. Fit logistic regression model\nlog_model <- glm(formula, data = train, family = binomial)\n\n# 4. Predict probabilities\npred_probs <- predict(log_model, test, type = \"response\")\n\n# Convert to class prediction (optional threshold = 0.5)\npred_class <- ifelse(pred_probs > 0.5, 1, 0)\n\n# 5. Assess model\ntable(Predicted = pred_class, Actual = test$outcome)\n```\n:::\n\n\n\n## Breast Cancer Data\nLet's do an example with logistic regression to classify cancer diagnosis. \nWe will:\n1. Load and lightly clean the dataset.  \n2. Select predictors whose names end with `\"mean\"`.  \n3. Split the data into training and testing sets (80/20).  \n4. Fit a logistic regression (`glm`, binomial family).  \n5. Predict class probabilities on the test set.  \n6. Evaluate performance using **binary cross-entropy (log loss)**.\n\n\nWe import the Breast Cancer dataset and drop any rows with missing values to ensure the model can be fit without errors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbc <- read.csv(\n  \"04-Data/BreastCancer.csv\",\n  stringsAsFactors = FALSE\n)\n\nbc <- na.omit(bc)\nnrow(bc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 569\n```\n\n\n:::\n:::\n\n\nThe outcome is diagnosis (Benign B vs Malignant M). As predictors we only use columns whose names end in \"mean\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# columns ending with \"mean\"\npredictors <- grep(\"mean$\", names(bc), value = TRUE)\n\n# modeling frame: outcome + predictors\ndf <- data.frame(\n  diagnosis = factor(bc$diagnosis, levels = c(\"B\",\"M\")), # B=0, M=1\n  bc[, predictors]\n)\n\n\nstr(df[, c(\"diagnosis\", predictors[1:5])])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t569 obs. of  6 variables:\n $ diagnosis      : Factor w/ 2 levels \"B\",\"M\": 2 2 2 2 2 2 2 2 2 2 ...\n $ radius_mean    : num  18 20.6 19.7 11.4 20.3 ...\n $ texture_mean   : num  10.4 17.8 21.2 20.4 14.3 ...\n $ perimeter_mean : num  122.8 132.9 130 77.6 135.1 ...\n $ area_mean      : num  1001 1326 1203 386 1297 ...\n $ smoothness_mean: num  0.1184 0.0847 0.1096 0.1425 0.1003 ...\n```\n\n\n:::\n:::\n\n\nWe split once and keep a fixed seed for reproducibility. The model will train on train and be evaluated on test.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nn <- nrow(df)\nidx_train <- sample.int(n, size = floor(0.8 * n))\ntrain <- df[idx_train, ]\ntest  <- df[-idx_train, ]\nc(n_train = nrow(train), n_test = nrow(test))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nn_train  n_test \n    455     114 \n```\n\n\n:::\n:::\n\n\n\nWe specify a formula that uses all \"mean\" predictors and fit a logistic regression using the binomial family.\n\n::: {.cell}\n\n```{.r .cell-code}\nformula <- as.formula(paste(\"diagnosis ~\", paste(predictors, collapse = \" + \")))\nlog_model <- glm(formula, data = train, family = binomial)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(log_model)$coefficients[1:6, , drop = FALSE]  # peek at a few coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   Estimate  Std. Error    z value     Pr(>|z|)\n(Intercept)     -8.33989439 14.53963337 -0.5735973 5.662403e-01\nradius_mean     -0.74101892  3.99281153 -0.1855883 8.527677e-01\ntexture_mean     0.38855965  0.07352802  5.2845112 1.260408e-07\nperimeter_mean  -0.27927112  0.54267867 -0.5146160 6.068214e-01\narea_mean        0.04017311  0.01952026  2.0580216 3.958806e-02\nsmoothness_mean 72.05651889 34.28104247  2.1019349 3.555898e-02\n```\n\n\n:::\n:::\n\n\nWe obtain predicted probabilities of **malignancy** for each test observation.\n\n::: {.cell}\n\n```{.r .cell-code}\np_test <- predict(log_model, newdata = test, type = \"response\")\nhead(p_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        1         9        15        17        18        28 \n0.9999327 0.9878218 0.9184778 0.7153792 0.9987491 0.9998869 \n```\n\n\n:::\n:::\n\n\nLower log loss indicates better calibrated probability predictions. We map B -> 0 and M -> 1 and compute the average cross-entropy.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Binary cross-entropy / log loss helper\nlog_loss <- function(y, p, eps = 1e-15) {\n  p <- pmin(pmax(p, eps), 1 - eps)    # avoid log(0)\n  -mean(y * log(p) + (1 - y) * log(1 - p))\n}\n\ny_test <- ifelse(test$diagnosis == \"M\", 1, 0)\n\nloss <- log_loss(y_test, p_test)\nloss\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1193837\n```\n\n\n:::\n:::\n\n\nA log loss of **0.119** indicates strong model performance. Log loss measures how well the model's predicted probabilities match the true outcomes where lower is better. A value close to 0 means the model is making accurate and well-calibrated predictions, showing high confidence when it is correct and low confidence when uncertain.\n\n::: callout-note\n- Diagnosis is encoded with levels c(\"B\",\"M\") so that M corresponds to the positive class (1) for log-loss computation.\n- No feature scaling is required for logistic regression to work, but standardization can sometimes help convergence or interpretability.\n- Log loss evaluates the quality of probabilities, not just the final class labels.\n:::\n\t\n\n\nLogistic Regression coefficients are by default in terms of *log odds* meaning that they tell you how much the *predicted log odds* of being in group 1 will change when the *predictor* increases by 1-unit.\nWe grab the coefficients for the model above:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ---- Extract Coefficients ----\ncoefs <- summary(log_model)$coefficients   # from glm()\n\n# Convert to a data frame with names\ncoef_df <- data.frame(\n  Name = rownames(coefs),\n  Coef = coefs[, \"Estimate\"],\n  row.names = NULL\n)\n\n# ---- Add Odds Ratios ----\ncoef_df$Odds <- exp(coef_df$Coef)\n\ncoef_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                     Name         Coef         Odds\n1             (Intercept)  -8.33989439 2.387976e-04\n2             radius_mean  -0.74101892 4.766280e-01\n3            texture_mean   0.38855965 1.474855e+00\n4          perimeter_mean  -0.27927112 7.563348e-01\n5               area_mean   0.04017311 1.040991e+00\n6         smoothness_mean  72.05651889 1.966747e+31\n7        compactness_mean  -1.35444224 2.580912e-01\n8          concavity_mean   6.66078793 7.811662e+02\n9     concave.points_mean  74.10069490 1.518878e+32\n10          symmetry_mean  14.00627785 1.210178e+06\n11 fractal_dimension_mean -39.60760668 6.289773e-18\n```\n\n\n:::\n:::\n\n\n\n## Question\nHow do you interpret the results?\n\n::: {.callout-tip collapse=\"true\"}\n### Interpreting the Coefficients\n\n- **Positive coefficients (Odds > 1)** increase the likelihood of a **malignant** diagnosis.  \n  Variables like `texture_mean`, `area_mean`, `concavity_mean`, `concave.points_mean`, and `symmetry_mean` strongly raise the probability of cancer, with very large odds ratios indicating powerful predictors.\n\n- **Negative coefficients (Odds < 1)** decrease the likelihood of malignancy.  \n  Higher `radius_mean`, `perimeter_mean`, `compactness_mean`, and `fractal_dimension_mean` values point more toward **benign** tumors.\n\n- The **intercept** represents the baseline log-odds when predictors are zero (not directly interpretable on its own).\n\n Overall, features related to **concavity, smoothness, and symmetry** strongly increase cancer risk, while higher **compactness, radius, and fractal dimension** values are associated with benign masses.\n:::\n\n## The Problem with Logistic Regression Coefficients\nWhen you're presenting your Logistic Regression Models to non-data people, you might want to be able to tell them which variables have the biggest impact on the predicted value. Typically, we might use coefficients for this because they give us a single number that summarizes the relationship between our *predictors* and our *predicted value*. \n\nHowever, log odds are difficult to understand intuitively, especially if you're not a data person. Thus, we might want a different way to present our results. Luckily, if we *exponentiate* our **log odds** coefficients, we get **odds** coefficients. These are easier to understand, as most people understand intuitively what **odds** are. \n\nRemember, for **odds** the important threshold value is $1$. So any **odds** coefficient $>1$ has a direct/positive relationship with the outcome and anything with an **odds** coefficient $< 1$ has an inverse/negative relationship with the outcome. \n\nYou can also use the **odds** coefs to give people an intuitive understanding of the relationship. If the odds coef is $2$ then increasing the predictor by 1-unit causes your predicted odds to *double*. Similarly, if the odds coef is $0.5$ then increasing the predictor by 1-unit causes your predicted odds to *halve*. If the odds coef is $1.25$ then increasing the predictor by 1-unit causes your predicted odds to increase by $25\\%$.\n\n\n# KNN\nKNN is a simple, distance based algorithm that let's us CLASSIFY data points based on what class the data points around them are. Birds of a feather...\n\nDespite it being distance based, KNN is a *classification* algorithm. In other words, it is supervised machine learning, as it requires truth labels (the actual class/group). However it does share characteristics with clustering algorithms we will see later.\n\nKNN *can* work with binary/categorical variables, but not without some tweaking which we do not cover here.\n\n## Hyperparameters\n\nHyperparameters are parameters in our model that are NOT chosen by the algorithm (we must supply them). We can either choose them:\n\n- based on domain expertise (knowledge about the data)\n- based on the data (hyperparameter tuning)\n\nWhy do we have to use a validation set when hyperparameter tuning?\n\n\nIn this classwork we'll use ggplot to plot the boundaries of knn, and see how the size, shape, and overlap of clusters affect these boundries.\n\n> Note: this will only work with specific 2D data, if you wanted to use it for your own data you'd need to change the code to do so\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplotKNN2D <- function(Xdf, y, k = 5) {\n  # Xdf: data frame with exactly 2 numeric features\n  # y: factor labels\n  \n  if (ncol(Xdf) != 2) stop(\"Xdf must have exactly 2 columns (2D only)\")\n  if (!is.factor(y)) y <- factor(y)\n  \n  library(class)\n  library(ggplot2)\n  \n  # Feature names\n  f1 <- colnames(Xdf)[1]\n  f2 <- colnames(Xdf)[2]\n  \n  # Create grid range\n  x0_range <- seq(min(Xdf[[f1]]) - sd(Xdf[[f1]]),\n                  max(Xdf[[f1]]) + sd(Xdf[[f1]]),\n                  length.out = 100)\n  \n  x1_range <- seq(min(Xdf[[f2]]) - sd(Xdf[[f2]]),\n                  max(Xdf[[f2]]) + sd(Xdf[[f2]]),\n                  length.out = 100)\n  \n  grid <- expand.grid(\n    f1 = x0_range,\n    f2 = x1_range\n  )\n  colnames(grid) <- c(f1, f2)\n  \n  # Predict using KNN\n  pred <- knn(train = Xdf, test = grid, cl = y, k = k)\n  grid$pred <- pred\n  \n  # Plot using tidy eval with .data\n  p <- ggplot() +\n    geom_point(\n      data = grid,\n      aes(x = .data[[f1]], y = .data[[f2]], color = pred),\n      alpha = 0.25, size = 0.6\n    ) +\n    geom_point(\n      data = Xdf,\n      aes(x = .data[[f1]], y = .data[[f2]], color = y),\n      size = 2\n    ) +\n    theme_minimal() +\n    labs(color = \"Class\",\n         title = paste(\"KNN Decision Boundary (k =\", k, \")\")) +\n    scale_color_manual(values = c(\"#E69F00\", \"#0072B2\"))\n  \n  p\n}\n```\n:::\n\n\n## Let's Explore\nLet's test this function with some fake data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# --- Generate Fake Data (two blobs) ---\nset.seed(1)\nn <- 200\nn_per <- n / 2\n\n# centers: (-5, -5) and (5, 5); cluster_std = 1\nX1 <- cbind(rnorm(n_per, mean = -5, sd = 1),\n            rnorm(n_per, mean = -5, sd = 1))\nX2 <- cbind(rnorm(n_per, mean =  5, sd = 1),\n            rnorm(n_per, mean =  5, sd = 1))\n\nX <- rbind(X1, X2)\ncolnames(X) <- c(\"X1\", \"X2\")\nX <- as.data.frame(X)\n\n# labels 0/1 as a factor\ny <- factor(c(rep(0, n_per), rep(1, n_per)))\n\n# --- Plot KNN decision boundary (k = 1) ---\n# assumes plotKNN2D(Xdf, y, k) is already defined\nplotKNN2D(X, y, k = 1)\n```\n\n::: {.cell-output-display}\n![](04_Classification_1_files/figure-html/knn-1-1.png){width=672}\n:::\n:::\n\n\n\nUsing the dataset `knnclasswork.csv` and using the `plotKNN2d()` function, build KNN models with K = 1, 3, 5, 20, 50, 100.\n\n**How does the decision boundary change as K changes?**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndd <- read.csv(\n  \"04-Data/KNNclasswork.csv\"\n)\n# k = 1\nplotKNN2D(dd[, c(\"X1\", \"X2\")], dd$y, 1)\n```\n\n::: {.cell-output-display}\n![](04_Classification_1_files/figure-html/KNN-varK-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# k = 3\n\n# k = 5\n\n# k = 20\n\n# k = 50\n\n# k = 100\n```\n:::\n\n\n##  How does changing k affect the decision boundary (imbalanced classes)?\n\nNow let's see how changing k affects the boundary when the groups have different numbers of samples. Using the `plotKNN2d()` function, and the data loaded below (`dd2`), examine what happens to the decision boundaries as you try different k's (try 1,3,5,10, 25, 50, and **100**).\n\n**How does changing k affect the decision boundary when the groups are imbalanced?**\n\n::: {.cell}\n\n```{.r .cell-code}\ndd2 <- read.csv(\n  \"04-Data/KNNclasswork2.csv\"\n)\n# k = 1\nplotKNN2D(dd2[, c(\"X1\", \"X2\")], dd2$y, 1)\n```\n\n::: {.cell-output-display}\n![](04_Classification_1_files/figure-html/KNN-varK-imb-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# k = 3\n\n# k = 5\n\n# k = 20\n\n# k = 50\n\n# k = 100\n```\n:::\n\n\n::: {.callout-tip collapse=\"true\"}\n# Answers\nIn balanced data, increasing k shifts the boundary from very wiggly and noise-sensitive to smooth and generalized.\n\n- Too small k = overfitting\n- too large k = underfitting.\n\nFor imbalanced case, as k increases, the decision boundary becomes smoother and shifts toward the majority class, eventually overwhelming the minority class and reducing its predicted region, especially at very high k.\n\n- Small k = preserves minority class but noisy\n- Large k = smooth but biased toward majority\n:::\n\n# Exercises\n\n## Logistic Regression\n\nPractice building Logistic Regression models. Using the `purchases` dataset, build a Logistic Regression model that predicts whether or not customers signed up for a rewards program based on their age, income, and whether they had made a previous purchase. Use an 80/20 Train-Test-Split. Note that logistic regression is not scale-based (it’s a linear model, not a distance-based one), so it doesn’t need standardization to function correctly. However, standardizing can improve training stability and interpretation consistency, especially when variables differ wildly in scale. Interpret the coefficients in terms of log-odds, odds and probability.\n\n## Recommendation Systems (KNN)\n\n\"If you like _________________ you should listen to ___________________ by Taylor Swift.\"\n\nWe're going to build a **Recommendation System** to recommend Taylor Swift songs for people by letting users select a song, and then recommending the most similar songs (according to `danceability`, `energy`, `instrumentalness`, `valence`, `loudness`, `liveness`, `speechiness`, `acousticness`).\n\nTo do this, we're going to load in our **training data** called `04-Data/TaylorSwiftSpotify.csv`, fit a `NearestNeighbors()` model, and then for each song in our **new data** called `04-Data/Data/KNNCompareSpotify.csv)` we'll find the 10 most similar songs and recommend them!\n\nBelow you have some code to get you started, note that you will nedd to install the package `FNN`. Fill in the missing parts!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# --- Read data ---\ntraining_data <- read.csv(\n  \"04-Data/TaylorSwiftSpotify.csv\",\n  stringsAsFactors = FALSE\n)\nnew_data <- read.csv(\n  \"04-Data//KNNCompareSpotify.csv\",\n  stringsAsFactors = FALSE\n)\n\n# --- Features ---\nfeat <- c(\"danceability\", \"energy\", \"instrumentalness\", \"valence\",\n          \"loudness\", \"liveness\", \"speechiness\", \"acousticness\")\n\n# --- Z-score using TRAINING stats only (avoid division by zero) ---\n\n# --- Nearest Neighbors (k = 10) ---\n\n# --- Attach neighbors to new_data ---\n# Store neighbor indices as a semicolon-separated string for each row (CSV-friendly)\n\n# --- Write as CSV file\n```\n:::\n\n",
    "supporting": [
      "04_Classification_1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}