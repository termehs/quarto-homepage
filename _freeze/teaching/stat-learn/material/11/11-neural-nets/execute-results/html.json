{
  "hash": "b6a0d888e2db3759bae2ef992cede7e7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Neural Networks\"\nauthor: \"Termeh Shafie\"\nformat:\n  html:\n    embed-resources: true\nnumber-sections: true\ntoc: true         \neditor: visual\nexecute:\n  cache:  true\n---\n\n# Review\n\nNeural Networks are great. Their flexibility (layers...connections...activation functions...and more!) allows you to build complex models that can accurately model complex relationships between predictors and outcomes. But I want to caution you: Neural Networks aren't magic. I often see people using them unnecessarily, just because they sound cool. If you're going to use NN's, make sure they're the right tool for your problem.\n\nWhen building a neural network you need to think about 2 (main) things:\n\n1.  The Structure of the model (nodes/connections/activation functions)\n2.  The Loss Function (how do we measure how well our model is doing?)\n\n## Installing necessary packages\n\nWhen working with neural networks in R, you may encounter two packages: **`keras`** and **`keras3`**. The older **`keras`** package is an R interface to the TensorFlow-specific version of Keras (often called Keras 2), which means it only works with **TensorFlow**. The newer **`keras3`** package connects R to **Keras 3**, the modern version of **Keras**, which is designed to work with multiple backends (such as TensorFlow, JAX, or PyTorch). Because Keras 3 represents the current and future direction of the framework, **`keras3` is the recommended choice for new neural network work**.\n\nWhen learning neural networks, you should generally use **`keras3` with the TensorFlow backend**. This setup is actively maintained, aligns with up-to-date tutorials, and is well supported in both R and Python ecosystems. You should only use the older **`keras`** package if you are working with legacy code that specifically depends on TensorFlow-only Keras. To get started, first install the R package and then install a backend. If you are new to neural networks, TensorFlow is the recommended backend:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"keras3\")\nkeras3::install_keras()\n```\n:::\n\n\nThis command automatically sets up a compatible Python environment and installs TensorFlow for you. After completing these steps, you can immediately begin building and training neural networks in R using keras3.\n\n## Simple NN\n\nThe code below loads a music dataset, selects four audio features to predict valence, and standardizes the inputs so they are on the same scale. It then builds and trains a simple neural network with one linear output node, equivalent in form to a linear regression model, using mean squared error and stochastic gradient descent over five training epochs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)  \n\n# Read the data\ndf <- read.csv(\"11-data/Music_data.csv\")\n\n# Define feature columns and target\nfeats <- c(\"danceability\", \"energy\", \"loudness\", \"acousticness\")\npredict <- \"valence\"\n\n# Print the shape of the data frame (rows, columns)\nprint(dim(df))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2553   14\n```\n\n\n:::\n\n```{.r .cell-code}\n# Select features and target\nX <- df[, feats]\ny <- df[, predict]\n\n# Standardize the features (mean = 0, sd = 1)\nX <- scale(X)\n```\n:::\n\n\nThe model below has the same shape as a simple linear regression, like we talked about in lecture. It has an input layer with 4 inputs (\"danceability\", \"energy\", \"loudness\",\"acousticness\"), and 1 output layer for \"valence\".\n\nWe will use the package `keras3`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(keras3)\n\n# structure of the model\nnn_model <- keras_model_sequential() %>%\n  layer_dense(\n    units = 1,\n    input_shape = c(4)   # same as input_shape=[4]\n  )\n\n# how to train the model\nnn_model %>% compile(\n  loss = \"mean_squared_error\",\n  optimizer = optimizer_sgd()\n)\n\n# fit the model (same idea as sklearn / Python Keras)\nnn_model %>% fit(\n  x = X,\n  y = y,\n  epochs = 5\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEpoch 1/5\n80/80 - 0s - 2ms/step - loss: 0.6473\nEpoch 2/5\n80/80 - 0s - 412us/step - loss: 0.1129\nEpoch 3/5\n80/80 - 0s - 406us/step - loss: 0.0529\nEpoch 4/5\n80/80 - 0s - 400us/step - loss: 0.0397\nEpoch 5/5\n80/80 - 0s - 406us/step - loss: 0.0363\n```\n\n\n:::\n:::\n\n\nWe next fit linear regression model is using the selected features to predict the target variable. The model estimates the relationship between each predictor and the response by finding the coefficients that minimize the sum of squared errors, using the entire dataset without any validation or train–test split.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert to data frame for lm()\ndf_lm <- data.frame(X, y = y)\n\n# Build and fit the linear regression model\nmodel <- lm(y ~ ., data = df_lm)\n\n# View model summary\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ ., data = df_lm)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.55286 -0.13372 -0.00401  0.13306  0.53369 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.471304   0.003703 127.288  < 2e-16 ***\ndanceability  0.108905   0.003828  28.446  < 2e-16 ***\nenergy        0.097514   0.006552  14.883  < 2e-16 ***\nloudness     -0.034970   0.005981  -5.847 5.66e-09 ***\nacousticness  0.035117   0.005127   6.850 9.23e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1871 on 2548 degrees of freedom\nMultiple R-squared:  0.2829,\tAdjusted R-squared:  0.2818 \nF-statistic: 251.3 on 4 and 2548 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nNow we extract the coefficients and intercept from the linear model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get coefficients (including intercept)\ncoef(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n (Intercept) danceability       energy     loudness acousticness \n  0.47130376   0.10890452   0.09751431  -0.03497002   0.03511732 \n```\n\n\n:::\n:::\n\n\nWe get the weights from the neural net, which for this model with one dense layer will be a list containing:\n\n1.  A weight matrix (coefficients for each input feature)\n2.  A bias term\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get weights from Neural Network\nweights <- get_weights(nn_model) \n\nweights\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n            [,1]\n[1,]  0.10631825\n[2,]  0.11201244\n[3,] -0.06878705\n[4,]  0.01332233\n\n[[2]]\n[1] 0.473378\n```\n\n\n:::\n:::\n\n\nThe output lists the intercept and feature coefficients of the linear regression model, which directly correspond to the learned weights indicating how strongly and in what direction each input feature influences the predicted target value.\n\n> What happens to the weights from our neural net as you **increase** the number of epochs (compare to the coefs from the linear regression model)?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ----- Linear regression coefficients -----\nlm_model <- lm(y ~ ., data = data.frame(X, y))\nlm_coefs <- coef(lm_model)[-1]   # exclude intercept\nlm_intercept <- coef(lm_model)[1]\n\n# ----- Train neural nets with increasing epochs -----\nepoch_list <- c(1, 5, 20, 100, 200)\n\nnn_weights <- lapply(epoch_list, function(e) {\n  \n  nn_model <- keras_model_sequential() %>%\n    layer_dense(units = 1, input_shape = c(ncol(X)))\n  \n  nn_model %>% compile(\n    loss = \"mean_squared_error\",\n    optimizer = optimizer_sgd()\n  )\n  \n  nn_model %>% fit(\n    X, y,\n    epochs = e,\n    verbose = 0\n  )\n  \n  # extract weights (matrix) and bias\n  w <- get_weights(nn_model)\n  list(\n    epochs = e,\n    weights = as.vector(w[[1]]),\n    bias = w[[2]]\n  )\n})\n\n# ----- View results -----\nnn_weights\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[[1]]$epochs\n[1] 1\n\n[[1]]$weights\n[1] -0.116555884 -0.009368724  0.182410553  0.070211098\n\n[[1]]$bias\n[1] 0.3754594\n\n\n[[2]]\n[[2]]$epochs\n[1] 5\n\n[[2]]$weights\n[1]  0.11654302  0.17983818 -0.12111755  0.02648513\n\n[[2]]$bias\n[1] 0.472273\n\n\n[[3]]\n[[3]]$epochs\n[1] 20\n\n[[3]]$weights\n[1]  0.10762285  0.09810586 -0.03503770  0.03266637\n\n[[3]]$bias\n[1] 0.4706939\n\n\n[[4]]\n[[4]]$epochs\n[1] 100\n\n[[4]]$weights\n[1]  0.10737380  0.09639432 -0.03592658  0.03644631\n\n[[4]]$bias\n[1] 0.4691057\n\n\n[[5]]\n[[5]]$epochs\n[1] 200\n\n[[5]]$weights\n[1]  0.10761727  0.09660057 -0.03519304  0.03625473\n\n[[5]]$bias\n[1] 0.4701196\n```\n\n\n:::\n\n```{.r .cell-code}\nlm_coefs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndanceability       energy     loudness acousticness \n  0.10890452   0.09751431  -0.03497002   0.03511732 \n```\n\n\n:::\n:::\n\n\nNote the following:\n\n-   As epochs increase, the neural network weights stabilize and converge\n-   With a single dense layer and MSE loss, the neural net is effectively learning a linear regression\n-   The final neural network weights become very close to the linear regression coefficients\n-   Differences at low epochs are due to incomplete optimization\n\n## Parameter Bloat\n\nRemember that a densely connected layer is connected to EVERY node in the layer before and after it. The parameters can add up QUICKLY.\n\n> What do you think can happen when you have a ton of parameters and only a little data?\n\nWhen you have many parameters but very little data, the model is likely to overfit, meaning it learns noise and random fluctuations instead of the true underlying pattern, resulting in poor performance on new, unseen data.\n\n# MNIST\n\nFor this part of our practical, we will need some helper mdodeling packages besides `keras3`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Modeling packages\nlibrary(keras3)         # for fitting DNNs\n```\n:::\n\n\nWe’ll use the `MNIST` data to illustrate various DNN concepts. With DNNs, it is important to note a few items:\n\n-   Feedforward DNNs require all feature inputs to be numeric. Consequently, if your data contains categorical features they will need to be numerically encoded (e.g., one-hot encoded, integer label encoded, etc.).\n-   Due to the data transformation process that DNNs perform, they are highly sensitive to the individual scale of the feature values. Consequently, we should standardize our features first. Although the MNIST features are measured on the same scale (0–255), they are not standardized (i.e., have mean zero and unit variance); the code chunk below standardizes the MNIST data to resolve this.35\n-   Since we are working with a multinomial response (0–9), keras requires our response to be a one-hot encoded matrix, which can be accomplished with the keras function `to_categorical()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Import MNIST training data\nmnist <- dslabs::read_mnist()\nmnist_x <- mnist$train$images\nmnist_y <- mnist$train$labels\n\n# Rename columns and standardize feature values\ncolnames(mnist_x) <- paste0(\"V\", 1:ncol(mnist_x))\nmnist_x <- mnist_x / 255\n\n# One-hot encode response\nmnist_y <- to_categorical(mnist_y, 10)\n```\n:::\n\n\nNext we focus on the two features that are needed for the network architecture of a feedforward DNN: (1) layers and nodes, and (2) activation.\n\nFirst, we initiate our sequential feedforward DNN architecture with `keras_model_sequential()` and then add some dense layers. This example creates two hidden layers, the first with 128 nodes and the second with 64, followed by an output layer with 10 nodes. One thing to point out is that the first layer needs the `input_shape` argument to equal the number of features in your data; however, the successive layers are able to dynamically interpret the number of expected inputs based on the previous layer.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 128, input_shape = ncol(mnist_x)) %>%\n  layer_dense(units = 64) %>%\n  layer_dense(units = 10)\n```\n:::\n\n\nTo control the activation functions used in our layers we specify the `activation` argument. For the two hidden layers we add the ReLU activation function and for the output layer we specify `activation = softmax` (since MNIST is a multinomial classification problem).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 128, activation = \"relu\", input_shape = ncol(mnist_x)) %>%\n  layer_dense(units = 64, activation = \"relu\") %>%\n  layer_dense(units = 10, activation = \"softmax\")\n```\n:::\n\n\nNext, we need to incorporate a feedback mechanism to help our model learn.\n\n## Backpropagation\n\nOn the first run (or forward pass), the DNN will select a batch of observations, randomly assign weights across all the node connections, and predict the output. The engine of neural networks is how it assesses its own accuracy and automatically adjusts the weights across all the node connections to improve that accuracy. This process is called **backpropagation**. To perform backpropagation we need two things:\n\n-   An objective function;\n-   An optimizer.\n\nFirst, you need to establish an objective (loss) function to measure performance. For regression problems this might be mean squared error (MSE) and for classification problems it is commonly binary and multi-categorical cross entropy. DNNs can have multiple loss functions but we’ll just focus on using one.\n\nOn each forward pass the DNN will measure its performance based on the loss function chosen. The DNN will then work backwards through the layers, compute the gradient of the loss with regards to the network weights, adjust the weights a little in the opposite direction of the gradient, grab another batch of observations to run through the model,... rinse and repeat until the loss function is minimized. This process is known as mini-batch stochastic gradient descent (mini-batch SGD). There are several variants of mini-batch SGD algorithms; they primarily differ in how fast they descend the gradient (controlled by the learning rate). These different variations make up the different optimizers that can be used.\n\nTo incorporate the backpropagation piece of our DNN we include `compile()` in our code sequence. In addition to the optimizer and loss function arguments, we can also identify one or more metrics in addition to our loss function to track and report.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- keras_model_sequential() %>%\n  \n  # Network architecture\n  layer_dense(units = 128, activation = \"relu\", input_shape = ncol(mnist_x)) %>%\n  layer_dense(units = 64, activation = \"relu\") %>%\n  layer_dense(units = 10, activation = \"softmax\") %>%\n  \n  # Backpropagation\n  compile(\n    loss = 'categorical_crossentropy',\n    optimizer = optimizer_rmsprop(),\n    metrics = c('accuracy')\n  )\n```\n:::\n\n\nWe’ve created a base model, now we just need to train it with some data. To do so we feed our model into a `fit()` function along with our training data. We also provide a few other arguments that are worth mentioning:\n\n-   `batch_size`: As we mentioned in the last section, the DNN will take a batch of data to run through the mini-batch SGD process. Batch sizes can be between one and several hundred. Small values will be more computationally burdensome while large values provide less feedback signal. Values are typically provided as a power of two that fit nicely into the memory requirements of the GPU or CPU hardware like 32, 64, 128, 256, and so on.\n-   `epochs`: An epoch describes the number of times the algorithm sees the entire data set. So, each time the algorithm has seen all samples in the data set, an epoch has completed. In our training set, we have 60,000 observations so running batches of 128 will require 469 passes for one epoch. The more complex the features and relationships in your data, the more epochs you’ll require for your model to learn, adjust the weights, and minimize the loss function.\n-   `validation_split`: The model will hold out XX% of the data so that we can compute a more accurate estimate of an out-of-sample error rate.\n-   `verbose`: We set this to FALSE for brevity; however, when TRUE you will see a live update of the loss function in your RStudio IDE.\n\nPlotting the output shows how our loss function (and specified metrics) improve for each epoch. We see that our model’s performance is optimized at 5–10 epochs and then proceeds to overfit, which results in a flatlined accuracy rate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Train the model\nfit1 <- model %>%\n  fit(\n    x = mnist_x,\n    y = mnist_y,\n    epochs = 25,\n    batch_size = 128,\n    validation_split = 0.2,\n    verbose = FALSE\n  )\n\n# Display output\nfit1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFinal epoch (plot to see history):\n    accuracy: 0.9996\n        loss: 0.002017\nval_accuracy: 0.9767\n    val_loss: 0.1442 \n```\n\n\n:::\n\n```{.r .cell-code}\nplot(fit1)\n```\n\n::: {.cell-output-display}\n![](11-neural-nets_files/figure-html/ffDNN-fit-1.png){width=672}\n:::\n:::\n\n\nThis plot shows that as training progresses, training accuracy continues to increase and training loss keeps decreasing, while validation accuracy plateaus and validation loss begins to rise, indicating that the model starts to overfit after a certain number of epochs; learning the training data very well but no longer improving (and even worsening) its performance on unseen data.\n\n## Model Tuning\n\nNow that we have an understanding of producing and running a DNN model, the next task is to find an optimal one by tuning different hyperparameters. There are many ways to tune a DNN. Typically, the tuning process follows these general steps; however, there is often a lot of iteration among these:\n\n-   Adjust model capacity (layers & nodes);\n-   Add batch normalization;\n-   Add regularization;\n-   Adjust learning rate.\n\n### Model Capacity\n\nWe aim to maximize predictive performance while keeping model capacity as low as possible, since higher capacity allows a model to learn more patterns but also increases the risk of overfitting. Therefore, we focus on improving validation performance rather than training performance, and compare multiple model capacity settings with different numbers of layers and nodes while keeping all other parameters fixed.\n\n#### Exercise\n\nThe table below summarizes different model capacities you should evaluate, defined by the number of hidden layers and nodes per layer.\n\n| Size   | 1 Hidden Layer | 2 Hidden Layers | 3 Hidden Layers |\n|--------|----------------|-----------------|-----------------|\n| Small  | 16             | 16, 8           | 16, 8, 4        |\n| Medium | 64             | 64, 32          | 64, 32, 16      |\n| Large  | 256            | 256, 128        | 256, 128, 64    |\n\n: Model capacities assessed, represented by the number of hidden layers and nodes per layer.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# -------------------------------------------------\n# Table variants to run (size x # hidden layers)\n# -------------------------------------------------\nvariants <- tribble(\n  ~size,   ~layers, ~units,\n  \"small\",  1,      c(16),\n  \"small\",  2,      c(16, 8),\n  \"small\",  3,      c(16, 8, 4),\n  \"medium\", 1,      c(64),\n  \"medium\", 2,      c(64, 32),\n  \"medium\", 3,      c(64, 32, 16),\n  \"large\",  1,      c(256),\n  \"large\",  2,      c(256, 128),\n  \"large\",  3,      c(256, 128, 64)\n) %>%\n  mutate(layers_lab = paste(layers, \"layer\"))\n\n\n### Continue code here.....\n```\n:::\n\n\n::: callout-note\nIf your models do not reach a stable (flatlined) validation error, increase the number of training epochs. Conversely, if validation error stabilizes early, continuing to train wastes computational resources without improving performance. To address this, you can use callbacks within `fit()` to automate training decisions. One commonly used callback is early stopping, which halts training when the loss function fails to improve for a specified number of epochs.\n:::\n\n### Batch Normalization\n\nAlthough we normalized the input data before feeding it into the model, normalization remains important throughout the entire network, not just at the input stage. As data passes through each layer, its distribution can change during training. Batch normalization addresses this issue by adaptively normalizing layer outputs as their mean and variance shift over time. The primary benefit of batch normalization is improved gradient propagation, which makes training deeper neural networks more stable and efficient. As a result, the deeper your network becomes, the more important batch normalization is, and it can lead to better overall performance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_w_norm <- keras_model_sequential() %>%\n  \n  # Network architecture with batch normalization\n  layer_dense(units = 256, activation = \"relu\", input_shape = ncol(mnist_x)) %>%\n  layer_batch_normalization() %>%\n  layer_dense(units = 128, activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_dense(units = 64, activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_dense(units = 10, activation = \"softmax\") %>%\n\n  # Backpropagation\n  compile(\n    loss = \"categorical_crossentropy\",\n    optimizer = optimizer_rmsprop(),\n    metrics = c(\"accuracy\")\n  )\n```\n:::\n\n\nNow try to add batch normalization to each of the previously assessed models, you should see a couple patterns emerge. One, batch normalization often helps to minimize the validation loss sooner, which increases efficiency of model training. Two, we see that for the larger, more complex models (3-layer medium and 2- and 3-layer large), batch normalization helps to reduce the overall amount of overfitting.\n\n### Regularization\n\nPlacing constraints on a model’s complexity through regularization is a common way to reduce overfitting, and deep neural networks are no exception. Two widely used regularization approaches are the (L_1) and (L_2) penalties, which add a cost based on the magnitude of the model’s weights. In practice, the (L_2) norm, often referred to as weight decay in neural networks, is the most commonly used. Weight regularization encourages small, noisy signals to have weights close to zero while allowing consistently strong signals to retain larger weights.\n\n::: callout-note\nAs the number of layers and nodes increases, regularization using (L_1) or (L_2) penalties tends to have a greater impact on model performance. Because large models are more prone to overparameterization, these penalties help shrink unnecessary weights toward zero, reducing the risk of overfitting.\n:::\n\nWe can apply (L_1), (L_2), or a combination of both penalties by adding `regularizer_XX()` to each layer.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# L2 (weight decay) regularizer\nl2_reg <- regularizer_l2(0.001)\n\nmodel_w_reg <- keras_model_sequential()  %>% \n  layer_dense(\n    units = 256,\n    activation = \"relu\",\n    input_shape = c(ncol(mnist_x)),\n    kernel_regularizer = l2_reg\n  )  %>% \n  layer_batch_normalization()  %>% \n  layer_dense(\n    units = 128,\n    activation = \"relu\",\n    kernel_regularizer = l2_reg\n  )  %>% \n  layer_batch_normalization()  %>% \n  layer_dense(\n    units = 64,\n    activation = \"relu\",\n    kernel_regularizer = l2_reg\n  )  %>% \n  layer_batch_normalization()  %>% \n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel_w_reg  %>% \n  compile(\n    optimizer = optimizer_rmsprop(),\n    loss = \"categorical_crossentropy\",\n    metrics = \"accuracy\"\n  )\n\n# Fit (same settings you used)\nfit2 <- model_w_reg  %>% \n  fit(\n    x = mnist_x,\n    y = mnist_y,\n    epochs = 25,\n    batch_size = 128,\n    validation_split = 0.2,\n    verbose = 0\n  )\n\nfit2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFinal epoch (plot to see history):\n    accuracy: 0.985\n        loss: 0.1021\nval_accuracy: 0.975\n    val_loss: 0.1495 \n```\n\n\n:::\n\n```{.r .cell-code}\nplot(fit2)\n```\n\n::: {.cell-output-display}\n![](11-neural-nets_files/figure-html/regul-1.png){width=672}\n:::\n:::\n\n\nCompared to the model before regularization, this figure shows that regularization reduces overfitting but slightly limits peak performance.\n\nIn the unregularized model, training accuracy quickly approaches 100% and training loss goes to near zero, while validation loss begins to increase after several epochs, clear evidence that the model is memorizing the training data. After adding regularization, training improves more gradually and does not reach the same extreme levels, but validation loss stabilizes instead of rising and validation accuracy remains more consistent. This indicates that regularization constrains the model’s complexity, preventing it from fitting noise and leading to better generalization to unseen data.\n\nDropout is another widely used regularization technique for reducing overfitting in neural networks. During training, dropout randomly sets a proportion of a layer’s output units to zero, which prevents the model from relying too heavily on any single feature or accidental patterns in the data. Typical dropout rates range from 0.2 to 0.5, though the optimal value depends on the dataset and must be tuned. Dropout is applied between layers using `layer_dropout()`.\n\n## Adjust learning rate\n\nAnother important consideration is whether the optimization process converges to a global minimum or becomes trapped in a local minimum. Mini-batch stochastic gradient descent updates the model by taking small steps along the loss gradient, and the learning rate controls the size of these steps. If the learning rate is poorly chosen, the optimizer may stall in a local minimum rather than progressing toward the global minimum.\n\nThere are two main ways to address this issue. First, different optimizers (such as RMSProp, Adam, and Adagrad) use distinct strategies for adapting the learning rate, so we can either switch optimizers or manually tune the learning rate for a given optimizer. Second, the learning rate can be reduced automatically, often by a factor of 2 to 10, once the validation loss stops improving. Building on an optimal model, we switch to the Adam optimizer and decrease the learning rate by a factor of 0.05 as loss improvements begin to stall, while also incorporating early stopping to avoid unnecessary training time.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_w_adj_lrn <- keras_model_sequential() %>%\n  layer_dense(units = 256, activation = \"relu\", input_shape = ncol(mnist_x)) %>%\n  layer_batch_normalization() %>%\n  layer_dropout(rate = 0.4) %>%\n  layer_dense(units = 128, activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_dropout(rate = 0.3) %>%\n  layer_dense(units = 64, activation = \"relu\") %>%\n  layer_batch_normalization() %>%\n  layer_dropout(rate = 0.2) %>%\n  layer_dense(units = 10, activation = \"softmax\") %>%\n  compile(\n    loss = 'categorical_crossentropy',\n    optimizer = optimizer_adam(),\n    metrics = c('accuracy')\n  ) %>%\n  fit(\n    x = mnist_x,\n    y = mnist_y,\n    epochs = 35,\n    batch_size = 128,\n    validation_split = 0.2,\n    callbacks = list(\n      callback_early_stopping(patience = 5),\n      callback_reduce_lr_on_plateau(factor = 0.05)\n      ),\n    verbose = FALSE\n  )\n\nmodel_w_adj_lrn\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFinal epoch (plot to see history):\n     accuracy: 0.9828\n         loss: 0.05595\n val_accuracy: 0.9795\n     val_loss: 0.06854\nlearning_rate: 0.001 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Optimal\nmin(model_w_adj_lrn$metrics$val_loss)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.06786273\n```\n\n\n:::\n\n```{.r .cell-code}\nmax(model_w_adj_lrn$metrics$val_acc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.98025\n```\n\n\n:::\n\n```{.r .cell-code}\n# Learning rate\nplot(model_w_adj_lrn)\n```\n\n::: {.cell-output-display}\n![](11-neural-nets_files/figure-html/lern-rate-1.png){width=672}\n:::\n:::\n\n\nThis plot shows training behavior that is consistent with a well-regularized model using an adaptive optimizer and early stopping. The training loss decreases rapidly and then levels off, while the validation loss follows a similar trajectory and stabilizes without increasing, indicating that the model stops training before overfitting occurs. Training and validation accuracy increase together and converge to nearly the same value, suggesting strong generalization and minimal performance gap between the two. The learning rate remains constant throughout training, implying that learning-rate reduction was either not triggered or stabilized early, which is consistent with steady improvement in validation loss. Overall, the model training appears stable, efficient, and well controlled.\n\nOverall, we observe a modest improvement in performance, and the loss curve shows that training is halted at the point where overfitting begins to emerge.\n\n## Hyperparameter \nTuning Hyperparameter tuning for deep neural networks is often more involved than for other machine learning models because of the large number of hyperparameters and the dependencies between them. In practice, this requires deciding in advance on aspects such as the number of hidden layers and then defining a search grid over relevant parameters (e.g., number of units, learning rate, regularization strength). This process is similar in spirit to grid searches used for other models, but typically requires more manual coordination.\n\nIn the following example, we demonstrate a grid search by defining a set of hyperparameter combinations and iteratively training models on the MNIST dataset, recording performance metrics for comparison.\n\n> This run takes a very long time so by default `eval:false` and you need to change it when you want to perform the grid search.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# -----------------------------\n# Hyperparameter grid\n# -----------------------------\ngrid <- crossing(\n  nodes1 = c(128, 256),\n  nodes2 = c(64, 128),\n  nodes3 = c(32, 64),\n  dropout1 = c(0.3, 0.4),\n  dropout2 = c(0.2, 0.3),\n  dropout3 = c(0.1, 0.2),\n  lr_annealing = c(0.1, 0.5)\n)\n\n# -----------------------------\n# Model training function\n# -----------------------------\ntrain_model <- function(nodes1, nodes2, nodes3,\n                        dropout1, dropout2, dropout3,\n                        lr_annealing) {\n\n  model <- keras_model_sequential()  %>% \n    layer_dense(nodes1, activation = \"relu\",\n                input_shape = c(ncol(mnist_x)))  %>% \n    layer_batch_normalization()  %>% \n    layer_dropout(dropout1)  %>% \n    layer_dense(nodes2, activation = \"relu\")  %>% \n    layer_batch_normalization()  %>% \n    layer_dropout(dropout2)  %>% \n    layer_dense(nodes3, activation = \"relu\")  %>% \n    layer_batch_normalization()  %>% \n    layer_dropout(dropout3)  %>% \n    layer_dense(10, activation = \"softmax\")\n\n  model  %>% \n    compile(\n      optimizer = optimizer_rmsprop(),\n      loss = \"categorical_crossentropy\",\n      metrics = \"accuracy\"\n    )\n\n  history <- model  %>% \n    fit(\n      mnist_x,\n      mnist_y,\n      epochs = 35,\n      batch_size = 128,\n      validation_split = 0.2,\n      callbacks = list(\n        callback_early_stopping(patience = 5, restore_best_weights = TRUE),\n        callback_reduce_lr_on_plateau(factor = lr_annealing)\n      ),\n      verbose = 0\n    )\n\n  tibble(\n    best_val_loss = min(history$metrics$val_loss),\n    best_val_acc  = max(history$metrics$val_accuracy),\n    epochs_run    = length(history$metrics$loss)\n  )\n}\n\n# -----------------------------\n# Run grid search\n# -----------------------------\nresults <- grid  %>% \n  mutate(\n    metrics = pmap(\n      list(nodes1, nodes2, nodes3,\n           dropout1, dropout2, dropout3,\n           lr_annealing),\n      train_model\n    )\n  )  %>% \n  unnest(metrics)\n\n# -----------------------------\n# View best models\n# -----------------------------\nresults  %>% \n  arrange(desc(best_val_acc))  %>% \n  slice_head(n = 5)\n\n# Best model overall\nresults  %>%  arrange(desc(best_val_acc))  %>%  slice(1)\n\n# Plot accuracy vs dropout\nresults  %>%  \n  ggplot(aes(dropout1, best_val_acc)) +\n  geom_point()\n```\n:::\n\n\nThis plot shows the relationship between the first-layer dropout rate (dropout1) and the best validation accuracy achieved across all combinations of the other hyperparameters (layer sizes, additional dropout rates, and learning-rate annealing). Each point represents a different model configuration from the grid search.\n\nThe results indicate that increasing dropout1 from 0.30 to 0.40 does not substantially change overall validation performance, as both values produce models with similar peak accuracies clustered around 98%. However, the models with `dropout1 = 0.30` show slightly less variability and fewer low-performing runs, suggesting more stable learning. In contrast, `dropout1 = 0.40` occasionally leads to reduced performance, likely because stronger regularization removes too much signal early in the network.\n\nOverall, this suggests that the model is not highly sensitive to modest changes in first-layer dropout, but slightly lower dropout provides more consistent results within this hyperparameter range.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}