{
  "hash": "d6b1de722438644aedb12a86c020bdea",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"'Non-Linear' Linear Regression\"\nauthor: \"Termeh Shafie\"\nformat: html\neditor: visual\nexecute:\n  cache:  true\n---\n\n\n# Introduction\n\nWe will use multiple approaches for modelling non-linearity and apply it to the `Boston` dataset included in the R library `MASS`. This dataset consists of 506 samples. The response variable is median value of owner-occupied homes in Boston (`medv`). The dataset has 13 associated predictor variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\nhelp(Boston)\nnames(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n```\n\n\n:::\n\n```{.r .cell-code}\nhead(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     crim zn indus chas   nox    rm  age    dis rad tax ptratio  black lstat\n1 0.00632 18  2.31    0 0.538 6.575 65.2 4.0900   1 296    15.3 396.90  4.98\n2 0.02731  0  7.07    0 0.469 6.421 78.9 4.9671   2 242    17.8 396.90  9.14\n3 0.02729  0  7.07    0 0.469 7.185 61.1 4.9671   2 242    17.8 392.83  4.03\n4 0.03237  0  2.18    0 0.458 6.998 45.8 6.0622   3 222    18.7 394.63  2.94\n5 0.06905  0  2.18    0 0.458 7.147 54.2 6.0622   3 222    18.7 396.90  5.33\n6 0.02985  0  2.18    0 0.458 6.430 58.7 6.0622   3 222    18.7 394.12  5.21\n  medv\n1 24.0\n2 21.6\n3 34.7\n4 33.4\n5 36.2\n6 28.7\n```\n\n\n:::\n:::\n\n\nWe will analyse `medv` with respect to the predictor `lstat` (percentage of lower status population).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(cbind(Boston$medv, Boston$lstat))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,] 24.0 4.98\n[2,] 21.6 9.14\n[3,] 34.7 4.03\n[4,] 33.4 2.94\n[5,] 36.2 5.33\n[6,] 28.7 5.21\n```\n\n\n:::\n:::\n\n\nFor convenience we can name the response as `y` and the predictor `x`. We will also pre-define the labels for the x and y-axes that we will use repeatedly in figures throughout this practical.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny = Boston$medv\nx = Boston$lstat\ny.lab = 'Median Property Value'\nx.lab = 'Lower Status (%)'\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot( x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n      main = \"\", bty = 'l' )\n```\n\n::: {.cell-output-display}\n![](08-nonlinearity_files/figure-html/unnamed-chunk-4-1.png){width=576}\n:::\n:::\n\n\n# Polynomial Regression\n\nStart by fitting to the data a degree-2 polynomial using the command `lm()` and summarizing the results using `summary()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoly2 = lm(y ~ poly(x,  2,  raw = TRUE))\nsummary(poly2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ poly(x, 2, raw = TRUE))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             42.862007   0.872084   49.15   <2e-16 ***\npoly(x, 2, raw = TRUE)1 -2.332821   0.123803  -18.84   <2e-16 ***\npoly(x, 2, raw = TRUE)2  0.043547   0.003745   11.63   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,\tAdjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nThe argument `raw = TRUE` In terms of fitting the curve `poly(x, 2, raw = TRUE))` and `poly(x, 2))` will give the same result! They are just based on different (orthogonal) basis but with polynomial regression we are almost never interested in the regression coefficients.\n\nFor plotting th results, we need to create an object, which we name `sort.x`, which has the sorted values of predictor `x` in a ascending order. Without `sort.x` we will not be able to produce the plots since in lecture. Then, we need to use `predict()` with `sort.x` as input in order to proceed to the next steps.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsort.x = sort(x)\nsort.x[1:10]     # the first 10 sorted values of x \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 1.73 1.92 1.98 2.47 2.87 2.88 2.94 2.96 2.97 2.98\n```\n\n\n:::\n\n```{.r .cell-code}\npred2 = predict(poly2, newdata = list(x = sort.x), se = TRUE)\nnames(pred2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"fit\"            \"se.fit\"         \"df\"             \"residual.scale\"\n```\n\n\n:::\n:::\n\n\nThe object `pred2` contains fit, which are the fitted values, and `se.fit`, which are the standard errors of the mean prediction, that we need in order to construct the approximate 95% confidence intervals (of the mean prediction). With this information we can construct the confidence intervals using `cbind()`. Lets see how the first 10 fitted values and confidence intervals look like.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred2$fit[1:10]  # the first 10 fitted values of the curve\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1        2        3        4        5        6        7        8 \n38.95656 38.54352 38.41374 37.36561 36.52550 36.50468 36.37992 36.33840 \n       9       10 \n36.31765 36.29691 \n```\n\n\n:::\n\n```{.r .cell-code}\nse.bands2 = cbind( pred2$fit - 2 * pred2$se.fit, \n                   pred2$fit + 2 * pred2$se.fit )\nse.bands2[1:10,] # the first 10 confidence intervals of the curve\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       [,1]     [,2]\n1  37.58243 40.33069\n2  37.20668 39.88036\n3  37.08853 39.73895\n4  36.13278 38.59845\n5  35.36453 37.68647\n6  35.34546 37.66390\n7  35.23118 37.52865\n8  35.19314 37.48365\n9  35.17413 37.46117\n10 35.15513 37.43870\n```\n\n\n:::\n:::\n\n\nNow we can plot the results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"Degree-2 polynomial\", bty = 'l')\nlines(sort.x, pred2$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands2, lwd = 1.4, col = \"firebrick\", lty = 3)\n```\n\n::: {.cell-output-display}\n![](08-nonlinearity_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nNote: We use `lines()` for `pred2$fit` because this is a vector, but for `se.bands2`, which is a matrix, we have to use `matlines()`.\n\nThen we do similar steps to produce a plot of degree-2 up to degree-5 polynomial fits.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoly3 = lm(y ~ poly(x,  3))\npoly4 = lm(y ~ poly(x,  4))\npoly5 = lm(y ~ poly(x, 5))\n\npred3 = predict(poly3, newdata = list(x = sort.x), se = TRUE)\npred4 = predict(poly4, newdata = list(x = sort.x), se = TRUE)\npred5 = predict(poly5, newdata = list(x = sort.x), se = TRUE)\n\nse.bands3 = cbind(pred3$fit + 2*pred3$se.fit, pred3$fit-2*pred3$se.fit)\nse.bands4 = cbind(pred4$fit + 2*pred4$se.fit, pred4$fit-2*pred4$se.fit)\nse.bands5 = cbind(pred5$fit + 2*pred5$se.fit, pred5$fit-2*pred5$se.fit)\n\n\npar(mfrow = c(2,2))\n# Degree-2\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"Degree-2 polynomial\", bty = 'l')\nlines(sort.x, pred2$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands2, lwd = 2, col = \"firebrick\", lty = 3)\n\n# Degree-3\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"Degree-3 polynomial\", bty = 'l')\nlines(sort.x, pred3$fit, lwd = 2, col = \"darkviolet\")\nmatlines(sort.x, se.bands3, lwd = 2, col = \"darkviolet\", lty = 3)\n\n# Degree-4\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"Degree-4 polynomial\", bty = 'l')\nlines(sort.x, pred4$fit, lwd = 2, col = \"royalblue\")\nmatlines(sort.x, se.bands4, lwd = 2, col = \"royalblue\", lty = 3)\n\n# Degree-5\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"Degree-5 polynomial\", bty = 'l')\nlines(sort.x, pred5$fit, lwd = 2, col = \"darkgreen\")\nmatlines(sort.x, se.bands5, lwd = 2, col = \"darkgreen\", lty = 3)\n```\n\n::: {.cell-output-display}\n![](08-nonlinearity_files/figure-html/unnamed-chunk-9-1.png){width=864}\n:::\n:::\n\n\nAll four curves look reasonable given the data available. We may choose the degree-2 polynomial since it is simpler and seems to do about as well as the others. However, if we want to base our decision on a more formal procedure, we can use analysis-of-variance (ANOVA). Specifically, we will perform sequential comparisons based on the F-test, comparing first the linear model vs. the quadratic model (degree-2 polynomial), then the quadratic model vs. the cubic model (degree-3 polynomial) and so on. We therefore have to fit the simple linear model, and we also choose to fit the degree-6 polynomial to investigate the effects of an additional predictor as well. We can perform this analysis in RStudio using the command `anova()` as displayed below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoly1 = lm(y ~ x)\npoly6 = lm(y ~ poly(x, 6))\nanova(poly1, poly2, poly3, poly4, poly5, poly6)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: y ~ x\nModel 2: y ~ poly(x, 2, raw = TRUE)\nModel 3: y ~ poly(x, 3)\nModel 4: y ~ poly(x, 4)\nModel 5: y ~ poly(x, 5)\nModel 6: y ~ poly(x, 6)\n  Res.Df   RSS Df Sum of Sq        F    Pr(>F)    \n1    504 19472                                    \n2    503 15347  1    4125.1 151.8623 < 2.2e-16 ***\n3    502 14616  1     731.8  26.9390 3.061e-07 ***\n4    501 13968  1     647.8  23.8477 1.406e-06 ***\n5    500 13597  1     370.7  13.6453 0.0002452 ***\n6    499 13555  1      42.4   1.5596 0.2123125    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\nWhich model would you choose?\n\n# Step Functions\n\nFor step function regression we can make use of the command `cut()`, which automatically assigns samples to intervals given a specific number of intervals. We can check how this works by executing the following syntax:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(cut(x, 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n(1.69,19.9]   (19.9,38] \n        430          76 \n```\n\n\n:::\n:::\n\n\nWhat we see is that `cut(x, 2)` automatically created a factor with two levels, corresponding to the intervals $(1.69,19.9]$ and $(19.9,38]$ and assigned each entry in\\\n`x` to one of these factors depending on which interval it was in. The command `table()` tells us that 430 samples of `x` fall within the first interval and that 76 samples fall within the second interval. Note that `cut(x, 2)` generated 2 intervals, but this means there is only 1 cutpoint (at 19.9). The number of cutpoints is naturally one less than the number of intervals, but it is important to be aware that cut requires specification of the number of required intervals.\n\nSo, we can use `cut()` within `lm()` to easily fit regression models with step functions. Below we consider 4 models with 1, 2, 3 and 4 cutpoints (2, 3, 4 and 5 intervals) respectively.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstep2 = lm(y ~ cut(x, 2))\nstep3 = lm(y ~ cut(x, 3))\nstep4 = lm(y ~ cut(x, 4))\nstep5 = lm(y ~ cut(x, 5))\n```\n:::\n\n\nThe analysis then is essentially the same as previously. We plot the fitted lines of the four models, along with approximate 95% confidence intervals for the mean predictions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred2 = predict(step2, newdata = list(x = sort(x)), se = TRUE)\npred3 = predict(step3, newdata = list(x = sort(x)), se = TRUE)\npred4 = predict(step4, newdata = list(x = sort(x)), se = TRUE)\npred5 = predict(step5, newdata = list(x = sort(x)), se = TRUE)\n\nse.bands2 = cbind(pred2$fit + 2*pred2$se.fit, pred2$fit-2*pred2$se.fit)\nse.bands3 = cbind(pred3$fit + 2*pred3$se.fit, pred3$fit-2*pred3$se.fit)\nse.bands4 = cbind(pred4$fit + 2*pred4$se.fit, pred4$fit-2*pred4$se.fit)\nse.bands5 = cbind(pred5$fit + 2*pred5$se.fit, pred5$fit-2*pred5$se.fit)\n\npar(mfrow = c(2,2))\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"1 cutpoint\", bty = 'l')\nlines(sort(x), pred2$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort(x), se.bands2, lwd = 1.4, col = \"firebrick\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"2 cutpoints\", bty = 'l')\nlines(sort(x), pred3$fit, lwd = 2, col = \"darkviolet\")\nmatlines(sort(x), se.bands3, lwd = 1.4, col = \"darkviolet\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"3 cutpoints\", bty = 'l')\nlines(sort(x), pred4$fit, lwd = 2, col = \"royalblue\")\nmatlines(sort(x), se.bands4, lwd = 1.4, col = \"royalblue\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab,\n     main = \"4 cutpoints\", bty = 'l')\nlines(sort(x), pred5$fit, lwd = 2, col = \"darkgreen\")\nmatlines(sort(x), se.bands5, lwd = 1.4, col = \"darkgreen\", lty = 3)\n```\n\n::: {.cell-output-display}\n![](08-nonlinearity_files/figure-html/unnamed-chunk-13-1.png){width=864}\n:::\n:::\n\n\nNote that we do not necessarily need to rely on the automatic selections of cutpoints used by `cut()`. We can define the intervals if we want to. For instance, if we want cutpoints at 10, 20 and 30 we can do the following\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbreaks4 = c(min(x), 10, 20, 30, max(x))\ntable(cut(x, breaks = breaks4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n(1.73,10]   (10,20]   (20,30]   (30,38] \n      218       213        62        12 \n```\n\n\n:::\n:::\n\n\nBy including `min(x)` and `max(x)` at the start and end, we ensure the intervals covered the entire range of `x`. Our model is then\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstep.new4 = lm(y ~ cut(x, breaks = breaks4))\nsummary(step.new4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ cut(x, breaks = breaks4))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.4803  -4.6239  -0.4239   2.8968  20.6197 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                      29.3803     0.4415  66.540   <2e-16 ***\ncut(x, breaks = breaks4)(10,20] -10.4563     0.6281 -16.648   <2e-16 ***\ncut(x, breaks = breaks4)(20,30] -16.6770     0.9383 -17.773   <2e-16 ***\ncut(x, breaks = breaks4)(30,38] -18.6886     1.9331  -9.668   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.519 on 501 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.4925,\tAdjusted R-squared:  0.4895 \nF-statistic: 162.1 on 3 and 501 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\nWe can now make predictions at new data points using the constructed linear model as usual.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnewx <- c(10.56, 5.89)\npreds = predict(step.new4, newdata = list(x = newx), se = TRUE)\npreds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$fit\n       1        2 \n18.92394 29.38028 \n\n$se.fit\n        1         2 \n0.4466955 0.4415432 \n\n$df\n[1] 501\n\n$residual.scale\n[1] 6.519307\n```\n\n\n:::\n:::\n\n\n# Regression Splines\n\nFor this analysis we will require package `splines`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(splines)\n```\n:::\n\n\nInitially let’s fit regression splines by specifying knots. From the previous plot it is not clear where exactly we should place knots, so we will make use of the command summary in order to find the 25th, 50th and 75th percentiles of`x`, which will be the positions where we will place the knots. We also sort the variable `x` before fitting the splines.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.73    6.95   11.36   12.65   16.95   37.97 \n```\n\n\n:::\n\n```{.r .cell-code}\ncuts = summary(x)[c(2, 3, 5)] \ncuts\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1st Qu.  Median 3rd Qu. \n  6.950  11.360  16.955 \n```\n\n\n:::\n\n```{.r .cell-code}\nsort.x = sort(x)\n```\n:::\n\n\nFor a start lets fit a linear spline using our selected placement of knots. For this we can use command `lm()` and inside it we use the command `bs()` in which we specify `degree = 1` for a linear spline and `knots = cuts` for the placement of the knots at the three percentiles. We also calculate the corresponding fitted values and confidence intervals exactly in the same way we did in previous practical demonstrations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspline1 = lm(y ~  splines::bs(x, degree = 1, knots = cuts))\npred1 = predict(spline1, newdata = list(x = sort.x), se = TRUE)\nse.bands1 = cbind(pred1$fit + 2 * pred1$se.fit, \n                  pred1$fit - 2 * pred1$se.fit)\n```\n:::\n\n\nLet's plot the results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Linear Spline\", bty = 'l')\nlines(sort.x, pred1$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands1, lwd = 2, col = \"firebrick\", lty = 3)\n```\n\n::: {.cell-output-display}\n![](08-nonlinearity_files/figure-html/unnamed-chunk-20-1.png){width=480}\n:::\n:::\n\n\nUsing `?bs` we see that instead of using the argument `knots` we can use the argument `df`, which are the degrees of freedom. Splines have $(d+1)+K$ degrees of freedom, where $d$ is the degree of the polynomial and $K$ the number of knots. So in this case we have 1+1+3 = 5 degrees of freedom. Selecting `df = 5` in `bs()` will automatically use 3 knots placed at the 25th, 50th and 75th percentiles. Below we check whether the plot based on `df=5` is indeed the same as the previous plot and as we can see it is.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspline1df = lm(y ~ splines::bs(x, degree = 1, df = 5))\npred1df = predict(spline1df, newdata = list(x = sort.x), se = TRUE)\nse.bands1df = cbind( pred1df$fit + 2 * pred1df$se.fit, \n                     pred1df$fit - 2 * pred1df$se.fit )\n\npar(mfrow = c(1, 2))\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Linear Spline (with knots)\", bty = 'l')\nlines(sort.x, pred1$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands1, lwd = 2, col = \"firebrick\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Linear Spline (with df)\", bty = 'l')\nlines(sort.x, pred1df$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands1df, lwd = 2, col = \"firebrick\", lty = 3)\nmatlines(sort.x, se.bands1, lwd = 2, col = \"firebrick\", lty = 3)\n```\n\n::: {.cell-output-display}\n![](08-nonlinearity_files/figure-html/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n\nHaving seen how this works we can also fit a degree-2 (quadratic) and degree-3 (cubic) spline to the data, all we have to do is change `degree = 1` to `degree = 2` and `degree = 3` respectively. Also we increase the respective degrees of freedom from `df = 5` to `df = 6` and `df = 7` in order to keep the same number (and position) of knots in the quadratic and cubic spline models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspline2 = lm(y ~ splines::bs(x, degree = 2, df = 6))\npred2 = predict(spline2, newdata = list(x = sort.x), se = TRUE)\nse.bands2 = cbind(pred2$fit + 2 * pred2$se.fit, pred2$fit - 2 * pred2$se.fit)\n\nspline3 = lm(y ~ splines::bs(x, degree = 3, df = 7))\npred3 = predict(spline3, newdata = list(x = sort.x), se = TRUE)\nse.bands3 = cbind(pred3$fit + 2 * pred3$se.fit, pred3$fit - 2 * pred3$se.fit)\n\npar(mfrow = c(1,3))\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Linear Spline\", bty = 'l')\nlines(sort.x, pred1$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands1, lwd = 2, col = \"firebrick\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Quadratic Spline\", bty = 'l')\nlines(sort.x, pred2$fit, lwd = 2, col = \"darkgreen\")\nmatlines(sort.x, se.bands2, lwd = 2, col = \"darkgreen\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Cubic Spline\", bty = 'l')\nlines(sort.x, pred3$fit, lwd = 2, col = \"royalblue\")\nmatlines(sort.x, se.bands3, lwd = 2, col = \"royalblue\", lty = 3)\n```\n\n::: {.cell-output-display}\n![](08-nonlinearity_files/figure-html/unnamed-chunk-22-1.png){width=864}\n:::\n:::\n\n\n# Natural Splines\n\nFor natural splines, we can use the command `ns()`. As with the command `bs()` previously, we again have the option to either specify the knots manually (via the argument knots) or to simply pre-define the degrees of freedom (via the argument df). Below we use the latter option to fit four natural splines with 1, 2, 3 and 4 degrees of freedom. As we see using 1 degree of freedom actually results in just a linear model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspline.ns1 = lm(y ~ splines::ns(x, df = 1))\npred.ns1 = predict(spline.ns1, newdata = list(x = sort.x), se = TRUE)\nse.bands.ns1 = cbind(pred.ns1$fit + 2 * pred.ns1$se.fit, \n                     pred.ns1$fit - 2 * pred.ns1$se.fit)\n\nspline.ns2 = lm(y ~ splines::ns(x, df = 2))\npred.ns2 = predict(spline.ns2, newdata = list(x = sort.x), se = TRUE)\nse.bands.ns2 = cbind(pred.ns2$fit + 2 * pred.ns2$se.fit, \n                     pred.ns2$fit - 2 * pred.ns2$se.fit)\n\nspline.ns3 = lm(y ~ splines::ns(x, df = 3))\npred.ns3 = predict(spline.ns3, newdata = list(x = sort.x), se = TRUE)\nse.bands.ns3 = cbind(pred.ns3$fit + 2 * pred.ns3$se.fit, \n                     pred.ns3$fit - 2 * pred.ns3$se.fit)\n\nspline.ns4 = lm(y ~ splines::ns(x, df = 4))\npred.ns4 = predict(spline.ns4, newdata = list(x = sort.x), se = TRUE)\nse.bands.ns4 = cbind(pred.ns4$fit + 2 * pred.ns4$se.fit, \n                     pred.ns4$fit - 2 * pred.ns4$se.fit)\n\npar(mfrow = c(2, 2))\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Natural Spline (1 df)\", bty = 'l')\nlines(sort.x, pred.ns1$fit, lwd = 2, col = \"firebrick\")\nmatlines(sort.x, se.bands.ns1, lwd = 2, col = \"firebrick\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Natural Spline (2 df)\", bty = 'l')\nlines(sort.x, pred.ns2$fit, lwd = 2, col = \"darkviolet\")\nmatlines(sort.x, se.bands.ns2, lwd = 2, col = \"darkviolet\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Natural Spline (3 df)\", bty = 'l')\nlines(sort.x, pred.ns3$fit, lwd = 2, col = \"royalblue\")\nmatlines(sort.x, se.bands.ns3, lwd = 2, col = \"royalblue\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Natural Spline (4 df)\", bty = 'l')\nlines(sort.x, pred.ns4$fit, lwd = 2, col = \"darkgreen\")\nmatlines(sort.x, se.bands.ns4, lwd = 2, col = \"darkgreen\", lty = 3)\n```\n\n::: {.cell-output-display}\n![](08-nonlinearity_files/figure-html/unnamed-chunk-23-1.png){width=864}\n:::\n:::\n\n\nBelow we plot the cubic spline next to the natural cubic spline for comparison. As we can see, the natural cubic spline is generally smoother and closer to linear on the right boundary of the predictor space, where it has, additionally, narrower confidence intervals in comparison to the cubic spline.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1,2))\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Cubic Spline\", bty = 'l')\nlines(sort.x, pred3$fit, lwd = 2, col = \"darkblue\")\nmatlines(sort.x, se.bands3, lwd = 2, col = \"darkblue\", lty = 3)\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Natural Spline (3 df)\", bty = 'l')\nlines(sort.x, pred.ns3$fit, lwd = 2, col = \"royalblue\")\nmatlines(sort.x, se.bands.ns3, lwd = 2, col = \"royalblue\", lty = 3)\n```\n\n::: {.cell-output-display}\n![](08-nonlinearity_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n# Smoothing Splines\n\nFor fitting smoothing splines we use the command `smooth.splines()` instead of `lm()`. Under smoothing splines there are no knots to specify; the only parameter is $\\lambda$. This can be specified via cross-validation by specifying `cv = TRUE` inside `smooth.splines()`. Alternatively, we can specify the effective degrees of freedom which correspond to some value of $\\lambda$. Below we first fit a smoothing spline with 3 effective degrees of freedom (via the argument `df = 3`), and then also by tuning $\\lambda$ via cross-validation. In this case we see that tuning $\\lambda$ through cross-validation results in a curve which is slightly wiggly on the right boundary of the predictor space.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsmooth1 = smooth.spline(x, y, df = 3)\nsmooth2 = smooth.spline(x, y, cv = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in smooth.spline(x, y, cv = TRUE): cross-validation with non-unique 'x'\nvalues seems doubtful\n```\n\n\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1,2))\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Smoothing Spline (3 df)\", bty = 'l')\nlines(smooth1, lwd = 2, col = \"brown\")\n\nplot(x, y, cex.lab = 1.1, col=\"darkgrey\", xlab = x.lab, ylab = y.lab, \n     main = \"Smoothing Spline (CV)\", bty = 'l')\nlines(smooth2, lwd = 2, col = \"darkorange\")\n```\n\n::: {.cell-output-display}\n![](08-nonlinearity_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\nNote: the effective degrees of freedom of a smoothing spline are similar to the degrees of freedom in standard spline models and can be used as an alternative to cross-validation as a way to fix $\\lambda$.\n\n# GAMs\n\nIn this final part we will fit a generalised additive model (GAM) utilising more than one predictor from the Boston dataset.We first use the command `names()` in order to check once again the available predictor variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(Boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n```\n\n\n:::\n:::\n\n\nLet’s say that we want to use predictors `lstat`, `indus` and `chas` for the analysis (use ?Boston again to check what these refer to).\n\nFor GAMs we will make use of the library `gam`, so the first thing that we have to do is to install this package by executing `install.packages(\"gam\")` once. Then we load the library.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gam)\n```\n:::\n\n\nThe main function is `gam()`. Inside this function we can use any combination of non-linear and linear modelling of the various predictors. For example below we use a cubic spline with 5 degrees of freedom for `lstat`, a smoothing spline with 5 degrees of freedom for indus and a simple linear model for variable chas. We then plot the contributions of each predictor using the command `plot()`. As we can see, GAMs are very useful as they estimate the contribution of the effects of each predictor.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngam = gam( medv ~ bs(lstat, degree = 3, df = 5) + s(indus, df = 5) + chas, \n           data = Boston )\npar( mfrow = c(1,3) )\nplot( gam,  se = TRUE, col = \"blue\" )\n```\n\n::: {.cell-output-display}\n![](08-nonlinearity_files/figure-html/unnamed-chunk-28-1.png){width=768}\n:::\n:::\n\n\nNote that simply using `chas` inside `gam()` is just fitting a linear model for this variable. However, one thing that we observe is that `chas` is a binary variable as it only takes the values of 0 and 1. This we can see from the x-axis of the `chas` plot on the right above. So, it would be preferable to use a step function for this variable. In order to do this we have to change the variable `chas` to a factor. We first create a second object called `Boston1` (in order not to change the initial dataset `Boston`) and then we use the command `factor()` to change variable `chas`. Then we fit again the same model. As we can see below now `gam()` fits a step function for variable chas which is more appropriate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBoston1 = Boston\nBoston1$chas = factor(Boston1$chas)\n\ngam1 = gam( medv ~ bs(lstat, degree = 3, df = 5) + s(indus, df = 5) + chas, \n            data = Boston1 )\npar(mfrow = c(1,3))\nplot(gam1,  se = TRUE, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](08-nonlinearity_files/figure-html/unnamed-chunk-29-1.png){width=768}\n:::\n:::\n\n\nWe can make predictions from `gam` objects, just like `lm` objects, using the `predict()` method for the class `gam`. Here we make predictions on some new data. Note that when assigning the value 0 to `chas`, we enclose it in \"\" since we informed R to treat `chas` as a categorical factor with two levels: \"0\" and \"1\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds <- predict( gam1, \n                  newdata = data.frame( chas = \"0\", indus = 3, lstat = 5 )  )\npreds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       1 \n32.10065 \n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}