{
  "hash": "447a54f74a8eba60d6ac185965c06c88",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear Regression II\"\nauthor: \"Termeh Shafie\"\nformat: html\neditor: visual\nexecute:\n  cache:  true\n---\n\n\n\n# Evaluate Model Performance\n\nOur goal is to run a few models of different complexity and evaluate the performance based on a test-train split of the data. Can you understand why we do not use the full data to test the different models?\n\nWe will start by creating a function that allows us to assess model performance:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse = function(actual, predicted) {\n  sqrt(mean((actual - predicted) ^ 2))\n}\n```\n:::\n\n\nWe will also use a function that gives the complexity of the linear model as number of independent variables defined.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_complexity = function(model) {\n  length(coef(model)) - 1\n}\n```\n:::\n\n\nCan you understand why we take -1 in the above function?\n\n## Data\n\nWe will use the Advertisement data from ISLR2 which is in a compressed folder for download. Make sure you have set the correct working directory for reading the data. You can also load the data from the `ISLR2` package.  The `Advertising` data set consists of the sales of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: TV, radio, and newspaper.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr) # install if not in your library\nAdvertising = read_csv(\"03-data/Advertising.csv\") \nknitr::kable(head(Advertising,10)) # look at 10 first rows of data\n```\n\n::: {.cell-output-display}\n\n\n|    TV| Radio| Newspaper| Sales|\n|-----:|-----:|---------:|-----:|\n| 230.1|  37.8|      69.2|  22.1|\n|  44.5|  39.3|      45.1|  10.4|\n|  17.2|  45.9|      69.3|   9.3|\n| 151.5|  41.3|      58.5|  18.5|\n| 180.8|  10.8|      58.4|  12.9|\n|   8.7|  48.9|      75.0|   7.2|\n|  57.5|  32.8|      23.5|  11.8|\n| 120.2|  19.6|      11.6|  13.2|\n|   8.6|   2.1|       1.0|   4.8|\n| 199.8|   2.6|      21.2|  10.6|\n\n\n:::\n:::\n\n\nExplore the data by doing some plots. Example shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(Sales ~ TV, data = Advertising, col = \"dodgerblue\", pch = 20, cex = 1.5,\n     main = \"Sales vs Television Advertising\")\n```\n\n::: {.cell-output-display}\n![](03-linear-regression-II_files/figure-html/unnamed-chunk-4-1.png){width=864}\n:::\n:::\n\n\nWe can also explore the data by using a correlation matrix plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(Advertising)\n```\n\n::: {.cell-output-display}\n![](03-linear-regression-II_files/figure-html/unnamed-chunk-5-1.png){width=864}\n:::\n:::\n\nWhat are the main patterns you detect?\n\n## Test-Train Split\n\nWe will now split the data in two. One part of the datasets will be used to fit (train) a model, which we will call the **training data**. The remainder of the original data will be used to assess how well the model is predicting, which we will call the **test data**.\n\nWe use the `sample()` function to obtain a random sample of the rows of the original data. We then use those row numbers (and remaining row numbers) to split the data accordingly. Notice we used the `set.seed()` function to reproduce the same random split each time we perform this analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nnum_obs = nrow(Advertising)\n\ntrain_index = sample(num_obs, size = trunc(0.50 * num_obs))\ntrain_data = Advertising[train_index, ]\ntest_data = Advertising[-train_index, ]\n```\n:::\n\n\n## The Model\n\nWe start by fitting the simplest linear model, a model with *no* predictors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_0 = lm(Sales ~ 1, data = train_data)\nget_complexity(fit_0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\nAs seen, the complexity of this model is 0. We compute the Test and Train RMSE for this model, via the function specified above and a direct formula:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# train RMSE\nrmse(actual = train_data$Sales, predicted = predict(fit_0, train_data))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.297333\n```\n\n\n:::\n\n```{.r .cell-code}\nsqrt(mean((train_data$Sales - predict(fit_0, train_data)) ^ 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.297333\n```\n\n\n:::\n\n```{.r .cell-code}\n# test RMSE\nrmse(actual = test_data$Sales, predicted = predict(fit_0, test_data))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.112073\n```\n\n\n:::\n\n```{.r .cell-code}\nsqrt(mean((test_data$Sales - predict(fit_0, test_data)) ^ 2)) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5.112073\n```\n\n\n:::\n:::\n\n\nLet's make the computations of RMSE even more easy by using the below function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# train RMSE\nget_rmse = function(model, data, response) {\n  rmse(actual = subset(data, select = response, drop = TRUE),\n       predicted = predict(model, data))\n}\n```\n:::\n\n\nCan you figure out how to use the above function?\n\nWe will fit 5 models, compute the train and test MSE for each model to evaluate it and visualize how it relates to the complexity (i.e. model size in terms of parameters) of the model. Furthermore, we will interpret the results in terms of underfitting or overfitting. We will conclude with dertemrineing which model to choose. The five models to fit are listed below:\n\n\n-   `Sales ~ Radio + Newspaper + TV`\n-   `Sales ~ Radio * Newspaper * TV`\n-   `Sales ~ Radio * Newspaper * TV + I(TV ^ 2)`\n-   `Sales ~ Radio * Newspaper * TV + I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2)`\n-   `Sales ~ Radio * Newspaper * TV + I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2)`\n\nNote that the specification first\\*second indicates the cross of first and second. This is the same as first + second + first:second.\n\n\nWe fit all five models and print the output by creating a list of the model fits. We then obtain train RMSE, test RMSE, and model complexity for each. Finally, plot the results. The train RMSE can be seen in blue, while the test RMSE is given in orange.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_1 = lm(Sales ~ Radio + Newspaper + TV, data = train_data)\nfit_2 = lm(Sales ~ Radio * Newspaper * TV, data = train_data)\nfit_3 = lm(Sales ~ Radio * Newspaper * TV + I(TV ^ 2), data = train_data)\nfit_4 = lm(Sales ~ Radio * Newspaper * TV + \n             I(TV ^ 2) + I(Radio ^ 2) + I(Newspaper ^ 2), data = train_data)\nfit_5 = lm(Sales ~ Radio * Newspaper * TV +\n             I(TV ^ 2) * I(Radio ^ 2) * I(Newspaper ^ 2), data = train_data)\n\nmodel_list = list(fit_1, fit_2, fit_3, fit_4, fit_5)\n#model_list #uncomment if you wish to see this list object\n\ntrain_rmse = sapply(model_list, get_rmse, data = train_data, response = \"Sales\")\ntest_rmse = sapply(model_list, get_rmse, data = test_data, response = \"Sales\")\nmodel_complexity = sapply(model_list, get_complexity)\n\ntrain_rmse = sapply(model_list, get_rmse, data = train_data, response = \"Sales\")\ntest_rmse = sapply(model_list, get_rmse, data = test_data, response = \"Sales\")\nmodel_complexity = sapply(model_list, get_complexity)\n\nplot(model_complexity, train_rmse, type = \"b\", \n     ylim = c(min(c(train_rmse, test_rmse)) - 0.02, \n              max(c(train_rmse, test_rmse)) + 0.02), \n     col = \"blue\", \n     lwd = 2,\n     xlab = \"Complexity (Model Size)\",\n     ylab = \"RMSE\")\nlines(model_complexity, test_rmse, type = \"b\", col = \"red\", lwd = 2)\ngrid()\n```\n\n::: {.cell-output-display}\n![](03-linear-regression-II_files/figure-html/unnamed-chunk-10-1.png){width=864}\n:::\n:::\n\n\nConclusion:\n\n-   **Underfitting models**: In general High Train RMSE, High Test RMSE. Seen in fit_1 and fit_2.\n\n-   **Overfitting models**: In general Low Train RMSE, High Test RMSE. Seen in fit_4 and fit_5.\n\nWe see that the Test RMSE is smallest for fit_3, and it is the model we believe will perform the best on future data not used to train the model.\n\n## Exercises\n 1. Write functions for the some of the other loss functions we covered in the lecture (e.g. MAE, MAPE). Apply them to asses the same five models above. Do you see similar or deviant results in terms of overfitting/underfitting?\n \n 2. Replicate the simulation example from lecture. Try other models based on different order polynomials.\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}