{
  "hash": "370b78f1edaedc59b0d0662da9e2f899",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Statistical Learning Lab 12 - Clustering\"\neditor: visual\nformat:\n  html:\n    embed-resources: true\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(jsonlite)\nlibrary(ggplot2)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()  masks stats::filter()\n✖ purrr::flatten() masks jsonlite::flatten()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(dbscan)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'dbscan'\n\nThe following object is masked from 'package:stats':\n\n    as.dendrogram\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(mclust)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nPackage 'mclust' version 6.1.1\nType 'citation(\"mclust\")' for citing this R package in publications.\n\nAttaching package: 'mclust'\n\nThe following object is masked from 'package:purrr':\n\n    map\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(randomForest)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrandomForest 4.7-1.2\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: 'randomForest'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n```\n\n\n:::\n\n```{.r .cell-code}\ndf_snakes = fromJSON('Snake_educlust.json')\ndf_snake = df_snakes$data\ndf_snake$xpo = as.numeric(df_snake$xpo)\ndf_snake  = df_snake %>% select(xpo, ypo)\n\ndf_hier = fromJSON('Hierarchical_educlust.json')\ndf_hier = df_hier$data\ndf_hier$xpo = as.numeric(df_hier$xpo)\ndf_hier  = df_hier %>% select(xpo, ypo)\n```\n:::\n\n\n## Clustering\n\n### $K$-Means Clustering\n\nThe function `kmeans()` performs $K$-means clustering in `R`. We begin with a simple simulated example in which there truly are two clusters in the data: the first 25 observations have a mean shift relative to the next 25 observations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2)\nx <- matrix(rnorm(50 * 2), ncol = 2)\nx[1:25, 1] <- x[1:25, 1] + 3\nx[1:25, 2] <- x[1:25, 2] - 4\n```\n:::\n\n\nWe now perform $K$-means clustering with $K=2$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkm.out <- kmeans(x, 2, nstart = 20)\n```\n:::\n\n\nThe cluster assignments of the 50 observations are contained in `km.out$cluster`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkm.out$cluster\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n[39] 2 2 2 2 2 2 2 2 2 2 2 2\n```\n\n\n:::\n:::\n\n\nThe $K$-means clustering perfectly separated the observations into two clusters even though we did not supply any group information to `kmeans()`. We can plot the data, with each observation colored according to its cluster assignment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#par(mfrow = c(1, 2))\nplot(x, col = (km.out$cluster + 1),\n    main = \"K-Means Clustering Results with K = 2\",\n    xlab = \"\", ylab = \"\", pch = 20, cex = 2)\n```\n\n::: {.cell-output-display}\n![](12_Clustering_files/figure-html/chunk31-1.png){width=672}\n:::\n:::\n\n\nHere the observations can be easily plotted because they are two-dimensional. If there were more than two variables then we could instead perform PCA and plot the first two principal components score vectors.\n\nIn this example, we knew that there really were two clusters because we generated the data. However, for real data, in general we do not know the true number of clusters. We could instead have performed $K$-means clustering on this example with $K=3$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(4)\nkm.out <- kmeans(x, 3, nstart = 20)\nkm.out\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nK-means clustering with 3 clusters of sizes 10, 23, 17\n\nCluster means:\n        [,1]        [,2]\n1  2.3001545 -2.69622023\n2 -0.3820397 -0.08740753\n3  3.7789567 -4.56200798\n\nClustering vector:\n [1] 3 1 3 1 3 3 3 1 3 1 3 1 3 1 3 1 3 3 3 3 3 1 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2\n[39] 2 2 2 2 2 1 2 1 2 2 2 2\n\nWithin cluster sum of squares by cluster:\n[1] 19.56137 52.67700 25.74089\n (between_SS / total_SS =  79.3 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n```\n\n\n:::\n\n```{.r .cell-code}\nplot(x, col = (km.out$cluster + 1),\n    main = \"K-Means Clustering Results with K = 3\",\n    xlab = \"\", ylab = \"\", pch = 20, cex = 2)\n```\n\n::: {.cell-output-display}\n![](12_Clustering_files/figure-html/chunk32-1.png){width=672}\n:::\n:::\n\n\nWhen $K=3$, $K$-means clustering splits up the two clusters.\n\nTo run the `kmeans()` function in `R` with multiple initial cluster assignments, we use the `nstart` argument. If a value of `nstart` greater than one is used, then $K$-means clustering will be performed using multiple random assignments in Step\\~1 of Algorithm 12.2, and the `kmeans()` function will report only the best results. Here we compare using `nstart = 1` to `nstart = 20`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(4)\nkm.out <- kmeans(x, 3, nstart = 1)\nkm.out$tot.withinss\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 104.3319\n```\n\n\n:::\n\n```{.r .cell-code}\nkm.out <- kmeans(x, 3, nstart = 20)\nkm.out$tot.withinss\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 97.97927\n```\n\n\n:::\n:::\n\n\nNote that `km.out$tot.withinss` is the total within-cluster sum of squares, which we seek to minimize by performing $K$-means clustering (Equation 12.17). The individual within-cluster sum-of-squares are contained in the vector `km.out$withinss`.\n\nWe *strongly* recommend always running $K$-means clustering with a large value of `nstart`, such as 20 or 50, since otherwise an undesirable local optimum may be obtained.\n\nWhen performing $K$-means clustering, in addition to using multiple initial cluster assignments, it is also important to set a random seed using the `set.seed()` function. This way, the initial cluster assignments in Step\\~1 can be replicated, and the $K$-means output will be fully reproducible.\n\nLet's try k-means clustering on a more complicated dataset. Open questions:\n\n-   How many clusters are in this dataset? (Hyperparameter choice)\n\n-   How many can we expect k-means to detect?\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_hier %>% ggplot(aes(x = xpo, y = ypo)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](12_Clustering_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nWe can attempt to tune our clustering algorithm by observing the total within-cluster sum of squares over each level of the hyperparameter k. Keep in mind this is by no means a \"correct\" partition of the dataspace but rather a tool to aid in decisionmaking.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntune_kmeans = function(data, kmax) {\n  result = c()\n  for (i in c(1:kmax)) {\n    km.out <- kmeans(data, i, nstart = 20)\n    result[i] = km.out$tot.withinss\n  }\n  df_viz = data.frame(k = c(1:kmax),\n                      within_ss = result)\n  \n}\n\n\ndf_viz = tune_kmeans(df_hier, 10)\n\ndf_viz %>%\n  ggplot(aes(x = k, y = within_ss)) +\n  geom_line() +  scale_x_continuous(breaks=seq(1,10,1))\n```\n\n::: {.cell-output-display}\n![](12_Clustering_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(4)\nkm.out <- kmeans(df_hier, 6, nstart = 50)\nplot(df_hier, col = (km.out$cluster + 1),\n    main = \"K-Means Clustering Results\",\n    xlab = \"\", ylab = \"\", pch = 20, cex = 2)\n```\n\n::: {.cell-output-display}\n![](12_Clustering_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n### Hierarchical Clustering\n\nThe `hclust()` function implements hierarchical clustering in `R`. In the following example we use the data from the previous lab to plot the hierarchical clustering dendrogram using complete, single, and average linkage clustering, with Euclidean distance as the dissimilarity measure. We begin by clustering observations using complete linkage. The `dist()` function is used to compute the $50 \\times 50$ inter-observation Euclidean distance matrix.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhc.complete <- hclust(dist(x), method = \"complete\")\nhc.single <- hclust(dist(x), method = \"single\")\n```\n:::\n\n\nWe can now plot the dendrograms obtained using the usual `plot()` function. The numbers at the bottom of the plot identify each observation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 2))\nplot(hc.complete, main = \"Complete Linkage\",\n    xlab = \"\", sub = \"\", cex = .9)\nplot(hc.single, main = \"Single Linkage\",\n    xlab = \"\", sub = \"\", cex = .9)\n```\n\n::: {.cell-output-display}\n![](12_Clustering_files/figure-html/chunk36-1.png){width=672}\n:::\n:::\n\n\nTo determine the cluster labels for each observation associated with a given cut of the dendrogram, we can use the `cutree()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncutree(hc.complete, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n[39] 2 2 2 2 2 2 2 2 2 2 2 2\n```\n\n\n:::\n\n```{.r .cell-code}\ncutree(hc.single, 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[39] 1 1 1 1 1 1 1 1 1 1 1 1\n```\n\n\n:::\n:::\n\n\nThe second argument to `cutree()` is the number of clusters we wish to obtain. For this data, complete and average linkage generally separate the observations into their correct groups. However, single linkage identifies one point as belonging to its own cluster. A more sensible answer is obtained when four clusters are selected, although there are still two singletons.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncutree(hc.single, 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3\n[39] 3 3 3 4 3 3 3 3 3 3 3 3\n```\n\n\n:::\n:::\n\n\nLets try our hierarchical dataset again:\\\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(hclust(dist(df_hier), method = \"complete\"),\n     labels = FALSE, hang = -1, ann = FALSE, xlim = c(0,10))\n```\n\n::: {.cell-output-display}\n![](12_Clustering_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(4)\nhc.complete <- hclust(dist(df_hier), method = \"complete\")\n\ndf_viz = df_hier\ndf_viz$cluster = factor(cutree(hc.complete, 9))\n\ndf_viz %>% ggplot(aes( x = xpo, y = ypo, color = cluster)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](12_Clustering_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nUsing hierarchical clustering we have a nice understanding of the hierarchical nature of our data. The dendrogram shows clearly the order in which the communities are constructed and can be used to detect nested communities. We can observe that the model is quite susceptible to the random noise between the \"expected\" clusters.\n\n### DBSCAN\n\nIf we use a density based approach the result changes slightly: The model fails in helping us understand the hierarchical data structure on the right. Keep in mind that the algorithm directly uses the density of observations in the dataspace. A mix of high and low clustering makes a proper choice for epsilon difficult.\n\nIt is however much less susceptible to noise as the minPts parameter does not allow the model fit arbitrarily small clusters, instead all the noise is grouped into one additional cluster.\n\n\n::: {.cell}\n\n```{.r .cell-code}\neps <- 5  \nminPts <- 3\n\ndb <- dbscan(df_hier, eps = eps, minPts = minPts)\n\ndf_viz = df_hier\ndf_viz$col = factor(db$cluster)\ndf_viz %>% ggplot(aes( x = xpo, y = ypo, color = col)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](12_Clustering_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n### Model Based Clustering - Gaussian Mixture Model\n\nGaussian-mixture model is as the name implies a model-based approach, meaning we can get uncertainty estimations for each data point and each cluster!\n\nThe \"classification\" vector stores the highest probability class of each observation. The \"uncertainty\" vector stores the related uncertainty of that classification.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngmm = Mclust(df_hier)\ndf_viz = df_hier\ndf_viz$col = factor(gmm$classification)\ndf_viz %>% ggplot(aes( x = xpo, y = ypo, color = col)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](12_Clustering_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngmm_uncertainty = data.frame(gmm$uncertainty)\ngmm_uncertainty %>%\n  ggplot(aes(x = gmm.uncertainty)) + geom_density()\n```\n\n::: {.cell-output-display}\n![](12_Clustering_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nZ is a G x n matrix that stores each individual inclusion probability between observation-cluster pairs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuncertainty_matrix = data.frame(gmm[[\"z\"]])\nhead(uncertainty_matrix, n = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            X1           X2           X3           X4           X5\n1 9.999863e-01 1.025362e-56 3.861321e-21 1.371037e-05 1.370416e-49\n2 9.999901e-01 2.107018e-60 5.143002e-24 9.894991e-06 3.416674e-51\n3 9.999928e-01 4.353228e-66 1.303422e-28 7.183974e-06 9.689301e-54\n4 3.425475e-27 9.999989e-01 9.836350e-29 1.090513e-06 1.875254e-60\n5 1.308746e-26 9.999988e-01 1.575384e-27 1.182247e-06 1.262486e-59\n             X6            X7\n1  1.209802e-92 1.639829e-130\n2  3.117873e-93 5.294943e-133\n3  2.487486e-94 5.873509e-137\n4 1.603307e-157 1.367950e-131\n5 8.962820e-156 1.208672e-130\n```\n\n\n:::\n:::\n\n\nUnfortunately we loose some flexibility with a model based approach: The gaussian assumption does not hold for many real world datasets and we cannot really detect arbitrarily shaped communities as the model expects convex / ellipsoid distributions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngmm = Mclust(df_snake)\ndf_viz = df_snake\ndf_viz$col = factor(gmm$classification)\ndf_viz %>% ggplot(aes( x = xpo, y = ypo, color = col)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](12_Clustering_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nCompare this to the previous models: Hierarchical clustering identifies our expected communities when using single linkage and DBSCAN can be tuned to do so as well.\\\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(4)\nhc.complete <- hclust(dist(df_snake), method = \"single\")\n\ndf_viz = df_snake\ndf_viz$cluster = factor(cutree(hc.complete, 4))\n\ndf_viz %>% ggplot(aes( x = xpo, y = ypo, color = cluster)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](12_Clustering_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\neps <- 5  \nminPts <- 3\n\ndb <- dbscan(df_snake, eps = eps, minPts = minPts)\n\ndf_viz = df_snake\ndf_viz$col = factor(db$cluster)\ndf_viz %>% ggplot(aes( x = xpo, y = ypo, color = col)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](12_Clustering_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n## Application - Young People Survey:\n\nLet's use our newfound skills to classify some people. Below we find survey data of 1000 people in Slovakia aged 15 to 30. We will try to detect distinct communities using clustering.\n\nFirst we load the dataset and keep only numeric data (we could also transform the categorical)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresponses = read.csv('responses.csv')\ndf_youth = select_if(responses, is.numeric)\ndf_youth = df_youth[complete.cases(df_youth), ]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(4)\nhc.complete <- hclust(dist(df_youth), method = \"complete\")\n\nplot(hclust(dist(df_youth), method = \"complete\"),\n     labels = FALSE, hang = -1, ann = FALSE)\n```\n\n::: {.cell-output-display}\n![](12_Clustering_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_viz = df_youth\ndf_viz$cluster = factor(cutree(hc.complete, 4))\n\ndf_viz %>% ggplot(aes( x = Age,\n                       y = Number.of.friends, color = cluster)) + geom_jitter()\n```\n\n::: {.cell-output-display}\n![](12_Clustering_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nThis tells us very little as we have no idea which variables to visualize! There appear to be at least two interesting communities here but to learn more we need to go back to supervised learning.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_viz$Cluster2 = ifelse(df_viz$cluster == 2, 1, 0)\ndf_viz = df_viz %>% select(-cluster)\n\n\nrf.fit = randomForest(Cluster2 ~ ., data=df_viz, mtry = 20, importance = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in randomForest.default(m, y, ...): The response has five or fewer\nunique values.  Are you sure you want to do regression?\n```\n\n\n:::\n:::\n\n\nInterestingly enough we see factors stereotypically associated with \"outsiders\" as most relevant for belonging in the second cluster. We will continue here next week with PCA.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvarImpPlot(rf.fit, n.var = 10)\n```\n\n::: {.cell-output-display}\n![](12_Clustering_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "12_Clustering_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}