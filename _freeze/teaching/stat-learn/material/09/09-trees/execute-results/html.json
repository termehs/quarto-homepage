{
  "hash": "c0886b29a4991061af242f3dbcf4c65b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Tree Based Methods\"\nauthor: \"Termeh Shafie\"\nformat:\n  html:\n    embed-resources: true\nnumber-sections: true\ntoc: true         \neditor: visual\nexecute:\n  cache:  true\n---\n# Review\n\n## Models\n\n|                     | **Decision Tree**         | **Random Forest**                  | **Gradient Boosting Tree** |\n|---------------------|---------------------------|------------------------------------|----------------------------|\n| _num trees_         | one                       | many                               | many                       |\n| _make predictions_  | mode or mean of leaf node | each tree votes                    | sum of tree outputs        |\n| _tree independence_ | NOT applicable            | independent                        | dependent                  |\n| _Data Used_         | all                       | bagging + random feature selection | all                        |\n\n\n## Decision Trees, Graphically\n\nLet's load the [penguin data set](https://github.com/allisonhorst/palmerpenguins), and plot the bill length and bill depth for our three species:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load libraries\nlibrary(readr)\nlibrary(ggplot2)\n\n# Read the data\npengwing <- read_csv(\"09-data/penguins.csv\")\n\n# View first few rows (equivalent to .head())\nhead(pengwing)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 9\n   ...1 species island    bill_length_mm bill_depth_mm flipper_length_mm\n  <dbl> <chr>   <chr>              <dbl>         <dbl>             <dbl>\n1     0 Adelie  Torgersen           39.1          18.7               181\n2     1 Adelie  Torgersen           39.5          17.4               186\n3     2 Adelie  Torgersen           40.3          18                 195\n4     3 Adelie  Torgersen           NA            NA                  NA\n5     4 Adelie  Torgersen           36.7          19.3               193\n6     5 Adelie  Torgersen           39.3          20.6               190\n# ℹ 3 more variables: body_mass_g <dbl>, sex <chr>, year <dbl>\n```\n\n\n:::\n\n```{.r .cell-code}\n# Create the plot\nggplot(pengwing, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point() +\n  theme_minimal() +\n  labs(\n    x = \"Bill Length (in mm)\",\n    y = \"Bill Depth (in mm)\",\n    title = \"Bill Length vs. Bill Depth by Species\"\n  )\n```\n\n::: {.cell-output-display}\n![](09-trees_files/figure-html/load-plot-data-1.png){width=672}\n:::\n:::\n\n\nWe could use a decision tree based on bill length and bill depth to classify penguins as different species. First, we could split on Bill Depth and decide that any penguin with a depth less than 16.5 mm, should be classified as a Gentoo penguin.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Choose a split value\nsplit1 <- 16.5\n\nggplot(pengwing, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point() +\n  theme_minimal() +\n  labs(\n    x = \"Bill Length (in mm)\",\n    y = \"Bill Depth (in mm)\",\n    title = \"Bill Length vs. Bill Depth by Species\"\n  ) +\n  geom_hline(yintercept = split1, linewidth = 1, linetype = \"dashed\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](09-trees_files/figure-html/split-ex1-1.png){width=672}\n:::\n:::\n\n\nThat bottom group looks GREAT. Now let's look at the top group. Most of the Chinstrap penguins have longer bill lengths. Let's say that if a penguin has a bill depth > 16.5mm, then we will split on bill length at 44 to separate the Adelie and Chinstrap penguins.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Choose a split value \nsplit2 <- 44\n\nggplot(pengwing, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) +\n  geom_point() +\n  theme_minimal() +\n  labs(\n    x = \"Bill Length (in mm)\",\n    y = \"Bill Depth (in mm)\",\n    title = \"Bill Length vs. Bill Depth by Species\"\n  ) +\n  geom_hline(yintercept = split1, linewidth = 1, linetype = \"dashed\") +\n  geom_segment(\n    x = split2, xend = split2,\n    y = split1, yend = 22,\n    linewidth = 0.6, linetype = \"dashed\", color = \"black\"\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](09-trees_files/figure-html/split-ex2-1.png){width=672}\n:::\n:::\n\n\nVoila! We've built a (very short) decision tree! It would look like this:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install if needed\n# install.packages(\"DiagrammeR\")\n\nlibrary(DiagrammeR)\n\ngrViz(\"\ndigraph penguin_tree {\n\n  graph [layout = dot, rankdir = TB]\n\n  node [\n    shape = box,\n    style = rounded,\n    fontname = Helvetica,\n    fontsize = 14\n  ]\n\n  # Decision nodes\n  node1 [label = 'bill_depth < 16.5']\n  node2 [label = 'bill_length < 44']\n\n  # Leaf nodes\n  gentoo    [label = 'Gentoo', fillcolor = '#dbe9f6', style = 'rounded,filled']\n  chinstrap[label = 'Chinstrap', fillcolor = '#e3f0dd', style = 'rounded,filled']\n  adelie   [label = 'Adelie', fillcolor = '#f4d6d6', style = 'rounded,filled']\n\n  # Edges\n  node1 -> gentoo     [label = 'yes']\n  node1 -> node2      [label = 'no']\n\n  node2 -> adelie     [label = 'yes']\n  node2 -> chinstrap  [label = 'no']\n}\n\")\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"grViz html-widget html-fill-item\" id=\"htmlwidget-2846f02eb7b4a8644256\" style=\"width:100%;height:464px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-2846f02eb7b4a8644256\">{\"x\":{\"diagram\":\"\\ndigraph penguin_tree {\\n\\n  graph [layout = dot, rankdir = TB]\\n\\n  node [\\n    shape = box,\\n    style = rounded,\\n    fontname = Helvetica,\\n    fontsize = 14\\n  ]\\n\\n  # Decision nodes\\n  node1 [label = \\\"bill_depth < 16.5\\\"]\\n  node2 [label = \\\"bill_length < 44\\\"]\\n\\n  # Leaf nodes\\n  gentoo    [label = \\\"Gentoo\\\", fillcolor = \\\"#dbe9f6\\\", style = \\\"rounded,filled\\\"]\\n  chinstrap[label = \\\"Chinstrap\\\", fillcolor = \\\"#e3f0dd\\\", style = \\\"rounded,filled\\\"]\\n  adelie   [label = \\\"Adelie\\\", fillcolor = \\\"#f4d6d6\\\", style = \\\"rounded,filled\\\"]\\n\\n  # Edges\\n  node1 -> gentoo     [label = \\\"yes\\\"]\\n  node1 -> node2      [label = \\\"no\\\"]\\n\\n  node2 -> adelie     [label = \\\"yes\\\"]\\n  node2 -> chinstrap  [label = \\\"no\\\"]\\n}\\n\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\n## Entropy \nEntropy is a measure of disorder/chaos. We want ordered and organized data in the leaf nodes of our decision trees. So we want LOW entropy. **Entropy** is defined as:\n\n$$ E = -\\sum_1^N p_i* log_2(p_i) $$\n\nWhere $N$ is the number of categories or labels in our outcome variable.\n\nThis is compared to **gini impurity** which is:\n\n$$GI = 1 - \\sum_1^N p_i^2$$\n\n### Question\n\nWHY do we want the leaf nodes of our tree to be ordered (have low entropy or impurity?)?\n\n## Measures of Chaos for a Split\n\nWhen you split a node, we now have two new nodes. In order to calculate the chaos (entropy or gini impurity) of the split, we have to calculate the chaos (entropy or gini impurity) for EACH of the new nodes and then calculate the weighted average chaos (entropy or gini impurity).  \n\nThe reason we weight each node differently in this calculation, is because if a node has more data in it, than it has more impact, and therefore its measure of chaos (entropy or gini impurity) should count more.\n\nIn general, once you've calculated the chaos (entropy or gini impurity) for each of the new nodes, you'll use this formula to calculate the weighted average:\n\n\n$$ WC = (\\frac{N_L}{Total}* C_L) + (\\frac{N_R}{Total}* C_R)$$\n\nWhere $N_L$ is the number of data points in the Left Node, $N_R$ is the number of data points in the Right Node, and $Total$ is the total number of data points in that split. $C_R$ and $C_L$ are the chaos measure (entropy of gini impurity) for the right and left nodes, respectively.\n\n## Decision Trees\n\nLet's first build a Decision Tree to **classify** patients as diabetic or not diabetic.\n\nGini impurity is probability of misclassifying a random data point from that node.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Packages\nlibrary(readr)\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: lattice\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(rpart)\n\n# Read data + peek\nd <- read_csv(\"09-data/diabetes2.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 768 Columns: 9\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (9): Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, D...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 9\n  Pregnancies Glucose BloodPressure SkinThickness Insulin   BMI\n        <dbl>   <dbl>         <dbl>         <dbl>   <dbl> <dbl>\n1           6     148            72            35       0  33.6\n2           1      85            66            29       0  26.6\n3           8     183            64             0       0  23.3\n4           1      89            66            23      94  28.1\n5           0     137            40            35     168  43.1\n6           5     116            74             0       0  25.6\n# ℹ 3 more variables: DiabetesPedigreeFunction <dbl>, Age <dbl>, Outcome <dbl>\n```\n\n\n:::\n\n```{.r .cell-code}\n# Predictors / outcome\npredictors <- c(\"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\n                \"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\")\n\nX <- d[, predictors]\ny <- d$Outcome   # should be 0/1\n\n# Train/test split (80/20), like random_state=1234\nset.seed(1234)\nidx_train <- createDataPartition(y, p = 0.8, list = FALSE)\nX_train <- X[idx_train, , drop = FALSE]\nX_test  <- X[-idx_train, , drop = FALSE]\ny_train <- y[idx_train]\ny_test  <- y[-idx_train]\n\n# Standardize using TRAIN stats only, then apply to both\npp <- preProcess(X_train, method = c(\"center\", \"scale\"))\nX_train_sc <- predict(pp, X_train)\nX_test_sc  <- predict(pp, X_test)\n\n# Fit decision tree (classification)\ntrain_df <- data.frame(X_train_sc, Outcome = factor(y_train))\ntest_df  <- data.frame(X_test_sc,  Outcome = factor(y_test))\n\nset.seed(1234)\ntree <- rpart(Outcome ~ ., data = train_df, method = \"class\")\n\n# Predict classes\npred_test  <- predict(tree, newdata = test_df, type = \"class\")\npred_train <- predict(tree, newdata = train_df, type = \"class\")\n\n# Confusion matrices (test + train), like ConfusionMatrixDisplay\nconfusionMatrix(pred_test,  test_df$Outcome)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 76 26\n         1 15 36\n                                          \n               Accuracy : 0.732           \n                 95% CI : (0.6545, 0.8003)\n    No Information Rate : 0.5948          \n    P-Value [Acc > NIR] : 0.0002754       \n                                          \n                  Kappa : 0.4279          \n                                          \n Mcnemar's Test P-Value : 0.1183498       \n                                          \n            Sensitivity : 0.8352          \n            Specificity : 0.5806          \n         Pos Pred Value : 0.7451          \n         Neg Pred Value : 0.7059          \n             Prevalence : 0.5948          \n         Detection Rate : 0.4967          \n   Detection Prevalence : 0.6667          \n      Balanced Accuracy : 0.7079          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n\n\n:::\n\n```{.r .cell-code}\nconfusionMatrix(pred_train, train_df$Outcome)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 368  60\n         1  41 146\n                                          \n               Accuracy : 0.8358          \n                 95% CI : (0.8041, 0.8642)\n    No Information Rate : 0.665           \n    P-Value [Acc > NIR] : < 2e-16         \n                                          \n                  Kappa : 0.6227          \n                                          \n Mcnemar's Test P-Value : 0.07328         \n                                          \n            Sensitivity : 0.8998          \n            Specificity : 0.7087          \n         Pos Pred Value : 0.8598          \n         Neg Pred Value : 0.7807          \n             Prevalence : 0.6650          \n         Detection Rate : 0.5984          \n   Detection Prevalence : 0.6959          \n      Balanced Accuracy : 0.8042          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n\n\n:::\n:::\n\n\n\n### Limiting Max_Depth\nWe talked about different ways to \"prune\" or limit the depth of a tree, both directly and indirectly via `max_depth` and `min_samples_leaf`. Run the following code, what does it tell you:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to run repeated train/test evaluation for a given maxdepth\neval_depth <- function(data, depth = NULL, reps = 50, test_prop = 0.2, seed = 1234) {\n  set.seed(seed)\n  n <- nrow(data)\n\n  out <- data.frame(rep = integer(0), split = character(0), acc = numeric(0))\n\n  for (r in 1:reps) {\n    idx_test <- sample.int(n, size = floor(test_prop * n))\n    train_df <- data[-idx_test, c(predictors, \"Outcome\")]\n    test_df  <- data[idx_test,  c(predictors, \"Outcome\")]\n\n    ctrl <- rpart.control(cp = 0, xval = 0)\n    if (!is.null(depth)) ctrl$maxdepth <- depth\n\n    fit <- rpart(Outcome ~ ., data = train_df, method = \"class\", control = ctrl)\n\n    pred_train <- predict(fit, newdata = train_df, type = \"class\")\n    pred_test  <- predict(fit, newdata = test_df,  type = \"class\")\n\n    acc_train <- mean(pred_train == train_df$Outcome)\n    acc_test  <- mean(pred_test  == test_df$Outcome)\n\n    out <- rbind(out,\n                 data.frame(rep = r, split = \"Test\",  acc = acc_test),\n                 data.frame(rep = r, split = \"Train\", acc = acc_train))\n  }\n  out\n}\n\n# Run across depths 2-9 and \"none\"\ndepths <- 2:9\nall_res <- data.frame()\n\nfor (dep in depths) {\n  tmp <- eval_depth(d, depth = dep, reps = 60, test_prop = 0.2, seed = 1234)\n  tmp$depth <- as.character(dep)\n  all_res <- rbind(all_res, tmp)\n}\n\n# \"none\" = no maxdepth cap (use rpart default maxdepth)\ntmp_none <- eval_depth(d, depth = NULL, reps = 60, test_prop = 0.2, seed = 1234)\ntmp_none$depth <- \"none\"\nall_res <- rbind(all_res, tmp_none)\n\nall_res$depth <- factor(all_res$depth, levels = c(as.character(2:9), \"none\"))\nall_res$split <- factor(all_res$split, levels = c(\"Test\", \"Train\"))\n\n# Plot \nggplot(all_res, aes(x = depth, y = acc, fill = split)) +\n  geom_boxplot(width = 0.7, position = position_dodge(width = 0.8)) +\n  scale_fill_manual(values = c(\"Test\" = \"#D76B63\", \"Train\" = \"#78D7E6\")) +\n  labs(\n    x = \"Restriction of Depth of Tree\",\n    y = \"Accuracy\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](09-trees_files/figure-html/sim-pruning-1.png){width=672}\n:::\n:::\n\n\n### Random Forests and Gradient Boosting Trees\n\nNow let's copy and paste the code from above and build a **Random Forest** to predict diabetes instead of a single tree, and then using a **Gradient Boosting Tree**.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest)\n\n\n# Predictors and outcome\npredictors <- c(\n  \"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\n  \"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\"\n)\n\nX <- d[, predictors]\ny <- d$Outcome\n\n# Train/test split (80/20)\nset.seed(1234)\ntrain_idx <- createDataPartition(y, p = 0.8, list = FALSE)\n\nX_train <- X[train_idx, , drop = FALSE]\nX_test  <- X[-train_idx, , drop = FALSE]\ny_train <- y[train_idx]\ny_test  <- y[-train_idx]\n\n# Standardize predictors (z-score using TRAIN stats)\npp <- preProcess(X_train, method = c(\"center\", \"scale\"))\nX_train_sc <- predict(pp, X_train)\nX_test_sc  <- predict(pp, X_test)\n# z scoring not important, because none of the variables are influencing/compared to each other directly scale doesn't matter here. But z scoring wont hurt.\n\n\n# Combine into data frames for modeling\ntrain_df <- data.frame(X_train_sc, Outcome = factor(y_train))\ntest_df  <- data.frame(X_test_sc,  Outcome = factor(y_test))\n\n# Fit Random Forest\nset.seed(1234)\nrf_model <- randomForest(\n  Outcome ~ .,\n  data = train_df,\n  ntree = 500,        # number of trees\n  mtry = 3,           # variables tried at each split (default ~ sqrt(p))\n  importance = TRUE\n)\n\n# Predictions\npred_train <- predict(rf_model, train_df)\npred_test  <- predict(rf_model, test_df)\n\n# Confusion matrices\nconfusionMatrix(pred_train, train_df$Outcome)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 409   0\n         1   0 206\n                                    \n               Accuracy : 1         \n                 95% CI : (0.994, 1)\n    No Information Rate : 0.665     \n    P-Value [Acc > NIR] : < 2.2e-16 \n                                    \n                  Kappa : 1         \n                                    \n Mcnemar's Test P-Value : NA        \n                                    \n            Sensitivity : 1.000     \n            Specificity : 1.000     \n         Pos Pred Value : 1.000     \n         Neg Pred Value : 1.000     \n             Prevalence : 0.665     \n         Detection Rate : 0.665     \n   Detection Prevalence : 0.665     \n      Balanced Accuracy : 1.000     \n                                    \n       'Positive' Class : 0         \n                                    \n```\n\n\n:::\n\n```{.r .cell-code}\nconfusionMatrix(pred_test,  test_df$Outcome)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 77 29\n         1 14 33\n                                          \n               Accuracy : 0.719           \n                 95% CI : (0.6407, 0.7886)\n    No Information Rate : 0.5948          \n    P-Value [Acc > NIR] : 0.0009444       \n                                          \n                  Kappa : 0.3936          \n                                          \n Mcnemar's Test P-Value : 0.0327626       \n                                          \n            Sensitivity : 0.8462          \n            Specificity : 0.5323          \n         Pos Pred Value : 0.7264          \n         Neg Pred Value : 0.7021          \n             Prevalence : 0.5948          \n         Detection Rate : 0.5033          \n   Detection Prevalence : 0.6928          \n      Balanced Accuracy : 0.6892          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gbm)\n\n# Predictors and outcome\npredictors <- c(\n  \"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\n  \"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\"\n)\n\nX <- d[, predictors]\ny <- d$Outcome # must be numeric 0/1 and not factor as earlier\n\n\n# Train/test split (80/20)\nset.seed(1234)\ntrain_idx <- createDataPartition(y, p = 0.8, list = FALSE)\n\nX_train <- X[train_idx, , drop = FALSE]\nX_test  <- X[-train_idx, , drop = FALSE]\ny_train <- y[train_idx]\ny_test  <- y[-train_idx]\n\n# Standardize predictors (z-score using TRAIN stats)\npp <- preProcess(X_train, method = c(\"center\", \"scale\"))\nX_train_sc <- predict(pp, X_train)\nX_test_sc  <- predict(pp, X_test)\n# z scoring not important, because none of the variables are influencing/compared to each other directly scale doesn't matter here. But z scoring wont hurt.\n\n\n# Combine into data frames for modeling\ntrain_df <- data.frame(X_train_sc, Outcome = y_train) \ntest_df  <- data.frame(X_test_sc,  Outcome = y_test)\n\n# Fit Gradient Boosting model\nset.seed(1234)\ngb_model <- gbm(\n  Outcome ~ .,\n  data = train_df,\n  distribution = \"bernoulli\",   # binary classification\n  n.trees = 300,\n  interaction.depth = 3,        # tree depth\n  shrinkage = 0.05,              # learning rate\n  n.minobsinnode = 10,\n  bag.fraction = 0.8,\n  verbose = FALSE\n)\n\n# Predict probabilities\nprob_train <- predict(gb_model, train_df, n.trees = 300, type = \"response\")\nprob_test  <- predict(gb_model, test_df,  n.trees = 300, type = \"response\")\n\n# Convert probabilities to class labels (0.5 threshold)\npred_train <- factor(ifelse(prob_train > 0.5, 1, 0))\npred_test  <- factor(ifelse(prob_test  > 0.5, 1, 0))\n\n# Convert truth to factor with matching levels\ntrain_truth <- factor(train_df$Outcome, levels = c(0, 1))\ntest_truth  <- factor(test_df$Outcome,  levels = c(0, 1))\n\npred_train <- factor(pred_train, levels = c(0, 1))\npred_test  <- factor(pred_test,  levels = c(0, 1))\n\n# Confusion matrices\nconfusionMatrix(pred_train, train_truth)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 392  43\n         1  17 163\n                                          \n               Accuracy : 0.9024          \n                 95% CI : (0.8762, 0.9247)\n    No Information Rate : 0.665           \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.7739          \n                                          \n Mcnemar's Test P-Value : 0.001249        \n                                          \n            Sensitivity : 0.9584          \n            Specificity : 0.7913          \n         Pos Pred Value : 0.9011          \n         Neg Pred Value : 0.9056          \n             Prevalence : 0.6650          \n         Detection Rate : 0.6374          \n   Detection Prevalence : 0.7073          \n      Balanced Accuracy : 0.8748          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n\n\n:::\n\n```{.r .cell-code}\nconfusionMatrix(pred_test,  test_truth)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 78 29\n         1 13 33\n                                          \n               Accuracy : 0.7255          \n                 95% CI : (0.6476, 0.7945)\n    No Information Rate : 0.5948          \n    P-Value [Acc > NIR] : 0.0005179       \n                                          \n                  Kappa : 0.4061          \n                                          \n Mcnemar's Test P-Value : 0.0206376       \n                                          \n            Sensitivity : 0.8571          \n            Specificity : 0.5323          \n         Pos Pred Value : 0.7290          \n         Neg Pred Value : 0.7174          \n             Prevalence : 0.5948          \n         Detection Rate : 0.5098          \n   Detection Prevalence : 0.6993          \n      Balanced Accuracy : 0.6947          \n                                          \n       'Positive' Class : 0               \n                                          \n```\n\n\n:::\n:::\n\n\n### Using Trees for Regression\nLastly, let's take a quick look at what we'd need to change if we wanted to predict a *continuous* value instead of a categorical one. Let's look at this data set that measures risk propensity. We're going to predict BART Scores (a score where higher values mean you're riskier), based on a bunch of different measures. \n\nThese are the variables in the data set:\n\nBART: Balloon Analogue Risk Task\n - Measures risk-taking behavior\n - Higher scores means more willingness to take risks\n - This is the outcome (target variable)\n  \nBIS/BAS: Behavioral Inhibition / Behavioral Activation Scales\n\t-\tBISmeans sensitivity to punishment / avoidance\n\t-\tBAS means sensitivity to reward / approach behavior\n\nFemale\n\t- Binary variable (0/1)\n\t\n\t\nGoal of the model is to use psychological traits (BIS/BAS + gender) to predict risk-taking behavior (BART score) using linear regression, evaluated with 5-fold cross-validation.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read and clean data----\nbart <- read_csv(\n  \"09-data/bart.csv\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 1000 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (7): BIS.Score, BAS.Drive.Score, BAS.Fun.Seeking.Score, BAS.Reward.Respo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\n# Drop missing values\nbart <- na.omit(bart)\n\n# Reset row names \nrownames(bart) <- NULL\n\n# Define predictors and outcome----\n# Outcome\ny <- bart$BART\n\n# Predictors\npredictors <- setdiff(colnames(bart), \"BART\")\n\n# Continuous predictors (everything except Female)\ncontin <- setdiff(predictors, \"Female\")\n\nX <- bart[, predictors]\n\n# set up CV with 5 folds-----\nset.seed(1234)\nkf <- createFolds(y, k = 5, returnTrain = TRUE)\n\n# Storage for metrics ----\nmse  <- list(train = c(), test = c())\nmae  <- list(train = c(), test = c())\nmape <- list(train = c(), test = c())\nr2   <- list(train = c(), test = c())\n\n\n# Cross-validation loop\nfor (i in seq_along(kf)) {\n\n  train_idx <- kf[[i]]\n  test_idx  <- setdiff(seq_len(nrow(X)), train_idx)\n\n  X_train <- X[train_idx, ]\n  X_test  <- X[test_idx, ]\n  y_train <- y[train_idx]\n  y_test  <- y[test_idx]\n\n  # Z-score continuous predictors only\n  pp <- preProcess(X_train[, contin], method = c(\"center\", \"scale\"))\n\n  X_train_sc <- X_train\n  X_test_sc  <- X_test\n\n  X_train_sc[, contin] <- predict(pp, X_train[, contin])\n  X_test_sc[, contin]  <- predict(pp, X_test[, contin])\n\n  # Fit linear regression\n  train_df <- data.frame(X_train_sc, BART = y_train)\n  test_df  <- data.frame(X_test_sc,  BART = y_test)\n\n  lm_fit <- lm(BART ~ ., data = train_df)\n\n  # Predictions\n  pred_train <- predict(lm_fit, train_df)\n  pred_test  <- predict(lm_fit, test_df)\n\n  # Metrics\n  mse$train  <- c(mse$train, mean((y_train - pred_train)^2))\n  mse$test   <- c(mse$test,  mean((y_test  - pred_test)^2))\n\n  mae$train  <- c(mae$train, mean(abs(y_train - pred_train)))\n  mae$test   <- c(mae$test,  mean(abs(y_test  - pred_test)))\n\n  mape$train <- c(mape$train, mean(abs((y_train - pred_train) / y_train)))\n  mape$test  <- c(mape$test,  mean(abs((y_test  - pred_test)  / y_test)))\n\n  r2$train   <- c(r2$train, cor(y_train, pred_train)^2)\n  r2$test    <- c(r2$test,  cor(y_test,  pred_test)^2)\n}\n\n\n# Create summary table-----\nresults_table <- data.frame(\n  Metric = c(\"MSE\", \"MAE\", \"MAPE\", \"R²\"),\n  Train = c(\n    mean(mse$train),\n    mean(mae$train),\n    mean(mape$train),\n    mean(r2$train)\n  ),\n  Test = c(\n    mean(mse$test),\n    mean(mae$test),\n    mean(mape$test),\n    mean(r2$test)\n  )\n)\n\n# Print table\nresults_table\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Metric        Train         Test\n1    MSE 143.01960222 146.17530363\n2    MAE  10.49211192  10.59655360\n3   MAPE   0.87809521   0.88646195\n4     R²   0.06309417   0.05407431\n```\n\n\n:::\n:::\n\n\nWe can interpret the results as follows:\nThe linear regression model shows poor predictive performance, explaining only 5–6% of the variance in BART scores. Train and test errors were nearly identical, indicating no overfitting but substantial underfitting. These results suggest that BIS/BAS traits and gender alone are insufficient predictors of risk-taking behavior as measured by the BART.\n\nNext step would be to try non-linear models (Random Forest, Gradient Boosting).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nfolds <- createFolds(y, k = 5, returnTrain = TRUE)\n\nmse  <- list(train = c(), test = c())\nmae  <- list(train = c(), test = c())\nmape <- list(train = c(), test = c())\nr2   <- list(train = c(), test = c())\n\nfor (i in seq_along(folds)) {\n  tr <- folds[[i]]\n  te <- setdiff(seq_len(nrow(X)), tr)\n\n  X_train <- X[tr, , drop = FALSE]\n  X_test  <- X[te, , drop = FALSE]\n  y_train <- y[tr]\n  y_test  <- y[te]\n\n  # z-score continuous predictors using TRAIN stats\n  pp <- preProcess(X_train[, contin, drop = FALSE], method = c(\"center\", \"scale\"))\n  X_train_sc <- X_train; X_test_sc <- X_test\n  X_train_sc[, contin] <- predict(pp, X_train[, contin, drop = FALSE])\n  X_test_sc[, contin]  <- predict(pp, X_test[, contin, drop = FALSE])\n\n  train_df <- data.frame(X_train_sc, BART = y_train)\n  test_df  <- data.frame(X_test_sc,  BART = y_test)\n\n  set.seed(1234)\n  rf_fit <- randomForest(\n    BART ~ .,\n    data = train_df,\n    ntree = 500,\n    mtry = max(1, floor(sqrt(length(predictors))))\n  )\n\n  pred_train <- predict(rf_fit, train_df)\n  pred_test  <- predict(rf_fit, test_df)\n\n  mse$train  <- c(mse$train, mean((y_train - pred_train)^2))\n  mse$test   <- c(mse$test,  mean((y_test  - pred_test)^2))\n\n  mae$train  <- c(mae$train, mean(abs(y_train - pred_train)))\n  mae$test   <- c(mae$test,  mean(abs(y_test  - pred_test)))\n\n  mape$train <- c(mape$train, mean(abs((y_train - pred_train) / y_train)))\n  mape$test  <- c(mape$test,  mean(abs((y_test  - pred_test)  / y_test)))\n\n  r2$train   <- c(r2$train, cor(y_train, pred_train)^2)\n  r2$test    <- c(r2$test,  cor(y_test,  pred_test)^2)\n}\n\nrf_results <- data.frame(\n  Model = \"Random Forest\",\n  Metric = c(\"MSE\", \"MAE\", \"MAPE\", \"R^2\"),\n  Train = c(mean(mse$train), mean(mae$train), mean(mape$train), mean(r2$train)),\n  Test  = c(mean(mse$test),  mean(mae$test),  mean(mape$test),  mean(r2$test))\n)\n\nrf_results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Model Metric      Train        Test\n1 Random Forest    MSE 49.4503557 133.4441967\n2 Random Forest    MAE  5.7125156   9.5465513\n3 Random Forest   MAPE  0.4700485   0.7912291\n4 Random Forest    R^2  0.7497229   0.1468698\n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nfolds <- createFolds(y, k = 5, returnTrain = TRUE)\n\nmse_g  <- list(train = c(), test = c())\nmae_g  <- list(train = c(), test = c())\nmape_g <- list(train = c(), test = c())\nr2_g   <- list(train = c(), test = c())\n\nfor (i in seq_along(folds)) {\n  tr <- folds[[i]]\n  te <- setdiff(seq_len(nrow(X)), tr)\n\n  X_train <- X[tr, , drop = FALSE]\n  X_test  <- X[te, , drop = FALSE]\n  y_train <- y[tr]\n  y_test  <- y[te]\n\n  # z-score continuous predictors using TRAIN stats\n  pp <- preProcess(X_train[, contin, drop = FALSE], method = c(\"center\", \"scale\"))\n  X_train_sc <- X_train; X_test_sc <- X_test\n  X_train_sc[, contin] <- predict(pp, X_train[, contin, drop = FALSE])\n  X_test_sc[, contin]  <- predict(pp, X_test[, contin, drop = FALSE])\n\n  train_df <- data.frame(X_train_sc, BART = y_train)\n  test_df  <- data.frame(X_test_sc,  BART = y_test)\n\n  set.seed(1234)\n  gb_fit <- gbm(\n    BART ~ .,\n    data = train_df,\n    distribution = \"gaussian\",     # regression\n    n.trees = 1500,\n    interaction.depth = 3,\n    shrinkage = 0.01,\n    n.minobsinnode = 10,\n    bag.fraction = 0.8,\n    verbose = FALSE\n  )\n\n  pred_train <- predict(gb_fit, train_df, n.trees = 1500)\n  pred_test  <- predict(gb_fit, test_df,  n.trees = 1500)\n\n  mse_g$train  <- c(mse_g$train, mean((y_train - pred_train)^2))\n  mse_g$test   <- c(mse_g$test,  mean((y_test  - pred_test)^2))\n\n  mae_g$train  <- c(mae_g$train, mean(abs(y_train - pred_train)))\n  mae_g$test   <- c(mae_g$test,  mean(abs(y_test  - pred_test)))\n\n  mape_g$train <- c(mape_g$train, mean(abs((y_train - pred_train) / y_train)))\n  mape_g$test  <- c(mape_g$test,  mean(abs((y_test  - pred_test)  / y_test)))\n\n  r2_g$train   <- c(r2_g$train, cor(y_train, pred_train)^2)\n  r2_g$test    <- c(r2_g$test,  cor(y_test,  pred_test)^2)\n}\n\ngb_results <- data.frame(\n  Model = \"Gradient Boosting\",\n  Metric = c(\"MSE\", \"MAE\", \"MAPE\", \"R^2\"),\n  Train = c(mean(mse_g$train), mean(mae_g$train), mean(mape_g$train), mean(r2_g$train)),\n  Test  = c(mean(mse_g$test),  mean(mae_g$test),  mean(mape_g$test),  mean(r2_g$test))\n)\n\ngb_results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              Model Metric       Train        Test\n1 Gradient Boosting    MSE 100.5563924 131.6130762\n2 Gradient Boosting    MAE   8.2969512   9.5293227\n3 Gradient Boosting   MAPE   0.6858956   0.7808945\n4 Gradient Boosting    R^2   0.3510419   0.1564954\n```\n\n\n:::\n:::\n\n\nLet's combine the results to compare:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall_results <- rbind(rf_results, gb_results)\nall_results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              Model Metric       Train        Test\n1     Random Forest    MSE  49.4503557 133.4441967\n2     Random Forest    MAE   5.7125156   9.5465513\n3     Random Forest   MAPE   0.4700485   0.7912291\n4     Random Forest    R^2   0.7497229   0.1468698\n5 Gradient Boosting    MSE 100.5563924 131.6130762\n6 Gradient Boosting    MAE   8.2969512   9.5293227\n7 Gradient Boosting   MAPE   0.6858956   0.7808945\n8 Gradient Boosting    R^2   0.3510419   0.1564954\n```\n\n\n:::\n:::\n\n\nWhat do you conclude?\n\n::: {.callout-note collapse=\"true\"}\n#### Interpretation\n\nWhile Random Forest achieved excellent training performance, it substantially overfit and failed to generalize. Gradient Boosting produced slightly better test performance and more stable behavior, though overall predictive power remained modest. These results suggest that nonlinear relationships exist but that BIS/BAS measures and gender alone account for only a small portion of variance in BART risk-taking behavior.\n\n:::\n\n### Feature Importance\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assuming rf_fit was trained on train_df\nrf_imp <- importance(rf_fit)\n\n# Convert to data frame\nrf_imp_df <- data.frame(\n  Feature = rownames(rf_imp),\n  Importance = rf_imp[, \"IncNodePurity\"]\n)\n\n# Sort by importance\nrf_imp_df <- rf_imp_df[order(rf_imp_df$Importance, decreasing = TRUE), ]\n\nrf_imp_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                                                        Feature Importance\nBAS.Drive.Score                                 BAS.Drive.Score  17676.021\nBAS.Fun.Seeking.Score                     BAS.Fun.Seeking.Score  16728.337\nAge                                                         Age  15398.407\nBIS.Score                                             BIS.Score  14628.120\nBAS.Reward.Responsiveness.Score BAS.Reward.Responsiveness.Score  14473.869\nFemale                                                   Female   3253.537\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Relative influence from gbm\ngb_imp <- summary(gb_fit, plotit = FALSE)\n\ngb_imp\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                                                            var   rel.inf\nBAS.Fun.Seeking.Score                     BAS.Fun.Seeking.Score 25.462993\nBAS.Drive.Score                                 BAS.Drive.Score 25.375267\nBAS.Reward.Responsiveness.Score BAS.Reward.Responsiveness.Score 21.312970\nBIS.Score                                             BIS.Score 13.361662\nAge                                                         Age 12.722097\nFemale                                                   Female  1.765011\n```\n\n\n:::\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(patchwork)\n\n# Random Forest plot\np_rf <- ggplot(rf_imp_df, aes(x = reorder(Feature, Importance), y = Importance)) +\n  geom_col(fill = \"#2F6F73\") +\n  coord_flip() +\n  labs(\n    title = \"Random Forest\",\n    x = \"Feature\",\n    y = \"Importance (IncNodePurity)\"\n  ) +\n  theme_minimal()\n\n# Gradient Boosting plot\np_gb <- ggplot(gb_imp, aes(x = reorder(var, rel.inf), y = rel.inf)) +\n  geom_col(fill = \"#F28E2B\") +\n  coord_flip() +\n  labs(\n    title = \"Gradient Boosting\",\n    x = \"Feature\",\n    y = \"Relative Influence\"\n  ) +\n  theme_minimal()\n\np_rf / p_gb\n```\n\n::: {.cell-output-display}\n![](09-trees_files/figure-html/plot-imp-1.png){width=672}\n:::\n:::\n\n\nFeature importance analyses from both Random Forest and Gradient Boosting consistently identified BAS-related traits, particularly Drive and Fun Seeking, as the most influential predictors of BART risk-taking behavior. In contrast, BIS, age, and gender showed relatively weak influence. Although these findings suggest that reward sensitivity plays a central role in risk-taking, the overall predictive power of the models remained modest, indicating substantial unexplained variability.\n\n\n\n# Classwork\nUsing the `bmd.csv`, fit a tree to predict bone mineral density (BMD) based on AGE.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#libraries that we will need\nset.seed(1234) #fix the random generator seed \n\nlibrary(rpart)  #library for CART\nlibrary(rpart.plot) # plotting\n#read the dataset\nbmd.data     <-  read.csv(\n  \"09-data/bmd.csv\", stringsAsFactors = TRUE\n)\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt1 <- rpart(bmd ~ age,\n            data = bmd.data, \n            method = \"anova\",         #indicates the outcome is continuous\n            control = rpart.control(\n                       minsplit = 1,  # min number of observ for a split \n                       minbucket = 1, # min nr of obs in terminal nodes\n                       cp=0)          #decrease in complex for a split \n)\n\n#the rpart.plot() may take a long time to plot\n#the complete tree\n#if you cannot run it, just try plot(t1); text(t1, pretty=1)\n#and you will see just the structure of the tree\nrpart.plot(t1)     \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: labs do not fit even at cex 0.15, there may be some overplotting\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](09-trees_files/figure-html/t1-1.png){width=672}\n:::\n:::\n\n\n\nWe can now prune the tree using a limit for the complexity parameter CP. This will indicate that only a split with CP higher than the limit is worth it. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nprintcp(t1)                      #CP for the complete tree\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRegression tree:\nrpart(formula = bmd ~ age, data = bmd.data, method = \"anova\", \n    control = rpart.control(minsplit = 1, minbucket = 1, cp = 0))\n\nVariables actually used in tree construction:\n[1] age\n\nRoot node error: 4.659/169 = 0.027568\n\nn= 169 \n\n            CP nsplit  rel error  xerror    xstd\n1   1.5011e-01      0 1.0000e+00 1.00752 0.11223\n2   2.2857e-02      1 8.4989e-01 0.88021 0.11329\n3   2.1625e-02      2 8.2703e-01 1.00527 0.12150\n4   2.0621e-02      8 6.9728e-01 1.02789 0.12227\n5   1.7896e-02     11 6.2374e-01 1.02015 0.12170\n6   1.6691e-02     13 5.8795e-01 1.04816 0.12398\n7   1.3855e-02     14 5.7126e-01 1.05215 0.12506\n8   1.2133e-02     15 5.5740e-01 1.09014 0.13382\n9   1.1410e-02     16 5.4527e-01 1.18367 0.14818\n10  1.0975e-02     17 5.3386e-01 1.22320 0.15014\n11  1.0843e-02     21 4.8996e-01 1.23199 0.15007\n12  1.0550e-02     26 4.3574e-01 1.21597 0.14484\n13  1.0318e-02     27 4.2519e-01 1.21718 0.14328\n14  9.5921e-03     28 4.1488e-01 1.22821 0.14316\n15  9.5636e-03     29 4.0528e-01 1.24631 0.14492\n16  9.4801e-03     35 3.4511e-01 1.24631 0.14492\n17  8.8498e-03     37 3.2615e-01 1.27487 0.14880\n18  8.7833e-03     38 3.1730e-01 1.30437 0.14966\n19  8.6510e-03     39 3.0851e-01 1.29524 0.14983\n20  8.1753e-03     42 2.8164e-01 1.26099 0.14307\n21  8.1257e-03     43 2.7347e-01 1.25794 0.14306\n22  7.9341e-03     45 2.5722e-01 1.27192 0.14317\n23  6.8660e-03     46 2.4928e-01 1.31851 0.14412\n24  6.2708e-03     47 2.4242e-01 1.40537 0.15478\n25  5.9903e-03     52 2.1106e-01 1.41452 0.15452\n26  5.9250e-03     54 1.9908e-01 1.40721 0.15436\n27  5.9229e-03     56 1.8723e-01 1.40721 0.15436\n28  5.8272e-03     57 1.8131e-01 1.40721 0.15436\n29  5.4523e-03     58 1.7548e-01 1.40090 0.15426\n30  5.0336e-03     59 1.7003e-01 1.42253 0.15466\n31  4.9113e-03     60 1.6500e-01 1.43632 0.15665\n32  4.6286e-03     63 1.5026e-01 1.44164 0.15661\n33  4.5364e-03     64 1.4563e-01 1.47586 0.15635\n34  4.3127e-03     66 1.3656e-01 1.47686 0.15634\n35  4.3104e-03     68 1.2794e-01 1.47835 0.15635\n36  4.2807e-03     69 1.2363e-01 1.47835 0.15635\n37  3.9800e-03     71 1.1506e-01 1.47621 0.15643\n38  3.6958e-03     72 1.1108e-01 1.46197 0.15633\n39  3.1052e-03     75 9.9997e-02 1.43906 0.15590\n40  3.0222e-03     76 9.6892e-02 1.44520 0.15595\n41  2.7468e-03     78 9.0848e-02 1.44816 0.15580\n42  2.6775e-03     79 8.8101e-02 1.45534 0.15572\n43  2.5828e-03     81 8.2746e-02 1.45691 0.15875\n44  2.5347e-03     82 8.0163e-02 1.45495 0.15876\n45  2.4665e-03     83 7.7628e-02 1.45095 0.15875\n46  2.4609e-03     84 7.5162e-02 1.45900 0.15906\n47  2.4243e-03     86 7.0240e-02 1.45900 0.15906\n48  2.4140e-03     87 6.7816e-02 1.45900 0.15906\n49  2.3335e-03     88 6.5402e-02 1.45900 0.15906\n50  2.3321e-03     89 6.3068e-02 1.46489 0.15935\n51  2.2471e-03     91 5.8404e-02 1.47050 0.15929\n52  2.1072e-03     92 5.6157e-02 1.46871 0.15786\n53  2.0870e-03     93 5.4050e-02 1.46963 0.15782\n54  2.0451e-03    100 3.7574e-02 1.46963 0.15782\n55  1.9386e-03    101 3.5529e-02 1.46560 0.15783\n56  1.8005e-03    102 3.3590e-02 1.46711 0.15776\n57  1.3686e-03    103 3.1790e-02 1.46865 0.15796\n58  1.3438e-03    107 2.6306e-02 1.47106 0.15801\n59  1.2138e-03    108 2.4962e-02 1.46963 0.15806\n60  1.1334e-03    110 2.2535e-02 1.46612 0.15641\n61  1.0723e-03    111 2.1401e-02 1.46176 0.15658\n62  1.0605e-03    113 1.9257e-02 1.46676 0.15654\n63  1.0035e-03    114 1.8196e-02 1.46518 0.15661\n64  9.4464e-04    115 1.7193e-02 1.46207 0.15669\n65  7.5924e-04    116 1.6248e-02 1.44567 0.15628\n66  7.5061e-04    118 1.4730e-02 1.44557 0.15628\n67  7.4468e-04    119 1.3979e-02 1.44557 0.15628\n68  7.3755e-04    120 1.3234e-02 1.44557 0.15628\n69  7.1811e-04    121 1.2497e-02 1.44820 0.15641\n70  7.1728e-04    122 1.1779e-02 1.45159 0.15641\n71  7.1635e-04    123 1.1061e-02 1.45159 0.15641\n72  7.1110e-04    124 1.0345e-02 1.45159 0.15641\n73  6.6133e-04    125 9.6340e-03 1.44761 0.15640\n74  6.5629e-04    126 8.9727e-03 1.44315 0.15666\n75  6.5461e-04    127 8.3164e-03 1.44315 0.15666\n76  5.8428e-04    128 7.6618e-03 1.44689 0.15544\n77  5.1243e-04    129 7.0775e-03 1.45836 0.15511\n78  4.7746e-04    130 6.5651e-03 1.45858 0.15495\n79  4.7460e-04    131 6.0876e-03 1.45858 0.15495\n80  4.5064e-04    132 5.6130e-03 1.45858 0.15495\n81  4.3443e-04    133 5.1624e-03 1.45931 0.15490\n82  4.0723e-04    134 4.7279e-03 1.46306 0.15490\n83  4.0723e-04    135 4.3207e-03 1.46502 0.15480\n84  4.0195e-04    136 3.9135e-03 1.46502 0.15480\n85  3.6728e-04    137 3.5115e-03 1.46765 0.15474\n86  3.3896e-04    138 3.1442e-03 1.46949 0.15465\n87  3.1877e-04    139 2.8053e-03 1.47112 0.15462\n88  3.1811e-04    140 2.4865e-03 1.47128 0.15461\n89  2.7369e-04    141 2.1684e-03 1.46930 0.15429\n90  2.0990e-04    142 1.8947e-03 1.46905 0.15431\n91  1.8394e-04    143 1.6848e-03 1.46767 0.15408\n92  1.8306e-04    144 1.5008e-03 1.47013 0.15416\n93  1.8237e-04    145 1.3178e-03 1.47013 0.15416\n94  1.3863e-04    146 1.1354e-03 1.47029 0.15415\n95  1.1758e-04    148 8.5816e-04 1.47000 0.15395\n96  9.2318e-05    149 7.4058e-04 1.46685 0.15235\n97  9.1614e-05    150 6.4826e-04 1.46632 0.15238\n98  8.9016e-05    152 4.6503e-04 1.46632 0.15238\n99  8.1752e-05    153 3.7601e-04 1.46632 0.15238\n100 5.6280e-05    154 2.9426e-04 1.46685 0.15239\n101 5.0071e-05    155 2.3798e-04 1.46893 0.15235\n102 4.1650e-05    156 1.8791e-04 1.46834 0.15237\n103 2.7818e-05    157 1.4626e-04 1.46775 0.15231\n104 2.2876e-05    158 1.1844e-04 1.46788 0.15226\n105 1.7583e-05    159 9.5567e-05 1.46788 0.15226\n106 1.5454e-05    160 7.7983e-05 1.46788 0.15226\n107 1.3947e-05    161 6.2529e-05 1.46697 0.15226\n108 1.3462e-05    162 4.8582e-05 1.46704 0.15228\n109 1.2059e-05    163 3.5120e-05 1.46648 0.15226\n110 1.2058e-05    164 2.3061e-05 1.46648 0.15226\n111 6.8126e-06    165 1.1003e-05 1.46648 0.15226\n112 2.4728e-06    166 4.1899e-06 1.46582 0.15227\n113 1.7171e-06    167 1.7171e-06 1.46569 0.15225\n114 0.0000e+00    168 0.0000e+00 1.46518 0.15217\n```\n\n\n:::\n\n```{.r .cell-code}\nprune.t1 <- prune(t1, cp=0.02)   #prune the tree with cp=0.02\n\nprintcp(prune.t1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRegression tree:\nrpart(formula = bmd ~ age, data = bmd.data, method = \"anova\", \n    control = rpart.control(minsplit = 1, minbucket = 1, cp = 0))\n\nVariables actually used in tree construction:\n[1] age\n\nRoot node error: 4.659/169 = 0.027568\n\nn= 169 \n\n        CP nsplit rel error  xerror    xstd\n1 0.150111      0   1.00000 1.00752 0.11223\n2 0.022857      1   0.84989 0.88021 0.11329\n3 0.021625      2   0.82703 1.00527 0.12150\n4 0.020621      8   0.69728 1.02789 0.12227\n5 0.020000     11   0.62374 1.02015 0.12170\n```\n\n\n:::\n\n```{.r .cell-code}\nrpart.plot(prune.t1)              #pruned tree   \n```\n\n::: {.cell-output-display}\n![](09-trees_files/figure-html/t1-prune-1.png){width=672}\n:::\n:::\n\n\nThe `printcp()` function is used to examine the complexity parameter table and identify an appropriate pruning threshold. The tree is then pruned using cp = 0.02, which removes splits that do not sufficiently reduce prediction error, resulting in a simpler and more generalizable model.\n\nIn summary:  CP controls tree size, i.e.\n\n  -\tIt penalizes adding splits that don’t improve the model enough\n  -\tA split is kept only if it reduces error by at least CP\n\n\nLet’s use the pruned tree from aboved to predict the BMD for an individual 70 years old and compare it with the predictions from a linear model. We plot the predictions from the tree for ages 40 through 90 together with the predictions from the linear model:\n\n::: {.cell}\n\n```{.r .cell-code}\nlm1 <- lm(bmd ~ age,\n           data = bmd.data)\n\npredict(prune.t1,                       #prediction using the tree\n        newdata = data.frame(age=70))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        1 \n0.8070857 \n```\n\n\n:::\n\n```{.r .cell-code}\npredict(lm1,                           #prediction using the linear model\n        newdata = data.frame(age=70))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        1 \n0.7567922 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Create prediction grid\nage_grid <- data.frame(age = seq(40, 90, 1))\n\n# Get predictions\nage_grid$tree <- predict(prune.t1, newdata = age_grid)\nage_grid$linear <- predict(lm1, newdata = age_grid)\n\n# Convert to long format for ggplot\nplot_df <- data.frame(\n  age = rep(age_grid$age, 2),\n  bmd = c(age_grid$tree, age_grid$linear),\n  model = factor(\n    rep(c(\"Tree\", \"Linear Model\"), each = nrow(age_grid))\n  )\n)\n\n# Plot\nggplot(plot_df, aes(x = age, y = bmd, color = model)) +\n  geom_line(linewidth = 1) +\n  scale_color_manual(\n    values = c(\n      \"Tree\" = \"blue\",\n      \"Linear Model\" = \"red\"\n    )\n  ) +\n  labs(\n    x = \"Age\",\n    y = \"BMD\",\n    color = \"Model\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](09-trees_files/figure-html/lm-comp-1.png){width=672}\n:::\n:::\n\n\nLet’s fit now a tree to predict bone mineral density (BMD) based on AGE, SEX and BMI (BMI has to be computed) and compute the MSE.\n\nNotice that the caret will prune the tree based on the cross-validated cp.\n\n::: {.cell}\n\n```{.r .cell-code}\n#compute BMI\nbmd.data$bmi <- bmd.data$weight_kg / (bmd.data$height_cm/100)^2\n\ntrctrl <- trainControl(method = \"repeatedcv\", number = 10, repeats = 10)\nt2.caret  <-  train(bmd ~ age + sex + bmi, \n                        data = bmd.data, \n                        method = \"rpart\",\n                        trControl=trctrl,\n                        tuneGrid = expand.grid(cp=seq(0.001, 0.1, 0.001))\n                        )\n\n#Plot the RMSE versus the CP\nplot(t2.caret)\n```\n\n::: {.cell-output-display}\n![](09-trees_files/figure-html/tree2-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#Plot the tree with the optimal CP\nrpart.plot(t2.caret$finalModel)\n```\n\n::: {.cell-output-display}\n![](09-trees_files/figure-html/tree2-2.png){width=672}\n:::\n:::\n\n\nWe can compare the RMSE and R2 of the tree above with the linear model:\n  \n\n::: {.cell}\n\n```{.r .cell-code}\ntrctrl <- trainControl(method = \"repeatedcv\", number = 5, repeats = 10)\nlm2.caret<-  train(bmd ~ age + sex + bmi, \n                        data = bmd.data, \n                        method = \"lm\",\n                        trControl=trctrl\n                        )\n\nlm2.caret$results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  intercept      RMSE  Rsquared       MAE     RMSESD RsquaredSD      MAESD\n1      TRUE 0.1374844 0.3307248 0.1041004 0.02212903  0.1310581 0.01472284\n```\n\n\n:::\n\n```{.r .cell-code}\nt2.caret$finalModel$cptable\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          CP nsplit rel error\n1 0.26960125      0 1.0000000\n2 0.07577601      1 0.7303988\n3 0.06448418      2 0.6546227\n4 0.05400352      3 0.5901386\n5 0.03600000      4 0.5361350\n```\n\n\n:::\n\n```{.r .cell-code}\n#extracts the row with the RMSE and R2 from the table of results\n#corresponding to the cp with lowest RMSE  (best tune)\nt2.caret$results[t2.caret$results$cp==t2.caret$bestTune[1,1], ]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      cp      RMSE  Rsquared       MAE     RMSESD RsquaredSD      MAESD\n36 0.036 0.1324524 0.3988161 0.1027905 0.02556867  0.1817756 0.01802933\n```\n\n\n:::\n:::\n\n\nNow by yourselves try to fit random forests and gradient boosting trees. Compare the results from these fits, and to the fit ofd the linear model.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../../../site_libs/htmltools-fill-0.5.8.1/fill.css\" rel=\"stylesheet\" />\n<script src=\"../../../../site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<script src=\"../../../../site_libs/viz-1.8.2/viz.js\"></script>\n<link href=\"../../../../site_libs/DiagrammeR-styles-0.2/styles.css\" rel=\"stylesheet\" />\n<script src=\"../../../../site_libs/grViz-binding-1.0.11/grViz.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}