{
  "hash": "d481c85b3a45f7e22c3c322464482845",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Support Vector Machines\"\nauthor: \"Termeh Shafie\"\nformat:\n  html:\n    embed-resources: true\nnumber-sections: true\ntoc: true         \neditor: visual\nexecute:\n  cache:  true\n---\n\n# Support Vector Machines\nSupport vector machines (SVMs) offer a direct approach to binary classification: try to find a hyperplane in some feature space that “best” separates the two classes. In practice, however, it is difficult (if not impossible) to find a hyperplane to perfectly separate the classes using just the original features. SVMs overcome this by extending the idea of finding a separating hyperplane in two ways: \n\n(1) loosen what we mean by “perfectly separates”, \n(2) use the so-called kernel trick to enlarge the feature space to the point that perfect separation of classes is (more) likely.\n\nSVMs are quite versatile and have been applied to a wide variety of domains ranging from chemistry to pattern recognition. They are best used in binary classification scenarios. This brings up a question as to where SVMs are to be preferred to other binary classification techniques such as logistic regression. The honest response is, “it depends”,  but here are some points to keep in mind when choosing between the two. A general point to keep in mind is that SVM algorithms tend to be expensive both in terms of memory and computation, issues that can start to hurt as the size of the dataset increases.\n\n\nWe use the `e1071` library in `R` to demonstrate the support vector classifier and the SVM. Another option is the `LiblineaR` library, which is useful for very large linear problems.\n\n\n##  Warm-Up: SVM on `iris` dataset\n###  Training and test datasets\n\n::: {.cell}\n\n```{.r .cell-code}\n#load required library\nlibrary(e1071)\n\n#load built-in iris dataset\ndata(iris)\n\n#set seed to ensure reproducible results\nset.seed(42)\n\n#split into training and test sets\niris[, \"train\"] <- ifelse(runif(nrow(iris)) < 0.8, 1, 0)\n\n#separate training and test sets\ntrainset <- iris[iris$train == 1,]\ntestset <- iris[iris$train == 0,]\n\n#get column index of train flag\ntrainColNum <- grep(\"train\", names(trainset))\n\n#remove train flag column from train and test sets\ntrainset <- trainset[,-trainColNum]\ntestset <- testset[,-trainColNum]\n\ndim(trainset)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 115   5\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(testset)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 35  5\n```\n\n\n:::\n:::\n\n\n### Build the SVM model\n\n::: {.cell}\n\n```{.r .cell-code}\n#get column index of predicted variable in dataset\ntypeColNum <- grep(\"Species\", names(iris))\n\n#build model – linear kernel and C-classification (soft margin) with default cost (C=1)\nsvm_model <- svm(Species~ ., data = trainset, \n                 method = \"C-classification\", \n                 kernel = \"linear\")\nsvm_model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nsvm(formula = Species ~ ., data = trainset, method = \"C-classification\", \n    kernel = \"linear\")\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  1 \n\nNumber of Support Vectors:  24\n```\n\n\n:::\n:::\n\n\n\n\n### Support Vectors\nThe output from the SVM model show that there are 24 support vectors. If desired, these can be examined using the SV variable in the model, i.e via `svm_model$SV`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# support vectors\nsvm_model$SV\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Sepal.Length Sepal.Width Petal.Length Petal.Width\n19   -0.25639203  1.76683447   -1.3228618  -1.3054201\n42   -1.70055936 -1.70445193   -1.5591789  -1.3054201\n45   -0.97847569  1.76683447   -1.2047033  -1.1709010\n53    1.18777530  0.14690082    0.5676747   0.3088091\n55    0.70638619 -0.54735646    0.3904369   0.3088091\n57    0.46569164  0.60973900    0.4495161   0.4433282\n58   -1.21917025 -1.47303284   -0.3775936  -0.3637864\n69    0.34534436 -1.93587102    0.3313576   0.3088091\n71   -0.01569747  0.37831991    0.5085954   0.7123664\n73    0.46569164 -1.24161374    0.5676747   0.3088091\n78    0.94708075 -0.08451828    0.6267539   0.5778473\n84    0.10464981 -0.77877556    0.6858332   0.4433282\n85   -0.61743386 -0.08451828    0.3313576   0.3088091\n86    0.10464981  0.84115809    0.3313576   0.4433282\n99   -0.97847569 -1.24161374   -0.5548314  -0.2292673\n107  -1.21917025 -1.24161374    0.3313576   0.5778473\n111   0.70638619  0.37831991    0.6858332   0.9814046\n117   0.70638619 -0.08451828    0.9221503   0.7123664\n124   0.46569164 -0.77877556    0.5676747   0.7123664\n130   1.54881714 -0.08451828    1.0993881   0.4433282\n138   0.58603892  0.14690082    0.9221503   0.7123664\n139   0.10464981 -0.08451828    0.5085954   0.7123664\n147   0.46569164 -1.24161374    0.6267539   0.8468855\n150  -0.01569747 -0.08451828    0.6858332   0.7123664\n```\n\n\n:::\n:::\n\n\n\n###  Predictions on training and test model\n\n::: {.cell}\n\n```{.r .cell-code}\n# training set predictions\npred_train <- predict(svm_model, trainset)\nmean(pred_train == trainset$Species)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9826087\n```\n\n\n:::\n\n```{.r .cell-code}\n# test set predictions\npred_test <-predict(svm_model, testset)\nmean(pred_test == testset$Species)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9142857\n```\n\n\n:::\n:::\n\n\nThe test prediction accuracy indicates that the linear performs quite well on this dataset, confirming that it is indeed near linearly separable. To check performance by class, one can create a confusion matri. Another point is that we have used a soft-margin classification scheme with a cost $C=1$. You can experiment with this by explicitly changing the value of $C$. Again, I’ll leave this for you an exercise.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# confusion matrix\ncm <- table(pred_test, testset$Species)\ncm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            \npred_test    setosa versicolor virginica\n  setosa         18          0         0\n  versicolor      0          5         3\n  virginica       0          0         9\n```\n\n\n:::\n\n```{.r .cell-code}\n# accuracy\nsum(diag(cm)) / sum(cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9142857\n```\n\n\n:::\n:::\n\n\n\n## SVM on Penguins Data\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(datasets) # penguins data come from here\nlibrary(tidyverse) \n# plot data\nggplot(penguins, aes(x = bill_len, y = bill_dep, color = species)) +\n  geom_point() +\n  theme_minimal() +\n  labs(\n    x = \"Bill Length (in mm)\",\n    y = \"Bill Depth (in mm)\",\n    title = \"Bill Length vs. Bill Depth by Species\"\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](10-svm_files/figure-html/peng-data-1.png){width=672}\n:::\n:::\n\n\n### Linear, Separable Case\nThe penguins data set contains morphological measurements for individual penguins collected in the Palmer Archipelago, Antarctica. It includes observations from three species: Adélie, Chinstrap, and Gentoo—and records key physical characteristics such as bill length, bill depth, flipper length, body mass, sex, and island of origin. The data are commonly used for demonstrating exploratory data analysis, visualization, and classification methods, as the species show both overlap and separation in their physical traits.\n\nHere, we merge the Adélie and Chinstrap groups and restrict the analysis to two dimensions, using bill depth and flipper width as the discriminating variables. A helper function is used to plot an approximate version of the margin. note that we use `ggplot2` here it needs to be loaded.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Helper to reproduce the plot (points, SV squares, margin lines)\nsepLinePlot <- function(svm_fit, data,\n                        xvar = \"flipper_len\",\n                        yvar = \"bill_dep\") {\n\n  # response + class colors\n  yname <- all.vars(formula(svm_fit))[1]\n  cls <- factor(data[[yname]])\n  cols <- c(\"red\", \"green\")[as.integer(cls)]\n\n  plot(data[[xvar]], data[[yvar]],\n       col = cols, pch = 1,\n       xlab = xvar, ylab = yvar)\n\n  # compute w and b from e1071 internals\n  w <- as.numeric(t(svm_fit$coefs) %*% svm_fit$SV)\n  names(w) <- colnames(svm_fit$SV)\n  b <- -svm_fit$rho\n\n  # pull the correct coefficients for the plotted axes\n  wx <- w[[xvar]]\n  wy <- w[[yvar]]\n\n  xseq <- seq(min(data[[xvar]], na.rm = TRUE),\n              max(data[[xvar]], na.rm = TRUE),\n              length.out = 200)\n\n  # if wy ~ 0 => vertical boundary in this coordinate system\n  if (abs(wy) < 1e-12) {\n    x0 <- -b / wx\n    abline(v = x0, col = \"cyan\", lwd = 2)\n    abline(v = (-b + 1) / wx, col = \"cyan\", lty = 2)\n    abline(v = (-b - 1) / wx, col = \"cyan\", lty = 2)\n  } else {\n    y0  <- -(wx * xseq + b) / wy\n    yP1 <- -(wx * xseq + b - 1) / wy\n    yM1 <- -(wx * xseq + b + 1) / wy\n\n    lines(xseq, y0,  col = \"cyan\", lwd = 2)\n    lines(xseq, yP1, col = \"cyan\", lty = 2)\n    lines(xseq, yM1, col = \"cyan\", lty = 2)\n  }\n\n  # support vectors as grey squares\n  points(svm_fit$SV[, xvar],\n         svm_fit$SV[, yvar],\n         pch = 0, cex = 1.6, lwd = 1.2, col = \"grey60\")\n}\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n#merge Adelie & Chinstrap species to make the problem easier\npenguins_12_3 <- penguins %>%\n  mutate(species=dplyr::recode(species,`Adelie`=\"Adel/Chin\",`Chinstrap`=\"Adel/Chin\"))\n\n\n# Fit model, no scaling (important for plotting lines in original units)\nsv.fit.lin0 <- e1071::svm(\n  species ~ bill_dep + flipper_len,\n  data   = penguins_12_3,\n  kernel = \"linear\",\n  scale  = FALSE\n)\n\n# Plot\nsepLinePlot(sv.fit.lin0, penguins_12_3)\n```\n\n::: {.cell-output-display}\n![](10-svm_files/figure-html/svm-lin-1.png){width=672}\n:::\n:::\n\nThe solid line represents the separating (hyper)plane, while the margins are shown as dashed lines. The margin is constructed by widening the separating line until it just touches points on either side of the boundary, known as support vectors, with the margin edges remaining parallel to the separating plane.\n\n### Non-Separable Case\nAs an example, consider the penguins data while ignoring the Gentoo species, which results in a non-separable case with imperfectly separated groups. The analysis is restricted to two dimensions, using bill length and bill depth as the discriminating variables. In this setting there is no clean separating line between the groups; instead, the algorithm accommodates this by allowing slack, permitting a limited amount of crossover within the margin, as will be discussed in more detail later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# keep only Adelie and Chinstrap\npenguins_1_2 <- penguins %>% \n  dplyr::filter(species != \"Gentoo\")\n\n# fit linear SVM \nsv.fit.lin <- e1071::svm(\n  species ~ bill_len + bill_dep,\n  data   = penguins_1_2,\n  kernel = \"linear\",\n  scale  = FALSE\n)\n\n# plot using the helper (bill_len on x, bill_dep on y)\nsepLinePlot(\n  sv.fit.lin,\n  penguins_1_2,\n  xvar = \"bill_dep\",\n  yvar = \"bill_len\"\n)\n```\n\n::: {.cell-output-display}\n![](10-svm_files/figure-html/svm-lin2-1.png){width=672}\n:::\n:::\n\nThis plot shows the result of fitting a linear soft-margin support vector machine to the Adélie and Chinstrap penguins using bill depth (horizontal axis) and bill length (vertical axis).\n\nEach point represents an individual penguin, with colour indicating species. The solid cyan line is the estimated separating hyperplane, and the dashed cyan lines indicate the margins on either side. Because the two species overlap substantially in this two-dimensional feature space, there is no clean separating line. As a result, the fitted margin passes through dense regions of both classes.\n\nThe square-boxed points are the support vectors. In this non-separable setting, these include not only points lying exactly on the margin, but also points inside the margin and some that are on the wrong side of the separating line. These observations are the ones that determine the position and orientation of the hyperplane.\n\nMany observations fall within the margin and several lie on the “wrong” side of the separating line. This illustrates the role of **slack variables** in a soft-margin SVM: the algorithm allows violations of the margin, and even misclassifications, in order to achieve the best overall trade-off between margin width and classification error. Given the restriction to two variables and a linear decision boundary, this plot represents about as good a separation as can be achieved for these two species.\n\nEach element of the slack vector  \n$$\n\\boldsymbol{\\varepsilon} = (\\varepsilon_1, \\ldots, \\varepsilon_n)^\\top\n$$\n\nis known as the *slack* for a given observation. The slack variables quantify how far each point lies from the margin for its assigned group and can be viewed as allowances for observations that fall on the “wrong side” of the separating plane, playing a role analogous to residuals in regression.\n\nWhen $\\varepsilon_i = 0$, observation $i$ lies on the correct side of the margin. If $0 < \\varepsilon_i \\leq 1$, observation $i$ is still on the correct side of the separating plane but lies within the margin; such points are classified correctly, although in the case of perfectly separable groups no observations fall within the margin. When $\\varepsilon_i > 1$, observation $i$ lies on the wrong side of the separating plane and is therefore misclassified.\n\nThe margin width $M$ and the cost parameter $C$ work together to control the use of slack. In this formulation, decreasing the cost $C$ allows more slack, which in turn results in a larger number of support vectors and greater tolerance for margin violations.\n\n### Using e1071 Package\nWe have tried to align the above formulation with the implementation of the support vector machine in the `svm` function from the `e1071` package. In particular, we use the cost parameter $C$ to modify the separating plane and illustrate the consequences of this choice. Shown next is the final SVM classification for the second example using the default cost $C = 1$, which results in several misclassified observations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(sv.fit.lin,penguins_1_2,formula=bill_dep ~ bill_len)\n```\n\n::: {.cell-output-display}\n![](10-svm_files/figure-html/svm-alt-1.png){width=768}\n:::\n:::\n\n\nIn this plot, group membership is indicated by the colour of the symbols, from which it can be seen that there are seven misclassified observations. Points are shown with an open circle when they are not support vectors and with a cross when they are support vectors. The plot reveals a relatively large number of support vectors, in contrast to the seven support vectors observed in the completely separable case. We now decrease the cost parameter $C$, forcing the margin $M$ to shrink in order to accommodate the slack associated with the misclassified observations, which in turn increases the number of support vectors used. The advantage of decreasing $C$, and hence increasing the number of support vectors, is that the resulting separating plane becomes less sensitive to a small number of outlying observations. The results for $C = 0.01$ are shown next.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsv.fit.lin.lowCost <- svm(species ~ bill_len+bill_dep, data = penguins_1_2,kernel='linear',cost=0.01)\nplot(sv.fit.lin.lowCost,penguins_1_2,formula=bill_dep ~ bill_len)\n```\n\n::: {.cell-output-display}\n![](10-svm_files/figure-html/svm-alt-lowC-1.png){width=768}\n:::\n\n```{.r .cell-code}\nsummary(sv.fit.lin.lowCost)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nsvm(formula = species ~ bill_len + bill_dep, data = penguins_1_2, \n    kernel = \"linear\", cost = 0.01)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  0.01 \n\nNumber of Support Vectors:  129\n\n ( 65 64 )\n\n\nNumber of Classes:  2 \n\nLevels: \n Adelie Chinstrap Gentoo\n```\n\n\n:::\n:::\n\n\nThe summary of the fitted model shows that 129 support vectors were used, and the separating line can be seen to have shifted in an interesting way. There are now more misclassified observations, almost all of which belong to the Chinstrap species. This suggests that some of the original support vectors may have been overly influential; however, the adjustment has also introduced additional classification error.\n\n## The Kernel Trick\nThe kernel trick is a concept that underlies a wide range of machine learning techniques and is based on two key ideas. First, groups that are not linearly separable in $p$ dimensions may become separable in a higher-dimensional space of dimension $q > p$ through an appropriate transformation. Second, rather than explicitly computing this transformation, training the model in the higher-dimensional space, and then making predictions on new data, we can instead use a kernel function with a suitable inner product, which leads to an equivalent optimization problem and solution without ever working directly in the higher-dimensional space.\n\nA rather extreme example using two circles helps illustrate the idea. Let one group be defined by the relationship  \n$X_1^2 + X_2^2 = 2^2$,  \nand the other by  \n$X_1^2 + X_2^2 = 1^2$.\n\nIf we transform the two-dimensional relationships into three dimensions using the mapping  \n$(X_1, X_2) \\mapsto (X_1, X_2, X_1^2 + X_2^2)$,  \nthe two groups become clearly separable.\n\nNote that we need an extra package for 3D plots. We also plot the 2D original set of relationships for comparison.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(plot3D) # install if needed\n\nset.seed(1001)\nx1 <- runif(100, -2, 2); y1 <- sqrt(4 - x1^2)\nx2 <- runif(100, -1, 1); y2 <- sqrt(1 - x2^2)\ndf <- data.frame(\n  x1 = c(x1, x1, x2, x2),\n  x2 = c(y1, -y1, y2, -y2)\n)\ndf$z <- df$x1^2 + df$x2^2\nplot3D::scatter3D(df$x1, df$x2, df$z)\n```\n\n::: {.cell-output-display}\n![](10-svm_files/figure-html/rad-example-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# 2d correspondence\nplot(x2~x1,data=df,col=c(4,2,2,3)[round(z)]) \n```\n\n::: {.cell-output-display}\n![](10-svm_files/figure-html/rad-example-2.png){width=672}\n:::\n:::\n\n\n\nBefore going into details, we run svm on the circle data using several different kernels. First the linear:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsv.circ.lin <- svm(factor(z)~ x1+x2,data = df, kernel='linear')\nplot(sv.circ.lin,df,formula=x2~x1,main=\"Linear Kernel\")\n```\n\n::: {.cell-output-display}\n![](10-svm_files/figure-html/svm-rad1-1.png){width=768}\n:::\n:::\n\n\n\nUsing the Radial Kernel we get:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsv.circ.rad <- svm(factor(z)~ x1+x2,data = df, kernel='radial')\nplot(sv.circ.rad,df,formula=x2~x1)\n```\n\n::: {.cell-output-display}\n![](10-svm_files/figure-html/svm-rad2-1.png){width=768}\n:::\n:::\n\n\n### Back to the Penguin Data\nHere a support vector machine with a radial kernel is fitted to the penguin data using bill length and bill depth as predictors. The radial kernel implicitly maps the data into a higher-dimensional feature space, allowing the classifier to learn a non-linear decision boundary between the species. The subsequent plot displays this non-linear separating boundary in the original two-dimensional feature space, illustrating how the radial kernel can capture complex class structure that a linear SVM cannot.\n\n::: {.cell}\n\n```{.r .cell-code}\nsv.fit.rad <- svm(species ~ bill_len+bill_dep, data = penguins_1_2, kernel='radial')\nplot(sv.fit.rad,penguins_1_2,formula=bill_dep ~ bill_len)\n```\n\n::: {.cell-output-display}\n![](10-svm_files/figure-html/svm-rad-peng-1.png){width=768}\n:::\n:::\n\nWe can adjust the model parameters to improve classification performance, but care must be taken to avoid overfitting; in practice, methods such as cross-validation should be used to select these parameters. \n\nBelow, the parameter $\\gamma$ is increased to 2, which accentuates large differences between points. \n\n::: {.cell}\n\n```{.r .cell-code}\nsv.fit.rad.gam2 <- svm(species ~ bill_len+bill_dep, data = penguins_1_2,kernel='radial', gamma=2)\nplot(sv.fit.rad.gam2, penguins_1_2, formula=bill_dep ~ bill_len)\n```\n\n::: {.cell-output-display}\n![](10-svm_files/figure-html/svm-rad-gam2-1.png){width=768}\n:::\n:::\n\n\n\n###  SVM with more than two groups\nThe basic idea is to apply the support vector machine recursively, comparing one group against all remaining groups, then comparing the second group against groups three and higher, and so on. As a final example, all species in the penguins data are used, with both linear and radial kernel fits under default settings. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsv.fit.lin_all3 <- svm(species ~ bill_len + bill_dep, data = penguins,kernel='linear')\nplot(sv.fit.lin_all3, penguins, formula=bill_dep ~ bill_len)\n```\n\n::: {.cell-output-display}\n![](10-svm_files/figure-html/svm-lin-cat3-1.png){width=768}\n:::\n:::\n\n\n\nAs before, Gentoo is easy to separate from the other species, but this multiclass extension does not substantially improve predictive performance. Distinguishing Adélie from Chinstrap remains difficult near the decision boundary, and the radial kernel provides only a modest improvement.\n\nThe radial variant is as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\nsv.fit.rad_all3 <- svm(species ~ bill_len + bill_dep, data = penguins, kernel='radial')\nplot(sv.fit.rad_all3,penguins,formula=bill_dep ~ bill_len)\n```\n\n::: {.cell-output-display}\n![](10-svm_files/figure-html/svm-rad-cat3-1.png){width=768}\n:::\n:::\n\n\n## Cross Validation\nAs done previously in this course, the basic idea is to withhold a portion of the training data and use it as a test set. First, one loops over the open tuning parameters, such as $C$ and $\\gamma$, fitting the model on the training set while holding these parameters fixed. The fitted model is then evaluated on the test set, and this process is repeated for all parameter combinations. The parameter values that yield the best performance are then selected. We applied this approach to the penguins data, still attempting classification using only two features. \n\nA grid search is conducted over $C = 0.01, 0.1, 1, 10, 100$ and $\\gamma = 0.5, 1, 2, 3, 4$. Somewhat surprisingly, the default parameter values performed best in this case.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1001)\ntune.out <- tune(svm, species ~., data=penguins, kernel='radial', ranges = list(cost=c(0.01,0.1,1,10,100),gamma=c(0.5,1,2,3,4)))\nsummary(tune.out)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost gamma\n    1   0.5\n\n- best performance: 0.01212678 \n\n- Detailed performance results:\n    cost gamma      error dispersion\n1  1e-02   0.5 0.56054495 0.07977601\n2  1e-01   0.5 0.03004679 0.02427130\n3  1e+00   0.5 0.01212678 0.02119780\n4  1e+01   0.5 0.02407531 0.02361808\n5  1e+02   0.5 0.02407531 0.02361808\n6  1e-02   1.0 0.56054495 0.07977601\n7  1e-01   1.0 0.21904650 0.06312732\n8  1e+00   1.0 0.01800914 0.02523584\n9  1e+01   1.0 0.02113414 0.02468684\n10 1e+02   1.0 0.02113414 0.02468684\n11 1e-02   2.0 0.56054495 0.07977601\n12 1e-01   2.0 0.53040393 0.08062374\n13 1e+00   2.0 0.04503629 0.02956429\n14 1e+01   2.0 0.04797746 0.03525477\n15 1e+02   2.0 0.04797746 0.03525477\n16 1e-02   3.0 0.56054495 0.07977601\n17 1e-01   3.0 0.56054495 0.07977601\n18 1e+00   3.0 0.09274589 0.07027869\n19 1e+01   3.0 0.08980472 0.06407604\n20 1e+02   3.0 0.08980472 0.06407604\n21 1e-02   4.0 0.56054495 0.07977601\n22 1e-01   4.0 0.56054495 0.07977601\n23 1e+00   4.0 0.13162131 0.07053983\n24 1e+01   4.0 0.12270865 0.06670692\n25 1e+02   4.0 0.12270865 0.06670692\n```\n\n\n:::\n:::\n\n\n\n::: {.callout-note}\nDispersion, as shown above, is analogous to a root mean squared error (RMSE),\n$$\n\\mathrm{rmse} = \\sqrt{\\frac{1}{n}\\sum_i \\left(\\hat{y}_i - y_i\\right)^2},\n$$\nand is a quantity we seek to minimize. However, it is possible to choose parameter values that yield improved within-sample performance but do not translate into improved out-of-sample performance.\n:::\n\nOur final model is thus:\n\n::: {.cell}\n\n```{.r .cell-code}\nsv.fit.rad_all3 <- svm(species ~ bill_len+bill_dep, data= penguins, kernel='radial', cost=1, gamma=.5)\nplot(sv.fit.rad_all3, penguins, formula = bill_dep ~ bill_len)\n```\n\n::: {.cell-output-display}\n![](10-svm_files/figure-html/svm-final-peng-1.png){width=768}\n:::\n:::\n\n\n## Exercise: Radial Kernels and Nonlinear Decision Boundaries\n\nThis section explores the use of **radial kernels** to allow decision boundaries to be both non-linear and non-contiguous. As a reminder, the linear and radial (sometimes called Gaussian) kernels are defined as follows.\n\n**Linear kernel:**\n\n$$\nK(\\mathbf{X}_i, \\mathbf{X}_j) = \\langle \\mathbf{X}_i, \\mathbf{X}_j \\rangle,\n$$\n\nwhere $\\langle \\cdot \\rangle$ denotes the standard vector inner product, as defined in the notes.\n\n**Radial kernel:**\n\n$$\nK(\\mathbf{X}_i, \\mathbf{X}_j) = \\exp\\!\\left(-\\gamma \\lVert \\mathbf{X}_i - \\mathbf{X}_j \\rVert^2\\right).\n$$\n\nThis kernel is based on a distance measure and includes a single tuning parameter $\\gamma$, which controls the influence of large distances between observations. Larger values of $\\gamma$ emphasize local structure, while smaller values lead to smoother decision boundaries. We briefly explore the effect of this parameter here, with a more detailed discussion deferred to the next class.\n\nFor the following exercises, we use the **Australian Crabs** data (`crabs`), a benchmark data set that is notoriously difficult to cluster and relatively challenging to classify. The five primary predictors (measurements) are abbreviated as **FL**, **RW**, **CL**, **CW**, and **BD**. \nThe data needs to be preprocessed so that sex and species are combined into a single categorical variable, `group`, with four levels:\n\n1. Blue Males  \n2. Orange Males  \n3. Blue Females  \n4. Orange Females  \n\nIn most analyses, we do not explicitly split the data into training and test sets. Instead, we rely on various forms of cross-validation to properly assess out-of-sample performance.\n\nYou are asked to compare and contrast SVM fits using the raw measures.\n\n- Run `svm` with the following formula using all five measures: `group ~ FL + RW + CL + CW + BD`, first with the **linear** kernel and then with the **radial** kernel.\n- Compare the **accuracy** (and optionally the **confusion matrices**) for these two runs.\n- If the radial-kernel fit performs worse, adjust the tuning parameter by setting `gamma` to 2, 4, or even 10, and then compare the accuracy of this updated fit.\n- If you would like to visualize the fits, try plotting **RW vs. CW**, but you must set `slice` to reasonable values for the remaining features.\n\n\nDetermine if the following statements are **true/false** based on your output:\n\n1. The crabs data, examined using raw features seems to be divisible into groups using linear 'cuts'\n2. In order to perform well, use of the radial kernel requires higher penalties for large distances between observations\n3. Visualization of the svm solution is hindered by our choice for the slice?\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}