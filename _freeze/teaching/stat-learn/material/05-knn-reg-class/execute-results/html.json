{
  "hash": "94b68385382af44e993659a06a2f55c8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"KNN classification + regression\"\nauthor: \"Termeh Shafie\"\nformat: html\neditor: visual\nexecute:\n  cache:  true\n---\n\n\n\n# KNN classifier algorithm (for univariate $x$'s and binary $y$'s)\n\nWe are going to look at the probability version of the KNN classifier algorithm. We create a function called `KNN_classifier`⁠ for performing KNN classification using the following arguments:\n\n-   $x_0$ as the new point at which we wish to predict $y$\n-   ${\\bf x} = (x_1,x_2, \\dots, x_n)$ as the vector of training $x$'s, where $x_i$ is real-valued\n-   ${\\bf y} = (y_1,y_2, \\dots, y_n)$ as the vector of training $y$'s, where $y_i$ is 0 or 1\n-   $K$ as number of neighbors to use\n-   $\\hat{p}_{1}$ as the estimated probability of $y_0=1$ given $x_0$\n\nThe function calculates the Euclidean distance between $x_0$ and each of the $x_i$'s in the training set $(x_1, x_2, \\dots, x_n)$. Then we order them from nearest to furthest away and computes the fraction of $y$ values of the $y$ values of the $K$ nearest training points that are equal to 1 and return this proportion as an estimated probability of $y_0=1$. We can transform $\\hat{p}_{1}$ to a prediction of the $y$ value at $x_0$ by using a threshold on $\\hat{p}_{1}$ and return\n\n\n::: {.cell}\n\n```{.r .cell-code}\nKNN_classifier = function(x0, x, y, K) {\n  distances = abs(x - x0)  \n  o = order(distances)  \n  p1_hat = mean(y[o[1:K]])  \n  return(p1_hat)  \n}\n```\n:::\n\n\n## Simulate training data\n\nWe simulate training vector $\\bf{x}$ from a uniform distribution on the interval $[0,5]$ and the true probability $p_1(x)$ of $y=1$ given $x$ (true relationship between $x$ and $y$) according to $$p_1(x) = \\frac{\\exp(2*\\cos(x))}{(1 + \\exp(2*\\cos(x)))}$$ We simulate simulate training $y$'s as Bernoulli random variables with probabilities $p_1(x)$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)  # set random number generator\nn = 20 \nx = 5*runif(n)  \np1 = function(x) { exp(2*cos(x))/(1 + exp(2*cos(x))) }  \ny = rbinom(n,1,p1(x)) \n```\n:::\n\n\n### Plot of the training data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x,y,col=2,pch=20,cex=2)  # plot training data\nx_grid = seq(from=0, to=5, by=0.01)  # grid of x values \nlines(x_grid,p1(x_grid))  # plot true p1(x) values for the grid\n```\n\n::: {.cell-output-display}\n![](05-knn-reg-class_files/figure-html/unnamed-chunk-3-1.png){width=864}\n:::\n:::\n\n\n## Predicting classes\n\nNow we run the `KNN_classifier` function to predict $y$ at each point on the grid of $x$ values. For that we need to define $K$, that is number of nearest neighbors to use. We start with setting it equal to 1 but this can be changed later as an exercise. Further, we predict the $y$ values for each $x$ in the grid by thresholding the estimated probabilities to $\\leq 0.5$ and $>0.5$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nK = 1\np1_grid_hat = sapply(x_grid, function(x0) { KNN_classifier(x0, x, y, K) })\ny_grid_hat = round(p1_grid_hat > 0.5)  \n```\n:::\n\n\nNext we add the predicted values to our plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x,y,col=2,pch=20,cex=2)  # plot training data\ntitle(paste(\"K =\",K))\nlines(x_grid,p1(x_grid))  # plot true p1(x) values \nlines(x_grid,p1_grid_hat,col=4)  # plot estimated probabilities of y=1 \nlines(x_grid,y_grid_hat,col=4)  # plot predicted y values for each x0\n```\n\n::: {.cell-output-display}\n![](05-knn-reg-class_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Error rates\n\nThe training error rate is given by $$\\textrm{training error} = \\frac{1}{n}\\sum_{i=1}^n  I(\\hat{y_i} \\neq y_i)$$ So we first run KNN classifier (probability version) at each $x$ in the training set, then we predict the $y$ values for each $x$ in the training set (prediction version of KNN), and finally compute the compute the training error rate which is the on average misclassification rate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\np1_hat = sapply(x, function(x0) { KNN_classifier(x0, x, y, K) }) \ny_hat = round(p1_hat > 0.5)  \ntrain_error = mean(y_hat != y)  \nprint(paste0(\"Training error rate (K = \",K,\") = \",train_error))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Training error rate (K = 1) = 0\"\n```\n\n\n:::\n:::\n\n\nNow we compute the test error rate. We again simulate a large number of samples as test set, namely 10000. We simulate test $x$'s and test $y$'s and run the KNN classifier at each $x$ in the test set. We then predict the $y$ values for each $x$ in the test set and compute the test error rate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_test = 10000 \nx_test = 5*runif(n_test)  \ny_test = rbinom(n_test,1,p1(x_test))  \np1_test_hat = sapply(x_test, function(x0) { KNN_classifier(x0, x, y, K) })  \ny_test_hat = round(p1_test_hat > 0.5) \ntest_error = mean(y_test_hat != y_test) \nprint(paste0(\"Test error rate (K = \",K,\"): \",test_error))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Test error rate (K = 1): 0.2869\"\n```\n\n\n:::\n:::\n\n\n\nHow can we tell if the above is a good test error rate? We compute the test error rate for the Bayes optimal classifier. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Bayes optimal classifier\n# use the true p1(x) to make the best possible predictions on the training set\ny_hat_optimal = p1(x) > 0.5  \n# compute the training error rate for the Bayes optimal classifier\ntrain_error_optimal = mean(y_hat_optimal != y)  \nprint(paste0(\"Training error rate (Optimal): \",train_error_optimal))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Training error rate (Optimal): 0.15\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# use the true p1(x) to make the best possible predictions on the test set\ny_test_hat_optimal = round(p1(x_test) > 0.5)\n# compute the test error rate for the Bayes optimal classifier\ntest_error_optimal = mean(y_test_hat_optimal != y_test) \nprint(paste0(\"Test error rate (Optimal): \",test_error_optimal))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Test error rate (Optimal): 0.2495\"\n```\n\n\n:::\n:::\n\n\n\n# KNN regression algorithm (for univariate $x$'s)\n\nWe create a function called `KNN`⁠ for performing KNN regression using the following arguments:\n\n-   $x_0$ as the new point at which we wish to predict $y$\n-   ${\\bf x} = (x_1,x_2, \\dots, x_n)$ as the vector of training $x$'s\n-   ${\\bf y} = (y_1,y_2, \\dots, y_n)$ as the vector of training $y$'s\n-   $K$ as number of neighbors to use\n-   $\\hat{y}_0$ as the predicted value of $y$ at $x_0$\n\nThe function calculates the Euclidean distance between $x_0$ and each of the $x_i$'s in the training set $(x_1, x_2, \\dots, x_n)$. Then we order them from nearest to furthest away and takes the mean of the $y$ values of the $K$ nearest points yielding the predicted value of $y$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#   x0 = new point at which to predict y\n#   x = (x_1,...,x_n) = vector of training x's\n#   y = (y_1,...,y_n) = vector of training y's\n#   K = number of neighbors to use\n#   y0_hat = predicted value of y at x0\n\nKNN = function(x0, x, y, K) {\n  distances = abs(x - x0) \n  o = order(distances) \n  y0_hat = mean(y[o[1:K]]) \n  return(y0_hat)  \n}\n```\n:::\n\n\nWhere do we get ${\\bf x}$ and ${\\bf y}$?\n\n## Simulate training data\n\nWe simulate training vector $\\bf{x}$ from a uniform distribution on the interval $[0,5]$ and simulate training vector $\\bf{y}$ by assuming $$y = f(x) + \\varepsilon$$ where $f(x) = \\cos(x)$ and $\\varepsilon \\sim N(0, \\sigma^2)$ and $\\sigma = 0.3$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)  # set random number generator\nn = 20  # number of samples\nx = 5*runif(n)  \nsigma = 0.3  \nf = function(x) { cos(x) }  \ny = f(x) + sigma*rnorm(n)  \n```\n:::\n\n\n### Plot of the training data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x,y,col=2,pch=20,cex=2)  # plot training data\nx_grid = seq(from=0, to=5, by=0.01)  # grid of x values for plotting f(x) values\nlines(x_grid,f(x_grid))  # plot true f(x) values for the grid\n```\n\n::: {.cell-output-display}\n![](05-knn-reg-class_files/figure-html/unnamed-chunk-11-1.png){width=864}\n:::\n:::\n\n\n## Predicting values\n\nNow we run the `KNN` function to predict $y$ at each point on the grid of $x$ values. For that we need to define $K$, that is number of nearest neighbors to use. We start with setting it equal to 1 but this can be changed later as an exercise.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nK = 1 \ny_grid_hat = sapply(x_grid, function(x0) { KNN(x0, x, y, K) })\n```\n:::\n\n\nNext we add the predicted values to our plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x,y,col=2,pch=20,cex=2)  # plot training data\ntitle(paste(\"K =\",K))\nlines(x_grid,f(x_grid))  # plot true f(x) values\nlines(x_grid,y_grid_hat,col=4)  # plot predicted y values \n```\n\n::: {.cell-output-display}\n![](05-knn-reg-class_files/figure-html/unnamed-chunk-13-1.png){width=864}\n:::\n:::\n\n\nWhat happens to predicted curve when you change the value of $K$?\n\n## Bias-Variance Trade-Off\n\nWe are going to run through some code in order to illustrate the trade-off between bias and variance. We set $x_0$ to 1.5, which is the point we wish to estimate $y$ at.\n\nWe simulate 10000 data sets to approximate expectations over the $Y$'s (given fixed $x$). We initialize two vectors of zeros to hold predicted and true $y$ values at $x_0$. Then for each of the 10000 datasets simulated, we repeat the above syntax for prediction. For the first 5 datasets simulated, we plot out the results to see what is happening.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nK = 1  \nx0 = 1.5  \nn_datasets = 10000  \ny0_hat = rep(0,n_datasets)  \ny0 = rep(0,n_datasets) \nfor (i in 1:n_datasets) {\n  y = f(x) + sigma*rnorm(n) \n  y0[i] = f(x0) + sigma*rnorm(1) \n  y0_hat[i] = KNN(x0, x, y, K)  \n  if (i <= 5) {\n    plot(x,y,col=2,pch=20,cex=2,ylim=c(-1.5,1.5))  \n    lines(x_grid,f(x_grid))  \n    y_grid_hat = sapply(x_grid, function(x0) { KNN(x0, x, y, K) })\n    lines(x_grid,y_grid_hat,col=4)  \n    points(x0,y0_hat[i],pch=20,cex=4,col=4)  # plot predicted value of y at x0\n  }\n}\n```\n\n::: {.cell-output-display}\n![](05-knn-reg-class_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](05-knn-reg-class_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](05-knn-reg-class_files/figure-html/unnamed-chunk-14-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](05-knn-reg-class_files/figure-html/unnamed-chunk-14-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](05-knn-reg-class_files/figure-html/unnamed-chunk-14-5.png){width=672}\n:::\n:::\n\n\nNext we calculate the bias and variance of the KNN predictions at $x_0$.We also compute the variance of the noise at $x_0$ in order to be able to get the test MSE both using the bias variance representation $$\\textrm{test MSE} = \\textrm{bias}^2 + \\textrm{variance} + \\textrm{noise}$$ and the direct formula: $$\\mathop{\\mathbb{E}} \\left(y_0- \\hat{f}(x_0)\\right)^2$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbias = mean(y0_hat) - f(x0)  # bias of KNN predictions at x0\nvariance = var(y0_hat)  # variance of KNN predictions at x0\nnoise = sigma^2  # variance of the noise at x0\n\nbias^2 + variance + noise \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2086705\n```\n\n\n:::\n\n```{.r .cell-code}\nmean((y0 - y0_hat)^2) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2097131\n```\n\n\n:::\n:::\n\n\nWhy do you think the two values differ?\n\n\nLet's visualize and explain the bias-variance trade-off using the syntax above. We show how the bias, variance and test MSE is influenced by the choice of $K$, and include three plots should be included showing the following:\n\n-   bias vs. $K$ (or flexibility)\n-   variance vs. $K$ (or flexibility)\n-   test MSE versus $K$ (or flexibility)\n\n\nWe plot of MSE, bias\\^2 and variance against number of neighbors $K$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nK = 1:5\ny0_hat_matrix = matrix(0, nrow = n_datasets, ncol = length(K))\n\n\nstatsout <- function(K) {\n  y0_hat_K = rep(0, n_datasets)\n  \n  for (i in 1:n_datasets) {\n    y = f(x) + sigma * rnorm(n)\n    y0[i] = f(x0) + sigma * rnorm(1)\n    y0_hat_K[i] = KNN(x0, x, y, K)\n  }\n  \n  bias_K = mean(y0_hat_K) - f(x0)\n  variance_K = var(y0_hat_K)\n  noise_K = sigma^2\n  MSE_K = bias_K^2 + variance_K + noise_K \n  \n  return(c(bias_K^2, variance_K, MSE_K))\n}\n\ntoplot = sapply(K, statsout)\n\n# Plot the bias-variance trade-off\nplot(K, toplot[1, ], type = \"l\", lty=1, xlab = \"K\", \n     ylab = \"Bias^2\", ylim = c(min(toplot), max(toplot)))\nlines(K, toplot[2, ], lty=2)\nlines(K, toplot[3, ],lty=3)\n# Add a legend\nlegend(\"topleft\", legend=c(\"Bias^2\", \"Variance\", \"MSE\"),lty=1:3, cex=0.8)\n```\n\n::: {.cell-output-display}\n![](05-knn-reg-class_files/figure-html/unnamed-chunk-16-1.png){width=864}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}