{
  "hash": "7c74137cfa0ba76d72ffa4a54b1368c5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Statistical Learning - Lab Session 09\"\nformat:\n  html:\n    embed-resources: true\neditor: visual\n---\n\n\n## Tree Methods\n\n# Decision Trees\n\n\n::: {.cell}\n\n:::\n\n\nThe `tree` library is used to construct classification and regression trees.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tree)\nlibrary(ISLR2)\nlibrary(ggplot2)\nlibrary(tidyverse)\n```\n:::\n\n\nWe first use classification trees to analyze the `Carseats` data set. In these data, `Sales` is a continuous variable, and so we begin by recoding it as a binary variable. We use the `ifelse()` function to create a variable, called `High`, which takes on a value of `Yes` if the `Sales` variable exceeds $8$, and takes on a value of `No` otherwise.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nHigh <- factor(ifelse(Carseats$Sales <= 8, \"No\", \"Yes\"))\nCarseats <- data.frame(Carseats, High)\n```\n:::\n\n\nWe now use the `tree()` function to fit a classification tree in order to predict `High` using all variables but `Sales`. The syntax of the `tree()` function is quite similar to that of the `lm()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(0)\ntree.carseats <- tree(High ~ . - Sales, Carseats)\n```\n:::\n\n\nThe `summary()` function lists the variables that are used as internal nodes in the tree, the number of terminal nodes, and the (training) error rate.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(tree.carseats)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nClassification tree:\ntree(formula = High ~ . - Sales, data = Carseats)\nVariables actually used in tree construction:\n[1] \"ShelveLoc\"   \"Price\"       \"Income\"      \"CompPrice\"   \"Population\" \n[6] \"Advertising\" \"Age\"         \"US\"         \nNumber of terminal nodes:  27 \nResidual mean deviance:  0.4575 = 170.7 / 373 \nMisclassification error rate: 0.09 = 36 / 400 \n```\n\n\n:::\n:::\n\n\nWe see that the training error rate is $9\\%$. For classification trees, the deviance reported in the output of `summary()` is given by\n\n\\$\\$-2 \\sum m\\* \\sum\\*k n\\*{mk} \\log \\hat{p}{mk}\\$\\$\n\nwhere $n_{mk}$ is the number of observations in the $m$th terminal node that belong to the $k$th class. This is closely related to the entropy, defined in (8.7). A small deviance indicates a tree that provides a good fit to the (training) data. The *residual mean deviance* reported is simply the deviance divided by $n-|{T}_0|$, which in this case is $400-27=373$.\n\nOne of the most attractive properties of trees is that they can be graphically displayed. We use the `plot()` function to display the tree structure, and the `text()` function to display the node labels. The argument `pretty = 0` instructs `R` to include the category names for any qualitative predictors, rather than simply displaying a letter for each category.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(tree.carseats)\ntext(tree.carseats, pretty = 1)\n```\n\n::: {.cell-output-display}\n![](09_Trees_files/figure-html/chunk6-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nCarseats$ShelveLoc <- factor(Carseats$ShelveLoc, levels = c(\"Bad\", \"Medium\", \"Good\"))\n\n\nCarseats %>%\n  ggplot(aes(x = ShelveLoc, y = Price, color = High)) +\n  geom_point() +\n  theme(legend.position = \"top\") +\n  geom_segment(aes(x = 2.5, xend = 4, y = 135, yend = 135), \n               linetype = \"dashed\", color = \"purple\", size = 1) + \n  geom_segment(aes(x = 0, xend = 2.5, y = 92.5, yend = 92.5), \n               linetype = \"dashed\", color = \"purple\", size = 1) + \n  geom_segment(aes(x = 2.5, xend = 2.5, y = 0, yend = 200), \n               linetype = \"dashed\", color = \"purple\", size = 1) \n```\n\n::: {.cell-output-display}\n![](09_Trees_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nThe most important indicator of `Sales` appears to be shelving location, since the first branch differentiates `Good` locations from `Bad` and `Medium` locations.\n\nIf we just type the name of the tree object, `R` prints output corresponding to each branch of the tree. `R` displays the split criterion (e.g. `Price < 92.5`), the number of observations in that branch, the deviance, the overall prediction for the branch (`Yes` or `No`), and the fraction of observations in that branch that take on values of `Yes` and `No`. Branches that lead to terminal nodes are indicated using asterisks.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree.carseats\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nnode), split, n, deviance, yval, (yprob)\n      * denotes terminal node\n\n  1) root 400 541.500 No ( 0.59000 0.41000 )  \n    2) ShelveLoc: Bad,Medium 315 390.600 No ( 0.68889 0.31111 )  \n      4) Price < 92.5 46  56.530 Yes ( 0.30435 0.69565 )  \n        8) Income < 57 10  12.220 No ( 0.70000 0.30000 )  \n         16) CompPrice < 110.5 5   0.000 No ( 1.00000 0.00000 ) *\n         17) CompPrice > 110.5 5   6.730 Yes ( 0.40000 0.60000 ) *\n        9) Income > 57 36  35.470 Yes ( 0.19444 0.80556 )  \n         18) Population < 207.5 16  21.170 Yes ( 0.37500 0.62500 ) *\n         19) Population > 207.5 20   7.941 Yes ( 0.05000 0.95000 ) *\n      5) Price > 92.5 269 299.800 No ( 0.75465 0.24535 )  \n       10) Advertising < 13.5 224 213.200 No ( 0.81696 0.18304 )  \n         20) CompPrice < 124.5 96  44.890 No ( 0.93750 0.06250 )  \n           40) Price < 106.5 38  33.150 No ( 0.84211 0.15789 )  \n             80) Population < 177 12  16.300 No ( 0.58333 0.41667 )  \n              160) Income < 60.5 6   0.000 No ( 1.00000 0.00000 ) *\n              161) Income > 60.5 6   5.407 Yes ( 0.16667 0.83333 ) *\n             81) Population > 177 26   8.477 No ( 0.96154 0.03846 ) *\n           41) Price > 106.5 58   0.000 No ( 1.00000 0.00000 ) *\n         21) CompPrice > 124.5 128 150.200 No ( 0.72656 0.27344 )  \n           42) Price < 122.5 51  70.680 Yes ( 0.49020 0.50980 )  \n             84) ShelveLoc: Bad 11   6.702 No ( 0.90909 0.09091 ) *\n             85) ShelveLoc: Medium 40  52.930 Yes ( 0.37500 0.62500 )  \n              170) Price < 109.5 16   7.481 Yes ( 0.06250 0.93750 ) *\n              171) Price > 109.5 24  32.600 No ( 0.58333 0.41667 )  \n                342) Age < 49.5 13  16.050 Yes ( 0.30769 0.69231 ) *\n                343) Age > 49.5 11   6.702 No ( 0.90909 0.09091 ) *\n           43) Price > 122.5 77  55.540 No ( 0.88312 0.11688 )  \n             86) CompPrice < 147.5 58  17.400 No ( 0.96552 0.03448 ) *\n             87) CompPrice > 147.5 19  25.010 No ( 0.63158 0.36842 )  \n              174) Price < 147 12  16.300 Yes ( 0.41667 0.58333 )  \n                348) CompPrice < 152.5 7   5.742 Yes ( 0.14286 0.85714 ) *\n                349) CompPrice > 152.5 5   5.004 No ( 0.80000 0.20000 ) *\n              175) Price > 147 7   0.000 No ( 1.00000 0.00000 ) *\n       11) Advertising > 13.5 45  61.830 Yes ( 0.44444 0.55556 )  \n         22) Age < 54.5 25  25.020 Yes ( 0.20000 0.80000 )  \n           44) CompPrice < 130.5 14  18.250 Yes ( 0.35714 0.64286 )  \n             88) Income < 100 9  12.370 No ( 0.55556 0.44444 ) *\n             89) Income > 100 5   0.000 Yes ( 0.00000 1.00000 ) *\n           45) CompPrice > 130.5 11   0.000 Yes ( 0.00000 1.00000 ) *\n         23) Age > 54.5 20  22.490 No ( 0.75000 0.25000 )  \n           46) CompPrice < 122.5 10   0.000 No ( 1.00000 0.00000 ) *\n           47) CompPrice > 122.5 10  13.860 No ( 0.50000 0.50000 )  \n             94) Price < 125 5   0.000 Yes ( 0.00000 1.00000 ) *\n             95) Price > 125 5   0.000 No ( 1.00000 0.00000 ) *\n    3) ShelveLoc: Good 85  90.330 Yes ( 0.22353 0.77647 )  \n      6) Price < 135 68  49.260 Yes ( 0.11765 0.88235 )  \n       12) US: No 17  22.070 Yes ( 0.35294 0.64706 )  \n         24) Price < 109 8   0.000 Yes ( 0.00000 1.00000 ) *\n         25) Price > 109 9  11.460 No ( 0.66667 0.33333 ) *\n       13) US: Yes 51  16.880 Yes ( 0.03922 0.96078 ) *\n      7) Price > 135 17  22.070 No ( 0.64706 0.35294 )  \n       14) Income < 46 6   0.000 No ( 1.00000 0.00000 ) *\n       15) Income > 46 11  15.160 Yes ( 0.45455 0.54545 ) *\n```\n\n\n:::\n:::\n\n\nIn order to properly evaluate the performance of a classification tree on these data, we must estimate the test error rather than simply computing the training error. We split the observations into a training set and a test set, build the tree using the training set, and evaluate its performance on the test data. The `predict()` function can be used for this purpose. In the case of a classification tree, the argument `type = \"class\"` instructs `R` to return the actual class prediction. This approach leads to correct predictions for around $77 \\%$ of the locations in the test data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2)\ntrain <- sample(1:nrow(Carseats), 200)\nCarseats.test <- Carseats[-train, ]\nHigh.test <- High[-train]\ntree.carseats <- tree(High ~ . - Sales, Carseats,\n    subset = train)\ntree.pred <- predict(tree.carseats, Carseats.test,\n    type = \"class\")\ntable(tree.pred, High.test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         High.test\ntree.pred  No Yes\n      No  104  33\n      Yes  13  50\n```\n\n\n:::\n\n```{.r .cell-code}\n(104 + 50) / 200\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.77\n```\n\n\n:::\n:::\n\n\n(If you re-run the `predict()` function then you might get slightly different results, due to \"ties\": for instance, this can happen when the training observations corresponding to a terminal node are evenly split between `Yes` and `No` response values.)\n\nNext, we consider whether pruning the tree might lead to improved results. The function `cv.tree()` performs cross-validation in order to determine the optimal level of tree complexity; cost complexity pruning is used in order to select a sequence of trees for consideration. We use the argument `FUN = prune.misclass` in order to indicate that we want the classification error rate to guide the cross-validation and pruning process, rather than the default for the `cv.tree()` function, which is deviance. The `cv.tree()` function reports the number of terminal nodes of each tree considered (`size`) as well as the corresponding error rate and the value of the cost-complexity parameter used (`k`, which corresponds to $\\alpha$ in (8.4)).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(7)\ncv.carseats <- cv.tree(tree.carseats, FUN = prune.misclass)\nnames(cv.carseats)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"size\"   \"dev\"    \"k\"      \"method\"\n```\n\n\n:::\n\n```{.r .cell-code}\ncv.carseats\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$size\n[1] 21 19 14  9  8  5  3  2  1\n\n$dev\n[1] 75 75 75 74 82 83 83 85 82\n\n$k\n[1] -Inf  0.0  1.0  1.4  2.0  3.0  4.0  9.0 18.0\n\n$method\n[1] \"misclass\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n```\n\n\n:::\n:::\n\n\nDespite its name, `dev` corresponds to the number of cross-validation errors. The tree with 9 terminal nodes results in only 74 cross-validation errors. We plot the error rate as a function of both `size` and `k`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndfviz = data.frame(size = cv.carseats$size,\n                   k = cv.carseats$k,\n                   dev = cv.carseats$dev)\ndfviz_long <- dfviz %>%\n  pivot_longer(cols = c(size, k), \n               names_to = \"Variable\", \n               values_to = \"Value\")\n\ndfviz_long %>%\n  ggplot(aes(x = Value, y = dev)) +\n  geom_line() +\n  geom_point() +\n  facet_wrap(~ Variable, scales = \"free_x\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](09_Trees_files/figure-html/chunk10-1.png){width=672}\n:::\n:::\n\n\nWe now apply the `prune.misclass()` function in order to prune the tree to obtain the nine-node tree.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprune.carseats <- prune.misclass(tree.carseats, best = 9)\nplot(prune.carseats)\ntext(prune.carseats, pretty = 0)\n```\n\n::: {.cell-output-display}\n![](09_Trees_files/figure-html/chunk11-1.png){width=672}\n:::\n:::\n\n\nHow well does this pruned tree perform on the test data set? Once again, we apply the `predict()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntree.pred <- predict(prune.carseats, Carseats.test,\n    type = \"class\")\ntable(tree.pred, High.test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         High.test\ntree.pred No Yes\n      No  97  25\n      Yes 20  58\n```\n\n\n:::\n\n```{.r .cell-code}\n(97 + 58) / 200\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.775\n```\n\n\n:::\n:::\n\n\nNow $77.5 \\%$ of the test observations are correctly classified, so not only has the pruning process produced a more interpretable tree, but it has also slightly improved the classification accuracy.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCarseats %>%\n  ggplot(aes(x = ShelveLoc, y = Price, color = High)) +\n  geom_point() +\n  theme(legend.position = \"top\") +\n  geom_segment(aes(x = 0, xend = 4, y = 96.5, yend = 96.5), \n               linetype = \"dashed\", color = \"purple\", size = 1) +\n    geom_segment(aes(x = 2.5, xend = 2.5, y = 96.5, yend = 200), \n               linetype = \"dashed\", color = \"purple\", size = 1) \n```\n\n::: {.cell-output-display}\n![](09_Trees_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nIf we increase the value of `best`, we obtain a larger pruned tree with lower classification accuracy:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprune.carseats <- prune.misclass(tree.carseats, best = 14)\nplot(prune.carseats)\ntext(prune.carseats, pretty = 0)\n```\n\n::: {.cell-output-display}\n![](09_Trees_files/figure-html/chunk13-1.png){width=672}\n:::\n\n```{.r .cell-code}\ntree.pred <- predict(prune.carseats, Carseats.test,\n    type = \"class\")\ntable(tree.pred, High.test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         High.test\ntree.pred  No Yes\n      No  102  31\n      Yes  15  52\n```\n\n\n:::\n\n```{.r .cell-code}\n(102 + 52) / 200\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.77\n```\n\n\n:::\n:::\n\n\n## Fitting Regression Trees\n\nHere we fit a regression tree to the `Boston` data set. First, we create a training set, and fit the tree to the training data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\ntrain <- sample(1:nrow(Boston), nrow(Boston) / 2)\ntree.boston <- tree(medv ~ ., Boston, subset = train)\nsummary(tree.boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRegression tree:\ntree(formula = medv ~ ., data = Boston, subset = train)\nVariables actually used in tree construction:\n[1] \"rm\"    \"lstat\" \"crim\"  \"age\"  \nNumber of terminal nodes:  7 \nResidual mean deviance:  10.38 = 2555 / 246 \nDistribution of residuals:\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-10.1800  -1.7770  -0.1775   0.0000   1.9230  16.5800 \n```\n\n\n:::\n:::\n\n\nNotice that the output of `summary()` indicates that only four of the variables have been used in constructing the tree. In the context of a regression tree, the deviance is simply the sum of squared errors for the tree. We now plot the tree.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(tree.boston)\ntext(tree.boston, pretty = 0)\n```\n\n::: {.cell-output-display}\n![](09_Trees_files/figure-html/chunk15-1.png){width=672}\n:::\n:::\n\n\nThe variable `lstat` measures the percentage of individuals with {lower socioeconomic status}, while the variable `rm` corresponds to the average number of rooms. The tree indicates that larger values of `rm`, or lower values of `lstat`, correspond to more expensive houses. For example, the tree predicts a median house price of \\$$45{,}400$ for homes in census tracts in which `rm >= 7.553`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBoston %>%\n  ggplot(aes(x = rm, y = medv)) +geom_point() +\n  geom_segment(aes(x = 6.959, xend = 6.959, y = 0, yend = 60), \n               linetype = \"dashed\", color = \"purple\", size = 1) +\n    geom_segment(aes(x = 7.553, xend = 7.553, y = 0, yend = 60), \n               linetype = \"dashed\", color = \"purple\", size = 1) +\n    geom_segment(aes(x = 6.959, xend = 7.553, y = 33.42, yend = 33.42), \n               linetype = \"dashed\", color = \"darkorange\", size = 1.5) +\n    geom_segment(aes(x = 7.553, xend = 9, y = 45.38, yend = 45.38), \n               linetype = \"dashed\", color = \"darkorange\", size = 1.5) +\n    theme_minimal() +\n    labs(title  = \"Right side of regression tree\") +\n    theme(plot.title = element_text(hjust = 1))\n```\n\n::: {.cell-output-display}\n![](09_Trees_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nNow we use the `cv.tree()` function to see whether pruning the tree will improve performance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv.boston <- cv.tree(tree.boston)\ndf_viz_cv = data.frame(size = cv.boston[1], dev = cv.boston[2])\ndf_viz_cv %>%\n  ggplot(aes(x = size, y = dev)) +\n  geom_point(size = 1) +\n  geom_line() +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](09_Trees_files/figure-html/chunk16-1.png){width=672}\n:::\n:::\n\n\nIn this case, the most complex tree under consideration is selected by cross-validation. However, if we wish to prune the tree, we could do so as follows, using the `prune.tree()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprune.boston <- prune.tree(tree.boston, best = 5)\nplot(prune.boston)\ntext(prune.boston, pretty = 0)\n```\n\n::: {.cell-output-display}\n![](09_Trees_files/figure-html/chunk17-1.png){width=672}\n:::\n:::\n\n\nIn keeping with the cross-validation results, we use the unpruned tree to make predictions on the test set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyhat <- predict(tree.boston, newdata = Boston[-train, ])\nboston.test <- Boston[-train, \"medv\"]\nplot(yhat, boston.test)\nabline(0, 1)\n```\n\n::: {.cell-output-display}\n![](09_Trees_files/figure-html/chunk18-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmean((yhat - boston.test)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 35.28688\n```\n\n\n:::\n:::\n\n\nIn other words, the test set MSE associated with the regression tree is $35.29$. The square root of the MSE is therefore around $5.941$, indicating that this model leads to test predictions that are (on average) within approximately $5.941$ of the true median home value for the census tract.\n\n## Bagging and Random Forests\n\nHere we apply bagging and random forests to the `Boston` data, using the `randomForest` package in `R`. The exact results obtained in this section may depend on the version of `R` and the version of the `randomForest` package installed on your computer. Recall that bagging is simply a special case of a random forest with $m=p$. Therefore, the `randomForest()` function can be used to perform both random forests and bagging. We perform bagging as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(randomForest)\nset.seed(1)\nbag.boston <- randomForest(medv ~ ., data = Boston,\n    subset = train, mtry = 12, importance = TRUE)\nbag.boston\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(formula = medv ~ ., data = Boston, mtry = 12, importance = TRUE,      subset = train) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 12\n\n          Mean of squared residuals: 11.40162\n                    % Var explained: 85.17\n```\n\n\n:::\n:::\n\n\nThe argument `mtry = 12` indicates that all $12$ predictors should be considered for each split of the tree---in other words, that bagging should be done. How well does this bagged model perform on the test set?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyhat.bag <- predict(bag.boston, newdata = Boston[-train, ])\nplot(yhat.bag, boston.test)\nabline(0, 1)\n```\n\n::: {.cell-output-display}\n![](09_Trees_files/figure-html/chunk20-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmean((yhat.bag - boston.test)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 23.41916\n```\n\n\n:::\n:::\n\n\nThe test set MSE associated with the bagged regression tree is $23.42$, about two-thirds of that obtained using an optimally-pruned single tree. We could change the number of trees grown by `randomForest()` using the `ntree` argument:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbag.boston25 <- randomForest(medv ~ ., data = Boston, subset = train, mtry = 12, ntree = 25)\nyhat.bag25 <- predict(bag.boston25, newdata = Boston[-train, ])\nmean((yhat.bag25 - boston.test)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 25.75055\n```\n\n\n:::\n:::\n\n\nGrowing a random forest proceeds in exactly the same way, except that we use a smaller value of the `mtry` argument. By default, `randomForest()` uses $p/3$ variables when building a random forest of regression trees, and $\\sqrt{p}$ variables when building a random forest of classification trees. Here we use `mtry = 6`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nrf.boston <- randomForest(medv ~ ., data = Boston,\n    subset = train, mtry = 6, importance = TRUE)\nyhat.rf <- predict(rf.boston, newdata = Boston[-train, ])\nmean((yhat.rf - boston.test)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 20.06644\n```\n\n\n:::\n:::\n\n\nThe test set MSE is $20.07$; this indicates that random forests yielded an improvement over bagging in this case.\n\nUsing the `importance()` function, we can view the importance of each variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimportance(rf.boston)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          %IncMSE IncNodePurity\ncrim    19.435587    1070.42307\nzn       3.091630      82.19257\nindus    6.140529     590.09536\nchas     1.370310      36.70356\nnox     13.263466     859.97091\nrm      35.094741    8270.33906\nage     15.144821     634.31220\ndis      9.163776     684.87953\nrad      4.793720      83.18719\ntax      4.410714     292.20949\nptratio  8.612780     902.20190\nlstat   28.725343    5813.04833\n```\n\n\n:::\n:::\n\n\nTwo measures of variable importance are reported. The first is based upon the mean decrease of accuracy in predictions on the out of bag samples when a given variable is permuted. The second is a measure of the total decrease in node impurity that results from splits over that variable, averaged over all trees (this was plotted in Figure 8.9). In the case of regression trees, the node impurity is measured by the training RSS, and for classification trees by the deviance. Plots of these importance measures can be produced using the `varImpPlot()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvarImpPlot(rf.boston)\n```\n\n::: {.cell-output-display}\n![](09_Trees_files/figure-html/chunk24-1.png){width=672}\n:::\n:::\n\n\nThe results indicate that across all of the trees considered in the random forest, the wealth of the community (`lstat`) and the house size (`rm`) are by far the two most important variables.\n\nIf we want to limit the number of fitted trees, we consult the OutofBag-error, stored as $mse$ in our randomForest object. We observe that the error stabilizes after a while and additional trees bring little to no benefit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndfviz_B = data.frame(ntrees = rep(seq(1, length(rf.boston$mse)),2),\n                     mse = c(rf.boston$mse, bag.boston$mse),\n                     model = c(rep(\"rf\", 500), rep(\"bag\", 500)) )\n\ndfviz_B %>%\n  ggplot(aes(x = ntrees, y = mse, color = model)) +\n  geom_line(size = 1) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](09_Trees_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n## Boosting\n\nThe ISLR2 book uses the $gbm$ library which is being deprecated (can still be used but authors recommend switching). We will use the authors newer $gbm3$ library.\n\nTo install run the following chunk. This will take a couple of minutes.\\\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(\"remotes\")\n#remotes::install_github(\"gbm-developers/gbm3\", build_vignettes = TRUE, force = TRUE)\n```\n:::\n\n\nHere we use the `gbm3` package, and within it the `gbm()` function, to fit boosted regression trees to the `Boston` data set. We run `gbm()` with the option `distribution = \"gaussian\"` since this is a regression problem; if it were a binary classification problem, we would use `distribution = \"bernoulli\"`. The argument `n.trees = 5000` indicates that we want $5000$ trees, and the option `interaction.depth = 4` limits the depth of each tree.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gbm3)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in library(gbm3): there is no package called 'gbm3'\n```\n\n\n:::\n\n```{.r .cell-code}\nset.seed(0)\nboost.boston <- gbm(medv ~ ., data = Boston[train, ],\n    distribution = \"gaussian\", n.trees = 8000,\n    interaction.depth = 4)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in gbm(medv ~ ., data = Boston[train, ], distribution = \"gaussian\", : could not find function \"gbm\"\n```\n\n\n:::\n:::\n\n\nThe `summary()` function produces a relative influence plot and also outputs the relative influence statistics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(boost.boston)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'boost.boston' not found\n```\n\n\n:::\n:::\n\n\nWe see that `lstat` and `rm` are by far the most important variables. We can also produce *partial dependence plots* for these two variables. These plots illustrate the marginal effect of the selected variables on the response after *integrating* out the other variables. In this case, as we might expect, median house prices are increasing with `rm` and decreasing with `lstat`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(boost.boston, var_index = c(\"rm\", \"lstat\"))\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'boost.boston' not found\n```\n\n\n:::\n:::\n\n\nWe now use the boosted model to predict `medv` on the test set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyhat.boost <- predict(boost.boston, newdata = Boston[-train, ], n.trees = 8000)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'boost.boston' not found\n```\n\n\n:::\n\n```{.r .cell-code}\nmean((yhat.boost - boston.test)^2)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'yhat.boost' not found\n```\n\n\n:::\n:::\n\n\nThe test MSE obtained is $19.921$: this is superior to the test MSE of random forests and bagging. If we want to, we can perform boosting with a different value of the shrinkage parameter $\\lambda$ in (8.10). The default value is $0.001$, but this is easily modified. Here we take $\\lambda=0.2$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboost.boston_lam <- gbm(medv ~ ., data = Boston[train, ],\n                    distribution = \"gaussian\",\n                    n.trees = 8000,\n                    interaction.depth = 4,\n                    shrinkage = 0.2,\n                    verbose = F)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in gbm(medv ~ ., data = Boston[train, ], distribution = \"gaussian\", : could not find function \"gbm\"\n```\n\n\n:::\n\n```{.r .cell-code}\nyhat.boost_lam <- predict(boost.boston_lam,\n                      newdata = Boston[-train, ],\n                      n.trees = 8000)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'boost.boston_lam' not found\n```\n\n\n:::\n\n```{.r .cell-code}\nmean((yhat.boost_lam - boston.test)^2)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'yhat.boost_lam' not found\n```\n\n\n:::\n:::\n\n\nIn this case, using $\\lambda=0.2$ leads to a lower test MSE than $\\lambda=0.001$.\n\nTo understand why this happens we can look at the training performance of our model. Remember that a boosted model learns iteratively with each new tree. As we increase the learning rate, the model converges faster and displays a lower training error earlier.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_viz_boost = data.frame(errors = c(boost.boston$train.error, boost.boston_lam$train.error),\n                          trees = c(rep(seq(1,8000),2)),\n                          l2 = c(rep(\"l2 = 0.001\", 8000), rep(\"l2 = 0.2\", 8000)))\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'boost.boston' not found\n```\n\n\n:::\n\n```{.r .cell-code}\ndf_viz_boost %>%\n  ggplot(aes(x = trees, y = errors, color = l2)) + geom_point() #+ coord_cartesian(ylim = c(0,5))\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: object 'df_viz_boost' not found\n```\n\n\n:::\n:::\n",
    "supporting": [
      "09_Trees_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}