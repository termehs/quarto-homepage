{
  "hash": "d906ed75b1e77a073b390825ee0c0904",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Cross Validation and Model Selection\"\nauthor: \"Termeh Shafie\"\nformat: html\neditor: visual\nexecute:\n  cache:  true\n---\n\n\n\n# Part I: Iris Data\n\n   - `iris` data is a built-in data set in R that contains measurements for 50 flowers in 3 different species and 4 different attributes.\n  - `caret` package is short for Classification And REgression Training. This is a useful tool for data splitting, pre-processing, feature selection and model tuning. In this simple example I will use this package to illustrate cross-validation methods.\n  - `dplyr` package is a commonly used tool for data manipulation.\n  - `tidyverse`  package is for data manipulation and visualization (with `dplyr` included).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\nlibrary(tidyverse)\n# Load data\ndata(iris)\n\n# Take a look at data \nstr(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n```\n\n\n:::\n:::\n\n\n## Model Performance Metrics\nTo determine whether the designed model is performing well, we need to use the observations that are not being used during the training of the model. Therefore the test set will serve as the unseen data, then the values of the dependent variables are predicted and model accuracy will be evaluated based on the difference between actual values and predicted values of the dependent variable. We use following model performance metrics (consult lecture slides for more information):\n\n  - $R^2$\n  - Rooted Mean Squared Error (RMSE)\n  - Mean Absolute Error (MAE)\n \n \n## Procedure for each CV method applied\nEach methods below will be conducted in four steps:\n\n  - **Data splitting**: split the data set into different subsets.\n  - **Training**: build the model on the training data set.\n  - **Testing**: apply the resultant model to the unseen data (testing data set) to predict the outcome of new observations.\n  - **Evaluating**: calculate prediction error using the model performance metrics.\n\n## Validation Set Approach\nIn this approach, the available data is divided into two subsets: a training set and a validation set. The training set is used to train the model, and the validation set is used to evaluate its performance. Predictions done by this method could be largely affected by the subset of observations used in testing set. If the test set is not representative of the entire data, this method may lead to overfitting. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Data splitting\n\n# set seed to generate a reproducible random sample\nset.seed(123)\n\n# create training and testing data set using index, training data contains 80% of the data set\n# 'list = FALSE' allows us to create a matrix data structure with the indices of the observations in the subsets along the rows.\ntrain.index.vsa <- createDataPartition(iris$Species, p= 0.8, list = FALSE)\ntrain.vsa <- iris[train.index.vsa,]\ntest.vsa <- iris[-train.index.vsa,]\n\n# see how the the subsets are randomized\nrole = rep('train',nrow(iris))\nrole[-train.index.vsa] = 'test'\nggplot(data = cbind(iris,role)) + \n  geom_point(aes(x = Sepal.Length,\n                 y = Petal.Width,\n                 color = role)) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](06-model-validation-1_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n### Training: linear model is fit using all availbale predictors\nmodel.vsa <- lm(Petal.Width ~., data = train.vsa)\n\n\n### Testing\npredictions.vsa <- model.vsa %>% predict(test.vsa)\n\n\n### Evaluating\ndata.frame(RMSE = RMSE(predictions.vsa, test.vsa$Petal.Width),\n           R2 = R2(predictions.vsa, test.vsa$Petal.Width),\n           MAE = MAE(predictions.vsa, test.vsa$Petal.Width))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       RMSE        R2      MAE\n1 0.1675093 0.9497864 0.128837\n```\n\n\n:::\n:::\n\n\n## Leave-One-Out Cross-Validation: LOOCV\n\n::: {.cell}\n\n```{.r .cell-code}\n### Data splitting: leave one out\ntrain.loocv <- trainControl(method = \"LOOCV\")\n\n### Training\nmodel.loocv <- train(Petal.Width ~.,\n                     data = iris,\n                     method = \"lm\",\n                     trControl = train.loocv)\n\n### Present results\nprint(model.loocv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression \n\n150 samples\n  4 predictor\n\nNo pre-processing\nResampling: Leave-One-Out Cross-Validation \nSummary of sample sizes: 149, 149, 149, 149, 149, 149, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.1705606  0.9496003  0.1268164\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n```\n\n\n:::\n:::\n\n## K-Fold Cross Validation\n\n::: {.cell}\n\n```{.r .cell-code}\n### Data splitting\n\n# set seed to generate a reproducible random sample\nset.seed(123)\n# the number of K is set to be 5\ntrain.kfold <- trainControl(method = \"cv\", number = 5)\n\n### Training\nmodel.kfold <- train(Petal.Width ~.,\n                     data = iris,\n                     method = \"lm\",\n                     trControl = train.kfold)\n\n### Present results\nprint(model.kfold)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression \n\n150 samples\n  4 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 122, 120, 118, 121, 119 \nResampling results:\n\n  RMSE       Rsquared   MAE    \n  0.1704321  0.9514251  0.12891\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n```\n\n\n:::\n:::\n\n\n## Repeated K-Fold Cross Validation\n\n### Data splitting\n\n::: {.cell}\n\n```{.r .cell-code}\n# set seed to generate a reproducible random sample\nset.seed(123)\n# the number of K is set to be 5\ntrain.rkfold <- trainControl(method = \"repeatedcv\", number = 5, repeats = 3)\n\n### Training\nmodel.rkfold <- train(Petal.Width ~.,\n                     data = iris,\n                     method = \"lm\",\n                     trControl = train.rkfold)\n\n### Present results\nprint(model.rkfold)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression \n\n150 samples\n  4 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold, repeated 3 times) \nSummary of sample sizes: 122, 120, 118, 121, 119, 119, ... \nResampling results:\n\n  RMSE      Rsquared   MAE      \n  0.168445  0.9525634  0.1266377\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(model.kfold)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression \n\n150 samples\n  4 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 122, 120, 118, 121, 119 \nResampling results:\n\n  RMSE       Rsquared   MAE    \n  0.1704321  0.9514251  0.12891\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n```\n\n\n:::\n:::\n\n\n## Let's summarize the results\n\n| CV method| RMSE | R2 | MAE |\n|---------|:-----|------:|:------:|\n| Validation Set   |  0.1675| 0.9498| 0.1288|\n| LOOCV    | 0.1706 | 0.9496 | 0.1268|\n| K-Fold       |   0.1704  |  0.9514  |  0.1289 | \n| K-Fold repeat   |   0.1704 | 0.9514 | 0.1289|\n\nWhat do you note?\n\n\n# Part II: Simulation\n\n## KNN\n\nRecall our `KNN` classifier from earlier where the following function was used:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nKNN = function(x0, x, y, K) {\n    distances = abs(x - x0)  # Euclidean distance between x0 and each x_i\n    o = order(distances)  # order of the training points by distance from x0 (nearest to farthest)\n    y0_hat = mean(y[o[1:K]])  # take average of the y values of the K nearest training points\n    return(y0_hat)  # return predicted value of y\n}\n```\n:::\n\n\nwhere:\n\n-   $x_0$ as the new point at which we wish to predict $y$\n-   ${\\bf x} = (x_1,x_2, \\dots, x_n)$ as the vector of training $x$'s\n-   ${\\bf y} = (y_1,y_2, \\dots, y_n)$ as the vector of training $y$'s\n-   $K$ as number of neighbors to use\n-   $\\hat{y}_0$ as the predicted value of $y$ at $x_0$\n\n## Simulate data\n\nWe also simulate training data as before and plot it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)  \nn = 100 \nx = 5*runif(n)\nsigma = 0.3  \nf = function(x) { cos(x) }  \ny = f(x) + sigma*rnorm(n)  \nplot(x,y,col=2,pch=20,cex=2)  # plot training data\n```\n\n::: {.cell-output-display}\n![](06-model-validation-1_files/figure-html/unnamed-chunk-8-1.png){width=864}\n:::\n:::\n\n\n## K-Fold Cross Validation\n\nHere we are going to use cross-validation to estimate test performance of the KNN classifier. We set number of neighbors as $K=1$ and use the 10-fold cross validation. We do a random ordering of all the available data, and initialize a vector to hold MSE for each fold. For each fold, we then create a training and test (hold one out/validation) set, run KNN at each $x$ in this test set (the one left out), and compute MSE on this test set. Then we average the MSE over all folds to obtain the CV estimate of test MSE:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nK = 1  \nnfolds = 10 \npermutation = sample(1:n)  \nMSE_fold = rep(0,nfolds)  \nfor (j in 1:nfolds) {\n    test = permutation[floor((j-1)*n/nfolds+1) : floor(j*n/nfolds)]  \n    train = setdiff(1:n, test)  \n    y_hat = sapply(x[test], function(x0) { KNN(x0, x[train], y[train], K) }) \n    MSE_fold[j] = mean((y[test] - y_hat)^2) \n}\nMSE_cv = mean(MSE_fold)  \nMSE_cv\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1630092\n```\n\n\n:::\n:::\n\n\nNext we compare with the ground truth estimate of test performance, given this training set. Because this is a simulation example, we can generate lots of test data. We simulate $x$'s and $y$'s from the true data generating process. Then we run the KNN classifier at each $x$ in the test set and compute the MSE on the test set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_test = 100000\nx_test = 5*runif(n_test)  \ny_test = f(x_test) + sigma*rnorm(n_test)  \ny_test_hat = sapply(x_test, function(x0) { KNN(x0, x, y, K) })  \nMSE_test = mean((y_test - y_test_hat)^2)  \n```\n:::\n\n\nLet's compare the two values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMSE_test\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1682957\n```\n\n\n:::\n\n```{.r .cell-code}\nMSE_cv\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1630092\n```\n\n\n:::\n:::\n\n\nBe careful when calculating the *root* MSE (RMSE) since it corresponds to root mean squared error or square root of MSE: Let's try with\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt(MSE_test)  # test RMSE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4102386\n```\n\n\n:::\n\n```{.r .cell-code}\nsqrt(mean(MSE_fold))  # sqrt of MSE_cv\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.403744\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(sqrt(MSE_fold))  # can we use this?\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3970696\n```\n\n\n:::\n:::\n\n\n## Leave-One Out Cross-Validation (LOOCV)\nUse the leave-one out cross-validation (LOOCV) in the above example and report the CV estimate of test MSE and the MSE given ground truth.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nK = 1  \nnfolds = n\npermutation = sample(1:n)  \nMSE_fold = rep(0,nfolds)  \nfor (j in 1:nfolds) {\n    test = permutation[floor((j-1)*n/nfolds+1) : floor(j*n/nfolds)]  \n    train = setdiff(1:n, test)  \n    y_hat = sapply(x[test], function(x0) { KNN(x0, x[train], y[train], K) }) \n    MSE_fold[j] = mean((y[test] - y_hat)^2) \n}\nMSE_loocv = mean(MSE_fold)  \nMSE_test = mean((y_test - y_test_hat)^2)  \nMSE_loocv\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1682862\n```\n\n\n:::\n\n```{.r .cell-code}\nMSE_test\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1682957\n```\n\n\n:::\n:::\n\n\n\n## Hyperparameter Tuning: Choosing Model Settings\n\nWith the following example, we will illustrate how to use cross validation to choose the optimal number of neighbors $K$ in KNN. We start with a rather high number of $K$ to try for KNN ($K=30$) and use 10 folds for each of these cases in the cross validation. Then we do a random ordering of data and initialize vector for holding MSE's. For each number of folds in the range, we compute the training and test set as before (this is again the validation set). For each $K$ up to 30, we then run KNN at each $x$ in the test set (the one left out), and compute MSE on the this test set. We average across folds to obtain CV estimate of test MSE for each $K$ and plot the results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nK_max = 30 \nnfolds = 10  \npermutation = sample(1:n)  \nMSE_fold = matrix(0,nfolds,K_max)  \nfor (j in 1:nfolds) {\n    test = permutation[floor((j-1)*n/nfolds+1) : floor(j*n/nfolds)]  \n    train = setdiff(1:n, test) \n    for (K in 1:K_max) {\n        y_hat = sapply(x[test], function(x0) { KNN(x0, x[train], y[train], K) })\n        MSE_fold[j,K] = mean((y[test] - y_hat)^2)  \n    }\n}\nMSE_cv = colMeans(MSE_fold)  \n```\n:::\n\n\nWe plot CV estimate of test MSE against number of neighbors $K=1,2,\\dots,30$, and choose the value of $K$ that minimizes estimated test MSE. Compare with a ground truth estimate of test performance by using the chosen number of $K$ and running KNN on each $x$ in the test set (denoted `x_test` above). Why do you think the test performance estimate for the chosen $K$ tend to be smaller than the ground truth estimate of test performance in this example?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(1:K_max, MSE_cv, pch=19)  # plot CV estimate of test MSE for each K\n```\n\n::: {.cell-output-display}\n![](06-model-validation-1_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Choose the value of K that minimizes estimated test MSE\nK_cv = which.min(MSE_cv)\nK_cv\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 12\n```\n\n\n:::\n:::\n\n\nAnswer: MSE_cv\\[K_cv\\] may systematically underestimate or overestimate test MSE! There are two sources of bias: K_cv is the minimum, and the pseudo-training set is smaller than $n$.\n\n\n## Choosing the number of folds\n\nWe start by simulating training data as before:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1) \nn = 20\nx = 5*runif(n)  \nsigma = 0.3 \ny = f(x) + sigma*rnorm(n)  \n```\n:::\n\n\nWe then compute \"ground truth\" estimate of test performance, given this training set. We set $K=10$, and run KNN at each $x$ in the test set and compute MSE on the test set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nK = 10\ny_test_hat = sapply(x_test, function(x0) { KNN(x0, x, y, K) })  \nMSE_test = mean((y_test - y_test_hat)^2)  \n```\n:::\n\n\nNext, we repeatedly run CV for a range of number of folds `nfolds` up to maximum $n=20$ (same as $n$ above in our simulated data). We repeat the simulation 200 times, and for each repetition and number of folds, we split the training data into training and test (hold one out/validation set). We run KNN at each $x$ in this test set and compute MSE. We then average the MSE's for each case with a different number of folds:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnfolds_max = n  # maximum value of nfolds to use for CV\nnreps = 200  # number of times to repeat the simulation\nMSE_cv = matrix(0,nreps,nfolds_max)  \nfor (r in 1:nreps) {  \n    for (nfolds in 1:nfolds_max) {\n        permutation = sample(1:n) \n        MSE_fold = rep(0,nfolds)  \n        for (j in 1:nfolds) {\n            test = permutation[floor((j-1)*n/nfolds+1) : floor(j*n/nfolds)]  \n            train = setdiff(1:n, test)  \n            y_hat = sapply(x[test], function(x0) { KNN(x0, x[train], y[train], K) }) \n            MSE_fold[j] = mean((y[test] - y_hat)^2) \n        }\n        MSE_cv[r,nfolds] = mean(MSE_fold)\n    }\n}\n```\n:::\n\n\nWe compute the MSE, bias, and variance of the CV estimate of test MSE, for each value of nfolds and plot MSE, bias\\^2, and variance of the CV estimate, for each value of nfolds.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmse = colMeans((MSE_cv - MSE_test)^2)\nbias = colMeans(MSE_cv) - MSE_test\nvariance = apply(MSE_cv,2,var)\n\n# plot of MSE, bias^2 and variance against number of folds\nplot(1:nfolds_max, type=\"n\", ylim=c(0,max(mse[2:nfolds_max])*1.1), xlab=\"nfolds\", ylab=\"mse\", main=\"MSE of the CV estimates\")\nlines(1:nfolds_max, mse, col=1, lty=2, lwd=2, ylim=c(0,0.2))\nlines(1:nfolds_max, bias^2, col=2, lwd=2)\nlines(1:nfolds_max, variance, col=4, lwd=2)\nlegend(\"topright\", legend=c(\"mse\",\"bias^2\",\"variance\"), col=c(1,2,4), lwd=2)\n```\n\n::: {.cell-output-display}\n![](06-model-validation-1_files/figure-html/unnamed-chunk-19-1.png){width=864}\n:::\n\n```{.r .cell-code}\n# plot bias against number of folds\nplot(1:nfolds_max, bias)\nlines(1:nfolds_max, bias, col=2, lwd=2)\n```\n\n::: {.cell-output-display}\n![](06-model-validation-1_files/figure-html/unnamed-chunk-19-2.png){width=864}\n:::\n:::\n\n\n In the following plot below, why do you think  the bias of the CV estimate of test MSE is always positive?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot bias against number of folds\nplot(1:nfolds_max, bias)\nlines(1:nfolds_max, bias, col=2, lwd=2)\n```\n\n::: {.cell-output-display}\n![](06-model-validation-1_files/figure-html/unnamed-chunk-20-1.png){width=864}\n:::\n:::\n\n\n\nAnswer: Because the the \"pseudo\"-training set (each fold) is smaller than the training set.\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}