{
  "hash": "824a1a117c9fc8d5d81c3b4dc86a8d67",
  "result": {
    "engine": "knitr",
    "markdown": "---\neditor: \n  markdown: \n    wrap: sentence\n---\n\n::: {.cell}\n\n:::\n\n\n# Classification Methods II\n\n## Caravan data and KNN\n\nTo continue our dive into classification methods we will take a loook at the Insurance dataset stored as \"Caravan\".\n\nThis data set includes 85 predictors that measure demographic characteristics for 5,822 individuals.\nThe response variable is Purchase, which indicates whether or not a given individual purchases a caravan insurance policy.\nIn this data set, only 6% of people purchased caravan insurance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ISLR2)\nlibrary(class)\nattach(Caravan)\ndim(Caravan)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5822   86\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(Caravan$Purchase)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  No  Yes \n5474  348 \n```\n\n\n:::\n\n```{.r .cell-code}\n348 / 5474\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.06357326\n```\n\n\n:::\n:::\n\n\nBecause the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters.\nVariables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale.\nFor instance, imagine a data set that contains two variables, salary and age (measured in dollars and years, respectively).\nAs far as KNN is concerned, a difference of \\$1,000 in salary is enormous compared to a difference of 50 years in age.\nConsequently, salary will drive the KNN classification results, and age will have almost no effect.\nThis is contrary to our intuition that a salary difference of \\$1,000 is quite small compared to an age difference of 50 years.\nFurthermore, the importance of scale to the KNN classifier leads to another issue: if we measured salary in Japanese yen, or if we measured age in minutes, then weâ€™d get quite different classification results from what we get if these two variables are measured in dollars and years.\n\nA good way to handle this problem is to standardize the data so that all variables are given a mean of zero and a standard deviation of one.\nThen all variables will be on a comparable scale.\nThe scale() function does just this.\nIn standardizing the data, we exclude column 86, because that is the qualitative Purchase variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstandardized.X <- scale(Caravan[, -86])\ncat(\"Unstandardized:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnstandardized:\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(Caravan[, 1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 165.0378\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(Caravan[, 2])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1647078\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Standardized:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStandardized:\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(standardized.X[, 1])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(standardized.X[, 2])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n:::\n\n\nNow every column of standardized.X has a standard deviation of one and a mean of zero.\n\nWe now split the observations into a test set, containing the first 1,000 observations, and a training set, containing the remaining observations.\nWe fit a KNN model on the training data using K=1, and evaluate its performance on the test data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- 1:1000\ntrain.X <- standardized.X[-test, ]\ntest.X <- standardized.X[test, ]\ntrain.Y <- Purchase[-test]\ntest.Y <- Purchase[test]\nset.seed(1)\nknn.pred <- knn(train.X, test.X, train.Y, k = 1)\nmean(test.Y != knn.pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.118\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(knn.pred, test.Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        test.Y\nknn.pred  No Yes\n     No  873  50\n     Yes  68   9\n```\n\n\n:::\n\n```{.r .cell-code}\n9 / (68 + 9)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1168831\n```\n\n\n:::\n:::\n\n\nThis looks like a nice result - we have high accuracy.\nBut do we care about how many people are not buying insurance in this example?\n\n### Exercise:\n\nTune your hyperparameter k and compare to our previous result.\nAlso Compare to random guessing (50:50)\n\n\n::: {.cell}\n\n:::\n\n\n## LDA with the stocks data\n\nNow we will perform LDA on the `Smarket` data.\nIn `R`, we fit an LDA model using the `lda()` function, which is part of the `MASS` library.\nNotice that the syntax for the `lda()` function is identical to that of `lm()`, and to that of `glm()` except for the absence of the `family` option.\nWe fit the model using only the observations before 2005.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nattach(Smarket)\ntrain <- (Year < 2005)\nSmarket.2005 <- Smarket[!train, ]\ndim(Smarket.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 252   9\n```\n\n\n:::\n\n```{.r .cell-code}\nDirection.2005 <- Direction[!train]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'MASS'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:ISLR2':\n\n    Boston\n```\n\n\n:::\n\n```{.r .cell-code}\nlda.fit <- lda(Direction ~ Lag1 + Lag2, data = Smarket,\n    subset = train)\nlda.fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nlda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\nCoefficients of linear discriminants:\n            LD1\nLag1 -0.6420190\nLag2 -0.5135293\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(lda.fit)\n```\n\n::: {.cell-output-display}\n![](05_Class-II_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThe LDA output indicates that $\\hat\\pi_1=0.492$ and $\\hat\\pi_2=0.508$; in other words, $49.2$ % of the training observations correspond to days during which the market went down.\nIt also provides the group means; these are the average of each predictor within each class, and are used by LDA as estimates of $\\mu_k$.\nThese suggest that there is a tendency for the previous 2\\~days' returns to be negative on days when the market increases, and a tendency for the previous days' returns to be positive on days when the market declines.\nThe *coefficients of linear discriminants* output provides the linear combination of `lagone` and `lagtwo` that are used to form the LDA decision rule.\nIn other words, these are the multipliers of the elements of $X=x$ in (4.24).\nIf \\$-0.642 \\times $`lagone`$ - 0.514 \\times \\$`lagtwo` is large, then the LDA classifier will predict a market increase, and if it is small, then the LDA classifier will predict a market decline.\n\nThe `plot()` function produces plots of the *linear discriminants*, obtained by computing \\$-0.642 \\times $`lagone`$ - 0.514 \\times \\$`lagtwo` for each of the training observations.\nThe `Up` and `Down` observations are displayed separately.\n\nThe `predict()` function returns a list with three elements.\nThe first element, `class`, contains LDA's predictions about the movement of the market.\nThe second element, `posterior`, is a matrix whose $k$th column contains the posterior probability that the corresponding observation belongs to the $k$th class, computed from (4.15).\nFinally, `x` contains the linear discriminants, described earlier.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlda.pred <- predict(lda.fit, Smarket.2005)\nnames(lda.pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"class\"     \"posterior\" \"x\"        \n```\n\n\n:::\n:::\n\n\nAs we observed in Section 4.5, the LDA and logistic regression predictions are almost identical.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlda.class <- lda.pred$class\ntable(lda.class, Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Direction.2005\nlda.class Down  Up\n     Down   35  35\n     Up     76 106\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(lda.class == Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5595238\n```\n\n\n:::\n:::\n\n\nApplying a $50$ % threshold to the posterior probabilities allows us to recreate the predictions contained in `lda.pred$class`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(lda.pred$posterior[, 1] >= .5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 70\n```\n\n\n:::\n\n```{.r .cell-code}\nsum(lda.pred$posterior[, 1] < .5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 182\n```\n\n\n:::\n:::\n\n\nNotice that the posterior probability output by the model corresponds to the probability that the market will *decrease*:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlda.pred$posterior[1:20, 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      999      1000      1001      1002      1003      1004      1005      1006 \n0.4901792 0.4792185 0.4668185 0.4740011 0.4927877 0.4938562 0.4951016 0.4872861 \n     1007      1008      1009      1010      1011      1012      1013      1014 \n0.4907013 0.4844026 0.4906963 0.5119988 0.4895152 0.4706761 0.4744593 0.4799583 \n     1015      1016      1017      1018 \n0.4935775 0.5030894 0.4978806 0.4886331 \n```\n\n\n:::\n\n```{.r .cell-code}\nlda.class[1:20]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Down Up   Up   Up  \n[16] Up   Up   Down Up   Up  \nLevels: Down Up\n```\n\n\n:::\n:::\n\n\nIf we wanted to use a posterior probability threshold other than $50$ % in order to make predictions, then we could easily do so.\nFor instance, suppose that we wish to predict a market decrease only if we are very certain that the market will indeed decrease on that day---say, if the posterior probability is at least $90$ %.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(lda.pred$posterior[, 1] > .9)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\nNo days in 2005 meet that threshold!\nIn fact, the greatest posterior probability of decrease in all of 2005 was $52.02$\n\n## Quadratic Discriminant Analysis\n\nWe will now fit a QDA model to the `Smarket` data.\nQDA is implemented in `R` using the `qda()` function, which is also part of the `MASS` library.\nThe syntax is identical to that of `lda()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqda.fit <- qda(Direction ~ Lag1 + Lag2, data = Smarket,\n    subset = train)\nqda.fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall:\nqda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n```\n\n\n:::\n:::\n\n\nThe output contains the group means.\nBut it does not contain the coefficients of the linear discriminants, because the QDA classifier involves a quadratic, rather than a linear, function of the predictors.\nThe `predict()` function works in exactly the same fashion as for LDA.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqda.class <- predict(qda.fit, Smarket.2005)$class\ntable(qda.class, Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         Direction.2005\nqda.class Down  Up\n     Down   30  20\n     Up     81 121\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(qda.class == Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5992063\n```\n\n\n:::\n:::\n\n\nInterestingly, the QDA predictions are accurate almost $60$ % of the time, even though the 2005 data was not used to fit the model.\nThis level of accuracy is quite impressive for stock market data, which is known to be quite hard to model accurately.\nThis suggests that the quadratic form assumed by QDA may capture the true relationship more accurately than the linear forms assumed by LDA and logistic regression.\nHowever, we recommend evaluating this method's performance on a larger test set before betting that this approach will consistently beat the market!\n\n## Naive Bayes\n\nNext, we fit a naive Bayes model to the `Smarket` data.\nNaive Bayes is implemented in `R` using the `naiveBayes()` function, which is part of the `e1071` library.\nThe syntax is identical to that of `lda()` and `qda()`.\nBy default, this implementation of the naive Bayes classifier models each quantitative feature using a Gaussian distribution.\nHowever, a kernel density method can also be used to estimate the distributions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"e1071\")\nlibrary(e1071)\nnb.fit <- naiveBayes(Direction ~ Lag1 + Lag2, data = Smarket,\n    subset = train)\nnb.fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n    Down       Up \n0.491984 0.508016 \n\nConditional probabilities:\n      Lag1\nY             [,1]     [,2]\n  Down  0.04279022 1.227446\n  Up   -0.03954635 1.231668\n\n      Lag2\nY             [,1]     [,2]\n  Down  0.03389409 1.239191\n  Up   -0.03132544 1.220765\n```\n\n\n:::\n:::\n\n\nThe output contains the estimated mean and standard deviation for each variable in each class.\nFor example, the mean for `lagone` is $0.0428$ for\n\n`Direction=Down`, and the standard deviation is $1.23$.\nWe can easily verify this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(Lag1[train][Direction[train] == \"Down\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.04279022\n```\n\n\n:::\n\n```{.r .cell-code}\nsd(Lag1[train][Direction[train] == \"Down\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.227446\n```\n\n\n:::\n:::\n\n\nThe `predict()` function is straightforward.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnb.class <- predict(nb.fit, Smarket.2005)\ntable(nb.class, Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        Direction.2005\nnb.class Down  Up\n    Down   28  20\n    Up     83 121\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(nb.class == Direction.2005)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5912698\n```\n\n\n:::\n:::\n\n\nNaive Bayes performs very well on this data, with accurate predictions over $59\\%$ of the time.\nThis is slightly worse than QDA, but much better than LDA.\n\nThe `predict()` function can also generate estimates of the probability that each observation belongs to a particular class.\n%\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnb.preds <- predict(nb.fit, Smarket.2005, type = \"raw\")\nnb.preds[1:5, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Down        Up\n[1,] 0.4873164 0.5126836\n[2,] 0.4762492 0.5237508\n[3,] 0.4653377 0.5346623\n[4,] 0.4748652 0.5251348\n[5,] 0.4901890 0.5098110\n```\n\n\n:::\n:::\n",
    "supporting": [
      "05_Class-II_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}