{
  "hash": "bb07fe7340485f3b25372465d3a67a7e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Principal Component Analysis\"\nauthor: \"Termeh Shafie\"\nformat: html\neditor: visual\nexecute:\n  cache:  true\n---\n\n\n# PC Regression\n\nSome of the most notable advantages of performing PCA are the following:\n\n-   Dimensionality reduction\n-   Avoidance of multicollinearity between predictors.\n-   Variables are orthogonal, so including, say, PC9 in the model has no bearing on, say, PC3\n-   Variables are ordered in terms of standard error. Thus, they also tend to be ordered in terms of statistical significance\n-   Overfitting mitigation\n\nWith principal components regression, the new transformed variables (the principal components) are calculated in a totally unsupervised way:\n\n-   the response Y is not used to help determine the principal component directions).\n-   the response does not supervise the identification of the principal components.\n-   PCR just looks at the x variables\n\nThe PCA method can dramatically improve estimation and insight in problems where multicollinearity is a large problem – as well as aid in detecting it.\n\n## Very simple PCA example\n\nLet’s say we asked 16 participants four questions (on a 7 scale) about what they care about when choosing a new computer, and got the results like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tibble) \nPrice <- c(6,7,6,5,7,6,5,6,3,1,2,5,2,3,1,2)\nSoftware <- c(5,3,4,7,7,4,7,5,5,3,6,7,4,5,6,3)\nAesthetics <- c(3,2,4,1,5,2,2,4,6,7,6,7,5,6,5,7)\nBrand <- c(4,2,5,3,5,3,1,4,7,5,7,6,6,5,5,7)\nbuy_computer <- tibble(Price, Software, Aesthetics, Brand)\n```\n:::\n\n\nLet’s go on with the PCA. `prcomp` is part of the stats package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca_buycomputer <- prcomp(buy_computer, scale = TRUE, center = TRUE)\nnames(pca_buycomputer)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(pca_buycomputer)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStandard deviations (1, .., p=4):\n[1] 1.5589391 0.9804092 0.6816673 0.3792578\n\nRotation (n x k) = (4 x 4):\n                  PC1        PC2        PC3         PC4\nPrice      -0.5229138 0.00807487  0.8483525 -0.08242604\nSoftware   -0.1771390 0.97675554 -0.1198660 -0.01423081\nAesthetics  0.5965260 0.13369503  0.2950727 -0.73431229\nBrand       0.5825287 0.16735905  0.4229212  0.67363855\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(pca_buycomputer)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1    PC2    PC3     PC4\nStandard deviation     1.5589 0.9804 0.6817 0.37926\nProportion of Variance 0.6076 0.2403 0.1162 0.03596\nCumulative Proportion  0.6076 0.8479 0.9640 1.00000\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggbiplot)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: ggplot2\n```\n\n\n:::\n\n```{.r .cell-code}\ng <- ggbiplot(pca_buycomputer, obs.scale = 1, var.scale = 1,\n              ellipse = TRUE, circle = TRUE)\ng <- g + scale_color_discrete(name = '')\ng <- g +  theme_minimal() +\n          theme(legend.direction = 'horizontal', \n          legend.position = 'top') \nprint(g)\n```\n\n::: {.cell-output-display}\n![](pca_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nRemember that one of the disadventage of PCA is how difficult it is to interpret the model (ie. what does the PC1 is representing, what does PC2 is representing, etc.). The biplot graph help somehow to overcome that.\n\nIn the above graph, one can see that Brandand Aesthetic explain most of the variance in the new predictor PC1 while Software explain most of the variance in the new predictor PC2. It is also to be noted that Brand and Aesthetic are quite highly correlated.\n\nOnce you have done the analysis with PCA, you may want to look into whether the new variables can predict some phenomena well, i.e. whether features can classify the data well. Let’s say you have asked the participants one more thing, which OS they are using (Windows or Mac) in your survey, and the results are like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nOS <- c(0,0,0,0,1,0,0,0,1,1,0,1,1,1,1,1)\n```\n:::\n\n\nLet's first create a biplot with this new variable shown:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng <- ggbiplot(pca_buycomputer, obs.scale = 1, var.scale = 1, groups = as.character(OS),\n              ellipse = TRUE, circle = TRUE)\ng <- g + scale_color_discrete(name = '')\ng <- g +  theme_minimal() +\n          theme(legend.direction = 'horizontal', \n          legend.position = 'top') \n\nprint(g)\n```\n\n::: {.cell-output-display}\n![](pca_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nWhat do you note? Let's now test the model (recall this is a toy example so we won't bother with train-test split):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1 <- glm(OS ~ pca_buycomputer$x[,1] + pca_buycomputer$x[,2], family = binomial)\nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = OS ~ pca_buycomputer$x[, 1] + pca_buycomputer$x[, \n    2], family = binomial)\n\nCoefficients:\n                       Estimate Std. Error z value Pr(>|z|)  \n(Intercept)             -0.2138     0.7993  -0.268   0.7891  \npca_buycomputer$x[, 1]   1.5227     0.6621   2.300   0.0215 *\npca_buycomputer$x[, 2]   0.7337     0.9234   0.795   0.4269  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 22.181  on 15  degrees of freedom\nResidual deviance: 11.338  on 13  degrees of freedom\nAIC: 17.338\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n:::\n\n\nLet’s see how well this model predicts the kind of OS. You can use `fitted()` function to see the prediction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitted(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          1           2           3           4           5           6 \n0.114201733 0.009372181 0.217716320 0.066009817 0.440016243 0.031640529 \n          7           8           9          10          11          12 \n0.036189119 0.175766013 0.906761064 0.855587371 0.950088045 0.888272270 \n         13          14          15          16 \n0.781098710 0.757499202 0.842557931 0.927223453 \n```\n\n\n:::\n:::\n\n\nThese values represent the probabilities of being 1. For example, we can expect 11% chance that Participant 1 is using OS 1 based on the variable derived by PCA. Thus, in this case, Participant 1 is more likely to be using OS 0, which agrees with the survey response. In this way, PCA can be used with regression models for calculating the probability of a phenomenon or making a prediction.\n\n## Penguins\n\nHere we use the Penguin data set which includes variables on the penguin body features. Import this data and for simplicity, remove the rows that include NA's. Run PCA on the following four predictors:\n\n1.  `bill_length_mm`\n2.  `bill_depth_mm`\n3.  `flipper_length_mm`\n4.  `body_mass_g`\n\nInterpret the results using\n\n-   with a biplot and explain observed patterns\n-   in terms of proportion variance explained (use a scree plot and a cumulative proportion plot)\n\n## We start with the base R way\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr) # install if not in your library\npenguins = read_csv(\"penguins.csv\", col_names = T) \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNew names:\nRows: 344 Columns: 9\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): species, island, sex dbl (6): ...1, bill_length_mm, bill_depth_mm,\nflipper_length_mm, body_mass_g...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 9\n   ...1 species island    bill_length_mm bill_depth_mm flipper_length_mm\n  <dbl> <chr>   <chr>              <dbl>         <dbl>             <dbl>\n1     0 Adelie  Torgersen           39.1          18.7               181\n2     1 Adelie  Torgersen           39.5          17.4               186\n3     2 Adelie  Torgersen           40.3          18                 195\n4     3 Adelie  Torgersen           NA            NA                  NA\n5     4 Adelie  Torgersen           36.7          19.3               193\n6     5 Adelie  Torgersen           39.3          20.6               190\n# ℹ 3 more variables: body_mass_g <dbl>, sex <chr>, year <dbl>\n```\n\n\n:::\n\n```{.r .cell-code}\ntail(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 9\n   ...1 species   island bill_length_mm bill_depth_mm flipper_length_mm\n  <dbl> <chr>     <chr>           <dbl>         <dbl>             <dbl>\n1    62 Chinstrap Dream            45.7          17                 195\n2    63 Chinstrap Dream            55.8          19.8               207\n3    64 Chinstrap Dream            43.5          18.1               202\n4    65 Chinstrap Dream            49.6          18.2               193\n5    66 Chinstrap Dream            50.8          19                 210\n6    67 Chinstrap Dream            50.2          18.7               198\n# ℹ 3 more variables: body_mass_g <dbl>, sex <chr>, year <dbl>\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 344   9\n```\n\n\n:::\n\n```{.r .cell-code}\ndf <- na.omit(penguins)\ndim(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 333   9\n```\n\n\n:::\n\n```{.r .cell-code}\nreduced_df <-  cbind(df$bill_length_mm, df$bill_depth_mm, df$flipper_length_mm, df$body_mass_g)\ncolnames(reduced_df) <- c(\"bill_L\", \"bill_D\", \"flipper\", \"body\")\n\npenguin_pca = prcomp(reduced_df, scale = TRUE)\nbiplot(penguin_pca, scale = 0)\n```\n\n::: {.cell-output-display}\n![](pca_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\nget_PVE = function(pca_out) {\n  pca_out$sdev ^ 2 / sum(pca_out$sdev ^ 2)\n}\npve = get_PVE(penguin_pca)\npve\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.68633893 0.19452929 0.09216063 0.02697115\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(\n  pve,\n  xlab = \"Principal Component\",\n  ylab = \"Proportion of Variance Explained\",\n  ylim = c(0, 1),\n  type = 'b'\n)\n```\n\n::: {.cell-output-display}\n![](pca_files/figure-html/unnamed-chunk-10-2.png){width=672}\n:::\n\n```{.r .cell-code}\ncumsum(pve)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6863389 0.8808682 0.9730289 1.0000000\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(\n  cumsum(pve),\n  xlab = \"Principal Component\",\n  ylab = \"Cumulative Proportion of Variance Explained\",\n  ylim = c(0, 1),\n  type = 'b'\n)\n```\n\n::: {.cell-output-display}\n![](pca_files/figure-html/unnamed-chunk-10-3.png){width=672}\n:::\n:::\n\n\n## Let's now do it the **Tidy** way\n\nData prepration\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr) \nlibrary(tidyverse)\nlibrary(ggfortify)\npenguins = read_csv(\"penguins.csv\", col_names = T) \nhead(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 9\n   ...1 species island    bill_length_mm bill_depth_mm flipper_length_mm\n  <dbl> <chr>   <chr>              <dbl>         <dbl>             <dbl>\n1     0 Adelie  Torgersen           39.1          18.7               181\n2     1 Adelie  Torgersen           39.5          17.4               186\n3     2 Adelie  Torgersen           40.3          18                 195\n4     3 Adelie  Torgersen           NA            NA                  NA\n5     4 Adelie  Torgersen           36.7          19.3               193\n6     5 Adelie  Torgersen           39.3          20.6               190\n# ℹ 3 more variables: body_mass_g <dbl>, sex <chr>, year <dbl>\n```\n\n\n:::\n\n```{.r .cell-code}\npenguins <- \n  penguins %>% \n  drop_na()\n\npenguins[1] <- NULL # remove first column\n\nhead(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  <chr>   <chr>              <dbl>         <dbl>             <dbl>       <dbl>\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           36.7          19.3               193        3450\n5 Adelie  Torgersen           39.3          20.6               190        3650\n6 Adelie  Torgersen           38.9          17.8               181        3625\n# ℹ 2 more variables: sex <chr>, year <dbl>\n```\n\n\n:::\n\n```{.r .cell-code}\nstr(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [333 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : chr [1:333] \"Adelie\" \"Adelie\" \"Adelie\" \"Adelie\" ...\n $ island           : chr [1:333] \"Torgersen\" \"Torgersen\" \"Torgersen\" \"Torgersen\" ...\n $ bill_length_mm   : num [1:333] 39.1 39.5 40.3 36.7 39.3 38.9 39.2 41.1 38.6 34.6 ...\n $ bill_depth_mm    : num [1:333] 18.7 17.4 18 19.3 20.6 17.8 19.6 17.6 21.2 21.1 ...\n $ flipper_length_mm: num [1:333] 181 186 195 193 190 181 195 182 191 198 ...\n $ body_mass_g      : num [1:333] 3750 3800 3250 3450 3650 ...\n $ sex              : chr [1:333] \"male\" \"female\" \"female\" \"female\" ...\n $ year             : num [1:333] 2007 2007 2007 2007 2007 ...\n```\n\n\n:::\n:::\n\n\nLoad some packages:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(ggfortify)\n```\n:::\n\n\nWhen you’re running a PCA, the variables that you are collapsing need to be continuous. If you don’t have all continuous variables, then you’ll need to consider a different ordination method (e.g., Jaccard similarity indices use a binary presence/absence matrix). Similarly to running linear models (and its variations), it’s a good idea to scale and center our variables. Luckily, we can do this inside the `prcomp()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca_values <- \n  prcomp(penguins[, c(3:6)], center = TRUE, scale = TRUE)\nsummary(pca_values)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.6569 0.8821 0.60716 0.32846\nProportion of Variance 0.6863 0.1945 0.09216 0.02697\nCumulative Proportion  0.6863 0.8809 0.97303 1.00000\n```\n\n\n:::\n:::\n\n\nThe number of principal components will always equal the number of variables you’re collapsing - in our case, we have four (i.e., PC1, PC2, PC3, PC4). The table that is presented is telling you how well the PCA fits your data. Typically, we assess PCA “fit” based on how much of the variance can be explained on a single axis. Here, the proportion of variance on the first axis (PC1) is nearly 70%, which is great! The last row is describing the cumulative proportion, which is just the sum of the proportion of variance explained by each additional axis (the sum of all axes will equal 1.00).\n\nThe numbers are great (and you’ll have to report them in your results), but let’s visualize this. For a quick a dirty PCA plot, we can just use the `ggfortify::autoplot()` function. This produces a ggplot object, so you can still manipulate it quite a bit, but we’ll also provide code below so you can make your own.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(pca_values, loadings = TRUE, loadings.label = TRUE)\n```\n\n::: {.cell-output-display}\n![](pca_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nThe interpretation: arrows of similar length and direction are more correlated to one another than arrows that are perpendicular to one another. If two arrows are pointing in the exact opposite direction, they’re negatively correlated. You can double-check all this with a correlation matrix, and you’ll see that `flipper_length_mm` and `body_mass_g` are correlated (*r = 0.87*) and their arrows are nearly parallel!\n\nThe direction and magnitude of each arrow is also telling you how much of that variable loads on that axis. Let’s take bill_depth_mm as an example. Here we can see that decreasing values of PC1 equate to larger values of bill_depth_mm because its eigenvector is pointing towards the left side of the plot. We also see a similar pattern with PC2, where decreasing values of PC2 = increasing values of bill_depth_mm. Conversely, increasing values of PC1 would equate to increasing values of flipper_length_mm, body_mass_g, and bill_length_mm.\n\nThe clustering of points matters as well. Points that are clustering near each other are more similar than those that are further apart. This is easily visualized if we colour the points; as an example, we’ll colour the points by species:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(pca_values, loadings = TRUE, loadings.label = TRUE,\n         data = penguins, colour = 'species')\n```\n\n::: {.cell-output-display}\n![](pca_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nSo now we can see that the Adelie and Chinstrap points cluster, but they also overlap quite a bit. In this space, you would interpret them as more similar to one another. The Gentoo penguins are way to the right and don’t overlap with the other two species at all, so we would say that they are very different in terms of bill depth, bill length, flipper length, and body mass. Of course we can see this in the plot, but if you want to test clustering, then we’ll have to do that in a separate analysis.\n\n## Want to make even prettier plots?\n\nStart with basic plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca_points <- \n  # first convert the pca results to a tibble\n  as_tibble(pca_values$x) %>% \n  # now we'll add the penguins data\n  bind_cols(penguins)\n\nhead(pca_points)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 12\n    PC1     PC2     PC3    PC4 species island    bill_length_mm bill_depth_mm\n  <dbl>   <dbl>   <dbl>  <dbl> <chr>   <chr>              <dbl>         <dbl>\n1 -1.85 -0.0320  0.235   0.528 Adelie  Torgersen           39.1          18.7\n2 -1.31  0.443   0.0274  0.401 Adelie  Torgersen           39.5          17.4\n3 -1.37  0.161  -0.189  -0.528 Adelie  Torgersen           40.3          18  \n4 -1.88  0.0123  0.628  -0.472 Adelie  Torgersen           36.7          19.3\n5 -1.92 -0.816   0.700  -0.196 Adelie  Torgersen           39.3          20.6\n6 -1.77  0.366  -0.0284  0.505 Adelie  Torgersen           38.9          17.8\n# ℹ 4 more variables: flipper_length_mm <dbl>, body_mass_g <dbl>, sex <chr>,\n#   year <dbl>\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbasic_plot <- \n  ggplot(pca_points, aes(x = PC1, y = PC2)) +\n  geom_point(aes(colour = species)) +\n  theme_light()\n\nbasic_plot\n```\n\n::: {.cell-output-display}\n![](pca_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nAnd then pimp it up using `chull()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# first create a dataframe to extract the convex hull points\npca_hull <- \n  pca_points %>% \n  group_by(species) %>% \n  slice(chull(PC1, PC2))\n\n# now, we'll just continue to build on our ggplot object\nchull_plot <- \n  basic_plot +\n  geom_polygon(data = pca_hull,\n               aes(fill = as.factor(species)),\n               alpha = 0.3,\n               show.legend = FALSE)\n\n\nchull_plot\n```\n\n::: {.cell-output-display}\n![](pca_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nWe’re almost there! Lastly, let’s put the eigenvectors (i.e., the arrows) back on the plot. First, we’ll have to create another dataframe of eigenvectors and then we can throw them back onto the plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca_load <- \n  as_tibble(pca_values$rotation, rownames = 'variable') %>% \n  # we can rename the variables so they look nicer on the figure\n  mutate(variable = dplyr::recode(variable,\n                                  'bill_length_mm' = 'Bill length',\n                                  'bill_depth_mm' = 'Bill depth',\n                                  'flipper_length_mm' = 'Flipper length',\n                                  'body_mass_g' = 'Body mass'))\n\nhead(pca_load)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  variable          PC1      PC2    PC3    PC4\n  <chr>           <dbl>    <dbl>  <dbl>  <dbl>\n1 Bill length     0.454 -0.600   -0.642  0.145\n2 Bill depth     -0.399 -0.796    0.426 -0.160\n3 Flipper length  0.577 -0.00579  0.236 -0.782\n4 Body mass       0.550 -0.0765   0.592  0.585\n```\n\n\n:::\n\n```{.r .cell-code}\nchull_plot +\n  geom_segment(data = pca_load, \n               aes(x = 0, y = 0, \n                   xend = PC1*5,\n                   yend = PC2*5),\n               arrow = arrow(length = unit(1/2, 'picas'))) +\n  annotate('text', x = (pca_load$PC1*6), y = (pca_load$PC2*5.2),\n           label = pca_load$variable,\n           size = 3.5) \n```\n\n::: {.cell-output-display}\n![](pca_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}