{
  "hash": "2efd285edba26a7046725af4a411bd92",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear Regression I\"\nauthor: \"Termeh Shafie\"\nformat:\n  html:\n    embed-resources: true\nnumber-sections: true\ntoc: true         \neditor: visual\nexecute:\n  cache:  false\n---\n\n# Linear Regression\n\n*Linear* regression models relationships using straight lines (or flat planes/hyperplanes when there are multiple predictors), following the equation $Y = mx + b$. \n\nIf the true relationship between the predictor and the outcome isn’t actually linear, the model’s performance can suffer. More importantly, this poor performance often isn’t uniform and it may be especially bad for specific ranges of predictor values. \n\n$$\\underbrace{Y_i}_\\text{Observed Value for Person i} = \\underbrace{\\beta_0}_\\text{intercept} + \\underbrace{\\beta_1}_\\text{Coefficient for X1} * \\underbrace{X_{i1}}_\\text{Value for person i on Variable X1} + ... + \\beta_p * X_{ip}$$\n\n## Question\nCan you think of a situation where it would be particularly problematic if our model consistently under- or over-predicted within certain ranges of the predictor?\n\n\n::: {.callout-tip collapse=\"true\"}\n### Answer\nImagine we’re predicting house prices ($Y$) using square footage ($X$).  \nIf we fit a *linear* model, it might do a decent job for mid-sized homes but fail at the extremes.  \n\n- For **very small homes**, the model might **over-predict** price (since tiny houses often sell for disproportionately less).  \n- For **very large homes**, it might **under-predict** (since luxury properties tend to have higher prices per square foot).  \n\nThis would be especially problematic if those extreme cases are *important* to decision-making, for example, a real estate investor estimating renovation costs or an appraiser evaluating high-end homes.  \nIn such cases, a nonlinear model (like polynomial regression or a tree-based model) might capture those curved patterns better.\n:::\n\n## How to Choose Model Parameters\n\nLinear regression models include two types of parameters: **coefficients** which describe how changes in the predictors affect the predicted outcome, and an **intercept**, which represents the predicted value when all predictors are zero.  \n\nTo build an effective model, we need to estimate these parameters in a way that best fits the data. We covered two main approaches for doing this: **Least Squares** and **Maximum Likelihood Estimation (MLE)**.\n\n### Least Squares\nAs the name implies, Least Squares aims to choose parameter values that *minimize* the sum of squared errors. \n\n$$ \\text{SSE} = \\sum_{i = 1}^n(y_i - \\beta_0 - \\beta_1*x_i)^2 $$\n\nthe $\\hat{y_i}$ represents our model's *predicted* value of $y_i$. For a simple linear regression with only 1 predictor, we get our prediction using the formula:\n\n$$ \\hat{y} = \\beta_0 + \\beta_1*x_i $$\n\nSo let's plug that in for $\\hat{y_i}$:\n\n$$ \\text{SSE} = \\sum_{i = 1}^n(y_i - \\beta_0 - \\beta_1*x_i)^2 $$\n\nNow all we need to do is set the *partial derivatives* of the $\\text{SSE}$ to 0 and solve. The formula above has *two* parameters that we're interested in: $\\beta_0$ and $\\beta_1$, so we'll take the partial derivatives of $\\text{SSE}$ with respect to each of them:\n\n$$\\frac{\\partial SSE}{\\partial \\beta_0} = \\sum_{i = 1}^n 2(y_i - \\beta_0 - \\beta_1*x_i)(-1)$$\n$$\\frac{\\partial SSE}{\\partial \\beta_1} = \\sum_{i = 1}^n 2(y_i - \\beta_0 - \\beta_1*x_i)(-x_i)$$\n\nand set them equal to 0.\n\n$$\\frac{\\partial SSE}{\\partial \\beta_0} = \\sum_{i = 1}^n 2(y_i - \\beta_0 - \\beta_1*x_i)(-1) = 0$$\n$$\\frac{\\partial SSE}{\\partial \\beta_1} = \\sum_{i = 1}^n 2(y_i - \\beta_0 - \\beta_1*x_i)(-x_i) = 0$$\n\nthen we solve for $\\beta_0$ and $\\beta_1$ and we get:\n\n$$\\beta_0 = \\bar{y} - \\hat{\\beta_1}* \\bar{x}$$\n\nand\n\n$$ \\beta_1 = \\frac{Cov(x,y)}{Var(x)} = Corr(x,y) * \\frac{sd(x)}{sd(y)} $$\n\nThese values for $\\beta_0$ and $\\beta_1$ are the ones that *minimize* our Sum of Squared Errors ($\\text{SSE}$) and therefore give us a model that performs very well.\n\n### Maximum Likelihood Estimation\nAnother way to estimate the parameters (coefficients and intercept) of a linear regression model is through **Maximum Likelihood Estimation (MLE)**, a method we’ll revisit several times in this course. MLE selects parameter values that make the observed training data as *likely* as possible under the model.  \n\nRemember, a model is our mathematical description of the world. If a model assigns *very low probability* to data that looks like what we actually observed, it’s probably not a good description of reality.\n\n\nThe **likelihood** of an individual data point in our model is:\n\n$$ p(y_i | x_i; \\beta_0, \\beta_1, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(y_i - (\\beta_0 + \\beta_1 * x_i))^2}{2\\sigma^2}}$$\n\n$^\\text{(notice that the numerator in the exponent is just the squared error for that data point)}$\n\nIf we have *multiple* data points in our training data, we'll *multiply* their likelihoods.\n\n$$ \\prod_{i = 1}^{n} p(y_i | x_i; \\beta_0, \\beta_1, \\sigma^2) = \\prod_{i = 1}^{n}\\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(y_i - (\\beta_0 + \\beta_1 * x_i))^2}{2\\sigma^2}}$$\n\nthis gives us the overall likelihood of our training data *given* the values of $\\beta_0$, $\\beta_1$. We want to choose values of $\\beta_0$ and $\\beta_1$ that *maximize* the likelihood from the equation above. To do so, we typically take the *log* of the likelihood (remember logs turn multiplications into sums, which makes the math easier) and maximize *that* by setting it's partial derivatives (w.r.t $\\beta_0$ and $\\beta_1$) equal to 0.\n\nWhen we do that, it turns out we get **the exact same** estimates as from least squares!\n\n$$\\beta_0 = \\bar{y} - \\hat{\\beta_1}* \\bar{x}$$\n\nand\n\n$$ \\beta_1 = \\frac{Cov(x,y)}{Var(x)} = Corr(x,y) * \\frac{sd(x)}{sd(y)} $$\n\nThese values for $\\beta_0$ and $\\beta_1$ are the ones that *mazimize* the likelihood of our data, and therefore give us a model that performs very well.\n\n# Linear Regression in R\nR has a clear and consistent workflow for linear modeling:\n\n1. **Specify your model** using a formula interface (e.g., `y ~ x1 + x2`).\n2. **Fit the model** on the training data using a fitting function like `lm()`, `glm()`, or another modeling function.\n3. **Assess** model performance by comparing predictions to the observed outcomes.\n\n\n## Packages needed\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# We use base R mostly but sometimes explore the tidyverse option, so load tidyverse\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n:::\n\n\n## Read Amazon Book Data\nThe dataset contains information about a collection of books sold on Amazon, including their **list price**, **physical dimensions** (such as height, width, and thickness), **weight**, and **number of pages**. It also includes the **Amazon price**, which serves as the outcome variable we aim to predict.  \n\nWe’ll use this dataset to explore how different characteristics of a book relate to its selling price, applying linear regression to model and interpret these relationships.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the readr package (comes with tidyverse)\n# library(readr)\n\n# Read the Amazon data (tab-separated)\nama <- read_tsv(\"02-data/amazon-books.txt\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 325 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (5): Title, Author, Hard/ Paper, Publisher, ISBN-10\ndbl (8): List Price, Amazon Price, NumPages, Pub year, Height, Width, Thick,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\n# if you don't want to use tidyverse you can use:\n# ama <- read.delim(\n#  \"amazon-books.txt\",\n#  header = TRUE,\n#  stringsAsFactors = FALSE)\n\n# look at the 10 first rows of the data\nknitr::kable(head(ama,10))\n```\n\n::: {.cell-output-display}\n\n\n|Title                                                                                                           |Author             | List Price| Amazon Price|Hard/ Paper | NumPages|Publisher                  | Pub year|ISBN-10    | Height| Width| Thick| Weight (oz)|\n|:---------------------------------------------------------------------------------------------------------------|:------------------|----------:|------------:|:-----------|--------:|:--------------------------|--------:|:----------|------:|-----:|-----:|-----------:|\n|1,001 Facts that Will Scare the S#*t Out of You: The Ultimate Bathroom Reader                                   |Cary McNeal        |      12.95|         5.18|P           |      304|Adams Media                |     2010|1605506249 |    7.8|   5.5|   0.8|        11.2|\n|21: Bringing Down the House - Movie Tie-In: The Inside Story of Six M.I.T. Students Who Took Vegas for Millions |Ben Mezrich        |      15.00|        10.20|P           |      273|Free Press                 |     2008|1416564195 |    8.4|   5.5|   0.7|         7.2|\n|100 Best-Loved Poems (Dover Thrift Editions)                                                                    |Smith              |       1.50|         1.50|P           |       96|Dover Publications         |     1995|486285537  |    8.3|   5.2|   0.3|         4.0|\n|1421: The Year China Discovered America                                                                         |Gavin Menzies      |      15.99|        10.87|P           |      672|Harper Perennial           |     2008|0061564893 |    8.8|   6.0|   1.6|        28.8|\n|1493: Uncovering the New World Columbus Created                                                                 |Charles C. Mann    |      30.50|        16.77|P           |      720|Knopf                      |     2011|0307265722 |    8.0|   5.2|   1.4|        22.4|\n|1861:  The Civil War Awakening                                                                                  |Adam Goodheart     |      28.95|        16.44|H           |      460|Knopf                      |     2011|1400040159 |    8.9|   6.3|   1.7|        32.0|\n|A Christmas Carol and Other Christmas Writings (Penguin Classics)                                               |Dickens            |      20.00|        13.46|H           |      336|Penguin Classics Hardcover |     2010|141195851  |    7.8|   5.3|   1.2|        15.5|\n|A Confederacy of Dunces                                                                                         |John Kennedy Toole |      15.00|         8.44|P           |      405|Grove Weidenfeld           |     1987|802130208  |    8.2|   5.3|   0.8|        11.2|\n|A Dance With Dragons                                                                                            |Martin RR, George  |      35.00|        18.81|H           |       NA|Bantam                     |     2011|553801473  |    9.6|   6.5|   2.1|          NA|\n|A Farewell To Arms                                                                                              |Ernest Hemingway   |      30.00|        19.80|H           |      304|Scribner                   |     1997|0684837889 |    9.6|   6.4|   1.1|        19.2|\n\n\n:::\n:::\n\n\n## Clean the data\nLet's remove missing values...\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check for missing (NA) values in each column\ncolSums(is.na(ama))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       Title       Author   List Price Amazon Price  Hard/ Paper     NumPages \n           0            1            1            0            0            2 \n   Publisher     Pub year      ISBN-10       Height        Width        Thick \n           1            1            0            4            5            1 \n Weight (oz) \n           9 \n```\n\n\n:::\n\n```{.r .cell-code}\ndim(ama)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 325  13\n```\n\n\n:::\n\n```{.r .cell-code}\n# Drop rows with any missing values\nama <- na.omit(ama)\n\n# Reset the row index (Rrownames)\nrownames(ama) <- NULL\n\n# final dimensions of data\ndim(ama)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 310  13\n```\n\n\n:::\n:::\n\n\n\n## Separate data into `X` (predictors) and `y` (outcome)\n We want to prepare our data for modeling by separating the predictors ($X$) from the outcome variable ($y$). The predictors are the input features used to explain or predict the outcome — in this case, various physical attributes of books. The outcome (Amazon Price) is the numeric value we’re trying to predict.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define predictor column names\npredictors <- c(\"List Price\", \"NumPages\", \"Weight (oz)\", \"Thick\", \"Height\", \"Width\")\n\n# Subset predictors (X)\nX <- ama[, predictors]\n\n# Subset outcome (y)\ny <- ama[[\"Amazon Price\"]]  # or ama$`Amazon Price`\n```\n:::\n\n\n## Fitting the model\n\nBefore fitting a regression model, it’s often useful to **standardize** the predictor variables so they’re on the same scale. Here, we transform each predictor into a **z-score**, meaning we subtract its mean and divide by its standard deviation.  \nThis ensures that all predictors contribute comparably to the model, especially when their original units differ (for example, *weight in ounces* vs. *height in inches*).  \nIn R, this can be done easily using the `scale()` function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standardize (z-score) the predictor variables\nX_z <- as.data.frame(scale(X))\n```\n:::\n\n\nNow we fit a linear model with `lm()`.  This ensures that all steps are applied consistently and that the model is trained on properly scaled data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Fit a linear regression model using the standardized data\nmodel <- lm(y ~ ., data = X_z)\n\n# View model summary\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ ., data = X_z)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16.6291  -1.6496  -0.3563   1.2966  22.9981 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   12.58765    0.18715  67.261  < 2e-16 ***\n`List Price`  11.42032    0.23158  49.314  < 2e-16 ***\nNumPages       0.23189    0.34195   0.678  0.49820    \n`Weight (oz)` -0.42037    0.33361  -1.260  0.20862    \nThick         -1.16151    0.35601  -3.263  0.00123 ** \nHeight        -0.09905    0.24145  -0.410  0.68194    \nWidth         -0.19750    0.24447  -0.808  0.41981    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.295 on 303 degrees of freedom\nMultiple R-squared:  0.9206,\tAdjusted R-squared:  0.919 \nF-statistic: 585.4 on 6 and 303 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n## Fitted (predicted) values\ny_pred <- predict(model)\n\n# view some of the observed and predicted values\nhead(cbind(y,y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      y     y_pred\n1  5.18  8.6869171\n2 10.20 10.9158750\n3  1.50  0.6459632\n4 10.87  7.8749115\n5 16.77 21.7538857\n6 16.44 18.0935043\n```\n\n\n:::\n:::\n\n\n\n## Evaluate model perfromance\nAfter fitting and predicting, we assess how well our model performs by comparing the predicted values (`y_pred`) to the actual outcomes (`y`).  \nWe compute several common metrics:\n\n- **MSE (Mean Squared Error):** the average of squared prediction errors and penalizes large errors more heavily.  \n- **MAE (Mean Absolute Error):** the average of absolute prediction errors and gives an intuitive sense of the typical deviation.  \n- **MAPE (Mean Absolute Percentage Error):** expresses prediction errors as a percentage of actual values.  \n- **R² (R-squared):** the proportion of variance in the outcome explained by the model (higher is better).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Evaluate model performance\nMSE  <- mean((y - y_pred)^2)\nMAE  <- mean(abs(y - y_pred))\nMAPE <- mean(abs((y - y_pred) / y)) * 100\nR2   <- 1 - sum((y - y_pred)^2) / sum((y - mean(y))^2)\n\n# alternatively for R2: summary(model)$r.squared\n\ncbind(MSE,MAE,MAPE,R2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          MSE     MAE     MAPE        R2\n[1,] 10.61234 2.16044 19.62124 0.9205886\n```\n\n\n:::\n:::\n\n\n## Question\nWhat do these metrics tell us about the model’s performance?  \nDiscuss whether this model seems to fit the data well, and what potential issues (if any) might still exist. \n\n::: {.callout-tip collapse=\"true\"}\n### Answer\n\nThese results suggest that the model performs quite well:\n\n- **MSE (10.61)** and **MAE (2.16)** indicate that, on average, predictions are only a few units away from the actual values.  \n  MSE penalizes larger errors more heavily, while MAE provides the average absolute deviation.\n- **MAPE (19.6%)** means predictions are, on average, about **20% off** from the true values, thus a reasonably good level of accuracy depending on context.\n- **R² = 0.92** indicates that **92% of the variation** in the outcome variable is explained by the predictors, thus a strong fit.\n\nOverall, this model explains most of the variability in the data and makes relatively accurate predictions.  \nHowever, we’d still want to check **residual plots** for patterns or bias (e.g., under- or over-prediction at certain ranges) to confirm the model’s assumptions hold true.\n:::\n\n\n## Checking Assumptions\n\nRemember there are 3 main assumptions$^*$ of Linear Regression:\n\n- Linearity\n- Homoskedasticity\n- Normality of Errors\n\n\n$^*$ There's also an assumption of *Independence*, aka that the value of one data point does not affect the value of another data point.\n\n### Linearity\n\nWe assess linearity by either:\n\n- plotting one predictor at a time against the outcome, and see if there are any clear non-linear patterns.\n\nWe will use `ggplot` here and use a faceted plot:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The base R version:\n\n# Set up a 2x3 layout for the plots\npar(mfrow = c(2, 3))\n# Loop through each predictor and plot Amazon Price vs that predictor\nfor (c in predictors) {\n  plot(\n    ama[[c]], ama[[\"Amazon Price\"]],\n    main = paste(\"Amazon Price vs.\", c),\n    xlab = c,\n    ylab = \"Amazon Price\",\n    pch = 19, col = \"steelblue\"\n  )\n}\n```\n\n::: {.cell-output-display}\n![](02-linear-regression-I_files/figure-html/lin-assump-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# predictors vector from earlier:\n# predictors <- c(\"List Price\", \"NumPages\", \"Weight (oz)\", \"Thick\", \"Height\", \"Width\")\n\n# Reshape to long format: one column for predictor name, one for its values\nama_long <- ama |>\n  pivot_longer(\n    cols = all_of(predictors),\n    names_to = \"predictor\",\n    values_to = \"value\"\n  )\n\n# Faceted scatterplot: one panel per predictor\nggplot(ama_long, aes(x = value, y = `Amazon Price`)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~ predictor, scales = \"free_x\") +\n  labs(\n    title = \"Amazon Price vs. Predictors\",\n    x = NULL,\n    y = \"Amazon Price\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](02-linear-regression-I_files/figure-html/lin-assump-2.png){width=672}\n:::\n:::\n\n\n\nNext we plot the predicted values (x-axis) by the residuals (y-axis) and look for clear non-linear patterns:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The base R version:\n\n# Create a data frame of residuals and predictions\nfitdata <- data.frame(\n  errors = y - y_pred,\n  pred = y_pred\n)\n\n# Base R residual plot\nplot(\n  fitdata$pred, fitdata$errors,\n  main = \"Residuals vs. Predicted Values\",\n  xlab = \"Predicted Values\",\n  ylab = \"Residuals\",\n  pch = 19, col = \"steelblue\"\n)\n\n# Add horizontal reference line at 0\nabline(h = 0, col = \"red\", lwd = 2, lty = 2)\n```\n\n::: {.cell-output-display}\n![](02-linear-regression-I_files/figure-html/res-plot-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Residual plot using ggplot\nggplot(fitdata, aes(x = pred, y = errors)) +\n  geom_point(color = \"black\", alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  theme_minimal() +\n  labs(\n    title = \"Residuals vs. Predicted Values\",\n    x = \"Predicted Values\",\n    y = \"Residuals\"\n  )\n```\n\n::: {.cell-output-display}\n![](02-linear-regression-I_files/figure-html/res-plot-2.png){width=672}\n:::\n:::\n\n\n\n> Note: from now on I will only use ggplot.\n\n### Homoskedasticity\nWe assess homoskedasticity (constant variance of residuals) using the same residual plot as above. Recall that *“homo”* means *same* and *“hetero”* means *different*, while *“skedasticity”* refers to the **spread or variance** of the errors.\n\nWhen examining the plot, look for whether the residuals appear to have a *consistent spread* across the range of predicted values.  \nDo some areas along the x-axis show much larger or smaller errors than others?   The variance doesn’t need to be perfectly uniform, but noticeable patterns or changes in spread could suggest heteroskedasticity, meaning the model’s error variance isn’t constant.\n\n\n### Normality\n\nNormality doesn't really impact prediction, in fact many argue it doesnt even impact inference ($p$-values/confidence intervals). So we won't dwell on it here. Here's code to check it using something called a QQ (Quantile-Quantile plot).  A Q–Q plot compares the distribution of the model’s residuals to a theoretical normal distribution.  \nIf the residuals are approximately normal, the points should fall along the dashed red line.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Q–Q plot of residuals (normality check)\nggplot(fitdata, aes(sample = errors)) +\n  stat_qq(color = \"black\", alpha = 0.5) +\n  stat_qq_line(color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  theme_minimal() +\n  labs(\n    title = \"Q–Q Plot of Residuals\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  )\n```\n\n::: {.cell-output-display}\n![](02-linear-regression-I_files/figure-html/norm-plot-1.png){width=672}\n:::\n:::\n\n\n## Summary\n\nOverall, we can observe that:\n\n- **Linearity:**  \n\n  - From the individual predictor plots, *List Price* shows a clear linear relationship with *Amazon Price*, suggesting it fits well within a linear framework.  \n    \n   - The other predictors (e.g., *Height*, *Width*, *Thick*, *Weight (oz)*, *NumPages*) show weak or nonlinear patterns, often clustering at common physical book sizes.  This indicates that the linear model captures the trend in *List Price* effectively but may not fully represent the relationships of the other variables.\n    \n  - `List Price` displays a strong and roughly linear relationship  with Amazon Price. As the list price increases, the Amazon          price tends to increase in a nearly proportional way.\n    \n  - The remaining predictors: (`Height`, `Width`, `Thick`, `Weight (oz)`, and `NumPages`) show weak or nonlinear relationships.  Many of these variables form visible clusters or plateaus, likely reflecting standard book formats or physical constraints (e.g., most books share similar dimensions).\n      \n  - These patterns suggest that while `List Price` is likely a strong predictor, the other features may require nonlinear transformations or may contribute less explanatory power in a linear model.\n      \n- **Homoskedasticity (Constant Variance):**  \n\n  - In several cases, there appears to be heteroskedasticity, where the variability in Amazon Price increases with the predictor (especially noticeable for `List Price` and `Weight (oz)`).\n  - The residuals vs. predicted values plot shows that residuals are mostly centered around zero, but the spread is not perfectly uniform.  There appears to be slightly greater variability at lower predicted values, which suggests **mild heteroskedasticity**.  \n  However, no severe funneling pattern is observed, so the assumption is only moderately violated.\n\nIn summary, the data indicate that a simple linear regression may fit well for `List Price`, but more complex relationships could exist for the other predictors.\n\n## Model Coefficients\nAfter fitting a linear regression model, we can inspect the estimated coefficients to understand how each predictor contributes to the outcome.   Each coefficient represents the expected change in the response variable (`Amazon Price`) for a one-unit increase in that predictor, holding all other variables constant.  \n\nIn R, the `coef()` function returns both the intercept and the slope coefficients for each predictor.  \nBy converting these values into a data frame, we can easily view and interpret them in a tabular format.  \nThe intercept represents the model’s predicted value when all predictors are zero, while each coefficient shows the direction and strength of the predictor’s relationship with the outcome.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract model coefficients and intercept\n# (assuming you already have a fitted model called 'model' from lm())\n\n# Create a data frame of coefficients\ncoefficients_df <- data.frame(\n  Name = names(coef(model)),\n  Coef = as.numeric(coef(model))\n)\n\n# Display the table of coefficients\ncoefficients_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           Name        Coef\n1   (Intercept) 12.58764516\n2  `List Price` 11.42032143\n3      NumPages  0.23189316\n4 `Weight (oz)` -0.42036500\n5         Thick -1.16150737\n6        Height -0.09904558\n7         Width -0.19749780\n```\n\n\n:::\n:::\n\n\nThe model’s coefficients show how each predictor relates to the `Amazon Price`, holding all other variables constant.\n\n- The `Intercept (12.59)` represents the predicted Amazon Price when all predictors are zero though not meaningful in practice, it serves as a baseline for the model.  \n- `List Price (11.42)` has by far the largest positive effect: for each additional unit increase in list price, the Amazon Price is predicted to increase by roughly `$11.42`, all else equal.  \n- `NumPages (0.23)` also shows a small positive relationship, suggesting that books with more pages tend to cost slightly more.  \n- The remaining variables, `Weight (oz)`, `Thick`, `Height`, and `Width`, have negative coefficients, indicating that when holding the other factors constant, increases in these dimensions are associated with slightly lower predicted prices.  These effects are relatively small and may reflect overlapping information among the physical features of books.\n\nOverall, the model suggests that `List Price` is the dominant predictor of `Amazon Price`, while the other physical attributes contribute modestly or redundantly to the price prediction.\n\n## Question\n\nLooking at the coefficients, most physical attributes (`Weight (oz)`, `Thick`, `Height`, `Width`) have *negative* values.  \nWhat might explain why these coefficients are negative, even though we might expect larger or heavier books to cost more?\n\n\n::: {.callout-tip collapse=\"true\"}\n### Answer\nThe negative coefficients likely arise from **multicollinearity**; the physical attributes of a book (height, width, thickness, and weight) are strongly correlated with one another and with `NumPages` and `List Price`.  \nWhen these correlated predictors are included together in the same linear model, their individual coefficients can shift direction or appear negative, even if the overall relationship with price is positive.\n\nIn other words, once `List Price` and `NumPages` are accounted for, the remaining variation explained by size and weight might actually correspond to lower prices (e.g., large but inexpensive books such as textbooks or coffee-table books).  \nThis doesn’t mean heavier or thicker books are truly cheaper, it means their *unique contribution* to predicting price, after controlling for other variables, happens to be negative.\n:::\n\n# Classwork\n\n## Modeling Beyoncé Song Popularity\nThis dataset contains song-level data scraped from Spotify’s API, and includes a variety of numerical features that describe Beyoncé’s songs, such as `danceability`, `energy`, `loudness`, `speechiness`, Chhoose your own continuous response variable to model and predict.\n\nThink about what interests you most:\n- Do you want to predict how `danceable` a song is based on its acoustic and rhythmic qualities?\n- Or are you more curious about what makes a song sound `energetic`, `acoustic`, or `happy (valence)`?\n\nChoose any continuous numeric variable as your response, then use the other features as predictors to build a linear regression model.  \nYour goal is to see whether the musical characteristics of Beyoncé’s songs can meaningfully explain or predict that chosen outcome.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load data\nb <- read.csv(\"02-data/Beyonce_data.csv\")\n\n# Preview the data\n\n# Define predictor variables\n\n# Fit a linear regression model\n\n# Generate predictions\n\n# Assess model performance\n\n# Check residuals (assumptions)\n```\n:::\n\n\n## Simulation\nSometimes, we'll run a **simulation** to see how our models perform in different scenarios. This helps us learn more about how the model works. Below is a custom function `linear_simulation()` that \n\n1. Generates fake data about cats' weights and lengths\n2. Fits a Linear Regression Model\n3. Grabs the coefficients\n4. Returns the data and Coefficients for us to look at\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_simulation <- function(n = 100, trueCoef = 0.04, intercept = 0.2, error_sd = 1) {\n  \n  # 1. Generate fake data for cat length and weight -----------------------\n  # Simulate \"cat length\" from a standard normal distribution\n  length <- rnorm(n, mean = 0, sd = 1)\n  \n  # Simulate \"weight\" using a linear model with some random error\n  weight <- intercept + length * trueCoef + rnorm(n, mean = 0, sd = error_sd)\n  \n  cats <- data.frame(length = length, weight = weight)\n  \n  \n  # 2. Fit a Linear Regression Model -------------------------------------\n  model <- lm(weight ~ length, data = cats)\n  \n  \n  # 3. Extract Coefficients ----------------------------------------------\n  coef_table <- data.frame(\n    Names = names(coef(model)),\n    Coef = as.numeric(coef(model))\n  )\n  \n  \n  # 4. Return data and coefficients\n  return(list(coef = coef_table, data = cats))\n}\n```\n:::\n\n\nThe data are created using a known linear equation:\n\n$$\n\\text{weight} = \\text{intercept} + (\\text{trueCoef} \\times \\text{length}) + \\text{random error}\n$$\n\n\nWe can run this simulation 100's of times.\n\n- `n` is the number of samples in each fake dataset\n- `trueCoef` is the true coefficient for cat length\n- `intercept` is the true intercept for the model\n- `error_sd` tells us how spread out the data is around the regression line\n\nAfter generating the data, the function fits a linear regression model using `lm()` and returns both the estimated coefficients and the simulated data.   This allows us to explore how well the model recovers the *true* coefficient values (`trueCoef` and `intercept`) as we vary the number of observations or the noise level.\n\n\nTo explore how sampling variability affects our regression estimates, we run the `linear_simulation()` function 500 times.  \nEach run generates a new random dataset of cat lengths and weights, fits a linear regression, and stores the estimated coefficients.  \n\nWe then combine the results into two data frames:\n- `coef_df` contains the estimated coefficients (`intercept` and `length`) from each simulation.\n- `data_df` contains all the simulated data points, labeled by which simulation they came from.\n\nBy analyzing or visualizing `coef_df`, we can see how much the estimated slope and intercept vary across repeated samples, an empirical demonstration of sampling variability and the distribution of regression estimates.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Play around with these numbers \nn <- 100\ntrueCoef <- 0.45   # don't change\nintercept <- 6     # don't change\nerror_sd <- 1\n\n# Run regression simulation 500 times \nset.seed(123)  # for reproducibility\n\n# Run regression simulation 500 times \nsimulations <- replicate(\n  500,\n  linear_simulation(n = n, trueCoef = trueCoef, intercept = intercept, error_sd = error_sd),\n  simplify = FALSE\n)\n\n# Extract coefficients from 500 simulations\ncoef_df <- do.call(rbind, lapply(simulations, function(x) x$coef))\ncoef_df$simulation_no <- rep(0:499, each = nrow(simulations[[1]]$coef))\n\n# Extract data from 500 simulations\ndata_df <- do.call(rbind, lapply(simulations, function(x) x$data))\ndata_df$simulation_no <- rep(0:499, each = n)\n```\n:::\n\n\nNow that we've run a bunch of simulations with the SAME true coefficient and intercept (but different random samples), let's look at the results of our 500 regression models.\n\nFirst, let's just make some scatter plots to see some of the simulations. Notice how similar or different the simulations are from each other.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Choose number of simulations to visualize\nn_plot <- 9\n\n# Subset data for the first 9 simulations\nchosen_datasets <- subset(data_df, simulation_no < n_plot)\n\n# Faceted scatterplots for each simulation\nggplot(chosen_datasets, aes(x = length, y = weight, color = factor(simulation_no))) +\n  geom_point() +\n  facet_wrap(~ simulation_no) +\n  theme_minimal() +\n  labs(\n    title = \"Simulated Datasets of Cat Length vs Weight\",\n    x = \"Length (standardized units)\",\n    y = \"Weight\",\n    color = \"Simulation Number\"\n  )\n```\n\n::: {.cell-output-display}\n![](02-linear-regression-I_files/figure-html/cats-plot-1.png){width=672}\n:::\n:::\n\n\nLet's look at the coefficient values from all the linear regressions we ran. This histogram shows the estimated `length` coefficient from 500 repeated regression simulations.   Although each dataset was generated from the same true underlying relationship, random variation in the data causes the estimated slope to differ slightly across runs.\n\nThe red dashed line marks the mean estimated coefficient, which should be close to the true value (`trueCoef = 0.45`).  \nThis plot illustrates the sampling distribution of the slope estimate, that is how regression coefficients vary across repeated samples due to random noise, even when the underlying relationship remains constant.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Filter to include only the 'length' coefficients\ncoef_only <- subset(coef_df, Names == \"length\")\n\n# Calculate mean coefficient value\nmean_coef <- mean(coef_only$Coef)\n\n# Plot histogram of coefficient estimates\nggplot(coef_only, aes(x = Coef)) +\n  geom_histogram(color = \"black\", fill = \"steelblue\", bins = 30) +\n  geom_vline(xintercept = mean_coef, color = \"red\", linetype = \"dashed\", linewidth = 1.5) +\n  theme_minimal() +\n  labs(\n    title = \"Distribution of Length Coefficients Across 500 Simulations\",\n    x = \"Estimated Coefficient for Length\",\n    y = \"Frequency\"\n  )\n```\n\n::: {.cell-output-display}\n![](02-linear-regression-I_files/figure-html/cats-coeff-1.png){width=672}\n:::\n:::\n\n\n## Question\n\nLook at the different values you got for the coefficient of length. We set the TRUE coefficient value to be 0.45, think about and describe how spread apart the estimates from our 500 regression models are. Does seeing how different our coefficient estimates can be *change* how you think about the coefficient estimates you get in regression models on real data?\n\n::: {.callout-tip collapse=\"true\"}\n### Answer\n\nAcross the 500 simulated regressions, the estimated `length` coefficients cluster around the true value of `0.45`, but they vary from sample to sample.  \nThis spread reflects sampling variability; even though each dataset comes from the same true relationship, random noise in the data produces slightly different estimates each time.\n\nMost of the slopes are fairly close to 0.45, but some are noticeably higher or lower.  \nThis shows that our coefficient estimates aren’t fixed truths, they’re estimates with uncertainty.  \nIf we repeatedly sampled new data from the same population, our slope estimate would fluctuate around the true value, forming a distribution of possible outcomes.\n\nSeeing this variation highlights why we report confidence intervals and $p$-values in regression: they describe the uncertainty around our estimated coefficients.  \nIn real-world analyses, a single regression result is just one possible estimate from a range of plausible values.\n:::\n\n\n## Play with `n` and `error_sd`\nHere are some suggestions:\n\n- Change `n`, the number of data points in each sample, to be very small (say 10), how does this change the results you saw?\n-  Change `n`, the number of data points in each sample, to be very large (say 1,000), how does this change the results you saw?\n- Change the `error_sd` term, this is a measure of how much error is in the model. More error means that data is scattered tightly around the regression line, less error means that the data is scatters very loosely around the regression line. How does changing  `error_sd` change the results you originally saw?\n\n## Violations of Linearity\nHere, you’ll compare how linear regression performs on data that *does* and *does not* meet the assumption of linearity.\n\n- In the first code chunk (`#nonLinReg`), fit a linear regression model on data that is **nonlinear**, that is, data where the relationship between `x` and `y` bends or curves.\n- In the second code chunk (`#LinReg`), fit a model on **linear** data, that is data that satisfies the assumption of linearity.\n- For both models, create a residual plot showing the predicted values on the x-axis and the residuals (errors) on the y-axis.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load nonlinear data\ndf_nonlin <- read.csv(\"02-data/xy-nonlin.csv\")\n\n# Separate X and y\nX <- data.frame(x = df_nonlin$x)\ny <- df_nonlin$y\n\n# Z-score the predictor (standardize x)\nx_z <- as.numeric(scale(X$x))\n\n# Fit linear regression on standardized x\n\n# Predict on training data\n\n# Prediction + error (residuals) data frame\n\n# Residual plot \n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load nonlinear data\ndf_lin <- read.csv(\"02-data/xy-lin.csv\")\n\n# Separate X and y\nX <- data.frame(x = df_lin$x)\ny <- df_lin$y\n\n# Z-score the predictor (standardize x)\nx_z <- as.numeric(scale(X$x))\n\n# Fit linear regression on standardized x\n\n# Predict on training data\n\n# Prediction + error (residuals) data frame\n\n# Residual plot \n```\n:::\n\n\n\n\n\n## Questions\n\n- How do the residual patterns differ between the linear and nonlinear datasets?  What visual cues suggest that the linearity assumption is being violated?\n\n- What are some possible consequences of violating the assumption of linearity?\n\n- If we used a linear regression model on the nonlinear data, are there specific ranges of `x` where the model would consistently over-predict or under-predict the outcome?   How can you tell from the residual plot?",
    "supporting": [
      "02-linear-regression-I_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}