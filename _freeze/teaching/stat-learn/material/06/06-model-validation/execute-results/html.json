{
  "hash": "61312093ffa313129fcf3187b7b3c683",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Model Validation\"\nauthor: \"Termeh Shafie\"\nformat:\n  html:\n    embed-resources: true\nnumber-sections: true\ntoc: true         \neditor: visual\nexecute:\n  cache:  true\n---\n\n# Review: Model Validation {.unnumbered}\n\nWe learned about 3 types of model validation that help us estimate how well our model might do on data it has never seen before.\n\n-   **Train Test Split (TTS)** (aka validation vet approach): We take our data and break it up into two groups, training (used to fit model) and testing (used to see how the model does on data it has never seen before)\n-   **K-Fold Cross Validation (KF)**: We take our data and break it up into K groups. We train K different models using a different group as the test set each time. The other K-1 groups are used to train the model.\n-   **Leave One Out Cross Validation (LOOCV)**: Like K-Fold but each data point is it's own fold. This means we fit N models (where N is the number of data points) using N-1 data points to train, and 1 data point to test.\n\nRemember the purpose of a test set is to be UNSEEN data. We should NEVER fit ANYTHING on the test set. In fact we should not even TOUCH the test set until our model is completely done training.\n\n# Part I: Iris Data\n\n-   `iris` data is a built-in data set in R that contains measurements for 50 flowers in 3 different species and 4 different attributes.\n-   `caret` package is short for Classification And Regression Training. This is a useful tool for data splitting, pre-processing, feature selection and model tuning. In this simple example I will use this package to illustrate cross validation methods.\n-   `dplyr` package is a commonly used tool for data manipulation.\n-   `tidyverse` package is for data manipulation and visualization (with `dplyr` included).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\nlibrary(Metrics) # install if not on your machine\nlibrary(tidyverse)\n# Load data\ndata(iris)\n\n# Take a look at data \nstr(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n```\n\n\n:::\n:::\n\n\nTo determine whether the designed model is performing well, we need to use the observations that are not being used during the training of the model. Therefore the test set will serve as the unseen data, then the values of the dependent variables are predicted and model accuracy will be evaluated based on the difference between actual values and predicted values of the dependent variable. We use following model performance metrics (consult lecture slides for more information):\n\n-   $R^2$\n-   Rooted Mean Squared Error (RMSE)\n-   Mean Absolute Error (MAE)\n\nEach methods below will be conducted in four steps:\n\n-   **Data splitting**: split the data set into different subsets.\n-   **Training**: build the model on the training data set.\n-   **Testing**: apply the resultant model to the unseen data (testing data set) to predict the outcome of new observations.\n-   **Evaluating**: calculate prediction error using the model performance metrics.\n\n## Test-Train-split\n\nIn this approach, the available data is divided into two subsets: a training set and a validation set. The training set is used to train the model, and the validation set is used to evaluate its performance. Predictions done by this method could be largely affected by the subset of observations used in testing set. If the test set is not representative of the entire data, this method may lead to overfitting.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Data splitting\n\n# set seed to generate a reproducible random sample\nset.seed(123)\n\n# create training and testing data set using index, training data contains 80% of the data set\n# 'list = FALSE' allows us to create a matrix data structure with the indices of the observations in the subsets along the rows.\ntrain.index.vsa <- createDataPartition(iris$Species, p= 0.8, list = FALSE)\ntrain.vsa <- iris[train.index.vsa,]\ntest.vsa <- iris[-train.index.vsa,]\n\n# see how the the subsets are randomized\nrole = rep('train',nrow(iris))\nrole[-train.index.vsa] = 'test'\nggplot(data = cbind(iris,role)) + \n  geom_point(aes(x = Sepal.Length,\n                 y = Petal.Width,\n                 color = role)) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](06-model-validation_files/figure-html/data-split-tts-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Training: linear model is fit using all availbale predictors\nmodel.vsa <- lm(Petal.Width ~., data = train.vsa)\n\n\n### Testing\npredictions.vsa <- model.vsa %>% predict(test.vsa)\n\n\n### Evaluating\ndata.frame(RMSE = RMSE(predictions.vsa, test.vsa$Petal.Width),\n           R2 = R2(predictions.vsa, test.vsa$Petal.Width),\n           MAE = MAE(predictions.vsa, test.vsa$Petal.Width))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       RMSE        R2      MAE\n1 0.1675093 0.9497864 0.128837\n```\n\n\n:::\n:::\n\n\n## Leave-One-Out Cross Validation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data splitting: leave one out\ntrain.loocv <- trainControl(method = \"LOOCV\")\n\n# Training\nmodel.loocv <- train(Petal.Width ~.,\n                     data = iris,\n                     method = \"lm\",\n                     trControl = train.loocv)\n\n#  Present results\nprint(model.loocv)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression \n\n150 samples\n  4 predictor\n\nNo pre-processing\nResampling: Leave-One-Out Cross-Validation \nSummary of sample sizes: 149, 149, 149, 149, 149, 149, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.1705606  0.9496003  0.1268164\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n```\n\n\n:::\n:::\n\n\n## K-Fold Cross Validation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data splitting\n\n# set seed to generate a reproducible random sample\nset.seed(123)\n# the number of K is set to be 5\ntrain.kfold <- trainControl(method = \"cv\", number = 5)\n\n# Training\nmodel.kfold <- train(Petal.Width ~.,\n                     data = iris,\n                     method = \"lm\",\n                     trControl = train.kfold)\n\n# Present results\nprint(model.kfold)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression \n\n150 samples\n  4 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 122, 120, 118, 121, 119 \nResampling results:\n\n  RMSE       Rsquared   MAE    \n  0.1704321  0.9514251  0.12891\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n```\n\n\n:::\n:::\n\n\n## Repeated K-Fold Cross Validation\n\n### Data splitting\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set seed to generate a reproducible random sample\nset.seed(123)\n# the number of K is set to be 5\ntrain.rkfold <- trainControl(method = \"repeatedcv\", number = 5, repeats = 3)\n\n### Training\nmodel.rkfold <- train(Petal.Width ~.,\n                     data = iris,\n                     method = \"lm\",\n                     trControl = train.rkfold)\n\n# Present results\nmodel.rkfold\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression \n\n150 samples\n  4 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold, repeated 3 times) \nSummary of sample sizes: 122, 120, 118, 121, 119, 119, ... \nResampling results:\n\n  RMSE      Rsquared   MAE      \n  0.168445  0.9525634  0.1266377\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\nmodel.kfold\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLinear Regression \n\n150 samples\n  4 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 122, 120, 118, 121, 119 \nResampling results:\n\n  RMSE       Rsquared   MAE    \n  0.1704321  0.9514251  0.12891\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n```\n\n\n:::\n:::\n\n\n## Let's summarize the results\n\n| CV method      | RMSE   |     R2 |  MAE   |\n|----------------|:-------|-------:|:------:|\n| Validation Set | 0.1675 | 0.9498 | 0.1288 |\n| LOOCV          | 0.1706 | 0.9496 | 0.1268 |\n| K-Fold         | 0.1704 | 0.9514 | 0.1289 |\n| K-Fold repeat  | 0.1704 | 0.9514 | 0.1289 |\n\n### Question\n\n> What do you note?\n\n# Part II: KNN Regression Simulation\n\n## KNN classifier algorithm (for univariate $x$'s and binary $y$'s)\n\nWe create a function called `KNN`⁠ for performing KNN regression using the following arguments:\n\n-   $x_0$ as the new point at which we wish to predict $y$\n-   ${\\bf x} = (x_1,x_2, \\dots, x_n)$ as the vector of training $x$'s\n-   ${\\bf y} = (y_1,y_2, \\dots, y_n)$ as the vector of training $y$'s\n-   $K$ as number of neighbors to use\n-   $\\hat{y}_0$ as the predicted value of $y$ at $x_0$\n\nThe function calculates the Euclidean distance between $x_0$ and each of the $x_i$'s in the training set $(x_1, x_2, \\dots, x_n)$. Then we order them from nearest to furthest away and takes the mean of the $y$ values of the $K$ nearest points yielding the predicted value of $y$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#   x0 = new point at which to predict y\n#   x = (x_1,...,x_n) = vector of training x's\n#   y = (y_1,...,y_n) = vector of training y's\n#   K = number of neighbors to use\n#   y0_hat = predicted value of y at x0\n\nKNN = function(x0, x, y, K) {\n  distances = abs(x - x0) \n  o = order(distances) \n  y0_hat = mean(y[o[1:K]]) \n  return(y0_hat)  \n}\n```\n:::\n\n\n## Simulate data\n\nWe simulate training vector $\\bf{x}$ from a uniform distribution on the interval $[0,5]$ and simulate training vector $\\bf{y}$ by assuming $$y = f(x) + \\varepsilon$$ where $f(x) = \\cos(x)$ and $\\varepsilon \\sim N(0, \\sigma^2)$ and $\\sigma = 0.3$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)  # set random number generator\nn = 20  # number of samples\nx = 5*runif(n)  \nsigma = 0.3  \nf = function(x) { cos(x) }  \ny = f(x) + sigma*rnorm(n)  \n```\n:::\n\n\nLet's plot of the training data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x,y,col=2,pch=20,cex=2)  # plot training data\nx_grid = seq(from=0, to=5, by=0.01)  # grid of x values for plotting f(x) values\nlines(x_grid,f(x_grid))  # plot true f(x) values for the grid\n```\n\n::: {.cell-output-display}\n![](06-model-validation_files/figure-html/plot-sim-data-1.png){width=864}\n:::\n:::\n\n\nNow we run the `KNN` function to predict $y$ at each point on the grid of $x$ values. For that we need to define $K$, that is number of nearest neighbors to use. We start with setting it equal to 1 but this can be changed later as an exercise.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nK = 5 \ny_grid_hat = sapply(x_grid, function(x0) { KNN(x0, x, y, K) })\n```\n:::\n\n\nNext we add the predicted values to our plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x,y,col=2,pch=20,cex=2)  # plot training data\ntitle(paste(\"K =\",K))\nlines(x_grid,f(x_grid))  # plot true f(x) values\nlines(x_grid,y_grid_hat,col=4)  # plot predicted y values \n```\n\n::: {.cell-output-display}\n![](06-model-validation_files/figure-html/plot-KNN-5-1.png){width=864}\n:::\n:::\n\n\n### Question\n\n> What happens to predicted curve when you change the value of $K$?\n\n## K-Fold Cross Validation\n\nNow we are going to use cross validation to estimate test performance of the KNN classifier. We set number of neighbors as $K=1$ and use the 10-fold cross validation. We do a random ordering of all the available data, and initialize a vector to hold MSE for each fold. For each fold, we then create a training and test (hold one out/validation) set, run KNN at each $x$ in this test set (the one left out), and compute MSE on this test set. Then we average the MSE over all folds to obtain the CV estimate of test MSE:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nK = 1  \nnfolds = 10 \npermutation = sample(1:n)  \nMSE_fold = rep(0,nfolds)  \nfor (j in 1:nfolds) {\n    test = permutation[floor((j-1)*n/nfolds+1) : floor(j*n/nfolds)]  \n    train = setdiff(1:n, test)  \n    y_hat = sapply(x[test], function(x0) { KNN(x0, x[train], y[train], K) }) \n    MSE_fold[j] = mean((y[test] - y_hat)^2) \n}\nMSE_cv = mean(MSE_fold)  \nMSE_cv\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1241132\n```\n\n\n:::\n:::\n\n\nNext we compare with the ground truth estimate of test performance, given this training set. Because this is a simulation example, we can generate lots of test data. We simulate $x$'s and $y$'s from the true data generating process. Then we run the KNN classifier at each $x$ in the test set and compute the MSE on the test set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_test = 100000\nx_test = 5*runif(n_test)  \ny_test = f(x_test) + sigma*rnorm(n_test)  \ny_test_hat = sapply(x_test, function(x0) { KNN(x0, x, y, K) })  \nMSE_test = mean((y_test - y_test_hat)^2)  \n```\n:::\n\n\nLet's compare the two values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nMSE_test\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1656885\n```\n\n\n:::\n\n```{.r .cell-code}\nMSE_cv\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1241132\n```\n\n\n:::\n:::\n\n\nBe careful when calculating the *root* MSE (RMSE) since it corresponds to root mean squared error or square root of MSE: Let's try with\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt(MSE_test)  # test RMSE\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4070485\n```\n\n\n:::\n\n```{.r .cell-code}\nsqrt(mean(MSE_fold))  # sqrt of MSE_cv\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.352297\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(sqrt(MSE_fold))  # can we use this?\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3066843\n```\n\n\n:::\n:::\n\n\n## Leave-One Out Cross Validation\n\nUse the leave-one out cross validation (LOOCV) in the above example and report the CV estimate of test MSE and the MSE given ground truth.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nK = 1  \nnfolds = n\npermutation = sample(1:n)  \nMSE_fold = rep(0,nfolds)  \nfor (j in 1:nfolds) {\n    test = permutation[floor((j-1)*n/nfolds+1) : floor(j*n/nfolds)]  \n    train = setdiff(1:n, test)  \n    y_hat = sapply(x[test], function(x0) { KNN(x0, x[train], y[train], K) }) \n    MSE_fold[j] = mean((y[test] - y_hat)^2) \n}\nMSE_loocv = mean(MSE_fold)  \nMSE_test = mean((y_test - y_test_hat)^2)  \nMSE_loocv\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1241132\n```\n\n\n:::\n\n```{.r .cell-code}\nMSE_test\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1656885\n```\n\n\n:::\n:::\n\n\n## Hyperparameter Tuning: Choosing Model Settings\n\nWith the following example, we will illustrate how to use cross validation to choose the optimal number of neighbors $K$ in KNN. We start with a rather high number of $K$ to try for KNN ($K=30$) and use 10 folds for each of these cases in the cross validation. Then we do a random ordering of data and initialize vector for holding MSE's. For each number of folds in the range, we compute the training and test set as before (this is again the validation set). For each $K$ up to 30, we then run KNN at each $x$ in the test set (the one left out), and compute MSE on the this test set. We average across folds to obtain CV estimate of test MSE for each $K$ and plot the results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nK_max = 30 \nnfolds = 10  \npermutation = sample(1:n)  \nMSE_fold = matrix(0,nfolds,K_max)  \nfor (j in 1:nfolds) {\n    test = permutation[floor((j-1)*n/nfolds+1) : floor(j*n/nfolds)]  \n    train = setdiff(1:n, test) \n    for (K in 1:K_max) {\n        y_hat = sapply(x[test], function(x0) { KNN(x0, x[train], y[train], K) })\n        MSE_fold[j,K] = mean((y[test] - y_hat)^2)  \n    }\n}\nMSE_cv = colMeans(MSE_fold)  \n```\n:::\n\n\nWe plot CV estimate of test MSE against number of neighbors $K=1,2,\\dots,30$, and choose the value of $K$ that minimizes estimated test MSE. Compare with a ground truth estimate of test performance by using the chosen number of $K$ and running KNN on each $x$ in the test set (denoted `x_test` above).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(1:K_max, MSE_cv, pch=19)  # plot CV estimate of test MSE for each K\n```\n\n::: {.cell-output-display}\n![](06-model-validation_files/figure-html/plot-MSE-K-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Choose the value of K that minimizes estimated test MSE\nK_cv = which.min(MSE_cv)\nK_cv\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3\n```\n\n\n:::\n:::\n\n\n### Question?\n\n> Why do you think the test performance estimate for the chosen $K$ tend to be smaller than the ground truth estimate of test performance in this example?\n\n::: {.callout-tip collapse=\"true\"}\n### Answer\n\n$MSE_{cv}(K_{cv})$ may systematically underestimate or overestimate test MSE! There are two sources of bias: $K_{cv}$ is the minimum, and the pseudo-training set is smaller than $n$.\n:::\n\n## Choosing the number of folds\n\nWe start by simulating training data as before:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1) \nn = 20\nx = 5*runif(n)  \nsigma = 0.3 \ny = f(x) + sigma*rnorm(n)  \n```\n:::\n\n\nWe then compute \"ground truth\" estimate of test performance, given this training set. We set $K=10$, and run KNN at each $x$ in the test set and compute MSE on the test set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nK = 10\ny_test_hat = sapply(x_test, function(x0) { KNN(x0, x, y, K) })  \nMSE_test = mean((y_test - y_test_hat)^2)  \n```\n:::\n\n\nNext, we repeatedly run CV for a range of number of folds `nfolds` up to maximum $n=20$ (same as $n$ above in our simulated data). We repeat the simulation 200 times, and for each repetition and number of folds, we split the training data into training and test (hold one out/validation set). We run KNN at each $x$ in this test set and compute MSE. We then average the MSE's for each case with a different number of folds:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnfolds_max = n  # maximum value of nfolds to use for CV\nnreps = 200  # number of times to repeat the simulation\nMSE_cv = matrix(0,nreps,nfolds_max)  \nfor (r in 1:nreps) {  \n    for (nfolds in 1:nfolds_max) {\n        permutation = sample(1:n) \n        MSE_fold = rep(0,nfolds)  \n        for (j in 1:nfolds) {\n            test = permutation[floor((j-1)*n/nfolds+1) : floor(j*n/nfolds)]  \n            train = setdiff(1:n, test)  \n            y_hat = sapply(x[test], function(x0) { KNN(x0, x[train], y[train], K) }) \n            MSE_fold[j] = mean((y[test] - y_hat)^2) \n        }\n        MSE_cv[r,nfolds] = mean(MSE_fold)\n    }\n}\n```\n:::\n\n\nWe compute the MSE, bias, and variance of the CV estimate of test MSE, for each value of nfolds and plot MSE, bias\\^2, and variance of the CV estimate, for each value of nfolds.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmse = colMeans((MSE_cv - MSE_test)^2)\nbias = colMeans(MSE_cv) - MSE_test\nvariance = apply(MSE_cv,2,var)\n\n# plot of MSE, bias^2 and variance against number of folds\nplot(1:nfolds_max, type=\"n\", ylim=c(0,max(mse[2:nfolds_max])*1.1), xlab=\"nfolds\", ylab=\"mse\", main=\"MSE of the CV estimates\")\nlines(1:nfolds_max, mse, col=1, lty=2, lwd=2, ylim=c(0,0.2))\nlines(1:nfolds_max, bias^2, col=2, lwd=2)\nlines(1:nfolds_max, variance, col=4, lwd=2)\nlegend(\"topright\", legend=c(\"mse\",\"bias^2\",\"variance\"), col=c(1,2,4), lwd=2)\n```\n\n::: {.cell-output-display}\n![](06-model-validation_files/figure-html/plot-performance-1.png){width=864}\n:::\n\n```{.r .cell-code}\n# plot bias against number of folds\nplot(1:nfolds_max, bias)\nlines(1:nfolds_max, bias, col=2, lwd=2)\n```\n\n::: {.cell-output-display}\n![](06-model-validation_files/figure-html/plot-performance-2.png){width=864}\n:::\n:::\n\n\n### Question\n\nIn the above plot below, why do you think the bias of the CV estimate of test MSE is always positive?\n\n::: {.callout-tip collapse=\"true\"}\n### Answer\n\nBecause the \"pseudo\"-training set (each fold) is smaller than the training set.\n:::\n\n# Part III: Amazon Books\n\nLet's continue with our example from our earlier practical 2, where we fit a linear regression model to predict `Amazon Price`on books. We now include model validation. Read the data as before:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the readr package (comes with tidyverse)\n# library(readr)\n\n# Read the Amazon data (tab-separated)\nama <- read_tsv(\"06-data/amazon-books.txt\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 325 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (5): Title, Author, Hard/ Paper, Publisher, ISBN-10\ndbl (8): List Price, Amazon Price, NumPages, Pub year, Height, Width, Thick,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\n# Drop missing values\nama <- ama %>% drop_na()\n\n# Clean column names\n\ncolnames(ama) <- c(\n  \"Title\",\n  \"Author\",\n  \"List.Price\",\n  \"Amazon.Price\",\n  \"Hard.Paper\",\n  \"NumPages\",\n  \"Publisher\",\n  \"Pub.year\",\n  \"ISBN.10\",\n  \"Height\",\n  \"Width\",\n  \"Thick\",\n  \"Weight.oz\"\n)\n\n# Drop missing values\nama <- ama %>% drop_na() # using pipes from magrittr in tidyverse\n\n# Set up X and y\npredictors <- c(\"List.Price\", \"NumPages\", \"Weight.oz\", \"Thick\", \"Height\", \"Width\")\n\nX <- ama[, predictors]\ny <- ama$Amazon.Price\n```\n:::\n\n\n## K-Fold Cross Validation\n\nNow we cross validate as follows: - 5-fold CV - z-score predictors inside each fold - Train/test split per fold - Fit linear regression - Compute train & test MSE/MAE - Print per-fold metrics + averages - returns clean summary tables\n\nNote also that we the package called `Metrics` here (which you loaded earlier):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\n# Set up 5-fold cross validation indices\nset.seed(123)\nfolds <- createFolds(y, k = 5, returnTrain = TRUE)\n\nmse_train <- c()\nmse_test  <- c()\nmae_train <- c()\nmae_test  <- c()\n\n\n# Perform 5-fold CV\nfor (i in 1:5) {\n\n  train_idx <- folds[[i]]\n  test_idx  <- setdiff(seq_len(nrow(X)), train_idx)\n\n  X_train <- X[train_idx, ]\n  X_test  <- X[test_idx, ]\n  y_train <- y[train_idx]\n  y_test  <- y[test_idx]\n\n  # z-score within this fold\n  preproc <- preProcess(X_train, method = c(\"center\", \"scale\"))\n  X_train_scaled <- predict(preproc, X_train)\n  X_test_scaled  <- predict(preproc, X_test)\n\n  # fit\n  model <- lm(y_train ~ ., data = X_train_scaled)\n\n  # predict\n  y_pred_train <- predict(model, newdata = X_train_scaled)\n  y_pred_test  <- predict(model, newdata = X_test_scaled)\n\n  # metrics\n  mse_train[i] <- mse(y_train, y_pred_train)\n  mse_test[i]  <- mse(y_test, y_pred_test)\n\n  mae_train[i] <- mae(y_train, y_pred_train)\n  mae_test[i]  <- mae(y_test, y_pred_test)\n}\n\n# results using tidyverse tibble\ncv_results <- tibble(\n  Fold      = 1:5,\n  Train_MSE = mse_train,\n  Test_MSE  = mse_test,\n  Train_MAE = mae_train,\n  Test_MAE  = mae_test\n)\n\ncv_results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 5\n   Fold Train_MSE Test_MSE Train_MAE Test_MAE\n  <int>     <dbl>    <dbl>     <dbl>    <dbl>\n1     1      8.84    22.2       2.02     2.35\n2     2     11.1      9.21      2.17     2.24\n3     3     10.9     11.0       2.21     2.37\n4     4      9.84    14.2       2.16     2.05\n5     5     11.2      8.88      2.23     2.27\n```\n\n\n:::\n\n```{.r .cell-code}\n# averaged over all folds\nkfold_summary <- tibble(\n  Mean_Train_MSE = mean(mse_train),\n  Mean_Test_MSE  = mean(mse_test),\n  Mean_Train_MAE = mean(mae_train),\n  Mean_Test_MAE  = mean(mae_test)\n)\n\nkfold_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  Mean_Train_MSE Mean_Test_MSE Mean_Train_MAE Mean_Test_MAE\n           <dbl>         <dbl>          <dbl>         <dbl>\n1           10.4          13.1           2.16          2.26\n```\n\n\n:::\n:::\n\n\n## Leave-One-Out Cross Validation\n\nThe below code does the following: loads & cleans the data - extracts predictors and response - performs LOOCV manually - z-scores inside each fold - computes train/test MSE & MAE - returns clean summary tables\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\n\n# Set up predictors & response as before\n\npredictors <- c(\"List.Price\", \"NumPages\", \"Weight.oz\", \"Thick\", \"Height\", \"Width\")\n\nX <- ama[, predictors]\ny <- ama$Amazon.Price\nn <- nrow(X)\n\n# LOOCV: Leave-One-Out Cross Validation\n\nmse_train <- numeric(n)\nmse_test  <- numeric(n)\nmae_train <- numeric(n)\nmae_test  <- numeric(n)\n\nfor (i in 1:n) {\n\n  # train/test split\n  test_idx  <- i\n  train_idx <- setdiff(1:n, test_idx)\n\n  X_train <- X[train_idx, ]\n  X_test  <- X[test_idx, , drop = FALSE]\n  y_train <- y[train_idx]\n  y_test  <- y[test_idx]\n\n  # Z-score within fold \n  train_means <- apply(X_train, 2, mean)\n  train_sds   <- apply(X_train, 2, sd)\n\n  X_train_scaled <- scale(X_train, center = train_means, scale = train_sds)\n  X_test_scaled  <- sweep(sweep(X_test, 2, train_means), 2, train_sds, \"/\")\n\n  # Fit linear model\n\n  model <- lm(y_train ~ ., data = as.data.frame(X_train_scaled))\n\n  # Predictions\n  y_pred_train <- predict(model, newdata = as.data.frame(X_train_scaled))\n  y_pred_test  <- predict(model, newdata = as.data.frame(X_test_scaled))\n\n  # Metrics\n  mse_train[i] <- mse(y_train, y_pred_train)\n  mse_test[i]  <- mse(y_test, y_pred_test)\n\n  mae_train[i] <- mae(y_train, y_pred_train)\n  mae_test[i]  <- mae(y_test, y_pred_test)\n}\n\n# Results\nresults <- tibble(\n  Train_MSE = mse_train,\n  Test_MSE  = mse_test,\n  Train_MAE = mae_train,\n  Test_MAE  = mae_test\n)\n\n\n# averaged over all folds\nloo_summary <- tibble(\n  Mean_Train_MSE = mean(mse_train),\n  Mean_Test_MSE  = mean(mse_test),\n  Mean_Train_MAE = mean(mae_train),\n  Mean_Test_MAE  = mean(mae_test)\n)\n\nloo_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  Mean_Train_MSE Mean_Test_MSE Mean_Train_MAE Mean_Test_MAE\n           <dbl>         <dbl>          <dbl>         <dbl>\n1           10.6          13.8           2.16          2.27\n```\n\n\n:::\n:::\n\n\nNow visualize the results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Put into a single tibble\nresults <- bind_rows(\n  kfold_summary %>%  mutate(Method = \"K-Fold (K=5)\"),\n  loo_summary   %>%  mutate(Method = \"LOO\")\n)\n\n# Convert to long format\nresults_long <- results |> \n  pivot_longer(cols = starts_with(\"Mean\"),\n               names_to = \"Metric\",\n               values_to = \"Value\")\n\n# Nice readable metric labels\nresults_long$Metric <- recode(results_long$Metric,\n  \"Mean_Train_MSE\" = \"Train MSE\",\n  \"Mean_Test_MSE\"  = \"Test MSE\",\n  \"Mean_Train_MAE\" = \"Train MAE\",\n  \"Mean_Test_MAE\"  = \"Test MAE\"\n)\n\n# Plot\nggplot(results_long, aes(x = Metric, y = Value, fill = Method)) +\n  geom_col(position = \"dodge\") +\n  labs(title = \"K-Fold vs LOOCV: Average Performance Metrics\",\n       x = \"\",\n       y = \"Error\") +\n  theme_minimal(base_size = 14) +\n  scale_fill_brewer(palette = \"Set2\")\n```\n\n::: {.cell-output-display}\n![](06-model-validation_files/figure-html/plot-results-1.png){width=864}\n:::\n:::\n\n\n### Questions\n\nUsing the above plot, what can you conclude about:\n\n1.  How well the linear regression model generalizes to new data, and\n2.  Which validation method you would use to estimate the linear regression model’s test error, and why?\n3.  Using these results, explain how the bias–variance tradeoff helps us understand why K-Fold CV and LOOCV give slightly different test errors, even though both methods fit the same model on nearly the same data. What do your results suggest about how bias and variance differ between K-Fold CV and LOOCV for this problem?\n\n::: {.callout-tip collapse=\"true\"}\n### Answer\n\nBoth K-Fold CV and LOOCV are applied to *the same* linear regression model; the only thing that changes is how we estimate its out-of-sample error, not the model itself.\n\nFrom the plot:\n\n-   **Train MSE/MAE** are almost identical for K-Fold and LOOCV, which indicates that the fitted `lm` model itself is behaving the same in both setups (as expected, since it’s the same model and same data).\n-   **Test MSE** is *slightly lower* for K-Fold than for LOOCV, and **Test MAE** is essentially the same. This suggests that, for this `lm` model, K-Fold gives an estimate of test error that is at least as good as (and in this case a bit better than) LOOCV.\n\n1.  The `lm` model’s **generalization performance** appears very similar under both methods, with K-Fold showing marginally better test MSE, and\\\n2.  **LOOCV is far more computationally expensive** (it refits `lm` once per observation, versus only 5 times for 5-fold CV),\n\nThe reasonable choice is to use **K-Fold CV** to evaluate this linear regression model. It provides a practically equivalent (or slightly better) estimate of the `lm` model’s test error at a much lower computational cost.\n\n3.  The bias–variance tradeoff explains the small differences we observe between K-Fold CV and LOOCV in the plot.\n\n-   **LOOCV** uses almost the full dataset for every training fold, so its estimates have\n    -   **lower bias** (its training sets closely mimic the full dataset)\\\n    -   **higher variance** (each fold differs by only one observation, making the fitted model extremely sensitive to individual points)\n-   **K-Fold CV**, in contrast, uses smaller training sets and reshuffles data more substantially between folds, leading to\n    -   **slightly higher bias**\\\n    -   **lower variance**\n\nIn our results, the test MSE for LOOCV is only slightly lower than K-Fold’s, which is consistent with LOOCV’s lower bias.\\\nHowever, the difference is small, and K-Fold’s lower variance means it typically provides a more stable estimate of true generalization error.\n\nThus, the bias–variance tradeoff helps explain why LOOCV and K-Fold CV produce similar but not identical estimates: LOOCV trades stability (higher variance) for slightly lower bias, whereas K-Fold provides a more reliable, lower-variance estimate of the model’s test error.\n:::\n\n## Assumption Checks with TTS, KFold, LOO\n\nWhen checking assumptions AND using model validation, when do we check assumptions?\n\nCreating a residual plot requires we have residuals, which requires a model. Check assumptions *after* fitting the model by making residual plot(s).\n\n### TTS\n\nWe need a model to check residuals, so do it AFTER the model is built. This is done using an \"assumption\" plot (residuals vs fitted):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Train/Test Split (80/20)\nset.seed(123)\ntrain_index <- createDataPartition(y, p = 0.8, list = FALSE)\n\nX_train <- X[train_index, ]\nX_test  <- X[-train_index, ]\n\ny_train <- y[train_index]\ny_test  <- y[-train_index]\n\n\n# Z-scores\npreproc <- preProcess(X_train, method = c(\"center\", \"scale\"))\n\nX_train_scaled <- predict(preproc, X_train)\nX_test_scaled  <- predict(preproc, X_test)\n\n# Fit linear regression\nmodel <- lm(y_train ~ ., data = X_train_scaled)\n\n\n# Predict\ny_pred_train <- predict(model, newdata = X_train_scaled)\ny_pred_test  <- predict(model, newdata = X_test_scaled)\n\n\n# Assumption plot (residuals vs predictions)\nassump_train <- tibble(\n  predicted = y_pred_train,\n  errors = y_train - y_pred_train\n)\n\nggplot(assump_train, aes(x = predicted, y = errors)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Residuals vs Fitted (Train Data)\")\n```\n\n::: {.cell-output-display}\n![](06-model-validation_files/figure-html/tts-assump-1.png){width=864}\n:::\n:::\n\n\n### K-Fold\n\nSame check now but for K-fold CV used earlier with K=5.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(patchwork) # to have several panes next to each other\n\nset.seed(123)\n\n# K-fold setup\nfolds <- createFolds(y, k = 5, returnTrain = TRUE)\n\nmse_train <- c()\nmse_test  <- c()\n\nall_assump <- list()  # store residuals per fold\n\nfor (i in seq_along(folds)) {\n  train_idx <- folds[[i]]\n  test_idx  <- setdiff(seq_len(nrow(X)), train_idx)\n\n  X_train <- X[train_idx, ]\n  X_test  <- X[test_idx, ]\n  y_train <- y[train_idx]\n  y_test  <- y[test_idx]\n\n  # z-score within fold\n  preproc <- preProcess(X_train, method = c(\"center\", \"scale\"))\n  X_train_scaled <- predict(preproc, X_train)\n  X_test_scaled  <- predict(preproc, X_test)\n\n  # fit model\n  model <- lm(y_train ~ ., data = X_train_scaled)\n\n  # predict\n  y_pred_train <- predict(model, newdata = X_train_scaled)\n  y_pred_test  <- predict(model, newdata = X_test_scaled)\n\n  # store residuals with fold id\n  all_assump[[i]] <- tibble(\n    Fold = paste0(\"Fold \", i),\n    predicted = y_pred_train,\n    errors = y_train - y_pred_train\n  )\n\n  # metrics\n  mse_train[i] <- mse(y_train, y_pred_train)\n  mse_test[i]  <- mse(y_test,  y_pred_test)\n}\n\n# Bind all residuals into one data frame\nassump_df <- bind_rows(all_assump)\n\n# 1) Residual plots faceted by fold \n\np_resid <- ggplot(assump_df, aes(x = predicted, y = errors)) +\n  geom_point(alpha = 0.6) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  facet_wrap(~ Fold) +\n  theme_minimal() +\n  labs(\n    title = \"Residuals vs Fitted by Fold\",\n    x = \"Predicted (Train)\",\n    y = \"Residuals\"\n  )\n\n# 2) MSE barplot per fold\n\nmse_df <- tibble(\n  Fold = paste0(\"Fold \", seq_along(mse_train)),\n  Train_MSE = mse_train,\n  Test_MSE  = mse_test\n) %>% \n  pivot_longer(cols = c(Train_MSE, Test_MSE),\n               names_to = \"Set\",\n               values_to = \"MSE\")\n\np_mse <- ggplot(mse_df, aes(x = Fold, y = MSE, fill = Set)) +\n  geom_col(position = \"dodge\") +\n  theme_minimal() +\n  labs(\n    title = \"Train/Test MSE by Fold\",\n    x = \"\",\n    y = \"MSE\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# 3) Combine with patchwork \n\np_resid / p_mse\n```\n\n::: {.cell-output-display}\n![](06-model-validation_files/figure-html/kfold-assump-1.png){width=864}\n:::\n:::\n\n\nThe residual plots across the five folds show that the linear regression model behaves consistently regardless of how the data is split. Residuals remain centered around zero with a similar spread in each fold, suggesting that the model does not exhibit major bias and performs stably across training subsets. There is some mild heteroscedasticity, as errors tend to increase for higher predicted values, and a few outliers appear in each fold, which is expected in real-world price data.\n\nThe train and test MSE values show the expected pattern: training error is slightly lower than test error in every fold. Most folds have test MSE values in a similar range, indicating that the model generalizes reliably. Fold 1 exhibits a noticeably higher test MSE than the others, which likely reflects sampling variability rather than a structural issue with the model.\n\nOverall, the K-Fold results suggest that the linear regression model fits reasonably well, generalizes consistently across folds, and does not appear to be severely overfitting, although prediction accuracy decreases somewhat for higher-priced books.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}