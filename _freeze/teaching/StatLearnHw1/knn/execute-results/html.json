{
  "hash": "96d71add58f80fd0acf74b4a61104a4e",
  "result": {
    "markdown": "---\ntitle: \"K-nearest neighbors (KNN) regression and classifier\"\nauthor: \"Termeh Shafie\"\nformat: html\neditor: visual\nexecute:\n  cache:  true\n---\n\n\n# KNN regression algorithm (for univariate $x$'s)\nWe create a function called `KNN`⁠ for performing KNN regression using the following arguments: \n\n* $x_0$ as the new point at which we wish to predict $y$\n+ ${\\bf x} = (x_1,x_2, \\dots, x_n)$ as the vector of training $x$'s\n+ ${\\bf y} = (y_1,y_2, \\dots, y_n)$ as the vector of training $y$'s\n+ $K$ as number of neighbors to use\n+ $\\hat{y}_0$ as the predicted value of $y$ at $x_0$\n\n\nThe function calculates the Euclidean distance between $x_0$ and each of the $x_i$'s in the training set $(x_1, x_2, \\dots, x_n)$. Then we order them from nearest to furthest away and takes the mean of the $y$ values of the $K$ nearest points yielding the predicted value of $y$:\n\n::: {.cell hash='knn_cache/html/unnamed-chunk-1_55a837cb41a5b0e7350a975157df37d1'}\n\n```{.r .cell-code}\n#   x0 = new point at which to predict y\n#   x = (x_1,...,x_n) = vector of training x's\n#   y = (y_1,...,y_n) = vector of training y's\n#   K = number of neighbors to use\n#   y0_hat = predicted value of y at x0\n\nKNN = function(x0, x, y, K) {\n  distances = abs(x - x0) \n  o = order(distances) \n  y0_hat = mean(y[o[1:K]]) \n  return(y0_hat)  \n}\n```\n:::\n\nWhere do we get  ${\\bf x}$ and ${\\bf y}$?\n\n## Simulate training data\nWe simulate training vector $\\bf{x}$ from a uniform distribution on the interval $[0,5]$ and simulate training vector $\\bf{y}$ by assuming \n$$y = f(x) + \\varepsilon$$\nwhere $f(x) = \\cos(x)$ and $\\varepsilon \\sim N(0, \\sigma^2)$ and $\\sigma = 0.3$.\n\n::: {.cell hash='knn_cache/html/unnamed-chunk-2_d0ca6c6d38717625316d923301a21199'}\n\n```{.r .cell-code}\nset.seed(1)  # set random number generator\nn = 20  # number of samples\nx = 5*runif(n)  \nsigma = 0.3  \nf = function(x) { cos(x) }  \ny = f(x) + sigma*rnorm(n)  \n```\n:::\n\n### Plot of the training data\n\n::: {.cell hash='knn_cache/html/unnamed-chunk-3_31a1a17e4d5e38c53ce4d7538850e16c'}\n\n```{.r .cell-code}\nplot(x,y,col=2,pch=20,cex=2)  # plot training data\nx_grid = seq(from=0, to=5, by=0.01)  # grid of x values for plotting f(x) values\nlines(x_grid,f(x_grid))  # plot true f(x) values for the grid\n```\n\n::: {.cell-output-display}\n![](knn_files/figure-html/unnamed-chunk-3-1.png){width=864}\n:::\n:::\n\n\n## Predicting values\nNow we run the `KNN` function to predict $y$ at each point on the grid of $x$ values. For that we need to define $K$, that is number of nearest neighbors to use. We start with setting it equal to 1 but this can be changed later as an exercise.\n\n::: {.cell hash='knn_cache/html/unnamed-chunk-4_925ce0a460269e3308169e18907c1f99'}\n\n```{.r .cell-code}\nK = 1 \ny_grid_hat = sapply(x_grid, function(x0) { KNN(x0, x, y, K) })\n```\n:::\n\n\nNext we add the predicted values to our plot:\n\n::: {.cell hash='knn_cache/html/unnamed-chunk-5_a295cb692ced0085b2d0227448a93420'}\n\n```{.r .cell-code}\nplot(x,y,col=2,pch=20,cex=2)  # plot training data\ntitle(paste(\"K =\",K))\nlines(x_grid,f(x_grid))  # plot true f(x) values\nlines(x_grid,y_grid_hat,col=4)  # plot predicted y values \n```\n\n::: {.cell-output-display}\n![](knn_files/figure-html/unnamed-chunk-5-1.png){width=864}\n:::\n:::\n\n\nWhat happens to predicted curve when you change the value of $K$?\n\n## Bias-Variance Trade-Off\nWe are going to run through some code in order to illustrate the trade-off between bias and variance. We set $x_0$ to 1.5, which is the point we wish to estimate $y$ at.\n\nWe simulate 10000 data sets to approximate expectations over the $Y$'s (given fixed $x$). We initialize  two vectors of zeros to hold predicted and true $y$ values at $x_0$. Then for each of the 10000 datasets simulated, we repeat the above syntax for prediction. For the first 5 datasets simulated, we plot out the results to see what is happening.\n\n::: {.cell hash='knn_cache/html/unnamed-chunk-6_71dd19a141a60d6329448018bf08d067'}\n\n```{.r .cell-code}\nK = 1  \nx0 = 1.5  \nn_datasets = 10000  \ny0_hat = rep(0,n_datasets)  \ny0 = rep(0,n_datasets) \nfor (i in 1:n_datasets) {\n  y = f(x) + sigma*rnorm(n) \n  y0[i] = f(x0) + sigma*rnorm(1) \n  y0_hat[i] = KNN(x0, x, y, K)  \n  if (i <= 5) {\n    plot(x,y,col=2,pch=20,cex=2,ylim=c(-1.5,1.5))  \n    lines(x_grid,f(x_grid))  \n    y_grid_hat = sapply(x_grid, function(x0) { KNN(x0, x, y, K) })\n    lines(x_grid,y_grid_hat,col=4)  \n    points(x0,y0_hat[i],pch=20,cex=4,col=4)  # plot predicted value of y at x0\n  }\n}\n```\n\n::: {.cell-output-display}\n![](knn_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](knn_files/figure-html/unnamed-chunk-6-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](knn_files/figure-html/unnamed-chunk-6-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](knn_files/figure-html/unnamed-chunk-6-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](knn_files/figure-html/unnamed-chunk-6-5.png){width=672}\n:::\n:::\n\n\nNext we calculate the bias and variance of the KNN predictions at $x_0$.We also compute the variance of the noise at $x_0$ in order to be able to get the test MSE both using the bias variance representation\n$$\\textrm{test MSE} = \\textrm{bias}^2 + \\textrm{variance} + \\textrm{noise}$$\nand the direct formula:\n$$\\mathop{\\mathbb{E}} \\left(y_0- \\hat{f}(x_0)\\right)^2$$\n\n::: {.cell hash='knn_cache/html/unnamed-chunk-7_28f4953efba7300d1e1de5e05e9a8882'}\n\n```{.r .cell-code}\nbias = mean(y0_hat) - f(x0)  # bias of KNN predictions at x0\nvariance = var(y0_hat)  # variance of KNN predictions at x0\nnoise = sigma^2  # variance of the noise at x0\n\nbias^2 + variance + noise \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2086705\n```\n:::\n\n```{.r .cell-code}\nmean((y0 - y0_hat)^2) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2097131\n```\n:::\n:::\n\nWhy do you think the two values differ?\n\n\n::: {style=\"color: red;\"}\n**Homework 1.1** (6 points) \n\nVisualize and explain the bias-variance trade-off using the syntax above. You need to explain and show how the bias, variance and test MSE is influenced by the choice of $K$. For full points, three plots should be included showing the following:\n\n* bias vs. $K$ (or flexibility)\n+ variance vs. $K$  (or flexibility)\n+ test MSE versus $K$  (or flexibility)\n\n:::\n\n\n# KNN classifier algorithm (for univariate $x$'s and binary $y$'s) \nWe are going to look at the probability version of the KNN classifier algorithm. We create a function called `KNN_classifier`⁠ for performing KNN classification using the following arguments: \n\n* $x_0$ as the new point at which we wish to predict $y$\n+ ${\\bf x} = (x_1,x_2, \\dots, x_n)$ as the vector of training $x$'s, where $x_i$ is real-valued\n+ ${\\bf y} = (y_1,y_2, \\dots, y_n)$ as the vector of training $y$'s, where $y_i$ is 0 or 1\n+ $K$ as number of neighbors to use\n+ $\\hat{p}_{1}$ as the estimated probability of $y_0=1$ given $x_0$\n\nThe function calculates the Euclidean distance between $x_0$ and each of the $x_i$'s in the training set $(x_1, x_2, \\dots, x_n)$. Then we order them from nearest to furthest away and computes the fraction of $y$ values of the $y$ values of the $K$ nearest training points that are equal to 1 and return this proportion as an estimated probability of $y_0=1$.\nWe can transform $\\hat{p}_{1}$ to a prediction of the $y$ value at $x_0$ by using a threshold on $\\hat{p}_{1}$ and return\n\n\n::: {.cell hash='knn_cache/html/unnamed-chunk-8_b6206853841e58f6881763d8edf49050'}\n\n```{.r .cell-code}\nKNN_classifier = function(x0, x, y, K) {\n  distances = abs(x - x0)  \n  o = order(distances)  \n  p1_hat = mean(y[o[1:K]])  \n  return(p1_hat)  \n}\n```\n:::\n\n\n## Simulate training data\nWe simulate training vector $\\bf{x}$ from a uniform distribution on the interval $[0,5]$ and the true probability $p_1(x)$ of $y=1$ given $x$ (true relationship between $x$ and $y$) according to \n$$p_1(x) = \\frac{\\exp(2*\\cos(x))}{(1 + \\exp(2*\\cos(x)))}$$\nWe simulate simulate training $y$'s as Bernoulli random variables with probabilities $p_1(x)$.\n\n::: {.cell hash='knn_cache/html/unnamed-chunk-9_1b381c51c1fe5a89bc15de78e326c6b4'}\n\n```{.r .cell-code}\nset.seed(1)  # set random number generator\nn = 20 \nx = 5*runif(n)  \np1 = function(x) { exp(2*cos(x))/(1 + exp(2*cos(x))) }  \ny = rbinom(n,1,p1(x)) \n```\n:::\n\n### Plot of the training data\n\n::: {.cell hash='knn_cache/html/unnamed-chunk-10_f813800a424e0f0a89a30a5b748ece73'}\n\n```{.r .cell-code}\nplot(x,y,col=2,pch=20,cex=2)  # plot training data\nx_grid = seq(from=0, to=5, by=0.01)  # grid of x values \nlines(x_grid,p1(x_grid))  # plot true p1(x) values for the grid\n```\n\n::: {.cell-output-display}\n![](knn_files/figure-html/unnamed-chunk-10-1.png){width=864}\n:::\n:::\n\n\n## Predicting classes\nNow we run the `KNN_classifier` function to predict $y$ at each point on the grid of $x$ values. For that we need to define $K$, that is number of nearest neighbors to use. We start with setting it equal to 1 but this can be changed later as an exercise. Further, we predict the $y$ values for each $x$ in the grid by thresholding the estimated probabilities to $\\leq 0.5$ and $>0.5$.\n\n::: {.cell hash='knn_cache/html/unnamed-chunk-11_7a5554de851046f64eca8016d2a6f16a'}\n\n```{.r .cell-code}\nK = 1\np1_grid_hat = sapply(x_grid, function(x0) { KNN_classifier(x0, x, y, K) })\ny_grid_hat = round(p1_grid_hat > 0.5)  \n```\n:::\n\n\nNext we add the predicted values to our plot:\n\n::: {.cell hash='knn_cache/html/unnamed-chunk-12_2224cbf8bd3938c12f63d57862625153'}\n\n```{.r .cell-code}\nplot(x,y,col=2,pch=20,cex=2)  # plot training data\ntitle(paste(\"K =\",K))\nlines(x_grid,p1(x_grid))  # plot true p1(x) values \nlines(x_grid,p1_grid_hat,col=4)  # plot estimated probabilities of y=1 \nlines(x_grid,y_grid_hat,col=4)  # plot predicted y values for each x0\n```\n\n::: {.cell-output-display}\n![](knn_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n## Error rates\nThe training error rate is given by\n$$\\textrm{training error} = \\frac{1}{n}\\sum_{i=1}^n  I(\\hat{y_i} \\neq y_i)$$\nSo we first run KNN classifier (probability version) at each $x$ in the training set, then we predict the $y$ values for each $x$ in the training set (prediction version of KNN), and finally compute the compute the training error rate which is the on average misclassification rate.\n\n::: {.cell hash='knn_cache/html/unnamed-chunk-13_80ffd729bfa159eacaf79320e812c2c3'}\n\n```{.r .cell-code}\np1_hat = sapply(x, function(x0) { KNN_classifier(x0, x, y, K) }) \ny_hat = round(p1_hat > 0.5)  \ntrain_error = mean(y_hat != y)  \nprint(paste0(\"Training error rate (K = \",K,\") = \",train_error))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Training error rate (K = 1) = 0\"\n```\n:::\n:::\n\n\nNow we compute the test error rate.\nWe again simulate a large number of samples as test set, namely 10000. We simulate test $x$'s and test $y$'s and run the KNN classifier at each $x$ in the test set. We then predict the $y$ values for each $x$ in the test set and compute the test error rate.\n\n::: {.cell hash='knn_cache/html/unnamed-chunk-14_446daf4eeebf1e4f373ef35cd1f197b3'}\n\n```{.r .cell-code}\nn_test = 10000 \nx_test = 5*runif(n_test)  \ny_test = rbinom(n_test,1,p1(x_test))  \np1_test_hat = sapply(x_test, function(x0) { KNN_classifier(x0, x, y, K) })  \ny_test_hat = round(p1_test_hat > 0.5) \ntest_error = mean(y_test_hat != y_test) \nprint(paste0(\"Test error rate (K = \",K,\"): \",test_error))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Test error rate (K = 1): 0.2869\"\n```\n:::\n:::\n\n\n\n::: {style=\"color: red;\"}\n**Homework 1.2** (4 points) \n\nHow can we tell if the above is a good test error rate?   Compute the test error rate for the Bayes optimal classifier. Below you will find some *incomplete* code to assist you in this task. Note that the missing parts that you need to fill in are denoted by 'XXX'. Please include the R code you used for this task.\n\n\n::: {.cell hash='knn_cache/html/unnamed-chunk-15_1401004f87f6ed1fe5b578dd697ad03f'}\n\n```{.r .cell-code}\n# Bayes optimal classifier\n# use the true p1(x) to make the best possible predictions on the training set\ny_hat_optimal = p1(x) > 0.5  \n# compute the training error rate for the Bayes optimal classifier\ntrain_error_optimal = mean(y_hat_optimal != y)  \nprint(paste0(\"Training error rate (Optimal): \",train_error_optimal))\n# use the true p1(x) to make the best possible predictions on the test set\ny_test_hat_optimal = XXX\n# compute the test error rate for the Bayes optimal classifier\ntest_error_optimal = XXX \nprint(paste0(\"Test error rate (Optimal): \",test_error_optimal))\n```\n:::\n\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}