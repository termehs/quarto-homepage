{
  "hash": "797843ecdc7801b170b0b85949230a41",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Worksheet 3 - Evaluating models\"\neditor_options: \n  chunk_output_type: console\n---\n\n\n## Setup\n\nSetup from previous worksheet.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n✔ broom        1.0.7     ✔ recipes      1.2.0\n✔ dials        1.4.0     ✔ rsample      1.2.1\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.5.1     ✔ tidyr        1.3.1\n✔ infer        1.0.7     ✔ tune         1.3.0\n✔ modeldata    1.4.0     ✔ workflows    1.2.0\n✔ parsnip      1.3.1     ✔ workflowsets 1.1.0\n✔ purrr        1.0.4     ✔ yardstick    1.3.2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(forested)\n\nset.seed(123)\nforested_split <- initial_split(forested, prop = 0.8)\nforested_train <- training(forested_split)\nforested_test <- testing(forested_split)\n\n# decrease cost_complexity from its default 0.01 to make a more\n# complex and performant tree. see `?decision_tree()` to learn more.\ntree_spec <- decision_tree(cost_complexity = 0.0001, mode = \"classification\")\nforested_wflow <- workflow(forested ~ ., tree_spec)\nforested_fit <- fit(forested_wflow, forested_train)\n```\n:::\n\n\n## Metrics for model performance\n\n`conf_mat()` can be used to see how well the model is doing at prediction\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(forested_fit, new_data = forested_train) %>%\n  conf_mat(truth = forested, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Truth\nPrediction  Yes   No\n       Yes 2991  176\n       No   144 2374\n```\n\n\n:::\n:::\n\n\nand it has nice plotting features\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(forested_fit, new_data = forested_train) %>%\n  conf_mat(truth = forested, estimate = .pred_class) %>%\n  autoplot(type = \"heatmap\")\n```\n\n::: {.cell-output-display}\n![](3-evaluate-model_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nusing the same interface we can calculate metrics\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(forested_fit, new_data = forested_train) %>%\n  accuracy(truth = forested, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.944\n```\n\n\n:::\n:::\n\n\nMetric sets are a way to combine multiple similar metric functions together into a new function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_metrics <- metric_set(accuracy, specificity, sensitivity)\n\naugment(forested_fit, new_data = forested_train) %>%\n  forested_metrics(truth = forested, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  .metric     .estimator .estimate\n  <chr>       <chr>          <dbl>\n1 accuracy    binary         0.944\n2 specificity binary         0.931\n3 sensitivity binary         0.954\n```\n\n\n:::\n:::\n\n\nMetrics and metric sets work with grouped data frames!\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(forested_fit, new_data = forested_train) %>%\n  group_by(tree_no_tree) %>%\n  accuracy(truth = forested, estimate = .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  tree_no_tree .metric  .estimator .estimate\n  <fct>        <chr>    <chr>          <dbl>\n1 Tree         accuracy binary         0.946\n2 No tree      accuracy binary         0.941\n```\n\n\n:::\n:::\n\n\n## Your turn\n\nApply the `forested_metrics` metric set to `augment()` output grouped by `tree_no_tree`.\n\nDo any metrics differ substantially between groups?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your code here!\n```\n:::\n\n\n## Your turn\n\nCompute and plot an ROC curve for your current model.\n\nWhat data are being used for this ROC curve plot?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your code here!\n```\n:::\n\n\n## Dangers of overfitting\n\nRepredicting the training set, bad!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_fit %>%\n  augment(forested_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5,685 × 22\n   .pred_class .pred_Yes .pred_No forested  year elevation eastness northness\n   <fct>           <dbl>    <dbl> <fct>    <dbl>     <dbl>    <dbl>     <dbl>\n 1 No             0.0114   0.989  No        2016       464       -5       -99\n 2 Yes            0.636    0.364  Yes       2016       166       92        37\n 3 No             0.0114   0.989  No        2016       644      -85       -52\n 4 Yes            0.977    0.0226 Yes       2014      1285        4        99\n 5 Yes            0.977    0.0226 Yes       2013       822       87        48\n 6 Yes            0.808    0.192  Yes       2017         3        6       -99\n 7 Yes            0.977    0.0226 Yes       2014      2041      -95        28\n 8 Yes            0.977    0.0226 Yes       2015      1009       -8        99\n 9 No             0.0114   0.989  No        2017       436      -98        19\n10 No             0.0114   0.989  No        2018       775       63        76\n# ℹ 5,675 more rows\n# ℹ 14 more variables: roughness <dbl>, tree_no_tree <fct>, dew_temp <dbl>,\n#   precip_annual <dbl>, temp_annual_mean <dbl>, temp_annual_min <dbl>,\n#   temp_annual_max <dbl>, temp_january_min <dbl>, vapor_min <dbl>,\n#   vapor_max <dbl>, canopy_cover <dbl>, lon <dbl>, lat <dbl>, land_type <fct>\n```\n\n\n:::\n:::\n\n\n\"Resubstitution estimate\" - This should be the best possible performance that you could ever achieve, but it can be very misleading!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_fit %>%\n  augment(forested_train) %>%\n  accuracy(forested, .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.944\n```\n\n\n:::\n:::\n\n\nNow on the test set, see that it performs worse? This is closer to \"real\" performance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_fit %>%\n  augment(forested_test) %>%\n  accuracy(forested, .pred_class)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.886\n```\n\n\n:::\n:::\n\n\n## Your turn\n\nUse `augment()` and and a metric function to compute a classification metric like `brier_class()`.\n\nCompute the metrics for both training and testing data to demonstrate overfitting!\n\nNotice the evidence of overfitting!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your code here!\n\n# Use `augment()` and `brier_class()` with `forested_fit`\nforested_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nforested ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nn= 5685 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n   1) root 5685 2550 Yes (0.55145119 0.44854881)  \n     2) land_type=Tree 3064  300 Yes (0.90208877 0.09791123)  \n       4) tree_no_tree=Tree 2762  181 Yes (0.93446778 0.06553222)  \n         8) precip_annual>=287.5 2743  162 Yes (0.94094058 0.05905942)  \n          16) canopy_cover>=38.5 2356   98 Yes (0.95840407 0.04159593)  \n            32) temp_annual_mean< 10.255 1906   43 Yes (0.97743966 0.02256034) *\n            33) temp_annual_mean>=10.255 450   55 Yes (0.87777778 0.12222222)  \n              66) lon< -122.5845 320   23 Yes (0.92812500 0.07187500)  \n               132) elevation>=24 287   13 Yes (0.95470383 0.04529617) *\n               133) elevation< 24 33   10 Yes (0.69696970 0.30303030)  \n                 266) temp_annual_mean< 10.815 26    5 Yes (0.80769231 0.19230769) *\n                 267) temp_annual_mean>=10.815 7    2 No (0.28571429 0.71428571) *\n              67) lon>=-122.5845 130   32 Yes (0.75384615 0.24615385)  \n               134) year< 2014.5 59    7 Yes (0.88135593 0.11864407) *\n               135) year>=2014.5 71   25 Yes (0.64788732 0.35211268)  \n                 270) canopy_cover>=72.5 41    8 Yes (0.80487805 0.19512195)  \n                   540) lon< -122.4005 12    0 Yes (1.00000000 0.00000000) *\n                   541) lon>=-122.4005 29    8 Yes (0.72413793 0.27586207)  \n                    1082) elevation>=131 22    4 Yes (0.81818182 0.18181818) *\n                    1083) elevation< 131 7    3 No (0.42857143 0.57142857) *\n                 271) canopy_cover< 72.5 30   13 No (0.43333333 0.56666667)  \n                   542) year>=2017.5 13    4 Yes (0.69230769 0.30769231) *\n                   543) year< 2017.5 17    4 No (0.23529412 0.76470588) *\n          17) canopy_cover< 38.5 387   64 Yes (0.83462532 0.16537468)  \n            34) temp_annual_max< 15.475 351   45 Yes (0.87179487 0.12820513)  \n              68) roughness< 184 344   41 Yes (0.88081395 0.11918605)  \n               136) precip_annual>=485.5 294   28 Yes (0.90476190 0.09523810)  \n                 272) temp_annual_min< -4.47 194   11 Yes (0.94329897 0.05670103) *\n                 273) temp_annual_min>=-4.47 100   17 Yes (0.83000000 0.17000000)  \n                   546) vapor_min< 127.5 77    8 Yes (0.89610390 0.10389610)  \n                    1092) lat< 48.23524 66    4 Yes (0.93939394 0.06060606)  \n                      2184) year< 2016.5 41    0 Yes (1.00000000 0.00000000) *\n                      2185) year>=2016.5 25    4 Yes (0.84000000 0.16000000)  \n                        4370) canopy_cover< 32 18    0 Yes (1.00000000 0.00000000) *\n                        4371) canopy_cover>=32 7    3 No (0.42857143 0.57142857) *\n                    1093) lat>=48.23524 11    4 Yes (0.63636364 0.36363636) *\n                   547) vapor_min>=127.5 23    9 Yes (0.60869565 0.39130435)  \n                    1094) northness< 9 12    2 Yes (0.83333333 0.16666667) *\n                    1095) northness>=9 11    4 No (0.36363636 0.63636364) *\n               137) precip_annual< 485.5 50   13 Yes (0.74000000 0.26000000)  \n                 274) lon< -118.0235 41    7 Yes (0.82926829 0.17073171) *\n                 275) lon>=-118.0235 9    3 No (0.33333333 0.66666667) *\n              69) roughness>=184 7    3 No (0.42857143 0.57142857) *\n            35) temp_annual_max>=15.475 36   17 No (0.47222222 0.52777778)  \n              70) dew_temp< 6.455 22    8 Yes (0.63636364 0.36363636)  \n\n...\nand 106 more lines.\n```\n\n\n:::\n:::\n\n\n## Your turn\n\nIf we use 10 folds, what percent of the training data:\n\n- ends up in analysis?\n- ends up in assessment?\n\nfor each fold\n\n## Resampling\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# v = 10 is the default\nvfold_cv(forested_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits             id    \n   <list>             <chr> \n 1 <split [5116/569]> Fold01\n 2 <split [5116/569]> Fold02\n 3 <split [5116/569]> Fold03\n 4 <split [5116/569]> Fold04\n 5 <split [5116/569]> Fold05\n 6 <split [5117/568]> Fold06\n 7 <split [5117/568]> Fold07\n 8 <split [5117/568]> Fold08\n 9 <split [5117/568]> Fold09\n10 <split [5117/568]> Fold10\n```\n\n\n:::\n:::\n\n\nWhat is in a resampling result?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_folds <- vfold_cv(forested_train, v = 10)\n\n# Individual splits of analysis/assessment data\nforested_folds$splits[1:3]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n<Analysis/Assess/Total>\n<5116/569/5685>\n\n[[2]]\n<Analysis/Assess/Total>\n<5116/569/5685>\n\n[[3]]\n<Analysis/Assess/Total>\n<5116/569/5685>\n```\n\n\n:::\n:::\n\n\nWe'll use this setup:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nforested_folds <- vfold_cv(forested_train, v = 10)\nforested_folds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits             id    \n   <list>             <chr> \n 1 <split [5116/569]> Fold01\n 2 <split [5116/569]> Fold02\n 3 <split [5116/569]> Fold03\n 4 <split [5116/569]> Fold04\n 5 <split [5116/569]> Fold05\n 6 <split [5117/568]> Fold06\n 7 <split [5117/568]> Fold07\n 8 <split [5117/568]> Fold08\n 9 <split [5117/568]> Fold09\n10 <split [5117/568]> Fold10\n```\n\n\n:::\n:::\n\n\n## Evaluating model performance\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the workflow on each analysis set,\n# then compute performance on each assessment set\nforested_res <- fit_resamples(forested_wflow, forested_folds)\nforested_res\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Resampling results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits             id     .metrics         .notes          \n   <list>             <chr>  <list>           <list>          \n 1 <split [5116/569]> Fold01 <tibble [3 × 4]> <tibble [0 × 3]>\n 2 <split [5116/569]> Fold02 <tibble [3 × 4]> <tibble [0 × 3]>\n 3 <split [5116/569]> Fold03 <tibble [3 × 4]> <tibble [0 × 3]>\n 4 <split [5116/569]> Fold04 <tibble [3 × 4]> <tibble [0 × 3]>\n 5 <split [5116/569]> Fold05 <tibble [3 × 4]> <tibble [0 × 3]>\n 6 <split [5117/568]> Fold06 <tibble [3 × 4]> <tibble [0 × 3]>\n 7 <split [5117/568]> Fold07 <tibble [3 × 4]> <tibble [0 × 3]>\n 8 <split [5117/568]> Fold08 <tibble [3 × 4]> <tibble [0 × 3]>\n 9 <split [5117/568]> Fold09 <tibble [3 × 4]> <tibble [0 × 3]>\n10 <split [5117/568]> Fold10 <tibble [3 × 4]> <tibble [0 × 3]>\n```\n\n\n:::\n:::\n\n\nAggregate metrics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforested_res %>%\n  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 6\n  .metric     .estimator   mean     n std_err .config             \n  <chr>       <chr>       <dbl> <int>   <dbl> <chr>               \n1 accuracy    binary     0.894     10 0.00562 Preprocessor1_Model1\n2 brier_class binary     0.0817    10 0.00434 Preprocessor1_Model1\n3 roc_auc     binary     0.951     10 0.00378 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\nIf you want to analyze the assessment set (i.e. holdout) predictions, then you need to adjust the control object and tell it to save them:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Save the assessment set results\nctrl_forested <- control_resamples(save_pred = TRUE)\n\nforested_res <- fit_resamples(forested_wflow, forested_folds, control = ctrl_forested)\n\nforested_preds <- collect_predictions(forested_res)\nforested_preds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5,685 × 7\n   .pred_class .pred_Yes .pred_No id      .row forested .config             \n   <fct>           <dbl>    <dbl> <chr>  <int> <fct>    <chr>               \n 1 Yes           0.5       0.5    Fold01     2 Yes      Preprocessor1_Model1\n 2 Yes           0.982     0.0178 Fold01     5 Yes      Preprocessor1_Model1\n 3 No            0.00790   0.992  Fold01     9 No       Preprocessor1_Model1\n 4 No            0.4       0.6    Fold01    14 No       Preprocessor1_Model1\n 5 Yes           0.870     0.130  Fold01    18 Yes      Preprocessor1_Model1\n 6 Yes           0.982     0.0178 Fold01    59 Yes      Preprocessor1_Model1\n 7 No            0.00790   0.992  Fold01    67 No       Preprocessor1_Model1\n 8 Yes           0.982     0.0178 Fold01    89 Yes      Preprocessor1_Model1\n 9 No            0.00790   0.992  Fold01    94 No       Preprocessor1_Model1\n10 Yes           0.982     0.0178 Fold01   111 Yes      Preprocessor1_Model1\n# ℹ 5,675 more rows\n```\n\n\n:::\n:::\n\n\n## Bootstrapping\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(3214)\nbootstraps(forested_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Bootstrap sampling \n# A tibble: 25 × 2\n   splits              id         \n   <list>              <chr>      \n 1 <split [5685/2075]> Bootstrap01\n 2 <split [5685/2093]> Bootstrap02\n 3 <split [5685/2129]> Bootstrap03\n 4 <split [5685/2093]> Bootstrap04\n 5 <split [5685/2111]> Bootstrap05\n 6 <split [5685/2105]> Bootstrap06\n 7 <split [5685/2139]> Bootstrap07\n 8 <split [5685/2079]> Bootstrap08\n 9 <split [5685/2113]> Bootstrap09\n10 <split [5685/2101]> Bootstrap10\n# ℹ 15 more rows\n```\n\n\n:::\n:::\n\n\n## Your turn\n\nCreate:\n\n- Monte Carlo Cross-Validation sets\n- validation set\n\n(use the reference guide to find the functions)\n\nhttps://rsample.tidymodels.org/reference/index.html\n\nDon't forget to set a seed when you resample!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your code here!\n```\n:::\n\n\n## Create a random forest model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_spec <- rand_forest(trees = 1000, mode = \"classification\")\nrf_spec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  trees = 1000\n\nComputational engine: ranger \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_wflow <- workflow(forested ~ ., rf_spec)\nrf_wflow\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nforested ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  trees = 1000\n\nComputational engine: ranger \n```\n\n\n:::\n:::\n\n\n## Your turn\n\nUse `fit_resamples()` and `rf_wflow` to:\n\n- Keep predictions\n- Compute metrics\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your code here!\n```\n:::\n\n\n\n## The final fit\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# `forested_split` has train + test info\nfinal_fit <- last_fit(rf_wflow, forested_split) \n\nfinal_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits              id               .metrics .notes   .predictions .workflow \n  <list>              <chr>            <list>   <list>   <list>       <list>    \n1 <split [5685/1422]> train/test split <tibble> <tibble> <tibble>     <workflow>\n```\n\n\n:::\n:::\n\n\nTest set metrics:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(final_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  <chr>       <chr>          <dbl> <chr>               \n1 accuracy    binary        0.911  Preprocessor1_Model1\n2 roc_auc     binary        0.970  Preprocessor1_Model1\n3 brier_class binary        0.0651 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\nTest set predictions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_predictions(final_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,422 × 7\n   .pred_class .pred_Yes .pred_No id                .row forested .config       \n   <fct>           <dbl>    <dbl> <chr>            <int> <fct>    <chr>         \n 1 Yes             0.828   0.172  train/test split     3 No       Preprocessor1…\n 2 Yes             0.707   0.293  train/test split     4 Yes      Preprocessor1…\n 3 No              0.286   0.714  train/test split     7 Yes      Preprocessor1…\n 4 Yes             0.556   0.444  train/test split     8 Yes      Preprocessor1…\n 5 Yes             0.553   0.447  train/test split    10 Yes      Preprocessor1…\n 6 Yes             0.977   0.0229 train/test split    11 Yes      Preprocessor1…\n 7 Yes             0.969   0.0307 train/test split    12 Yes      Preprocessor1…\n 8 Yes             0.959   0.0412 train/test split    14 Yes      Preprocessor1…\n 9 Yes             0.938   0.0624 train/test split    15 Yes      Preprocessor1…\n10 Yes             0.976   0.0244 train/test split    19 Yes      Preprocessor1…\n# ℹ 1,412 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_predictions(final_fit) %>%\n  ggplot(aes(.pred_class, fill = forested)) + \n  geom_bar() \n```\n\n::: {.cell-output-display}\n![](3-evaluate-model_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nextract_workflow(final_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nforested ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nRanger result\n\nCall:\n ranger::ranger(x = maybe_data_frame(x), y = y, num.trees = ~1000,      num.threads = 1, verbose = FALSE, seed = sample.int(10^5,          1), probability = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  1000 \nSample size:                      5685 \nNumber of independent variables:  18 \nMtry:                             4 \nTarget node size:                 10 \nVariable importance mode:         none \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.06123267 \n```\n\n\n:::\n:::\n",
    "supporting": [
      "3-evaluate-model_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}